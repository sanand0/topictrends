id,update_date,title,abstract
0807.1750,2012-08-14,"A Markovian growth dynamics on rooted binary trees evolving according to
  the Gompertz curve","  Inspired by biological dynamics, we consider a growth Markov process taking
values on the space of rooted binary trees, similar to the Aldous-Shields
model. Fix $n\ge 1$ and $\beta>0$. We start at time 0 with the tree composed of
a root only. At any time, each node with no descendants, independently from the
other nodes, produces two successors at rate $\beta(n-k)/n$, where $k$ is the
distance from the node to the root. Denote by $Z_n(t)$ the number of nodes with
no descendants at time $t$ and let $T_n = \beta^{-1} n \ln(n /\ln 4) + (\ln
2)/(2 \beta)$. We prove that $2^{-n} Z_n(T_n + n \tau)$, $\tau\in\bb R$,
converges to the Gompertz curve $\exp (- (\ln 2) e^{-\beta \tau})$. We also
prove a central limit theorem for the martingale associated to $Z_n(t)$.
"
1006.5366,2012-06-12,"""Not only defended but also applied"": The perceived absurdity of
  Bayesian inference","  The missionary zeal of many Bayesians of old has been matched, in the other
direction, by a view among some theoreticians that Bayesian methods are
absurd-not merely misguided but obviously wrong in principle. We consider
several examples, beginning with Feller's classic text on probability theory
and continuing with more recent cases such as the perceived Bayesian nature of
the so-called doomsday argument. We analyze in this note the intellectual
background behind various misconceptions about Bayesian statistics, without
aiming at a complete historical coverage of the reasons for this dismissal.
"
1006.5471,2020-10-27,"Cognitive Constructivism and the Epistemic Significance of Sharp
  Statistical Hypotheses in Natural Sciences","  This book presents our case in defense of a constructivist epistemological
framework and the use of compatible statistical theory and inference tools. The
basic metaphor of decision theory is the maximization of a gambler's expected
fortune, according to his own subjective utility, prior beliefs an learned
experiences. This metaphor has proven to be very useful, leading the
development of Bayesian statistics since its XX-th century revival, rooted on
the work of de Finetti, Savage and others. The basic metaphor presented in this
text, as a foundation for cognitive constructivism, is that of an
eigen-solution, and the verification of its objective epistemic status. The
FBST - Full Bayesian Significance Test - is the cornerstone of a set of
statistical tolls conceived to assess the epistemic value of such
eigen-solutions, according to their four essential attributes, namely,
sharpness, stability, separability and composability. We believe that this
alternative perspective, complementary to the one ofered by decision theory,
can provide powerful insights and make pertinent contributions in the context
of scientific research.
"
1006.5690,2010-06-30,"Sweave Documentation for ""Implementing Markov chain Monte Carlo:
  Estimating with confidence""","  This file is the Sweave documentation for the examples provided in Flegal, J.
M. and Jones, G. L. (2010), ""Implementing Markov chain Monte Carlo: Estimating
with confidence"", in Handbook of Markov Chain Monte Carlo, edited by Brooks,
S., Gelman, A., Jones, G., and Meng, X. published by Chapman & Hall/CRC Press.
"
1006.5831,2013-11-27,Statistical Inference in Dynamic Treatment Regimes,"  Dynamic treatment regimes are of growing interest across the clinical
sciences as these regimes provide one way to operationalize and thus inform
sequential personalized clinical decision making. A dynamic treatment regime is
a sequence of decision rules, with a decision rule per stage of clinical
intervention; each decision rule maps up-to-date patient information to a
recommended treatment. We briefly review a variety of approaches for using data
to construct the decision rules. We then review an interesting challenge, that
of nonregularity that often arises in this area. By nonregularity, we mean the
parameters indexing the optimal dynamic treatment regime are nonsmooth
functionals of the underlying generative distribution.
  A consequence is that no regular or asymptotically unbiased estimator of
these parameters exists. Nonregularity arises in inference for parameters in
the optimal dynamic treatment regime; we illustrate the effect of nonregularity
on asymptotic bias and via sensitivity of asymptotic, limiting, distributions
to local perturbations. We propose and evaluate a locally consistent Adaptive
Confidence Interval (ACI) for the parameters of the optimal dynamic treatment
regime. We use data from the Adaptive Interventions for Children with ADHD
study as an illustrative example. We conclude by highlighting and discussing
emerging theoretical problems in this area.
"
1007.0566,2011-06-22,Organisation of signal flow in directed networks,"  Confining an answer to the question whether and how the coherent operation of
network elements is determined by the the network structure is the topic of our
work. We map the structure of signal flow in directed networks by analysing the
degree of edge convergence and the overlap between the in- and output sets of
an edge. Definitions of convergence degree and overlap are based on the
shortest paths, thus they encapsulate global network properties. Using the
defining notions of convergence degree and overlapping set we clarify the
meaning of network causality and demonstrate the crucial role of chordless
circles. In real-world networks the flow representation distinguishes nodes
according to their signal transmitting, processing and control properties. The
analysis of real-world networks in terms of flow representation was in
accordance with the known functional properties of the network nodes. It is
shown that nodes with different signal processing, transmitting and control
properties are randomly connected at the global scale, while local connectivity
patterns depart from randomness. Grouping network nodes according to their
signal flow properties was unrelated to the network's community structure. We
present evidence that signal flow properties of small-world-like, real-world
networks can not be reconstructed by algorithms used to generate small-world
networks. Convergence degree values were calculated for regular oriented trees,
and its probability density function for networks grown with the preferential
attachment mechanism. For Erd\H{o}s-R\'enyi graphs we calculated both the
probability density function of convergence degrees and of overlaps.
"
1007.1094,2010-07-08,Hotelling's test for highly correlated data,"  This paper is motivated by the analysis of gene expression sets, especially
by finding differentially expressed gene sets between two phenotypes. Gene
$\log_2$ expression levels are highly correlated and, very likely, have
approximately normal distribution. Therefore, it seems reasonable to use
two-sample Hotelling's test for such data. We discover some unexpected
properties of the test making it different from the majority of tests
previously used for such data. It appears that the Hotelling's test does not
always reach maximal power when all marginal distributions are differentially
expressed. For highly correlated data its maximal power is attained when about
a half of marginal distributions are essentially different. For the case when
the correlation coefficient is greater than 0.5 this test is more powerful if
only one marginal distribution is shifted, omparing to the case when all
marginal distributions are equally shifted. Moreover, when the correlation
coefficient increases the power of Hotelling's test increases as well.
"
1007.1787,2010-07-13,"The two sample problem: Exact distributions, numerical solutions,
  simulations","  The work presented in this article suggests a solution to the two sample
problem. Keywords: Two sample problem, Welch-Aspin solution, Fisher-Behrens
problem, nuisance parameter, similarity, the Linnik phenomenon.
"
1007.3210,2010-07-20,"Development and Initial Validation of a Scale to Measure Instructors'
  Attitudes toward Concept-Based Teaching of Introductory Statistics in the
  Health and Behavioral Sciences","  Despite more than a decade of reform efforts, students continue to experience
difficulty understanding and applying statistical concepts. The predominant
focus of reform has been on content, pedagogy, technology and assessment, with
little attention to instructor characteristics. However, there is strong
theoretical and empirical evidence that instructors' attitudes impact the
quality of teaching and learning. The objective of this study was to develop
and initially validate a scale to measure instructors' attitudes toward
reform-oriented (or concept-based) teaching of introductory statistics in the
health and behavioral sciences, at the tertiary level. This scale will be
referred to as FATS (Faculty Attitudes Toward Statistics). Data were obtained
from 227 instructors (USA and international), and analyzed using factor
analysis, multidimensional scaling and hierarchical cluster analysis. The
overall scale consists of five sub-scales with a total of 25 items, and an
overall alpha of 0.89. Construct validity was established. Specifically, the
overall scale, and subscales (except perceived difficulty) plausibly
differentiated between low-reform and high-reform practice instructors.
Statistically significant differences in attitude were observed with respect to
age, but not gender, employment status, membership status in professional
organizations, ethnicity, highest academic qualification, and degree
concentration. This scale can be considered a reliable and valid measure of
instructors' attitudes toward reform-oriented (concept-based or constructivist)
teaching of introductory statistics in the health and behavioral sciences at
the tertiary level. These five dimensions influence instructors' attitudes.
Additional studies are required to confirm these structural and psychometric
properties.
"
1008.0740,2010-08-05,$L_p$-nested symmetric distributions,"  Tractable generalizations of the Gaussian distribution play an important role
for the analysis of high-dimensional data. One very general super-class of
Normal distributions is the class of $\nu$-spherical distributions whose random
variables can be represented as the product $\x = r\cdot \u$ of a uniformly
distribution random variable $\u$ on the $1$-level set of a positively
homogeneous function $\nu$ and arbitrary positive radial random variable $r$.
Prominent subclasses of $\nu$-spherical distributions are spherically symmetric
distributions ($\nu(\x)=\|\x\|_2$) which have been further generalized to the
class of $L_p$-spherically symmetric distributions ($\nu(\x)=\|\x\|_p$). Both
of these classes contain the Gaussian as a special case. In general, however,
$\nu$-spherical distributions are computationally intractable since, for
instance, the normalization constant or fast sampling algorithms are unknown
for an arbitrary $\nu$. In this paper we introduce a new subclass of
$\nu$-spherical distributions by choosing $\nu$ to be a nested cascade of
$L_p$-norms. This class is still computationally tractable, but includes all
the aforementioned subclasses as a special case. We derive a general expression
for $L_p$-nested symmetric distributions as well as the uniform distribution on
the $L_p$-nested unit sphere, including an explicit expression for the
normalization constant. We state several general properties of $L_p$-nested
symmetric distributions, investigate its marginals, maximum likelihood fitting
and discuss its tight links to well known machine learning methods such as
Independent Component Analysis (ICA), Independent Subspace Analysis (ISA) and
mixed norm regularizers. Finally, we derive a fast and exact sampling algorithm
for arbitrary $L_p$-nested symmetric distributions, and introduce the Nested
Radial Factorization algorithm (NRF), which is a form of non-linear ICA.
"
1008.0756,2012-05-02,"Phasetype distributions, autoregressive processes and overshoot","  Autoregressive processes are intensively studied in statistics and other
fields of applied stochastics. For many applications the overshoot and the
threshold-time are of special interest. When the upward innovations are in the
class of phasetype distributions we determine the joint distribution of this
two quantities and apply this result to problems of optimal stopping. Using a
principle of continuous fit this leads to explicit solutions.
"
1009.1732,2010-10-19,Extreme shock models: an alternative perspective,"  Extreme shock models have been introduced in Gut and H\""usler (1999) to study
systems that at random times are subject to shock of random magnitude. These
systems break down when some shock overcomes a given resistance level. In this
paper we propose an alternative approach to extreme shock models using
reinforced urn processes. As a consequence of this we are able to look at the
same problem under a Bayesian nonparametric perspective, providing the
predictive distribution of systems' defaults.
"
1009.2723,2010-09-15,"Tests of Non-Equivalence among Absolutely Nonsingular Tensors through
  Geometric Invariants","  4x4x3 absolutely nonsingular tensors are characterized by their determinant
polynomial. Non-quivalence among absolutely nonsingular tensors with respect to
a class of linear transformations, which do not chage the tensor rank,is
studied. It is shown theoretically that affine geometric invariants of the
constant surface of a determinant polynomial is useful to discriminate
non-equivalence among absolutely nonsingular tensors. Also numerical
caluculations are presented and these invariants are shown to be useful indeed.
For the caluculation of invarinats by 20-spherical design is also commented. We
showed that an algebraic problem in tensor data analysis can be attacked by an
affine geometric method.
"
1009.4217,2012-08-21,Measurement error and deconvolution in spaces of generalized functions,"  This paper considers convolution equations that arise from problems such as
measurement error and non-parametric regression with errors in variables with
independence conditions. The equations are examined in spaces of generalized
functions to account for possible singularities; this makes it possible to
consider densities for arbitrary and not only absolutely continuous
distributions, and to operate with Fourier transforms for polynomially growing
regression functions. Results are derived for identification and well-posedness
in the topology of generalized functions for the deconvolution problem and for
some regression models. Conditions for consistency of plug-in estimation for
these models are derived.
"
1010.2326,2010-10-13,A brief history of the Fail Safe Number in Applied Research,"  Rosenthal's (1979) Fail-Safe-Number (FSN) is probably one of the best known
statistics in the context of meta-analysis aimed to estimate the number of
unpublished studies in meta-analyses required to bring the meta-analytic mean
effect size down to a statistically insignificant level. Already before
Scargle's (2000) and Schonemann & Scargle's (2008) fundamental critique on the
claimed stability of the basic rationale of the FSN approach, objections
focusing on the basic assumption of the FSN which treats the number of studies
as unbiased with averaging null were expressed throughout the history of the
FSN by different authors (Elashoff, 1978; Iyengar & Greenhouse, 1988a; 1988b;
see also Scargle, 2000). In particular, Elashoff's objection appears to be
important because it was the very first critique pointing directly to the
central problem of the FSN: ""R & R claim that the number of studies hidden in
the drawers would have to be 65,000 to achieve a mean effect size of zero when
combined with the 345 studies reviewed here. But surely, if we allowed the
hidden studies to be negative, on the average no more than 345 hidden studies
would be necessary to obtain a zero mean effect size"" (p. 392). Thus, users of
meta-analysis could have been aware right from the beginning that something was
wrong with the statistical reasoning of the FSN. In particular, from an applied
research perspective, it is therefore of interest whether any of the
fundamental objections on the FSN are reflected in standard handbooks on
meta-analysis as well as -and of course even more importantly- in meta-analytic
studies itself.
"
1011.1160,2010-11-05,A Conversation with James Hannan,"  Jim Hannan is a professor who has lived an interesting life and one whose
fundamental research in repeated games was not fully appreciated until late in
his career. During his service as a meteorologist in the Army in World War II,
Jim played poker and made weather forecasts. It is curious that his later
research included strategies for repeated play that apply to selecting the best
forecaster. James Hannan was born in Holyoke, Massachusetts on September 14,
1922. He attended St. Jerome's High School and in January 1943 received the
Ph.B. from St. Michael's College in Colchester, Vermont. Jim enlisted in the US
Army Air Force to train and serve as a meteorologist. This took him to army
airbases in China by the close of the war. Following discharge from the army,
Jim studied mathematics at Harvard and graduated with the M.S. in June 1947. To
prepare for doctoral work in statistics at the University of North Carolina
that fall, Jim went to the University of Michigan in the summer of 1947. The
routine admissions' physical revealed a spot on the lung and the possibility of
tuberculosis. This caused Jim to stay at Ann Arbor through the fall of 1947 and
then at a Veterans Administration Hospital in Framingham, Massachusetts to have
his condition followed more closely. He was discharged from the hospital in the
spring and started his study at Chapel Hill in the fall of 1948. There he began
research in compound decision theory under Herbert Robbins. Feeling the need
for teaching experience, Jim left Chapel Hill after two years and short of
thesis to take a three year appointment as an instructor at Catholic University
in Washington, DC. When told that renewal was not coming, Jim felt pressure to
finish his degree.
"
1011.6517,2010-12-01,A Conversation with Martin Bradbury Wilk,"  Martin Bradbury Wilk was born on December 18, 1922, in Montr\'{e}al,
Qu\'{e}bec, Canada. He completed a B.Eng. degree in Chemical Engineering in
1945 at McGill University and worked as a Research Engineer on the Atomic
Energy Project for the National Research Council of Canada from 1945 to 1950.
He then went to Iowa State College, where he completed a M.Sc. and a Ph.D.
degree in Statistics in 1953 and 1955, respectively. After a one-year post-doc
with John Tukey, he became Assistant Director of the Statistical Techniques
Research Group at Princeton University in 1956--1957, and then served as
Professor and Director of Research in Statistics at Rutgers University from
1959 to 1963. In parallel, he also had a 14-year career at Bell Laboratories,
Murray Hill, New Jersey. From 1956 to 1969, he was in turn Member of Technical
Staff, Head of the Statistical Models and Methods Research Department, and
Statistical Director in Management Sciences Research. He wrote a number of
influential papers in statistical methodology during that period, notably
testing procedures for normality (the Shapiro--Wilk statistic) and probability
plotting techniques for multivariate data. In 1970, Martin moved into higher
management levels of the American Telephone and Telegraph (AT&T) Company. He
occupied various positions culminating as Assistant Vice-President and Director
of Corporate Planning. In 1980, he returned to Canada and became the first
professional statistician to serve as Chief Statistician. His accomplishments
at Statistics Canada were numerous and contributed to a resurgence of the
institution's international standing. He played a crucial role in the
reinstatement of the Cabinet-cancelled 1986 Census.
"
1012.1069,2011-12-14,Degrees of Equivalence in a Key Comparison,"  In an interlaboratory key comparison, a data analysis procedure for this
comparison was proposed and recommended by CIPM [1, 2, 3], therein the degrees
of equivalence of measurement standards of the laboratories participated in the
comparison and the ones between each two laboratories were introduced but a
corresponding clear and plausible measurement model was not given. Authors in
[4] offered possible measurement models for a given comparison and a suitable
model was selected out after rigorous analyzing steps for expectation values of
these degrees of equivalence. The systematic laboratory-effects model was then
selected as a right one in this report. Those models were all based on the one
true value existence assumption. However in the year 2008, a new version of the
Vocabulary for International Metrology (VIM) [7] was issued where the true
value of a given measurement standard should be now perceived as multi true
values which following a given statistics distribution. Applying this
perception of true values of a measurement standard with combination of the
steps in [4], measurement models have been developed and degrees of equivalence
have been analyzed. The results show that although with new definition, the
systematic laboratory-effects model is still the reasonable one in a given key
comparison.
"
1012.2184,2011-11-08,"Inherent Difficulties of Non-Bayesian Likelihood-based Inference, as
  Revealed by an Examination of a Recent Book by Aitkin","  For many decades, statisticians have made attempts to prepare the Bayesian
omelette without breaking the Bayesian eggs; that is, to obtain probabilistic
likelihood-based inferences without relying on informative prior distributions.
A recent example is Murray Aitkin's recent book, {\em Statistical Inference},
which presents an approach to statistical hypothesis testing based on
comparisons of posterior distributions of likelihoods under competing models.
Aitkin develops and illustrates his method using some simple examples of
inference from iid data and two-way tests of independence. We analyze in this
note some consequences of the inferential paradigm adopted therein, discussing
why the approach is incompatible with a Bayesian perspective and why we do not
find it relevant for applied work.
"
1012.2596,2010-12-14,"A Unified MGF-Based Capacity Analysis of Diversity Combiners over
  Generalized Fading Channels","  Unified exact average capacity results for L-branch coherent diversity
receivers including equal-gain combining (EGC) and maximal-ratio combining
(MRC) are not known. This paper develops a novel generic framework for the
capacity analysis of $L$-branch EGC/MRC over generalized fading channels. The
framework is used to derive new results for the Gamma shadowed generalized
Nakagami-m fading model which can be a suitable model for the fading
environments encountered by high frequency (60 GHz and above) communications.
The mathematical formalism is illustrated with some selected numerical and
simulation results confirming the correctness of our newly proposed framework.
"
1101.0145,2011-01-04,"Squaring the Circle and Cubing the Sphere: Circular and Spherical
  Copulas","  Do there exist circular and spherical copulas in $R^d$? That is, do there
exist circularly symmetric distributions on the unit disk in $R^2$ and
spherically symmetric distributions on the unit ball in $R^d$, $d\ge3$, whose
one-dimensional marginal distributions are uniform? The answer is yes for $d=2$
and 3, where the circular and spherical copulas are unique and can be
determined explicitly, but no for $d\ge4$. A one-parameter family of elliptical
bivariate copulas is obtained from the unique circular copula in $R^2$ by
oblique coordinate transformations. Copulas obtained by a non-linear
transformation of a uniform distribution on the unit ball in $R^d$ are also
described, and determined explicitly for $d=2$.
"
1101.4207,2015-05-27,"Blind Channel Estimation for Amplify-and-Forward Two-Way Relay Networks
  Employing M-PSK Modulation","  We consider the problem of channel estimation for amplify-and-forward (AF)
two-way relay networks (TWRNs). Most works on this problem focus on pilot-based
approaches which impose a significant training overhead that reduces the
spectral efficiency of the system. To avoid such losses, this work proposes
blind channel estimation algorithms for AF TWRNs that employ constant-modulus
(CM) signaling. Our main algorithm is based on the deterministic maximum
likelihood (DML) approach. Assuming M-PSK modulation, we show that the
resulting estimator is consistent and approaches the true channel with high
probability at high SNR for modulation orders higher than 2. For BPSK, however,
the DML performs poorly and we propose an alternative algorithm that performs
much better by taking into account the BPSK structure of the data symbols. For
comparative purposes, we also investigate the Gaussian maximum-likelihood (GML)
approach which treats the data symbols as Gaussian-distributed nuisance
parameters. We derive the Cramer-Rao bound and use Monte-Carlo simulations to
investigate the mean squared error (MSE) performance of the proposed
algorithms. We also compare the symbol-error rate (SER) performance of the DML
algorithm with that of the training-based least-squares (LS) algorithm and
demonstrate that the DML offers a superior tradeoff between accuracy and
spectral efficiency.
"
1102.1128,2011-02-22,Simultaneous concentration of order statistics,"  Let $\mu$ be a probability measure on $\mathbb{R}$ with cumulative
distribution function $F$, $(x_{i})_{1}^{n}$ a large i.i.d. sample from $\mu$,
and $F_{n}$ the associated empirical distribution function. The
Glivenko-Cantelli theorem states that with probability 1, $F_{n}$ converges
uniformly to $F$. In so doing it describes the macroscopic structure of
$\{x_{i}\}_{1}^{n}$, however it is insensitive to the position of individual
points. Indeed any subset of $o(n)$ points can be perturbed at will without
disturbing the convergence. We provide several refinements of the
Glivenko-Cantelli theorem which are sensitive not only to the global structure
of the sample but also to individual points. Our main result provides
conditions that guarantee simultaneous concentration of all order statistics.
The example of main interest is the normal distribution.
"
1102.2146,2012-10-26,"Coexistence of cooperators and defectors in well mixed populations
  mediated by limiting resources","  Traditionally, resource limitation in evolutionary game theory is assumed
just to impose a constant population size. Here we show that resource
limitations may generate dynamical payoffs able to alter an original prisoner's
dilemma, and to allow for the stable coexistence between unconditional
cooperators and defectors in well-mixed populations. This is a consequence of a
self-organizing process that turns the interaction payoff matrix into
evolutionary neutral, and represents a resource-based control mechanism
preventing the spread of defectors. To our knowledge, this is the first example
of coexistence in well-mixed populations with a game structure different from a
snowdrift game.
"
1103.3543,2011-04-12,"Array Variate Elliptical Random Variables with Multiway Kronecker Delta
  Covariance Matrix Structure","  Standard statistical methods applied to matrix random variables often fail to
describe the underlying structure in multiway data sets. In this paper we will
discuss the concept of an array variate random variable and introduce a class
of elliptical array densities which have elliptical contours.
"
1104.0061,2015-03-19,"B\""o\""ogg Bang drives global climate change","  The B\""o\""ogg is a large model of a snowman, constructed of inflammable
materials and filled with explosives. During the traditional festival of
Sechsel\""auten, which takes place each spring in Zurich, Switzerland, the
B\""o\""ogg is placed atop a wooden pyre, which is set alight. According to
popular legend, the time that elapses until the B\""o\""ogg's head explodes (the
""head-bang"" time) is said to give a rough forecast of local weather conditions
prevailing during the following summer. However, recent research has questioned
the validity of this prediction. To study the B\""o\""ogg's predictive powers, we
analyzed the B\""o\""ogg head-bang time record from 1965-2010 within the context
of global climate change. Our analysis shows that the B\""o\""ogg head-bang time
is a good predictor not of short-term local weather, as might be expected from
the legend, but of the behavior of the entire global climate system.
"
1104.2434,2011-04-14,A Conversation with George G. Roussas,"  George G. Roussas was born in the city of Marmara in central Greece, on June
29, 1933. He received a B.A. with high honors in Mathematics from the
University of Athens in 1956, and a Ph.D. in Statistics from the University of
California, Berkeley, in 1964. In 1964--1966, he served as Assistant Professor
of Mathematics at the California State University, San Jose, and he was a
faculty member of the Department of Statistics at the University of Wisconsin,
Madison, in 1966--1976, starting as an Assistant Professor in 1966, becoming a
Professor in 1972. He was a Professor of Applied Mathematics and Director of
the Laboratory of Applied Mathematics at the University of Patras, Greece, in
1972--1984. He was elected Dean of the School of Physical and Mathematical
Sciences at the University of Patras in 1978, and Chancellor of the university
in 1981. He served for about three years as Vice President-Academic Affairs of
the then new University of Crete, Greece, in 1981--1985. In 1984, he was a
Visiting Professor in the Intercollege Division of Statistics at the University
of California, Davis, and he was appointed Professor, Associate Dean and Chair
of the Graduate Group in Statistics in the same university in 1985; he served
in the two administrative capacities in 1985--1999. He is an elected member of
the International Statistical Institute since 1974, a Fellow of the Royal
Statistical Society since 1975, a Fellow of the Institute of Mathematical
Statistics since 1983, and a Fellow of the American Statistical Association
since 1986. He served as a member of the Council of the Hellenic Mathematical
Society, and as President of the Balkan Union of Mathematicians.
"
1105.2582,2011-05-16,Baby Morse Theory in Data Analysis,"  A methodology is proposed for inferring the topology underlying point cloud
data. The approach employs basic elements of Morse Theory, and is capable of
producing not only a point estimate of various topological quantities (e.g.,
genus), but it can also assess their sampling uncertainty in a probabilistic
fashion. Several examples of point cloud data in three dimensions are utilized
to demonstrate how the method yields interval estimates for the topology of the
data as a 2-dimensional surface embedded in R^3.
"
1105.5038,2019-08-16,"Uniform bias study and Bahadur representation for local polynomial
  estimators of the conditional quantile function","  This paper investigates the bias and the weak Bahadur representation of a
local polynomial estimator of the conditional quantile function and its
derivatives. The bias and Bahadur remainder term are studied uniformly with
respect to the quantile level, the covariates and the smoothing parameter. The
order of the local polynomial estimator can be higher than the
differentiability order of the conditional quantile function. Applications of
the results deal with global optimal consistency rates of the local polynomial
quantile estimator, performance of random bandwidths and estimation of the
conditional quantile density function. The latter allows to obtain a simple
estimator of the conditional quantile function of the private values in a first
price sealed bids auctions under the independent private values paradigm and
risk neutrality.
"
1105.6145,2013-06-19,Maximum lilkelihood estimation in the $\beta$-model,"  We study maximum likelihood estimation for the statistical model for
undirected random graphs, known as the $\beta$-model, in which the degree
sequences are minimal sufficient statistics. We derive necessary and sufficient
conditions, based on the polytope of degree sequences, for the existence of the
maximum likelihood estimator (MLE) of the model parameters. We characterize in
a combinatorial fashion sample points leading to a nonexistent MLE, and
nonestimability of the probability parameters under a nonexistent MLE. We
formulate conditions that guarantee that the MLE exists with probability
tending to one as the number of nodes increases.
"
1106.2895,2011-06-23,Statistical Inference: The Big Picture,"  Statistics has moved beyond the frequentist-Bayesian controversies of the
past. Where does this leave our ability to interpret results? I suggest that a
philosophy compatible with statistical practice, labeled here statistical
pragmatism, serves as a foundation for inference. Statistical pragmatism is
inclusive and emphasizes the assumptions that connect statistical models with
observed data. I argue that introductory courses often mischaracterize the
process of statistical inference and I propose an alternative ""big picture""
depiction.
"
1106.2994,2015-05-28,"Widely Linear vs. Conventional Subspace-Based Estimation of SIMO
  Flat-Fading Channels: Mean-Squared Error Analysis","  We analyze the mean-squared error (MSE) performance of widely linear (WL) and
conventional subspace-based channel estimation for single-input multiple-output
(SIMO) flat-fading channels employing binary phase-shift-keying (BPSK)
modulation when the covariance matrix is estimated using a finite number of
samples. The conventional estimator suffers from a phase ambiguity that reduces
to a sign ambiguity for the WL estimator. We derive closed-form expressions for
the MSE of the two estimators under four different ambiguity resolution
scenarios. The first scenario is optimal resolution, which minimizes the
Euclidean distance between the channel estimate and the actual channel. The
second scenario assumes that a randomly chosen coefficient of the actual
channel is known and the third assumes that the one with the largest magnitude
is known. The fourth scenario is the more realistic case where pilot symbols
are used to resolve the ambiguities. Our work demonstrates that there is a
strong relationship between the accuracy of ambiguity resolution and the
relative performance of WL and conventional subspace-based estimators, and
shows that the less information available about the actual channel for
ambiguity resolution, or the lower the accuracy of this information, the higher
the performance gap in favor of the WL estimator.
"
1106.3259,2011-07-14,"A flexible observed factor model with separate dynamics for the factor
  volatilities and their correlation matrix","  Our article considers a regression model with observed factors. The observed
factors have a flexible stochastic volatility structure that has separate
dynamics for the volatilities and the correlation matrix. The correlation
matrix of the factors is time-varying and its evolution is described by an
inverse Wishart process. The model specifies the evolution of the observed
volatilities flexibly and is particularly attractive when the dimension of the
observations is high. A Markov chain Monte Carlo algorithm is developed to
estimate the model. It is straightforward to use this algorithm to obtain the
predictive distributions of future observations and to carry out model
selection. The model is illustrated and compared to other Wishart-type factor
multivariate stochastic volatility models using various empirical data
including monthly stock returns and portfolio weighted returns. The evidence
suggests that our model has better predictive performance. The paper also
allows the idiosyncratic errors to follow individual stochastic volatility
processes in order to deal with more volatile data such as daily or weekly
stock returns.
"
1106.3940,2011-06-21,Cooperative spectrum sensing over unreliable reporting channel,"  This article aims to analyze a cooperative spectrum sensing scheme using a
centralized approach with unreliable reporting channel. The spectrum sensing is
applied to a cognitive radio system, where each cognitive radio performs a
simple energy detection and send the decision to a fusion center through a
reporting channel. When the decisions are available at the fusion center, a
n-out-of-K rule is applied. The impact of the choice of the parameter n in the
cognitive radio system performance is analyzed in the case where the reporting
channel introduces errors.
"
1106.4513,2011-10-04,"A Markov Chain approach to determine the optimal performance period and
  bad definition for credit scorecard","  Performance period determination and bad definition for credit scorecard has
been a mix of fortune for the typical data modeler. The lack of literature on
these matters led to a proliferation of approaches and techniques to solve the
problems. However, the most commonly accepted approach involves subjective
interpretations of the performance period and bad definition as well as being
chicken and egg problem. These complications result in poorly developed credit
scorecard with minimal benefits to the banks. In this paper, we will be
recommending a simple and effective approach to resolve these issues.
"
1106.5598,2011-06-29,"Some notes on biasedness and unbiasedness of two-sample
  Kolmogorov-Smirnov test","  This paper deals with two-sample Kolmogorov-Smirnov test and its biasedness.
This test is not unbiased in general in case of different sample sizes. We
found out most biased distribution for some values of significance level
$\alpha$. Moreover we discovered that there exists number of observation and
significance level $\alpha$ such that this test is unbiased at level $\alpha$.
"
1107.0202,2011-07-04,Revealing Sub-Optimality Conditions of Strategic Decisions,"  Conceptual view of fitness and fitness measurement of strategic decisions on
information systems, technological systems and innovation are becoming more
important in recent years. This paper determines some dynamics of fitness
landscape which are lead to termination of decision makers' research before
reaching the global maximum in strategic decisions. These dynamics are
specified according to management decision making models and supported with
simulation results. This article determines simulation results by means of
""Fitness Value"" and ""Probability of Optimality"". Correlation between these two
concepts may be remarkable according to revealing optimal values in innovative
and research-based decision making approaches beside sub-optimal results of
traditional decision making approaches.
"
1107.5141,2011-07-27,"Which are the best cities for psychology research worldwide? A map
  visualizing city ratios of observed and expected numbers of highly-cited
  papers","  We present scientometric results about world-wide centers of excellence in
psychology. Based on Web of Science data, domain-specific excellence can be
identified for cities where highly cited papers are published. Data refer to
all psychology articles published in 2007 which are documented in the Social
Science Citation Index and to their citation frequencies from 2007 to May 2011.
Visualized are 214 cities with an article output of at least 50 in 2007.
Statistical z tests are used for the evaluation of the degree to which an
observed number of top-cited papers (top-10%) for a city differs from the
number expected on the basis of randomness in the selection of papers. Map
visualizing city ratios on significant differences between observed and
expected numbers of highly-cited papers point at excellence centers in cities
at the East and West Coast of the United States as well as in Great Britain,
Germany, the Netherlands, Ireland, Belgium, Sweden, Finland, Australia, and
Taiwan. Furthermore, positive but non-significant differences in favor of high
citation rates are documented for some cities in the United States, Great
Britain, the Netherlands, the Scandinavian and the German-speaking countries,
Belgium, France, Spain, Israel, South Korea, and China. Scientometric results
show convincingly that highly-cited psychological research articles come from
the Anglo-American countries and some of the non-English European countries in
which the number of English-language publications has increased during the last
decades.
"
1108.1879,2011-08-10,Boundary detection in disease mapping studies,"  In disease mapping, the aim is to estimate the spatial pattern in disease
risk over an extended geographical region, so that areas with elevated risks
can be identified. A Bayesian hierarchical approach is typically used to
produce such maps, which models the risk surface with a set of spatially smooth
random effects. However, in complex urban settings there are likely to be
boundaries in the risk surface, which separate populations that are
geographically adjacent but have very different risk profiles. Therefore this
paper proposes an approach for detecting such risk boundaries, and tests its
effectiveness by simulation. Finally, the model is applied to lung cancer
incidence data in Greater Glasgow, Scotland, between 2001 and 2005.
"
1108.3585,2011-08-19,"Some properties of the moment estimator of shape parameter for the gamma
  distribution","  Exact distribution of the moment estimator of shape parameter for the gamma
distribution for small samples is derived. Order preserving properties of this
estimator are presented.
"
1108.3586,2011-10-26,Order preserving property of moment estimators,"  Balakrishnan and Mi [1] considered order preserving property of maximum
likelihood estimators. In this paper there are given conditions under which the
moment estimators have the property of preserving stochastic orders. There is
considered property of preserving for usual stochastic order as well as for
likelihood ratio order. Mainly, sufficient conditions are given for one
parameter family of distributions and also for exponential family, location
family and scale family.
"
1109.0294,2015-05-30,"Extreme value and record statistics in heavy-tailed processes with
  long-range memory","  Extreme events are an important theme in various areas of science because of
their typically devastating effects on society and their scientific
complexities. The latter is particularly true if the underlying dynamics does
not lead to independent extreme events as often observed in natural systems.
Here, we focus on this case and consider stationary stochastic processes that
are characterized by long-range memory and heavy-tailed distributions, often
called fractional L\'evy noise. While the size distribution of extreme events
is not affected by the long-range memory in the asymptotic limit and remains a
Fr\'echet distribution, there are strong finite-size effects if the memory
leads to persistence in the underlying dynamics. Moreover, we show that this
persistence is also present in the extreme events, which allows one to make a
time-dependent hazard assessment of future extreme events based on events
observed in the past. This has direct applications in the field of space
weather as we discuss specifically for the case of the solar power influx into
the magnetosphere. Finally, we show how the statistics of records, or
record-breaking extreme events, is affected by the presence of long-range
memory.
"
1109.0828,2015-09-14,The Product Life Cycle of Durable Goods,"  A dynamic model of the product lifecycle of (nearly) homogeneous durables in
polypoly markets is established. It describes the concurrent evolution of the
unit sales and price of durable goods. The theory is based on the idea that the
sales dynamics is determined by a meeting process of demanded with supplied
product units. Taking advantage from the Bass model for first purchase and a
logistic model for repurchase the entire product lifecycle of a durable can be
established. For the case of a fast growing supply the model suggests that the
mean price of the good decreases according to a logistic law. Both, the
established unit sales and price evolution are in agreement with the empirical
data studied in this paper. The presented approach discusses further the
interference of the diffusion process with the supply dynamics. The model
predicts the occurrence of lost sales in the initial stages of the lifecycle
due to supply constraints. They are the origin for a retarded market
penetration. The theory suggests that the imitation rate B indicating social
contagion in the Bass model has its maximum magnitude for the case of a large
amount of available units at introduction and a fast output increase. The
empirical data of the investigated samples are in qualitative agreement with
this prediction.
"
1109.4371,2015-03-09,"High dimensional Bayesian inference for Gaussian directed acyclic graph
  models","  In this paper, we consider Gaussian models Markov with respect to an
arbitrary DAG. We first construct a family of conjugate priors for the Cholesky
parametrization of the covariance matrix of such models. This family has as
many shape parameters as the DAG has vertices, and naturally extends the work
of Geiger and Heckerman [8]. From these distributions, we derive prior
distributions for the covariance and precision parameters of the Gaussian DAG
Markov models. Our works thus extends the work of Dawid and Lauritzen [5] and
Letac and Massam [16] for Gaussian models Markov with respect to a decomposable
graph to arbitrary DAGs. For this reason, we call our distributions DAG-Wishart
distributions. An advantage of these distributions is that they possess strong
hyper Markov properties and thus allow for explicit estimation of the
covariance and precision parameters, regardless of the dimension of the
problem. They also allow us to develop methodology for model selection and
covariance estimation in the space of DAG-Markov models. We demonstrate via
several numerical examples that the proposed method scales well to
high-dimensions.
"
1109.5640,2011-11-04,Removing Gaussian Noise by Optimization of Weights in Non-Local Means,"  A new image denoising algorithm to deal with the additive Gaussian white
noise model is given. Like the non-local means method, the filter is based on
the weighted average of the observations in a neighborhood, with weights
depending on the similarity of local patches. But in contrast to the non-local
means filter, instead of using a fixed Gaussian kernel, we propose to choose
the weights by minimizing a tight upper bound of mean square error. This
approach makes it possible to define the weights adapted to the function at
hand, mimicking the weights of the oracle filter. Under some regularity
conditions on the target image, we show that the obtained estimator converges
at the usual optimal rate. The proposed algorithm is parameter free in the
sense that it automatically calculates the bandwidth of the smoothing kernel;
it is fast and its implementation is straightforward. The performance of the
new filter is illustrated by numerical simulations.
"
1109.5909,2014-03-13,Markov properties for mixed graphs,"  In this paper, we unify the Markov theory of a variety of different types of
graphs used in graphical Markov models by introducing the class of loopless
mixed graphs, and show that all independence models induced by $m$-separation
on such graphs are compositional graphoids. We focus in particular on the
subclass of ribbonless graphs which as special cases include undirected graphs,
bidirected graphs, and directed acyclic graphs, as well as ancestral graphs and
summary graphs. We define maximality of such graphs as well as a pairwise and a
global Markov property. We prove that the global and pairwise Markov properties
of a maximal ribbonless graph are equivalent for any independence model that is
a compositional graphoid.
"
1109.6531,2011-09-30,"Technical Report: Energy Evaluation of preamble Sampling MAC Protocols
  for Wireless Sensor Networks","  The paper presents a simple probabilistic analysis of the energy consumption
in preamble sampling MAC protocols. We validate the analytical results with
simulations. We compare the classical MAC protocols (B-MAC and X-MAC) with
LAMAC, a method proposed in a companion paper. Our analysis highlights the
energy savings achievable with LA-MAC with respect to B-MAC and X-MAC. It also
shows that LA-MAC provides the best performance in the considered case of high
density networks under traffic congestion.
"
1109.6565,2011-09-30,A Statistical Significance Simulation Study for the General Scientist,"  When a scientist performs an experiment they normally acquire a set of
measurements and are expected to demonstrate that their results are
""statistically significant"" thus confirming whatever hypothesis they are
testing. The main method for establishing statistical significance involves
demonstrating that there is a low probability that the observed experimental
results were the product of random chance. This is typically defined as p <
0.05, which indicates there is less than a 5% chance that the observed results
occurred randomly. This research study visually demonstrates that the commonly
used definition for ""statistical significance"" can erroneously imply a
significant finding. This is demonstrated by generating random Gaussian noise
data and analyzing that data using statistical testing based on the established
two-sample t-test. This study demonstrates that insignificant yet
""statistically significant"" findings are possible at moderately large sample
sizes which are very common in many fields of modern science.
"
1110.0349,2011-10-04,Modern Portfolio Theory using SAS\textregistered OR,"  Investment approaches in financial instruments have been varied and often
produce unpredictable results. Many investors in the earlier days of investment
banking suffered catastrophical losses due to poor strategy and lack of
understanding of the financial market. With the development of investment
banking, many innovative investment strategies have been proposed to make
portfolio returns higher than the overall market. One of the most famous
theories of portfolio creation and management is the modern portfolio theory
proposed by Harry Markowitz. In this paper, we shall apply the theory in
creating a portfolio of stocks as well as managing it.
"
1110.1462,2016-05-03,"Dynamic Clustering of Histogram Data Based on Adaptive Squared
  Wasserstein Distances","  This paper deals with clustering methods based on adaptive distances for
histogram data using a dynamic clustering algorithm. Histogram data describes
individuals in terms of empirical distributions. These kind of data can be
considered as complex descriptions of phenomena observed on complex objects:
images, groups of individuals, spatial or temporal variant data, results of
queries, environmental data, and so on. The Wasserstein distance is used to
compare two histograms. The Wasserstein distance between histograms is
constituted by two components: the first based on the means, and the second, to
internal dispersions (standard deviation, skewness, kurtosis, and so on) of the
histograms. To cluster sets of histogram data, we propose to use Dynamic
Clustering Algorithm, (based on adaptive squared Wasserstein distances) that is
a k-means-like algorithm for clustering a set of individuals into $K$ classes
that are apriori fixed.
  The main aim of this research is to provide a tool for clustering histograms,
emphasizing the different contributions of the histogram variables, and their
components, to the definition of the clusters. We demonstrate that this can be
achieved using adaptive distances. Two kind of adaptive distances are
considered: the first takes into account the variability of each component of
each descriptor for the whole set of individuals; the second takes into account
the variability of each component of each descriptor in each cluster. We
furnish interpretative tools of the obtained partition based on an extension of
the classical measures (indexes) to the use of adaptive distances in the
clustering criterion function. Applications on synthetic and real-world data
corroborate the proposed procedure.
"
1110.1615,2016-01-20,"Estimation of time-delayed mutual information and bias for irregularly
  and sparsely sampled time-series","  A method to estimate the time-dependent correlation via an empirical bias
estimate of the time-delayed mutual information for a time-series is proposed.
In particular, the bias of the time-delayed mutual information is shown to
often be equivalent to the mutual information between two distributions of
points from the same system separated by infinite time. Thus intuitively,
estimation of the bias is reduced to estimation of the mutual information
between distributions of data points separated by large time intervals. The
proposed bias estimation techniques are shown to work for Lorenz equations data
and glucose time series data of three patients from the Columbia University
Medical Center database.
"
1110.1892,2015-09-22,Confidence-based Reasoning in Stochastic Constraint Programming,"  In this work we introduce a novel approach, based on sampling, for finding
assignments that are likely to be solutions to stochastic constraint
satisfaction problems and constraint optimisation problems. Our approach
reduces the size of the original problem being analysed; by solving this
reduced problem, with a given confidence probability, we obtain assignments
that satisfy the chance constraints in the original model within prescribed
error tolerance thresholds. To achieve this, we blend concepts from stochastic
constraint programming and statistics. We discuss both exact and approximate
variants of our method. The framework we introduce can be immediately employed
in concert with existing approaches for solving stochastic constraint programs.
A thorough computational study on a number of stochastic combinatorial
optimisation problems demonstrates the effectiveness of our approach.
"
1110.3860,2011-10-19,"Contending Parties: A Logistic Choice Analysis of Inter- and Intra-group
  Blog Citation Dynamics in the 2004 US Presidential Election","  The 2004 US Presidential Election cycle marked the debut of Internet-based
media such as blogs and social networking websites as institutionally
recognized features of the American political landscape. Using a longitudinal
sample of all DNC/RNC-designated blog-citation networks we are able to test the
influence of various strategic, institutional, and balance-theoretic mechanisms
and exogenous factors such as seasonality and political events on the
propensity of blogs to cite one another over time. Capitalizing on the temporal
resolution of our data, we utilize an autoregressive network regression
framework to carry out inference for a logistic choice process. Using a
combination of deviance-based model selection criteria and simulation-based
model adequacy tests, we identify the combination of processes that best
characterizes the choice behavior of the contending blogs.
"
1110.4168,2013-12-18,Stable mixed graphs,"  In this paper, we study classes of graphs with three types of edges that
capture the modified independence structure of a directed acyclic graph (DAG)
after marginalisation over unobserved variables and conditioning on selection
variables using the $m$-separation criterion. These include MC, summary, and
ancestral graphs. As a modification of MC graphs, we define the class of
ribbonless graphs (RGs) that permits the use of the $m$-separation criterion.
RGs contain summary and ancestral graphs as subclasses, and each RG can be
generated by a DAG after marginalisation and conditioning. We derive simple
algorithms to generate RGs, from given DAGs or RGs, and also to generate
summary and ancestral graphs in a simple way by further extension of the
RG-generating algorithm. This enables us to develop a parallel theory on these
three classes and to study the relationships between them as well as the use of
each class.
"
1110.4539,2011-10-21,Markov Equivalences for Subclasses of Loopless Mixed Graphs,"  In this paper we discuss four problems regarding Markov equivalences for
subclasses of loopless mixed graphs. We classify these four problems as finding
conditions for internal Markov equivalence, which is Markov equivalence within
a subclass, for external Markov equivalence, which is Markov equivalence
between subclasses, for representational Markov equivalence, which is the
possibility of a graph from a subclass being Markov equivalent to a graph from
another subclass, and finding algorithms to generate a graph from a certain
subclass that is Markov equivalent to a given graph. We particularly focus on
the class of maximal ancestral graphs and its subclasses, namely regression
graphs, bidirected graphs, undirected graphs, and directed acyclic graphs, and
present novel results for representational Markov equivalence and algorithms.
"
1110.5971,2015-05-30,Effects of dimers on cooperation in the spatial prisoner's dilemma game,"  We investigate the evolutionary prisoner's dilemma game in structured
populations by introducing dimers, which are defined as that two players in
each dimer always hold a same strategy. We find that influences of dimers on
cooperation depend on the type of dimers and the population structure. For
those dimers in which players interact with each other, the cooperation level
increases with the number of dimers though the cooperation improvement level
depends on the type of network structures. On the other hand, the dimers, in
which there are not mutual interactions, will not do any good to the
cooperation level in a single community, but interestingly, will improve the
cooperation level in a population with two communities. We explore the
relationship between dimers and self-interactions and find that the effects of
dimers are similar to that of self-interactions. Also, we find that the dimers,
which are established over two communities in a multi-community network, act as
one type of interaction through which information between communities is
communicated by the requirement that two players in a dimer hold a same
strategy.
"
1111.0559,2011-11-03,Model Selection in Undirected Graphical Models with the Elastic Net,"  Structure learning in random fields has attracted considerable attention due
to its difficulty and importance in areas such as remote sensing, computational
biology, natural language processing, protein networks, and social network
analysis. We consider the problem of estimating the probabilistic graph
structure associated with a Gaussian Markov Random Field (GMRF), the Ising
model and the Potts model, by extending previous work on $l_1$ regularized
neighborhood estimation to include the elastic net $l_1+l_2$ penalty.
Additionally, we show numerical evidence that the edge density plays a role in
the graph recovery process. Finally, we introduce a novel method for augmenting
neighborhood estimation by leveraging pair-wise neighborhood union estimates.
"
1111.2091,2012-03-28,Performance-based regularization in mean-CVaR portfolio optimization,"  We introduce performance-based regularization (PBR), a new approach to
addressing estimation risk in data-driven optimization, to mean-CVaR portfolio
optimization. We assume the available log-return data is iid, and detail the
approach for two cases: nonparametric and parametric (the log-return
distribution belongs in the elliptical family). The nonparametric PBR method
penalizes portfolios with large variability in mean and CVaR estimations. The
parametric PBR method solves the empirical Markowitz problem instead of the
empirical mean-CVaR problem, as the solutions of the Markowitz and mean-CVaR
problems are equivalent when the log-return distribution is elliptical. We
derive the asymptotic behavior of the nonparametric PBR solution, which leads
to insight into the effect of penalization, and justification of the parametric
PBR method. We also show via simulations that the PBR methods produce efficient
frontiers that are, on average, closer to the population efficient frontier
than the empirical approach to the mean-CVaR problem, with less variability.
"
1111.2368,2011-11-11,On a connection between Stein characterizations and Fisher information,"  We generalize the so-called density approach to Stein characterizations of
probability distributions. We prove an elementary factorization property of the
resulting Stein operator in terms of a generalized (standardized) score
function. We use this result to connect Stein characterizations with
information distances such as the generalized (standardized) Fisher
information.
"
1111.5110,2012-06-11,Grouped sparse paired comparisons in the Bradley-Terry model,"  In a wide class of paired comparisons, especially in the sports games, in
which all subjects are divided into several groups, the intragroup comparisons
are dense and the intergroup comparisons are sparse. Typical examples include
the NFL regular season. Motivated by these situations, we propose group
sparsity for paired comparisons and show the consistency and asymptotical
normality of the maximum likelihood estimate in the Bradley-Terry model when
the number of parameters goes to infinity in this paper. Simulations are
carried out to illustrate the group sparsity and asymptotical results.
"
1112.3777,2011-12-19,"Parameter estimation for the discretely observed fractional
  Ornstein-Uhlenbeck process and the Yuima R package","  This paper proposes consistent and asymptotically Gaussian estimators for the
drift, the diffusion coefficient and the Hurst exponent of the discretely
observed fractional Ornstein-Uhlenbeck process. For the estimation of the
drift, the results are obtained only in the case when 1/2 < H < 3/4. This paper
also provides ready-to-use software for the R statistical environment based on
the YUIMA package.
"
1201.0846,2012-09-25,"Using complex surveys to estimate the $L_1$-median of a functional
  variable: application to electricity load curves","  Mean profiles are widely used as indicators of the electricity consumption
habits of customers. Currently, in \'Electricit\'e De France (EDF), class load
profiles are estimated using point-wise mean function. Unfortunately, it is
well known that the mean is highly sensitive to the presence of outliers, such
as one or more consumers with unusually high-levels of consumption. In this
paper, we propose an alternative to the mean profile: the $L_1$-median profile
which is more robust. When dealing with large datasets of functional data (load
curves for example), survey sampling approaches are useful for estimating the
median profile avoiding storing the whole data. We propose here estimators of
the median trajectory using several sampling strategies and estimators. A
comparison between them is illustrated by means of a test population. We
develop a stratification based on the linearized variable which substantially
improves the accuracy of the estimator compared to simple random sampling
without replacement. We suggest also an improved estimator that takes into
account auxiliary information. Some potential areas for future research are
also highlighted.
"
1201.2590,2012-01-13,It is Time to Stop Teaching Frequentism to Non-statisticians,"  We should cease teaching frequentist statistics to undergraduates and switch
to Bayes. Doing so will reduce the amount of confusion and over-certainty rife
among users of statistics.
"
1201.4743,2012-01-24,Voting Power : A Generalised Framework,"  This paper examines an area of Game Theory called Voting Power Theory. With
the adoption of a measure theoretic framework it argues that the many different
indices and tools currently used for measuring voting power can be replaced by
just three simple probabilities. The framework is sufficiently general to be
applicable to every conceivable type of voting game, and every possible
decision rule.
"
1201.6450,2012-02-01,"A Brief History of the Statistics Department of the University of
  California at Berkeley","  The early history of our department was dominated by Jerzy Neyman
(1894-1981), while the next phase was largely in the hands of Neyman's
students, with Erich Lehmann (1917-2009) being a central, long-lived and
much-loved member of this group. We are very fortunate in having Constance
Reid's biography ""Neyman -- From Life"" and Erich's ""Reminiscences of a
Statistician: The Company I Kept"" and other historical material documenting the
founding and growth of the department, and the people in it. In what follows,
we will draw heavily from these sources, describing what seems to us to be a
remarkable success story: one person starting ""a cell of statistical research
and teaching ... not being hampered by any existing traditions and routines""
and seeing that cell grow rapidly into a major force in academic statistics
worldwide. That it has remained so for (at least) the half-century after its
founding is a testament to the strength of Neyman's model for a department of
statistics.
"
1202.1436,2016-05-03,"Linear regression for numeric symbolic variables: an ordinary least
  squares approach based on Wasserstein Distance","  In this paper we present a linear regression model for modal symbolic data.
The observed variables are histogram variables according to the definition
given in the framework of Symbolic Data Analysis and the parameters of the
model are estimated using the classic Least Squares method. An appropriate
metric is introduced in order to measure the error between the observed and the
predicted distributions. In particular, the Wasserstein distance is proposed.
Some properties of such metric are exploited to predict the response variable
as direct linear combination of other independent histogram variables. Measures
of goodness of fit are discussed. An application on real data corroborates the
proposed method.
"
1202.3468,2012-02-17,"Partially-blind Estimation of Reciprocal Channels for AF Two-Way Relay
  Networks Employing M-PSK Modulation","  We consider the problem of channel estimation for amplify-and-forward two-way
relays assuming channel reciprocity and M-PSK modulation. In an earlier work, a
partially-blind maximum-likelihood estimator was derived by treating the data
as deterministic unknowns. We prove that this estimator approaches the true
channel with high probability at high signal-to-noise ratio (SNR) but is not
consistent. We then propose an alternative estimator which is consistent and
has similarly favorable high SNR performance. We also derive the Cramer-Rao
bound on the variance of unbiased estimators.
"
1202.4669,2012-08-27,A Concise Resolution to the Two Envelope Paradox,"  In this paper, I will demonstrate a new perspective on the Two Envelope
Problem. I hope to show with convincing clarity how the paradox results from an
inherent problem pertaining to the interpretation of Bayesian probability.
Specifically, a subjective probability that is inconsistent with reality can
mislead reasoning based on Bayesian decision theory.
"
1202.4746,2012-02-22,"A Proof on Asymptotics of Wavelet Variance of a Long Memory Process by
  Using Taylor Expansion","  A long memory process has self-similarity or scale-invariant properties in
low frequencies. We prove that the log of the scale-dependent wavelet variance
for a long memory process is asymptotically proportional to scales by using the
Taylor expansion of wavelet variances.
"
1203.3559,2012-03-19,A divergence formula for regularization methods with an L2 constraint,"  We derive a divergence formula for a group of regularization methods with an
L2 constraint. The formula is useful for regularization parameter selection,
because it provides an unbiased estimate for the number of degrees of freedom.
We begin with deriving the formula for smoothing splines and then extend it to
other settings such as penalized splines, ridge regression, and functional
linear regression.
"
1203.3747,2012-03-19,Note on the closed-form MLEs of k-component load-sharing systems,"  Recently Kim and Kvam (2004) and Singh, Sharma, Kumar (2008) proposed
different load-sharing models and developed parametric inference for the these
models. However, their parametric estimates are calculated using iterative
numerical methods. In this note, we provide the general closed-form MLEs for
the two load-sharing models provided by them.
"
1203.3880,2012-03-20,"Parameter Estimation from Censored Samples using the
  Expectation-Maximization Algorithm","  This paper deals with parameter estimation when the data are randomly right
censored. The maximum likelihood estimates from censored samples are obtained
by using the expectation-maximization (EM) and Monte Carlo EM (MCEM)
algorithms. We introduce the concept of the EM and MCEM algorithms and develop
parameter estimation methods for a variety of distributions such as normal,
Laplace and Rayleigh distributions. These proposed methods are illustrated with
three examples.
"
1203.4810,2012-03-22,"Estimating a Random Walk First-Passage Time from Noisy or Delayed
  Observations","  A random walk (or a Wiener process), possibly with drift, is observed in a
noisy or delayed fashion. The problem considered in this paper is to estimate
the first time \tau the random walk reaches a given level. Specifically, the
p-moment (p\geq 1) optimization problem \inf_\eta \ex|\eta-\tau|^p is
investigated where the infimum is taken over the set of stopping times that are
defined on the observation process.
  When there is no drift, optimal stopping rules are characterized for both
types of observations. When there is a drift, upper and lower bounds on
\inf_\eta \ex|\eta-\tau|^p are established for both types of observations. The
bounds are tight in the large-level regime for noisy observations and in the
large-level-large-delay regime for delayed observations. Noteworthy, for noisy
observations there exists an asymptotically optimal stopping rule that is a
function of a single observation.
  Simulation results are provided that corroborate the validity of the results
for non-asymptotic settings.
"
1203.5402,2012-04-11,"Sparse solution of overdetermined linear systems when the columns of $A$
  are orthogonal","  In this paper, we consider the problem of obtaining the best $k$-sparse
solution of $Ax=y$ subject to the constraint that the columns of $A$ are
orthogonal. The naive approach for obtaining a solution to this problem has
exponential complexity and there exist $l_1$ regularization methods such as
Lasso to obtain approximate solutions. In this paper, we show that we can
obtain an exact solution to the problem, with much less computational effort
compared to the brute force search when the columns of $A$ are orthogonal.
"
1203.6140,2012-03-29,Why FARIMA Models are Brittle,"  The FARIMA models, which have long-range-dependence (LRD), are widely used in
many areas. Through deriving a precise characterisation of the spectrum,
autocovariance function, and variance time function, we show that this family
is very atypical among LRD processes, being extremely close to the fractional
Gaussian noise in a precise sense. Furthermore, we show that this closeness
property is not robust to additive noise. We argue that the use of FARIMA, and
more generally fractionally differenced time series, should be reassessed in
some contexts, in particular when convergence rate under rescaling is important
and noise is expected.
"
1203.6249,2012-03-29,Reading Theorie Analytique des Probabilites,"  This note is an extended read of my read of Laplace's book Theorie Analytique
des Probabilites, when considered from a Bayesian viewpoint but without
historical nor comparative pretentions. A deeper analysis is provided in Dale
(1999).
"
1204.0165,2015-03-02,"Analytical Models for Power Networks: The case of the Western US and
  ERCOT grids","  The topological structure of the power grid plays a key role in the reliable
delivery of electricity and price settlement in the electricity market.
Incorporation of new energy sources and loads into the grid over time has led
to its structural and geographical expansion and can affect its stable
operation. This paper presents an intuitive analytical model for the temporal
evolution of large grids and uses it to understand common structural features
observed in grids across America. In particular, key graph parameters like
degree distribution, graph diameter, betweenness centralities, eigen-spread and
clustering coefficients, as well as graph processes like infection propagation
are used to quantify the model's benefits through comparison with the Western
US and ERCOT power grids. The most significant contribution of the developed
model is its analytical tractability, that provides a closed form expression
for the nodal degree distribution observed in large grids. The discussed model
can be used to generate realistic test cases to analyze topological effects on
grid functioning and new grid technologies.
"
1204.1561,2012-04-10,"The macroeconomic effect of the information and communication technology
  in Hungary","  It was not until the beginning of the 1990s that the effects of information
and communication technology on economic growth as well as on the profitability
of enterprises raised the interest of researchers. After giving a general
description on the relationship between a more intense use of ICT devices and
dynamic economic growth, the author identified and explained those four
channels that had a robust influence on economic growth and productivity. When
comparing the use of information technonology devices in developed as well as
in developing countries, the author highlighted the importance of the available
additional human capital and the elimination of organizational inflexibilities
in the attempt of narrowing the productivity gap between the developed and
developing nations. By processing a large quantitiy of information gained from
Hungarian enterprises operating in several economic sectors, the author made an
attempt to find a strong correlation between the development level of using ICT
devices and profitability together with total factor productivity. Although the
impact of using ICT devices cannot be measured unequivocally at the
microeconomic level because of certain statistical and methodological
imperfections, by applying such analytical methods as cluster analysis and
correlation and regression calculation, the author managed to prove that both
the correlation coefficient and the gradient of the regression trend line
showed a positive relationship between the extensive use of information and
communication technology and the profitability of enterprises.
"
1204.2410,2012-10-30,Densities of nested Archimedean copulas,"  Nested Archimedean copulas recently gained interest since they generalize the
well-known class of Archimedean copulas to allow for partial asymmetry.
Sampling algorithms and strategies have been well investigated for nested
Archimedean copulas. However, for likelihood based inference it is important to
have the density. The present work fills this gap. A general formula for the
derivatives of the nodes and inner generators appearing in nested Archimedean
copulas is developed. This leads to a tractable formula for the density of
nested Archimedean copulas in arbitrary dimensions if the number of nesting
levels is not too large. Various examples including famous Archimedean families
and transformations of such are given. Furthermore, a numerically efficient way
to evaluate the log-density is presented.
"
1204.3339,2023-03-06,Parameterization of Copulas and Covariance Decay of Stochastic Processes,"  In this work we study the problem of constructing stochastic processes with a
predetermined covariance decay by parameterizing its marginals and a given
family of copulas. We show that the proposed methodology is compatibility-free
and present several examples to illustrate the theory, including the important
Gaussian and Euclidean families of copulas. We associate the theory to common
applied time series models.
"
1204.4148,2012-04-19,Algorithm for multivariate data standardization up to third moment,"  An algorithm for transforming multivariate data to a form with normalized
first, second and third moments is presented.
"
1204.5334,2013-01-08,On individual neutrality and collective decision making,"  We derive a simple mathematical ""theory"" to show that two decision-making
entities can work better together only if at least one of them is occasionally
willing to stay neutral. This provides a mathematical ""justification"" for an
age-old cliche among marriage counselors.
"
1204.5635,2021-06-01,"Locally Most Powerful Invariant Tests for Correlation and Sphericity of
  Gaussian Vectors","  In this paper we study the existence of locally most powerful invariant tests
(LMPIT) for the problem of testing the covariance structure of a set of
Gaussian random vectors. The LMPIT is the optimal test for the case of close
hypotheses, among those satisfying the invariances of the problem, and in
practical scenarios can provide better performance than the typically used
generalized likelihood ratio test (GLRT). The derivation of the LMPIT usually
requires one to find the maximal invariant statistic for the detection problem
and then derive its distribution under both hypotheses, which in general is a
rather involved procedure. As an alternative, Wijsman's theorem provides the
ratio of the maximal invariant densities without even finding an explicit
expression for the maximal invariant. We first consider the problem of testing
whether a set of $N$-dimensional Gaussian random vectors are uncorrelated or
not, and show that the LMPIT is given by the Frobenius norm of the sample
coherence matrix. Second, we study the case in which the vectors under the null
hypothesis are uncorrelated and identically distributed, that is, the
sphericity test for Gaussian vectors, for which we show that the LMPIT is given
by the Frobenius norm of a normalized version of the sample covariance matrix.
Finally, some numerical examples illustrate the performance of the proposed
tests, which provide better results than their GLRT counterparts.
"
1204.5963,2013-06-28,On a Reliable Peer-Review Process,"  We propose an enhanced peer-review process where the reviewers are encouraged
to truthfully disclose their reviews. We start by modelling that process using
a Bayesian model where the uncertainty regarding the quality of the manuscript
is taken into account. After that, we introduce a scoring function to evaluate
the reported reviews. Under mild assumptions, we show that reviewers strictly
maximize their expected scores by telling the truth. We also show how those
scores can be used in order to reach consensus.
"
1205.0824,2022-11-16,"A Generalization of a Gaussian Semiparametric Estimator on Multivariate
  Long-Range Dependent Processes","  In this paper we propose and study a general class of Gaussian Semiparametric
Estimators (GSE) of the fractional differencing parameter in the context of
long-range dependent multivariate time series. We establish large sample
properties of the estimator without assuming Gaussianity. The class of models
considered here satisfies simple conditions on the spectral density function,
restricted to a small neighborhood of the zero frequency and includes important
class of VARFIMA processes. We also present a simulation study to assess the
finite sample properties of the proposed estimator based on a smoothed version
of the GSE which supports its competitiveness.
"
1205.1107,2012-05-08,Estimation of spatial max-stable models using threshold exceedances,"  Parametric inference for spatial max-stable processes is difficult since the
related likelihoods are unavailable. A composite likelihood approach based on
the bivariate distribution of block maxima has been recently proposed in the
literature. However modeling block maxima is a wasteful approach provided that
other information is available. Moreover an approach based on block, typically
annual, maxima is unable to take into account the fact that maxima occur or not
simultaneously. If time series of, say, daily data are available, then
estimation procedures based on exceedances of a high threshold could mitigate
such problems. In this paper we focus on two approaches for composing
likelihoods based on pairs of exceedances. The first one comes from the tail
approximation for bivariate distribution proposed by Ledford and Tawn (1996)
when both pairs of observations exceed the fixed threshold. The second one uses
the bivariate extension (Rootzen and Tajvidi, 2006) of the generalized Pareto
distribution which allows to model exceedances when at least one of the
components is over the threshold. The two approaches are compared through a
simulation study according to different degrees of spatial dependency. Results
show that both the strength of the spatial dependencies and the threshold
choice play a fundamental role in determining which is the best estimating
procedure.
"
1205.3217,2013-02-07,"A Generalized Fellegi-Sunter Framework for Multiple Record Linkage With
  Application to Homicide Record Systems","  We present a probabilistic method for linking multiple datafiles. This task
is not trivial in the absence of unique identifiers for the individuals
recorded. This is a common scenario when linking census data to coverage
measurement surveys for census coverage evaluation, and in general when
multiple record-systems need to be integrated for posterior analysis. Our
method generalizes the Fellegi-Sunter theory for linking records from two
datafiles and its modern implementations. The multiple record linkage goal is
to classify the record K-tuples coming from K datafiles according to the
different matching patterns. Our method incorporates the transitivity of
agreement in the computation of the data used to model matching probabilities.
We use a mixture model to fit matching probabilities via maximum likelihood
using the EM algorithm. We present a method to decide the record K-tuples
membership to the subsets of matching patterns and we prove its optimality. We
apply our method to the integration of three Colombian homicide record systems
and we perform a simulation study in order to explore the performance of the
method under measurement error and different scenarios. The proposed method
works well and opens some directions for future research.
"
1205.3588,2012-05-17,Uncertainties and Ambiguities in Percentiles and how to Avoid Them,"  The recently proposed fractional scoring scheme is used to attribute
publications to percentile rank classes. It is shown that in this way
uncertainties and ambiguities in the evaluation of percentile ranks do not
occur. Using the fractional scoring the total score of all papers exactly
reproduces the theoretical value.
"
1205.3845,2012-05-18,"Forecasting with Historical Data or Process Knowledge under
  Misspecification: A Comparison","  When faced with the task of forecasting a dynamic system, practitioners often
have available historical data, knowledge of the system, or a combination of
both. While intuition dictates that perfect knowledge of the system should in
theory yield perfect forecasting, often knowledge of the system is only
partially known, known up to parameters, or known incorrectly. In contrast,
forecasting using previous data without any process knowledge might result in
accurate prediction for simple systems, but will fail for highly nonlinear and
chaotic systems. In this paper, the authors demonstrate how even in chaotic
systems, forecasting with historical data is preferable to using process
knowledge if this knowledge exhibits certain forms of misspecification. Through
an extensive simulation study, a range of misspecification and forecasting
scenarios are examined with the goal of gaining an improved understanding of
the circumstances under which forecasting from historical data is to be
preferred over using process knowledge.
"
1205.4304,2012-05-22,In praise of the referee,"  There has been a lively debate in many fields, including statistics and
related applied fields such as psychology and biomedical research, on possible
reforms of the scholarly publishing system. Currently, referees contribute so
much to improve scientific papers, both directly through constructive criticism
and indirectly through the threat of rejection. We discuss ways in which new
approaches to journal publication could continue to make use of the valuable
efforts of peer reviewers.
"
1205.4807,2012-05-23,A Conversation with Eugenio Regazzini,"  Eugenio Regazzini was born on August 12, 1946 in Cremona (Italy), and took
his degree in 1969 at the University ""L. Bocconi"" of Milano. He has held
positions at the universities of Torino, Bologna and Milano, and at the
University ""L. Bocconi"" as assistant professor and lecturer from 1974 to 1980,
and then professor since 1980. He is currently professor in probability and
mathematical statistics at the University of Pavia. In the periods 1989-2001
and 2006-2009 he was head of the Institute for Applications of Mathematics and
Computer Science of the Italian National Research Council (C.N.R.) in Milano
and head of the Department of Mathematics at the University of Pavia,
respectively. For twelve years between 1989 and 2006, he served as a member of
the Scientific Board of the Italian Mathematical Union (U.M.I.). In 2007, he
was elected Fellow of the IMS and, in 2001, Fellow of the ""Istituto
Lombardo---Accademia di Scienze e Lettere."" His research activity in
probability and statistics has covered a wide spectrum of topics, including
finitely additive probabilities, foundations of the Bayesian paradigm,
exchangeability and partial exchangeability, distribution of functionals of
random probability measures, stochastic integration, history of probability and
statistics. Overall, he has been one of the most authoritative developers of de
Finetti's legacy. In the last five years, he has extended his scientific
interests to probabilistic methods in mathematical physics; in particular, he
has studied the asymptotic behavior of the solutions of equations, which are of
interest for the kinetic theory of gases. The present interview was taken in
occasion of his 65th birthday.
"
1205.4811,2015-06-05,Networks with time structure from time series,"  We propose a method of constructing a network, in which its time structure is
directly incorporated, based on a deterministic model from a time series. To
construct such a network, we transform a linear model containing terms with
different time delays into network topology. The terms in the model are
translated into temporal nodes of the network. On each link connecting these
nodes, we assign a positive real number representing the strength of
relationship, or the ""distance,"" between nodes specified by the parameters of
the model. The method is demonstrated by a known system and applied to two
actual time series.
"
1206.4208,2012-06-20,"A Fast Non-Gaussian Bayesian Matching Pursuit Method for Sparse
  Reconstruction","  A fast matching pursuit method using a Bayesian approach is introduced for
sparse signal recovery. This method, referred to as nGpFBMP, performs Bayesian
estimates of sparse signals even when the signal prior is non-Gaussian or
unknown. It is agnostic on signal statistics and utilizes a priori statistics
of additive noise and the sparsity rate of the signal, which are shown to be
easily estimated from data if not available. nGpFBMP utilizes a greedy approach
and order-recursive updates of its metrics to find the most dominant sparse
supports to determine the approximate minimum mean square error (MMSE) estimate
of the sparse signal. Simulation results demonstrate the power and robustness
of our proposed estimator.
"
1206.5478,2014-08-05,"Developing methods for identifying the inflection point of a
  convex/concave curve","  We are introducing two methods for revealing the true inflection point of
data that contains or not error. The starting point is a set of geometrical
properties that follow the existence of an inflection point p for a smooth
function. These properties connect the concept of convexity/concavity before
and after p respectively with three chords defined properly. Finally a set of
experiments is presented for the class of sigmoid curves and for the third
order polynomials.
"
1207.0407,2012-07-03,What is Statistics?; The Answer by Quantum Language,"  Since the problem: ""What is statistics?"" is most fundamental in sceince, in
order to solve this problem, there is every reason to believe that we have to
start from the proposal of a worldview. Recently we proposed measurement theory
(i.e., quantum language, or the linguistic interpretation of quantum
mechanics), which is characterized as the linguistic turn of the Copenhagen
interpretation of quantum mechanics. This turn from physics to language does
not only extend quantum theory to classical theory but also yield the quantum
mechanical world view (i.e., the (quantum) linguistic world view, and thus, a
form of quantum thinking, in other words, quantum philosophy). Thus, we believe
that the quantum lingistic formulation of statistics gives an answer to the
question: ""What is statistics?"". In this paper, this will be done through the
studies of inference interval, statistical hypothesis testing, Fisher maximum
likelihood method, Bayes method and regression analysis in meaurement theory.
"
1207.0700,2012-07-04,"A statistical view on team handball results: home advantage, team
  fitness and prediction of match outcomes","  We analyze the results of the German Team Handball Bundesliga for ten seasons
in a model-free statistical time series approach. We will show that the home
advantage is nearly negligible compared to the total sum of goals. Specific
interest has been spent on the time evolution of the team fitness expressed in
terms of the goal difference. In contrast to soccer, our results indicate a
decay of the team fitness values over a season while the long time correlation
behavior over years is nearly comparable. We are able to explain the dominance
of a few teams by the large value for the total number of goals in a match. A
method for the prediction of match winners is presented in good accuracy with
the real results. We analyze the properties of promoted teams and indicate
drastic level changes between the Bundesliga and the second league. Our
findings reflect in good agreement recent discussions on modern successful
attack strategies.
"
1207.1708,2012-11-05,Estimators for Archimedean copulas in high dimensions,"  The performance of known and new parametric estimators for Archimedean
copulas is investigated, with special focus on large dimensions and numerical
difficulties. In particular, method-of-moments-like estimators based on
pairwise Kendall's tau, a multivariate extension of Blomqvist's beta, minimum
distance estimators, the maximum-likelihood estimator, a simulated
maximum-likelihood estimator, and a maximum-likelihood estimator based on the
copula diagonal are studied. Their performance is compared in a large-scale
simulation study both under known and unknown margins (pseudo-observations), in
small and high dimensions, under small and large dependencies, various
different Archimedean families and sample sizes. High dimensions up to one
hundred are considered for the first time and computational problems arising
from such large dimensions are addressed in detail. All methods are implemented
in the open source \R{} package \pkg{copula} and can thus be easily accessed
and studied.
"
1207.2296,2013-08-23,"Extremal t processes: Elliptical domain of attraction and a spectral
  representation","  The extremal t process was proposed in the literature for modeling spatial
extremes within a copula framework based on the extreme value limit of
elliptical t distributions (Davison, Padoan and Ribatet (2012)). A major
drawback of this max-stable model was the lack of a spectral representation
such that for instance direct simulation was infeasible. The main contribution
of this note is to propose such a spectral construction for the extremal t
process. Interestingly, the extremal Gaussian process introduced by Schlather
(2002) appears as a special case. We further highlight the role of the extremal
t process as the maximum attractor for processes with finite-dimensional
elliptical distributions. All results naturally also hold within the
multivariate domain.
"
1207.4309,2012-09-21,Vine Constructions of Levy Copulas,"  Levy copulas are the most general concept to capture jump dependence in
multivariate Levy processes. They translate the intuition and many features of
the copula concept into a time series setting. A challenge faced by both,
distributional and Levy copulas, is to find flexible but still applicable
models for higher dimensions. To overcome this problem, the concept of pair
copula constructions has been successfully applied to distributional copulas.
In this paper, we develop the pair construction for Levy copulas (PLCC).
Similar to pair constructions of distributional copulas, the pair construction
of a d-dimensional Levy copula consists of d(d-1)/2 bivariate dependence
functions. We show that only d-1 of these bivariate functions are Levy copulas,
whereas the remaining functions are distributional copulas. Since there are no
restrictions concerning the choice of the copulas, the proposed pair
construction adds the desired flexibility to Levy copula models. We discuss
estimation and simulation in detail and apply the pair construction in a
simulation study.
"
1207.5781,2014-09-09,Confidence-based Optimization for the Newsvendor Problem,"  We introduce a novel strategy to address the issue of demand estimation in
single-item single-period stochastic inventory optimisation problems. Our
strategy analytically combines confidence interval analysis and inventory
optimisation. We assume that the decision maker is given a set of past demand
samples and we employ confidence interval analysis in order to identify a range
of candidate order quantities that, with prescribed confidence probability,
includes the real optimal order quantity for the underlying stochastic demand
process with unknown stationary parameter(s). In addition, for each candidate
order quantity that is identified, our approach can produce an upper and a
lower bound for the associated cost. We apply our novel approach to three
demand distribution in the exponential family: binomial, Poisson, and
exponential. For two of these distributions we also discuss the extension to
the case of unobserved lost sales. Numerical examples are presented in which we
show how our approach complements existing frequentist - e.g. based on maximum
likelihood estimators - or Bayesian strategies.
"
1208.1487,2016-11-24,Demmartingales and the functionnal Hill process for small parameters,"  Association of random variables and Demimartingales are recent fields for
handling asymptotic behaviors of sums of dependent random variables. We apply
their techniques to establish the asymptotic law of a demimartingale
  We next apply the results to find the asymptotic behavior the functional Hill
process for small parameters within the Extreme Value Theory (EVT) field. Such
a result would have been very hard to find whithout demimartingales techniques.
"
1208.2719,2012-08-15,"Unified Analysis of Transmit Antenna Selection/Space-Time Block Coding
  with Receive Selection and Combining over Nakagami-m Fading Channels in the
  Presence of Feedback Errors","  Examining the effect of imperfect transmit antenna selection (TAS) caused by
the feedback link errors on the performance of hybrid TAS/space-time block
coding (STBC) with selection combining (SC) (i.e., joint transmit and receive
antenna selection (TRAS)/STBC) and TAS/STBC (with receive maximal-ratio
combining (MRC)-like combining structure) over Nakagami-m fading channels is
the main objective of this paper. Under ideal channel estimation and delay-free
feedback assumptions, statistical expressions and several performance metrics
related to the post-processing signal-to-noise ratio (SNR) are derived for a
unified system model concerning both joint TRAS/STBC and TAS/STBC schemes.
Exact analytical expressions for outage probability and bit/symbol error rates
(BER/SER) of binary and M-ary modulations are presented in order to provide an
extensive examination on the capacity and error performance of the unified
system that experiences feedback errors. Also, the asymptotic diversity order
analysis, which shows that the diversity order of the investigated schemes is
lower bounded by the diversity order provided by STBC transmission itself, is
included in the paper. Moreover, all theoretical results are validated by
performing Monte Carlo simulations.
"
1208.3121,2014-06-24,The Multivariate $S_n$ Estimator,"  In this note we introduce the M$S_n$ estimator (for Multivariate $S_n$) a new
robust estimator of multivariate ranking. Like MVE and MCD it searches for an
$h$-subset which minimizes a criterion. The difference is that the new
criterion measures the degree of overlap between univariate projections of the
data. A primary advantage of this new criterion lies in its relative
independence from the configuration of the outliers. A second advantage is that
it easily lends itself to so-called ""symmetricizing"" transformations whereby
the observations only enter the objective function through their pairwise
differences: this makes our proposal well suited for models with an asymmetric
distribution. M$S_n$ is, therefore, more generally applicable than either MVE,
MCD or SDE. We also construct a fast algorithm for the M$S_n$ estimator, and
simulate its bias under various adversary configurations of outliers.
"
1208.5597,2012-08-29,"Biologists meet statisticians: A workshop for young scientists to foster
  interdisciplinary team work","  Life science and statistics have necessarily become essential partners. The
need to plan complex, structured experiments, involving elaborated designs, and
the need to analyse datasets in the era of systems biology and high throughput
technologies has to build upon professional statistical expertise. On the other
hand, conducting such analyses and also developing improved or new methods,
also for novel kinds of data, has to build upon solid biological understanding
and practise. However, the meeting of scientists of both fields is often
hampered by a variety of communicative hurdles - which are based on
field-specific working languages and cultural differences.
  As a step towards a better mutual understanding, we developed a workshop
concept bringing together young experimental biologists and statisticians, to
work as pairs and learn to value each others competences and practise
interdisciplinary communication in a casual atmosphere. The first
implementation of our concept was a cooperation of the German Region of the
International Biometrical Society and the Leibnitz Institute DSMZ-German
Collection of Microorganisms and Cell Cultures (short: DSMZ), Braunschweig,
Germany. We collected feedback in form of three questionnaires, oral comments,
and gathered experiences for the improvement of this concept. The long-term
challenge for both disciplines is the establishment of systematic schedules and
strategic partnerships which use the proposed workshop concept to foster mutual
understanding, to seed the necessary interdisciplinary cooperation network, and
to start training the indispensable communication skills at the earliest
possible phase of education.
"
1209.1350,2016-11-17,"Probability Distribution of the Quality Factor of a Mode-Stirred
  Reverberation Chamber","  We derive a probability distribution, confidence intervals and statistics of
the quality (Q) factor of an arbitrarily shaped mode-stirred reverberation
chamber, based on ensemble distributions of the idealized random cavity field
with assumed perfect stir efficiency. It is shown that Q exhibits a
Fisher-Snedecor F-distribution whose degrees of freedom are governed by the
number of simultaneously excited cavity modes per stir state. The most probable
value of Q is between a fraction 2/9 and 1 of its mean value, and between a
fraction 4/9 and 1 of its asymptotic (composite Q) value. The arithmetic mean
value is found to always exceed the values of all other theoretical metrics for
centrality of Q. For a rectangular cavity, we retrieve the known asymptotic Q
in the limit of highly overmoded regime.
"
1209.1371,2012-09-07,"On the age-, time- and migration dependent dynamics of diseases","  This paper generalizes a previously published differential equation that
describes the relation between the age-specific incidence, remission, and
mortality of a disease with its prevalence. The underlying model is a simple
compartment model with three states (illness-death model). In contrast to the
former work, migration- and calendar time-effects are included. As an
application of the theoretical findings, a hypothetical example of an
irreversible disease is treated.
"
1209.1670,2012-09-11,"Calculation of Some Expected Values for Parameterized Mean Model with
  Gaussian Noise","  This document derives several expected values related to the parameterized
mean model with Gaussian noise and their simplified forms.
"
1209.3215,2014-02-10,Cramer-Rao-Induced Bounds for CANDECOMP/PARAFAC tensor decomposition,"  This paper presents a Cramer-Rao lower bound (CRLB) on the variance of
unbiased estimates of factor matrices in Canonical Polyadic (CP) or
CANDECOMP/PARAFAC (CP) decompositions of a tensor from noisy observations,
(i.e., the tensor plus a random Gaussian i.i.d. tensor). A novel expression is
derived for a bound on the mean square angular error of factors along a
selected dimension of a tensor of an arbitrary dimension. The expression needs
less operations for computing the bound, O(NR^6), than the best existing
state-of-the art algorithm, O(N^3R^6) operations, where N and R are the tensor
order and the tensor rank. Insightful expressions are derived for tensors of
rank 1 and rank 2 of arbitrary dimension and for tensors of arbitrary dimension
and rank, where two factor matrices have orthogonal columns.
  The results can be used as a gauge of performance of different approximate CP
decomposition algorithms, prediction of their accuracy, and for checking
stability of a given decomposition of a tensor (condition whether the CRLB is
finite or not). A novel expression is derived for a Hessian matrix needed in
popular damped Gauss-Newton method for solving the CP decomposition of tensors
with missing elements. Beside computing the CRLB for these tensors the
expression may serve for design of damped Gauss-Newton algorithm for the
decomposition.
"
1209.4019,2018-01-31,Experimental design for Partially Observed Markov Decision Processes,"  This paper deals with the question of how to most effectively conduct
experiments in Partially Observed Markov Decision Processes so as to provide
data that is most informative about a parameter of interest. Methods from
Markov decision processes, especially dynamic programming, are introduced and
then used in an algorithm to maximize a relevant Fisher Information. The
algorithm is then applied to two POMDP examples. The methods developed can also
be applied to stochastic dynamical systems, by suitable discretization, and we
consequently show what control policies look like in the Morris-Lecar Neuron
model, and simulation results are presented. We discuss how parameter
dependence within these methods can be dealt with by the use of priors, and
develop tools to update control policies online. This is demonstrated in
another stochastic dynamical system describing growth dynamics of DNA template
in a PCR model.
"
1209.4065,2012-09-20,"On the Performance of Transmit Antenna Selection Based on Shadowing Side
  Information","  In this paper, a transmit antenna selection scheme, which is based on
shadowing side information, is investigated. In this scheme, the selected
single transmit antenna provides the highest shadowing coefficient between
transmitter and receiver. By the proposed technique, the frequency of the usage
of the feedback channel from the receiver to the transmitter and also channel
estimation complexity at the receiver can be reduced. We study the performance
of our proposed technique and in the analysis, we consider an independent but
not identically distributed Generalized-K composite fading model. More
specifically exact and closed-form expressions for the outage probability, the
moment generating function, the moments of signal-to-noise ratio, and the
average symbol error probability are derived. In addition, asymptotic outage
probability and symbol error probability expressions are also presented in
order to investigate the diversity order and the array gain. Finally, our
theoretical performance results are validated by Monte Carlo simulations.
"
1209.4340,2014-07-18,Moments and Absolute Moments of the Normal Distribution,"  We present formulas for the (raw and central) moments and absolute moments of
the normal distribution. We note that these results are not new, yet many
textbooks miss out on at least some of them. Hence, we believe that it is
worthwhile to collect these formulas and their derivations in these notes.
"
1209.4678,2012-09-24,On Control Charts for Monitoring the Variance of a Time Series,"  In this paper we derive control charts for the variance of a Gaussian process
using the likelihood ratio approach, the generalized likelihood ratio approach,
the sequential probability ratio method and a generalized sequential
probability ratio procedure, the Shiryaev-Roberts procedure and a generalized
Shiryaev-Roberts ap- proach. Recursive presentations for the calculation of the
control statistics are given for autoregressive processes of order 1. In an
extensive simulation study these schemes are compared with existing control
charts for the variance. In order to asses the performance of the schemes both
the average run length and the average delay are used.
"
1209.5272,2012-11-19,"Does the specification of uncertainty hurt the progress of
  scientometrics?","  In ""Caveats for using statistical significance tests in research
assessments,""--Journal of Informetrics 7(1)(2013) 50-62, available at
arXiv:1112.2516 -- Schneider (2013) focuses on Opthof & Leydesdorff (2010) as
an example of the misuse of statistics in the social sciences. However, our
conclusions are theoretical since they are not dependent on the use of one
statistics or another. We agree with Schneider insofar as he proposes to
develop further statistical instruments (such as effect sizes). Schneider
(2013), however, argues on meta-theoretical grounds against the specification
of uncertainty because, in his opinion, the presence of statistics would
legitimate decision-making. We disagree: uncertainty can also be used for
opening a debate. Scientometric results in which error bars are suppressed for
meta-theoretical reasons should not be trusted.
"
1210.0756,2013-02-15,"Stochastic dynamical model of a growing network based on self-exciting
  point process","  We perform experimental verification of the preferential attachment model
that is commonly accepted as a generating mechanism of the scale-free complex
networks. To this end we chose citation network of Physics papers and traced
citation history of 40,195 papers published in one year. Contrary to common
belief, we found that citation dynamics of the individual papers follows the
\emph{superlinear} preferential attachment, with the exponent $\alpha=
1.25-1.3$. Moreover, we showed that the citation process cannot be described as
a memoryless Markov chain since there is substantial correlation between the
present and recent citation rates of a paper. Basing on our findings we
constructed a stochastic growth model of the citation network, performed
numerical simulations based on this model and achieved an excellent agreement
with the measured citation distributions.
"
1210.0962,2013-05-23,Breaking Monotony with Meaning: Motivation in Crowdsourcing Markets,"  We conduct the first natural field experiment to explore the relationship
between the ""meaningfulness"" of a task and worker effort. We employed about
2,500 workers from Amazon's Mechanical Turk (MTurk), an online labor market, to
label medical images. Although given an identical task, we experimentally
manipulated how the task was framed. Subjects in the meaningful treatment were
told that they were labeling tumor cells in order to assist medical
researchers, subjects in the zero-context condition (the control group) were
not told the purpose of the task, and, in stark contrast, subjects in the
shredded treatment were not given context and were additionally told that their
work would be discarded. We found that when a task was framed more
meaningfully, workers were more likely to participate. We also found that the
meaningful treatment increased the quantity of output (with an insignificant
change in quality) while the shredded treatment decreased the quality of output
(with no change in quantity). We believe these results will generalize to other
short-term labor markets. Our study also discusses MTurk as an exciting
platform for running natural field experiments in economics.
"
1210.1773,2012-10-08,"Efficient Forward Simulation of Fisher-Wright Populations with
  Stochastic Population Size and Neutral Single Step Mutations in Haplotypes","  In both population genetics and forensic genetics it is important to know how
haplotypes are distributed in a population. Simulation of population dynamics
helps facilitating research on the distribution of haplotypes. In forensic
genetics, the haplotypes can for example consist of lineage markers such as
short tandem repeat loci on the Y chromosome (Y-STR). A dominating model for
describing population dynamics is the simple, yet powerful, Fisher-Wright
model. We describe an efficient algorithm for exact forward simulation of exact
Fisher-Wright populations (and not approximative such as the coalescent model).
The efficiency comes from convenient data structures by changing the
traditional view from individuals to haplotypes. The algorithm is implemented
in the open-source R package 'fwsim' and is able to simulate very large
populations. We focus on a haploid model and assume stochastic population size
with flexible growth specification, no selection, a neutral single step
mutation process, and self-reproducing individuals. These assumptions make the
algorithm ideal for studying lineage markers such as Y-STR.
"
1210.7225,2012-10-29,The anti-Bayesian moment and its passing,"  The present article is the reply to the discussion of our earlier ""Not only
defended but also applied"" (arXiv:1006.5366, to appear in The American
Statistician) that arose from our memory of a particularly intemperate
anti-Bayesian statement in Feller's beautiful and classic book on probability
theory. We felt that it was worth exploring the very extremeness of Feller's
words, along with similar anti-Bayesian remarks by others, in order to better
understand the background underlying controversies that still exist regarding
the foundations of statistics. We thank the four discussants of our article for
their contributions to our understanding of these controversies as they have
existed in the past and persist today.
"
1211.1183,2014-04-16,KernSmoothIRT: An R Package for Kernel Smoothing in Item Response Theory,"  Item response theory (IRT) models are a class of statistical models used to
describe the response behaviors of individuals to a set of items having a
certain number of options. They are adopted by researchers in social science,
particularly in the analysis of performance or attitudinal data, in psychology,
education, medicine, marketing and other fields where the aim is to measure
latent constructs. Most IRT analyses use parametric models that rely on
assumptions that often are not satisfied. In such cases, a nonparametric
approach might be preferable; nevertheless, there are not many software
applications allowing to use that. To address this gap, this paper presents the
R package KernSmoothIRT. It implements kernel smoothing for the estimation of
option characteristic curves, and adds several plotting and analytical tools to
evaluate the whole test/questionnaire, the items, and the subjects. In order to
show the package's capabilities, two real datasets are used, one employing
multiple-choice responses, and the other scaled responses.
"
1211.1184,2012-11-07,"DBKGrad: An R Package for Mortality Rates Graduation by Fixed and
  Adaptive Discrete Beta Kernel Techniques","  Kernel smoothing represents a useful approach in the graduation of mortality
rates. Though there exist several options for performing kernel smoothing in
statistical software packages, there have been very few contributions to date
that have focused on applications of these techniques in the graduation
context. Also, although it has been shown that the use of a variable or
adaptive smoothing parameter, based on the further information provided by the
exposed to the risk of death, provides additional benefits, specific
computational tools for this approach are essentially absent. Furthermore,
little attention has been given to providing methods in available software for
any kind of subsequent analysis with respect to the graduated mortality rates.
To facilitate analyses in the field, the R package DBKGrad is introduced. Among
the available kernel approaches, it considers a recent discrete beta kernel
estimator, in both its fixed and adaptive variants. In this approach, boundary
bias is automatically reduced and age is pragmatically considered as a discrete
variable. The bandwidth, fixed or adaptive, is allowed to be manually given by
the user or selected by cross-validation. Pointwise confidence intervals, for
each considered age, are also provided. An application to mortality rates from
the Sicily Region (Italy) for the year 2008 is also presented to exemplify the
use of the package.
"
1211.7008,2012-11-30,Benford's law: A theoretical explanation for base 2,"  In this paper, we present a possible theoretical explanation for benford's
law. We develop a recursive relation between the probabilities, using simple
intuitive ideas. We first use numerical solutions of this recursion and verify
that the solutions converge to the benford's law. Finally we solve the
recursion analytically to yeild the benford's law for base 2.
"
1212.6339,2024-11-27,A paradox on the spectral representation of stationary random processes,"  In this note our aim is to show a paradox in the spectral representation of
stationary random processes.
"
1301.0765,2013-01-15,Perceptive Statistical Variability Indicators,"  The concepts of variability and uncertainty, both epistemic and alleatory,
came from experience and coexist with different connotations. Therefore this
article attempts to express their relation by analytic means firstly setting
sights on their differences and then on their common characteristics. Inspired
with the definition of average number of equally probable events based on
entropy concept in probability theory, the article introduced two related
perceptive statistical measures which indicate the same variability as the
basic probability distribution. First is the equivalent number of a
hypothetical distribution with one sure and all the other impossible outcomes
which indicates variability. Second is the appropriate equivalent number of a
hypothetical distribution with all equal probabilities which indicates
invariability. The article interprets the common properties of variability and
uncertainty on theoretical distributions and on ocean-wide wind wave
directional properties by using the long term observations compiled in the
Global Wave Statistics.
"
1301.1277,2016-08-19,The generalized lognormal distribution and the Stieltjes moment problem,"  This paper studies a Stieltjes-type moment problem defined by the generalized
lognormal distribution, a heavy-tailed distribution with applications in
economics, finance and related fields. It arises as the distribution of the
exponential of a random variable following a generalized error distribution,
and hence figures prominently in the EGARCH model of asset price volatility.
Compared to the classical lognormal distribution it has an additional shape
parameter. It emerges that moment (in)determinacy depends on the value of this
parameter: for some values, the distribution does not have finite moments of
all orders, hence the moment problem is not of interest in these cases. For
other values, the distribution has moments of all orders, yet it is
moment-indeterminate. Finally, a limiting case is supported on a bounded
interval, and hence determined by its moments. For those generalized lognormal
distributions that are moment-indeterminate Stieltjes classes of
moment-equivalent distributions are presented.
"
1301.3748,2013-05-13,Deep Impact: Unintended consequences of journal rank,"  Most researchers acknowledge an intrinsic hierarchy in the scholarly journals
('journal rank') that they submit their work to, and adjust not only their
submission but also their reading strategies accordingly. On the other hand,
much has been written about the negative effects of institutionalizing journal
rank as an impact measure. So far, contributions to the debate concerning the
limitations of journal rank as a scientific impact assessment tool have either
lacked data, or relied on only a few studies. In this review, we present the
most recent and pertinent data on the consequences of our current scholarly
communication system with respect to various measures of scientific quality
(such as utility/citations, methodological soundness, expert ratings or
retractions). These data corroborate previous hypotheses: using journal rank as
an assessment tool is bad scientific practice. Moreover, the data lead us to
argue that any journal rank (not only the currently-favored Impact Factor)
would have this negative impact. Therefore, we suggest that abandoning journals
altogether, in favor of a library-based scholarly communication system, will
ultimately be necessary. This new system will use modern information technology
to vastly improve the filter, sort and discovery functions of the current
journal system.
"
1301.4628,2014-04-24,"Intrinsic posterior regret gamma-minimax estimation for the exponential
  family of distributions","  In practice, it is desired to have estimates that are invariant under
reparameterization. The invariance property of the estimators helps to
formulate a unified solution to the underlying estimation problem. In robust
Bayesian analysis, a frequent criticism is that the optimal estimators are not
invariant under smooth reparameterizations. This paper considers the problem of
posterior regret gamma-minimax (PRGM) estimation of the natural parameter of
the exponential family of distributions under intrinsic loss functions. We show
that under the class of Jeffrey's Conjugate Prior (JCP) distributions, PRGM
estimators are invariant to smooth one-to-one reparameterizations. We apply our
results to several distributions and different classes of JCP, as well as the
usual conjugate prior distributions. We observe that, in many cases, invariant
PRGM estimators in the class of JCP distributions can be obtained by some
modifications of PRGM estimators in the usual class of conjugate priors.
  Moreover, when the class of priors are convex or dependant on a
hyper-parameter belonging to a connected set, we show that the PRGM estimator
under the intrinsic loss function could be Bayes with respect to a prior
distribution in the original prior class. Theoretical results are supplemented
with several examples and illustrations.
"
1301.7006,2014-02-26,"A Unified Community Detection, Visualization and Analysis method","  Community detection in social graphs has attracted researchers' interest for
a long time. With the widespread of social networks on the Internet it has
recently become an important research domain. Most contributions focus upon the
definition of algorithms for optimizing the so-called modularity function. In
the first place interest was limited to unipartite graph inputs and partitioned
community outputs. Recently bipartite graphs, directed graphs and overlapping
communities have been investigated. Few contributions embrace at the same time
the three types of nodes. In this paper we present a method which unifies
commmunity detection for the three types of nodes and at the same time merges
partitionned and overlapping communities. Moreover results are visualized in
such a way that they can be analyzed and semantically interpreted. For
validation we experiment this method on well known simple benchmarks. It is
then applied to real data in three cases. In two examples of photos sets with
tagged people we reveal social networks. A second type of application is of
particularly interest. After applying our method to Human Brain Tractography
Data provided by a team of neurologists, we produce clusters of white fibers in
accordance with other well known clustering methods. Moreover our approach for
visualizing overlapping clusters allows better understanding of the results by
the neurologist team. These last results open up the possibility of applying
community detection methods in other domains such as data analysis with
original enhanced performances.
"
1302.2525,2019-09-02,Foundations of Descriptive and Inferential Statistics,"  These lecture notes were written with the aim to provide an accessible though
technically solid introduction to the logic of systematical analyses of
statistical data to both undergraduate and postgraduate students, in particular
in the Social Sciences, Economics, and the Financial Services. They may also
serve as a general reference for the application of quantitative--empirical
research methods. In an attempt to encourage the adoption of an
interdisciplinary perspective on quantitative problems arising in practice, the
notes cover the four broad topics (i) descriptive statistical processing of raw
data, (ii) elementary probability theory, (iii) the operationalisation of
one-dimensional latent statistical variables according to Likert's widely used
scaling approach, and (iv) null hypothesis significance testing within the
frequentist approach to probability theory concerning (a) distributional
differences of variables between subgroups of a target population, and (b)
statistical associations between two variables. The relevance of effect sizes
for making inferences is emphasised. These lecture notes are fully hyperlinked,
thus providing a direct route to original scientific papers as well as to
interesting biographical information. They also list many commands for running
statistical functions and data analysis routines in the software packages R,
SPSS, EXCEL and OpenOffice. The immediate involvement in actual data analysis
practices is strongly recommended.
"
1303.3518,2017-04-04,"Propagation of initial errors on the parameters for linear and Gaussian
  state space models","  For linear and Gaussian state space models parametrized by $\theta_0 \in
\Theta \subset \mathbb{R}^r, r \geq 1$ corresponding to the vector of
parameters of the model, the Kalman filter gives exactly the solution for the
optimal filtering under weak assumptions. This result supposes that $\theta_0$
is perfectly known. In most real applications, this assumption is not realistic
since $\theta_0$ is unknown and has to be estimated. In this paper, we analysis
the Kalman filter for a biased estimator of $\theta_0$. We show the propagation
of this bias on the estimation of the hidden state. We give an expression of
this propagation for linear and Gaussian state space models and we extend this
result for almost linear models estimated by the Extended Kalman filter. An
illustration is given for the autoregressive process with measurement noises
widely studied in econometrics to model economic and financial data.
"
1304.0718,2013-04-03,A Peer-based Model of Fat-tailed Outcomes,"  It is well known that the distribution of returns from various financial
instruments are leptokurtic, meaning that the distributions have ""fatter tails""
than a Normal distribution, and have skew toward zero. This paper presents a
graceful micro-level explanation for such fat-tailed outcomes, using agents
whose private valuations have Normally-distributed errors, but whose utility
function includes a term for the percentage of others who also buy.
"
1304.6617,2013-04-25,EM-based Semi-blind Channel Estimation in AF Two-Way Relay Networks,"  We propose an expectation maximization (EM)-based algorithm for semi-blind
channel estimation of reciprocal channels in amplify-and-forward (AF) two-way
relay networks (TWRNs). By incorporating both data samples and pilots into the
estimation, the proposed algorithm provides substantially higher accuracy than
the conventional training-based approach. Furthermore, the proposed algorithm
has a linear computational complexity per iteration and converges after a small
number of iterations.
"
1304.7920,2013-05-01,"From Ordinary Differential Equations to Structural Causal Models: the
  deterministic case","  We show how, and under which conditions, the equilibrium states of a
first-order Ordinary Differential Equation (ODE) system can be described with a
deterministic Structural Causal Model (SCM). Our exposition sheds more light on
the concept of causality as expressed within the framework of Structural Causal
Models, especially for cyclic models.
"
1305.1232,2013-05-07,"Bayesian Modeling and MCMC Computation in Linear Logistic Regression for
  Presence-only Data","  Presence-only data are referred to situations in which, given a censoring
mechanism, a binary response can be observed only with respect to on outcome,
usually called \textit{presence}. In this work we present a Bayesian approach
to the problem of presence-only data based on a two levels scheme. A
probability law and a case-control design are combined to handle the double
source of uncertainty: one due to the censoring and one due to the sampling. We
propose a new formalization for the logistic model with presence-only data that
allows further insight into inferential issues related to the model. We
concentrate on the case of the linear logistic regression and, in order to make
inference on the parameters of interest, we present a Markov Chain Monte Carlo
algorithm with data augmentation that does not require the a priori knowledge
of the population prevalence. A simulation study concerning 24,000 simulated
datasets related to different scenarios is presented comparing our proposal to
optimal benchmarks.
"
1305.6995,2013-05-31,Review of: Mixed Effects Models and Extensions in Ecology with R,"  This is a review of the book ""Mixed Effects Models and Extensions in Ecology
with R"" by Zuur, Ieno, Walker, Saveliev and Smith (2009, Springer). I was asked
to review this book for The American Statistician in 2010. After I wrote the
review, the invitation was revoked. This is the review.
"
1306.0364,2016-08-19,On moment indeterminacy of the Benini income distribution,"  The Benini distribution is a lognormal-like distribution generalizing the
Pareto distribution. Like the Pareto and the lognormal distributions it was
originally proposed for modeling economic size distributions, notably the size
distribution of personal income. This paper explores a probabilistic property
of the Benini distribution, showing that it is not determined by the sequence
of its moments although all the moments are finite. It also provides explicit
examples of distributions possessing the same set of moments. Related
distributions are briefly explored.
"
1306.3395,2013-06-17,Evolutionary Model of a Anonymous Consumer Durable Market,"  An analytic model is presented that considers the evolution of a market of
durable goods. The model suggests that after introduction goods spread always
according to a Bass diffusion. However, this phase will be followed by a
diffusion process for durable consumer goods governed by a
variation-selection-reproduction mechanism and the growth dynamics can be
described by a replicator equation. Describing the aggregate sales as the sum
of first, multiple and replacement purchase the product life cycle can be
derived. Replacement purchase causes periodic variations of the sales
determined by the finite lifetime of the good (Juglar cycles). The model
suggests that both, Bass- and Gompertz diffusion may contribute to the product
life cycle of a consumer durable. The theory contains the standard equilibrium
view of a market as a special case. It depends on the time scale, whether an
equilibrium or evolutionary description is more appropriate. The evolutionary
framework is used to derive also the size, growth rate and price distribution
of manufacturing business units. It predicts that the size distribution of the
business units (products) is lognormal, while the growth rates exhibit a
Laplace distribution. Large price deviations from the mean price are also
governed by a Laplace distribution (fat tails). These results are in agreement
with empirical findings. The explicit comparison of the time evolution of
consumer durables with empirical investigations confirms the close relationship
between price decline and Gompertz diffusion, while the product life cycle can
be described qualitatively for a long time period.
"
1307.4101,2013-09-27,"Decision Making for Inconsistent Expert Judgments Using Negative
  Probabilities","  In this paper we provide a simple random-variable example of inconsistent
information, and analyze it using three different approaches: Bayesian,
quantum-like, and negative probabilities. We then show that, at least for this
particular example, both the Bayesian and the quantum-like approaches have less
normative power than the negative probabilities one.
"
1307.6307,2013-07-25,Is there currently a scientific revolution in scientometrics?,"  The author of this letter to the editor would like to set forth the argument
that scientometrics is currently in a phase in which a taxonomic change, and
hence a revolution, is taking place. One of the key terms in scientometrics is
scientific impact which nowadays is understood to mean not only the impact on
science but the impact on every area of society.
"
1307.7498,2013-07-30,The normalization of citation counts based on classification systems,"  If we want to assess whether the paper in question has had a particularly
high or low citation impact compared to other papers, the standard practice in
bibliometrics is to normalize citations in respect of the subject category and
publication year. A number of proposals for an improved procedure in the
normalization of citation impact have been put forward in recent years. Against
the background of these proposals this study describes an ideal solution for
the normalization of citation impact: in a first step, the reference set for
the publication in question is collated by means of a classification scheme,
where every publication is associated with a single principal research field or
subfield entry (e. g. via Chemical Abstracts sections) and a publication year.
In a second step, percentiles of citation counts are calculated for this set
and used to assign the normalized citation impact score to the publications
(and also to the publication in question).
"
1308.1023,2013-09-17,On the asymptotics of Ajtai-Koml\'os-Tusn\'ady statistics,"  In our days there is a widespread analysis of Wasserstein distances between
theoretical and empirical measures. One of the first investigation of the topic
is given in the paper written by Ajtai, Koml\'os and Tusn\'ady in $1984.$
  Interestingly, all the neighboring questions posed by that paper were settled
already without the original one. In this paper we are going to delineate the
limit behavior of the original statistics with the help of computer
simulations. At the same time we kept an eye on theoretical grasping of the
problem. Based on our computer simulations our opinion is that the limit
distribution is Gaussian.
"
1308.4178,2015-12-18,"The factor paradox: Common factors can be correlated with the variance
  not accounted for by the common factors!","  The case that the factor model does not account for all the covariances of
the observed variables is considered. This is a quite realistic condition
because some model error as well as some sampling error should usually occur
with empirical data. It is shown that principal components representing
covariances not accounted for by the factors of the model can have a non-zero
correlation with the common factors of the factor model. Non-zero correlations
of components representing variance not accounted for by the factor model with
common factors were also found in a simulation study. Based on these results it
should be concluded that common factors can be correlated with variance
components representing model error as well as sampling error. In consequence,
even when researchers decide not to represent some small or trivial variance by
means of a common factor, these excluded variances can still be part of the
model.
"
1309.7445,2020-07-21,"I hear, I forget. I do, I understand: a modified Moore-method
  mathematical statistics course","  Moore introduced a method for graduate mathematics instruction that consisted
primarily of individual student work on challenging proofs (Jones, 1977). Cohen
(1982) described an adaptation with less explicit competition suitable for
undergraduate students at a liberal arts college. This paper details an
adaptation of this modified Moore-method to teach mathematical statistics, and
describes ways that such an approach helps engage students and foster the
teaching of statistics.
  Groups of students worked a set of 3 difficult problems (some theoretical,
some applied) every two weeks. Class time was devoted to coaching sessions with
the instructor, group meeting time, and class presentations. R was used to
estimate solutions empirically where analytic results were intractable, as well
as to provide an environment to undertake simulation studies with the aim of
deepening understanding and complementing analytic solutions. Each group
presented comprehensive solutions to complement oral presentations. Development
of parallel techniques for empirical and analytic problem solving was an
explicit goal of the course, which also attempted to communicate ways that
statistics can be used to tackle interesting problems. The group problem
solving component and use of technology allowed students to attempt much more
challenging questions than they could otherwise solve.
"
1310.2442,2013-10-10,A Conversation with Stephen E. Fienberg,"  The following conversation is based in part on a transcript of a 2009
interview funded by Pfizer Global Research-Connecticut, the American
Statistical Association and the Department of Statistics at the University of
Connecticut-Storrs as part of the ""Conversations with Distinguished
Statisticians in Memory of Professor Harry O. Posten"".
"
1310.7141,2013-10-29,"Meeting Student Needs for Multivariate Data Analysis: A Case Study in
  Teaching a Multivariate Data Analysis Course with No Pre-requisites","  Modern students encounter big, messy data sets long before setting foot in
our classrooms. Many of our students need to develop skills in exploratory data
analysis and multivariate analysis techniques for their jobs after college, but
these topics are not covered in introductory statistics courses. This case
study describes my experience in designing and teaching a course on
multivariate data analysis with no pre-requisites, using real data, active
learning, and other activities to help students tackle the material.
"
1310.7163,2013-10-29,Generalized Thompson Sampling for Contextual Bandits,"  Thompson Sampling, one of the oldest heuristics for solving multi-armed
bandits, has recently been shown to demonstrate state-of-the-art performance.
The empirical success has led to great interests in theoretical understanding
of this heuristic. In this paper, we approach this problem in a way very
different from existing efforts. In particular, motivated by the connection
between Thompson Sampling and exponentiated updates, we propose a new family of
algorithms called Generalized Thompson Sampling in the expert-learning
framework, which includes Thompson Sampling as a special case. Similar to most
expert-learning algorithms, Generalized Thompson Sampling uses a loss function
to adjust the experts' weights. General regret bounds are derived, which are
also instantiated to two important loss functions: square loss and logarithmic
loss. In contrast to existing bounds, our results apply to quite general
contextual bandits. More importantly, they quantify the effect of the ""prior""
distribution on the regret bounds.
"
1311.5939,2013-11-26,Hypergeometric tail inequalities: ending the insanity,"  The hypergeometric distribution is briefly and informally surveyed, including
popular notation, symmetries, and the tail inequalities $Pr[i \ge E[i]+tn] \le
e^{-2t^2n}$ and $Pr[i \le E[i]-tn] \le e^{-2t^2n}$.
"
1311.6257,2013-11-26,"Filters and smoothers for self-exciting Markov modulated counting
  processes","  We consider a self-exciting counting process, the parameters of which depend
on a hidden finite-state Markov chain. We derive the optimal filter and
smoother for the hidden chain based on observation of the jump process. This
filter is in closed form and is finite dimensional. We demonstrate the
performance of this filter both with simulated data, and by analysing the
`flash crash' of 6th May 2010 in this framework.
"
1312.1670,2013-12-06,An agent-based epidemiological model of incarceration,"  We build an agent-based model of incarceration based on the SIS model of
infectious disease propagation. Our central hypothesis is that the observed
racial disparities in incarceration rates between Black and White Americans can
be explained as the result of differential sentencing between the two
demographic groups. We demonstrate that if incarceration can be spread through
a social influence network, then even relatively small differences in
sentencing can result in the large disparities in incarceration rates.
Controlling for effects of transmissibility, susceptibility, and influence
network structure, our model reproduces the observed large disparities in
incarceration rates given the differences in sentence lengths for White and
Black drug offenders in the United States without extensive parameter tuning.
We further establish the suitability of the SIS model as applied to
incarceration, as the observed structural patterns of recidivism are an
emergent property of the model. In fact, our model shows a remarkably close
correspondence with California incarceration data, without requiring any
parameter tuning. This work advances efforts to combine the theories and
methods of epidemiology and criminology.
"
1312.2211,2014-03-18,Index Distribution of Cauchy Random Matrices,"  Using a Coulomb gas technique, we compute analytically the probability
$\mathcal{P}_\beta^{(C)}(N_+,N)$ that a large $N\times N$ Cauchy random matrix
has $N_+$ positive eigenvalues, where $N_+$ is called the index of the
ensemble. We show that this probability scales for large $N$ as
$\mathcal{P}_\beta^{(C)}(N_+,N)\approx \exp\left[-\beta N^2
\psi_C(N_+/N)\right]$, where $\beta$ is the Dyson index of the ensemble. The
rate function $\psi_C(\kappa)$ is computed in terms of single integrals that
are easily evaluated numerically and amenable to an asymptotic analysis. We
find that the rate function, around its minimum at $\kappa=1/2$, has a
quadratic behavior modulated by a logarithmic singularity. As a consequence,
the variance of the index scales for large $N$ as $\mathrm{Var}(N_+)\sim
\sigma_C\ln N$, where $\sigma_C=2/(\beta\pi^2)$ is twice as large as the
corresponding prefactor in the Gaussian and Wishart cases. The analytical
results are checked by numerical simulations and against an exact finite $N$
formula which, for $\beta=2$, can be derived using orthogonal polynomials.
"
1312.2248,2013-12-10,"Basic univariate and bivariate statistics for symbolic data: a critical
  review","  Some proofs of the problems of the basic statistics proposed for numeric
symbolic data.
"
1312.4731,2013-12-27,Statistical inference for exponential functionals of L\'evy processes,"  In this paper, we consider the exponential functional
\(A_{\infty}=\int_0^\infty e^{-\xi_s}ds\) of a L{\'e}vy process \(\xi_s\) and
aim to estimate the characteristics of \(\xi_{s}\) from the distribution of
\(A_{\infty}\). We present a new approach, which allows to statistically infer
on the L{\'e}vy triplet of \(\xi_{t}\), and study the theoretical properties of
the proposed estimators. The suggested algorithms are illustrated with
numerical simulations.
"
1312.5901,2014-03-25,"Trajectory composition of Poisson time changes and Markov counting
  systems","  Changing time of simple continuous-time Markov counting processes by
independent unit-rate Poisson processes results in Markov counting processes
for which we provide closed-form transition rates via composition of
trajectories and with which we construct novel, simpler infinitesimally
over-dispersed processes.
"
1312.5903,2013-12-23,Co-jumps and Markov counting systems in random environments,"  We provide transition rates for Markov counting systems subject to correlated
environmental noises motivated by multi-strain disease models. Such noises
induce simultaneous counts, which can help model infinitesimal count
correlation (regardless of whether such correlation is due to correlated
noises).
"
1312.7851,2017-01-06,Effective Degrees of Freedom: A Flawed Metaphor,"  To most applied statisticians, a fitting procedure's degrees of freedom is
synonymous with its model complexity, or its capacity for overfitting to data.
In particular, it is often used to parameterize the bias-variance tradeoff in
model selection. We argue that, contrary to folk intuition, model complexity
and degrees of freedom are not synonymous and may correspond very poorly. We
exhibit and theoretically explore various examples of fitting procedures for
which degrees of freedom is not monotonic in the model complexity parameter,
and can exceed the total dimension of the response space. Even in very simple
settings, the degrees of freedom can exceed the dimension of the ambient space
by an arbitrarily large amount. We show the degrees of freedom for any
non-convex projection method can be unbounded.
"
1401.3269,2014-01-15,"Teaching precursors to data science in introductory and second courses
  in statistics","  Statistics students need to develop the capacity to make sense of the
staggering amount of information collected in our increasingly data-centered
world. Data science is an important part of modern statistics, but our
introductory and second statistics courses often neglect this fact. This paper
discusses ways to provide a practical foundation for students to learn to
""compute with data"" as defined by Nolan and Temple Lang (2010), as well as
develop ""data habits of mind"" (Finzer, 2013). We describe how introductory and
second courses can integrate two key precursors to data science: the use of
reproducible analysis tools and access to large databases. By introducing
students to commonplace tools for data management, visualization, and
reproducible analysis in data science and applying these to real-world
scenarios, we prepare them to think statistically in the era of big data.
"
1401.4787,2015-08-18,On the Measurement of Economic Tail Risk,"  This paper attempts to provide a decision-theoretic foundation for the
measurement of economic tail risk, which is not only closely related to utility
theory but also relevant to statistical model uncertainty. The main result is
that the only risk measures that satisfy a set of economic axioms for the
Choquet expected utility and the statistical property of elicitability (i.e.
there exists an objective function such that minimizing the expected objective
function yields the risk measure) are the mean functional and the median
shortfall, which is the median of tail loss distribution. Elicitability is
important for backtesting. We also extend the result to address model
uncertainty by incorporating multiple scenarios. As an application, we argue
that median shortfall is a better alternative than expected shortfall for
setting capital requirements in Basel Accords.
"
1401.6849,2020-01-30,"A New Approach to Inference in Multi-Survey Studies with Unknown
  Population Size","  We investigate a Poisson sampling design in the presence of unknown selection
probabilities when applied to a population of unknown size for multiple
sampling occasions. The fixed-population model is adopted and extended upon for
inference. The complete minimal sufficient statistic is derived for the
sampling model parameters and fixed-population parameter vector. The
Rao-Blackwell version of population quantity estimators is detailed. An
application is applied to an emprical population. The extended inferential
framework is found to have much potential and utility for empirical studies.
"
1401.7474,2014-01-30,The phenotypic expansion and its boundaries,"  The development of sport performances in the future is a subject of myth and
disagreement among experts. As arguments favoring and opposing such methodology
were discussed, other publications empirically showed that the past development
of performances followed a non linear trend. Other works, while deeply
exploring the conditions leading to world records, highlighted that performance
is tied to the economical and geopolitical context. Here we investigated the
following human boundaries: development of performances with time in Olympic
and non-Olympic events, development of sport performances with aging among
humans and others species (greyhounds, thoroughbreds, mice). Development of
performances from a broader point of view (demography & lifespan) in a specific
sub-system centered on primary energy was also investigated. We show that the
physiological developments are limited with time. Three major and direct
determinants of sport performance are age, technology and climatic conditions
(temperature). However, all observed developments are related to the
international context including the efficient use of primary energies. This
last parameter is a major indirect propeller of performance development. We
show that when physiological and societal performance indicators such as
lifespan and population density depend on primary energies, the energy source,
competition and mobility are key parameters for achieving long term sustainable
trajectories. Otherwise, the vast majority (98.7%) of the studied trajectories
reaches 0 before 15 generations, due to the consumption of fossil energy and a
low mobility rate. This led us to consider that in the present turbulent
economical context and given the upcoming energy crisis, societal and physical
performances are not expected to grow continuously.
"
1401.8115,2014-02-03,Inhomogeneous K-function for germ-grain models,"  In this paper, we propose a generalization to germ-grain models of the
inhomogeneous K-function of Point Processes. We apply them to a sample of
images of peripheral blood smears obtained from patients with Sickle Cell
Disease, in order to decide whether the sample belongs to the thin, thick or
morphological region.
"
1402.0947,2014-04-23,On Renyi entropy convergence of the max domain of attraction,"  In this paper, we prove that the Renyi entropy of linearly normalized partial
maxima of independent and identically distributed random variables is
convergent to the corresponding limit Renyi entropy when the linearly
normalized partial maxima converges to some nondegenerate random variable.
"
1402.1089,2014-02-06,"Null hypothesis significance tests: A mix-up of two different theories,
  the basis for widespread confusion and numerous misinterpretations","  Null hypothesis statistical significance tests (NHST) are widely used in
quantitative research in the empirical sciences including scientometrics.
Nevertheless, since their introduction nearly a century ago significance tests
have been controversial. Many researchers are not aware of the numerous
criticisms raised against NHST. As practiced, NHST has been characterized as a
null ritual that is overused and too often misapplied and misinterpreted. NHST
is in fact a patchwork of two fundamentally different classical statistical
testing models, often blended with some wishful quasi-Bayesian interpretations.
This is undoubtedly a major reason why NHST is very often misunderstood. But
NHST also has intrinsic logical problems and the epistemic range of the
information provided by such tests is much more limited than most researchers
recognize. In this article we introduce to the scientometric community the
theoretical origins of NHST, which is mostly absent from standard statistical
textbooks, and we discuss some of the most prevalent problems relating to the
practice of NHST and trace these problems back to the mixup of the two
different theoretical origins. Finally, we illustrate some of the
misunderstandings with examples from the scientometric literature and bring
forward some modest recommendations for a more sound practice in quantitative
data analysis.
"
1402.1673,2016-07-04,Non-Orthogonal Tensor Diagonalization,"  Tensor diagonalization means transforming a given tensor to an exactly or
nearly diagonal form through multiplying the tensor by non-orthogonal
invertible matrices along selected dimensions of the tensor. It is
generalization of approximate joint diagonalization (AJD) of a set of matrices.
In particular, we derive (1) a new algorithm for symmetric AJD, which is called
two-sided symmetric diagonalization of order-three tensor, (2) a similar
algorithm for non-symmetric AJD, also called general two-sided diagonalization
of an order-3 tensor, and (3) an algorithm for three-sided diagonalization of
order-3 or order-4 tensors. The latter two algorithms may serve for canonical
polyadic (CP) tensor decomposition, and they can outperform other CP tensor
decomposition methods in terms of computational speed under the restriction
that the tensor rank does not exceed the tensor multilinear rank. Finally, we
propose (4) similar algorithms for tensor block diagonalization, which is
related to the tensor block-term decomposition.
"
1402.1894,2023-08-29,"R Markdown: Integrating A Reproducible Analysis Tool into Introductory
  Statistics","  Nolan and Temple Lang argue that ""the ability to express statistical
computations is an essential skill."" A key related capacity is the ability to
conduct and present data analysis in a way that another person can understand
and replicate. The copy-and-paste workflow that is an artifact of antiquated
user-interface design makes reproducibility of statistical analysis more
difficult, especially as data become increasingly complex and statistical
methods become increasingly sophisticated. R Markdown is a new technology that
makes creating fully-reproducible statistical analysis simple and painless. It
provides a solution suitable not only for cutting edge research, but also for
use in an introductory statistics course. We present evidence that R Markdown
can be used effectively in introductory statistics courses, and discuss its
role in the rapidly-changing world of statistical computation.
"
1402.2220,2014-02-11,"Donald Arthur Preece: A life in statistics, mathematics and music","  Biography and publications list for Donald Arthur Preece, who died on 6
January 2014, who made many contributions in statistics (experimental design)
and in combinatorics.
"
1402.6791,2014-03-18,A method for comparing chess openings,"  A quantitative method is described for comparing chess openings. Test
openings and baseline openings are run through chess engines under controlled
conditions and compared to evaluate the effectiveness of the test openings. The
results are intuitively appealing and in some cases they agree with expert
opinion. The specific contribution of this work is the development of an
objective measure that may be used for the evaluation and refutation of chess
openings, a process that had been left to thought experiments and subjective
conjectures and thereby to a large variety of opinion and a great deal of
debate.
"
1403.1185,2015-06-19,"Phase transitions in the condition number distribution of Gaussian
  random matrices","  We study the statistics of the condition number
$\kappa=\lambda_{\mathrm{max}}/\lambda_{\mathrm{min}}$ (the ratio between
largest and smallest squared singular values) of $N\times M$ Gaussian random
matrices. Using a Coulomb fluid technique, we derive analytically and for large
$N$ the cumulative $\mathcal{P}[\kappa<x]$ and tail-cumulative
$\mathcal{P}[\kappa>x]$ distributions of $\kappa$. We find that these
distributions decay as $\mathcal{P}[\kappa<x]\approx\exp\left(-\beta N^2
\Phi_{-}(x)\right)$ and $\mathcal{P}[\kappa>x]\approx\exp\left(-\beta N
\Phi_{+}(x)\right)$, where $\beta$ is the Dyson index of the ensemble. The left
and right rate functions $\Phi_{\pm}(x)$ are independent of $\beta$ and
calculated exactly for any choice of the rectangularity parameter
$\alpha=M/N-1>0$. Interestingly, they show a weak non-analytic behavior at
their minimum $\langle\kappa\rangle$ (corresponding to the average condition
number), a direct consequence of a phase transition in the associated Coulomb
fluid problem. Matching the behavior of the rate functions around
$\langle\kappa\rangle$, we determine exactly the scale of typical fluctuations
$\sim\mathcal{O}(N^{-2/3})$ and the tails of the limiting distribution of
$\kappa$. The analytical results are in excellent agreement with numerical
simulations.
"
1403.3371,2014-04-10,Spectral Correlation Hub Screening of Multivariate Time Series,"  This chapter discusses correlation analysis of stationary multivariate
Gaussian time series in the spectral or Fourier domain. The goal is to identify
the hub time series, i.e., those that are highly correlated with a specified
number of other time series. We show that Fourier components of the time series
at different frequencies are asymptotically statistically independent. This
property permits independent correlation analysis at each frequency,
alleviating the computational and statistical challenges of high-dimensional
time series. To detect correlation hubs at each frequency, an existing
correlation screening method is extended to the complex numbers to accommodate
complex-valued Fourier components. We characterize the number of hub
discoveries at specified correlation and degree thresholds in the regime of
increasing dimension and fixed sample size. The theory specifies appropriate
thresholds to apply to sample correlation matrices to detect hubs and also
allows statistical significance to be attributed to hub discoveries. Numerical
results illustrate the accuracy of the theory and the usefulness of the
proposed spectral framework.
"
1403.4471,2014-04-29,Principal bundles over statistical manifolds,"  In this paper, we introduce the concept of principal bundles on statistical
manifolds. After necessary preliminaries on information geometry and principal
bundles on manifolds, we study the $\alpha$-structure of frame bundles over
statistical manifolds with respect to $\alpha$-connections, by giving geometric
structures. The manifold of one-dimensional normal distributions appears in the
end as an application and a concrete example.
"
1403.5557,2014-03-25,"Claude Bouchu, intendant de Bourgogne au 17\`eme si\`ecle, a-t-il
  invent\'e le mot ""statistique""","  The objective of this paper is to examine the assertion that the word
""statistics"" would have been used for the first time in the 17th century, in a
report written by Claude Bouchu, administrator of Bourgogne. A historical and
bibliographical analysis is carried out to judge the credibility of this
thesis. The physical inspection of the report then makes it possible to bring a
final answer.
"
1403.7354,2014-08-07,Extremes of Order Statistics of Stationary Processes,"  Let $\{X_i(t),t\ge0\}, 1\le i\le n$ be independent copies of a stationary
process $\{X(t), t\ge0\}$. For given positive constants $u,T$, define the set
of $r$th conjunctions $ C_{r,T,u}:= \{t\in [0,T]: X_{r:n}(t) > u\}$ with
$X_{r:n}(t)$ the $r$th largest order statistics of $X_1(t), \ldots , X_n(t),
t\ge 0$. In numerous applications such as brain mapping and digital
communication systems, of interest is the approximation of the probability that
the set of conjunctions $C_{r,T,u}$ is not empty. Imposing the Albin's
conditions on $X$, in this paper we obtain an exact asymptotic expansion of
this probability as $u$ tends to infinity. Further, we establish the tail
asymptotics of the supremum of a generalized skew-Gaussian process and a Gumbel
limit theorem for the minimum order statistics of stationary Gaussian
processes. As a by-product we derive a version of Li and Shao's normal
comparison lemma for the minimum and the maximum of Gaussian random vectors.
"
1404.1789,2014-10-22,A Conversation with Donald B. Rubin,"  Donald Bruce Rubin is John L. Loeb Professor of Statistics at Harvard
University. He has made fundamental contributions to statistical methods for
missing data, causal inference, survey sampling, Bayesian inference, computing
and applications to a wide range of disciplines, including psychology,
education, policy, law, economics, epidemiology, public health and other social
and biomedical sciences.
"
1404.4287,2015-02-26,"Network impact on persistence in a finite population dynamic diffusion
  model: application to an emergent seed exchange network","  Dynamic extinction colonisation models (also called contact processes) are
widely studied in epidemiology and in metapopulation theory. Contacts are
usually assumed to be possible only through a network of connected patches.
This network accounts for a spatial landscape or a social organisation of
interactions. Thanks to social network literature, heterogeneous networks of
contacts can be considered. A major issue is to assess the influence of the
network in the dynamic model. Most work with this common purpose uses
deterministic models or an approximation of a stochastic
Extinction-Colonisation model (sEC) which are relevant only for large networks.
When working with a limited size network, the induced stochasticity is
essential and has to be taken into account in the conclusions. Here, a rigorous
framework is proposed for limited size networks and the limitations of the
deterministic approximation are exhibited. This framework allows exact
computations when the number of patches is small. Otherwise, simulations are
used and enhanced by adapted simulation techniques when necessary. A
sensitivity analysis was conducted to compare four main topologies of networks
in contrasting settings to determine the role of the network. A challenging
case was studied in this context: seed exchange of crop species in the R\'eseau
Semences Paysannes (RSP), an emergent French farmers' organisation. A
stochastic Extinction-Colonisation model was used to characterize the
consequences of substantial changes in terms of RSP's social organisation on
the ability of the system to maintain crop varieties.
"
1404.7208,2014-05-08,"Validating Sample Average Approximation Solutions with Negatively
  Dependent Batches","  Sample-average approximations (SAA) are a practical means of finding
approximate solutions of stochastic programming problems involving an extremely
large (or infinite) number of scenarios. SAA can also be used to find estimates
of a lower bound on the optimal objective value of the true problem which, when
coupled with an upper bound, provides confidence intervals for the true optimal
objective value and valuable information about the quality of the approximate
solutions. Specifically, the lower bound can be estimated by solving multiple
SAA problems (each obtained using a particular sampling method) and averaging
the obtained objective values. State-of-the-art methods for lower-bound
estimation generate batches of scenarios for the SAA problems independently. In
this paper, we describe sampling methods that produce negatively dependent
batches, thus reducing the variance of the sample-averaged lower bound
estimator and increasing its usefulness in defining a confidence interval for
the optimal objective value. We provide conditions under which the new sampling
methods can reduce the variance of the lower bound estimator, and present
computational results to verify that our scheme can reduce the variance
significantly, by comparison with the traditional Latin hypercube approach.
"
1405.2781,2014-05-13,Conditional quantile estimation through optimal quantization,"  In this paper, we use quantization to construct a nonparametric estimator of
conditional quantiles of a scalar response $Y$ given a d-dimensional vector of
covariates $X$. First we focus on the population level and show how optimal
quantization of $X$, which consists in discretizing $X$ by projecting it on an
appropriate grid of $N$ points, allows to approximate conditional quantiles of
$Y$ given $X$. We show that this is approximation is arbitrarily good as $N$
goes to infinity and provide a rate of convergence for the approximation error.
Then we turn to the sample case and define an estimator of conditional
quantiles based on quantization ideas. We prove that this estimator is
consistent for its fixed-$N$ population counterpart. The results are
illustrated on a numerical example. Dominance of our estimators over local
constant/linear ones and nearest neighbor ones is demonstrated through
extensive simulations in the companion paper Charlier et al.(2014b).
"
1405.4136,2014-05-19,"The Sociotype, a New Conceptual Construct on Human Social Networks:
  Application in Mental Health and Quality of Life","  The present work discusses the pertinence of a 'sociotype' construct, both
theoretically and empirically oriented. The term, based on the conceptual chain
genotype-phenotype-sociotype, suggests an evolutionary preference in the human
species for some determined averages of social relationships. This core pattern
or 'sociotype' has been explored herein for the networking relationships of
young people--165 university students filling in a 20-items questionnaire on
their social interactions. In spite that this is a preliminary study,
interesting results have been obtained on gender conversation time, mental
health, sociability level, and satisfaction with personal relationships. This
sociotype hypothesis could be a timely enterprise for mental health and quality
of life policies.
"
1405.6416,2014-05-27,"Discussion of ""Single and Two-Stage Cross-Sectional and Time Series
  Benchmarking Procedures for SAE""","  We congratulate the authors for a stimulating and valuable manuscript,
providing a careful review of the state-of the-art in cross-sectional and
time-series benchmarking procedures for small area estimation. They develop a
novel two-stage benchmarking method for hierarchical time series models, where
they evaluate their procedure by estimating monthly total unemployment using
data from the U.S. Census Bureau. We discuss three topics: linearity and model
misspecification, computational complexity and model comparisons, and, some
aspects on small area estimation in practice. More specifically, we pose the
following questions to the authors, that they may wish to answer: How robust is
their model to misspecification? Is it time to perhaps move away from linear
models of the type considered by (Battese et al. 1988; Fay and Herriot 1979)?
What is the asymptotic computational complexity and what comparisons can be
made to other models? Should the benchmarking constraints be inherently fixed
or should they be random?
"
1405.6676,2014-10-07,"Statistique et Big Data Analytics; Volum\'etrie, L'Attaque des Clones","  This article assumes acquired the skills and expertise of a statistician in
unsupervised (NMF, k-means, SVD) and supervised learning (regression, CART,
random forest). What skills and knowledge do a statistician must acquire to
reach the ""Volume"" scale of big data? After a quick overview of the different
strategies available and especially of those imposed by Hadoop, the algorithms
of some available learning methods are outlined in order to understand how they
are adapted to the strong stresses of the Map-Reduce functionalities
"
1405.7091,2014-05-29,"Bayesian hierarchical modelling for inferring genetic interactions in
  yeast","  Identifying genetic interactions for a given microorganism such as yeast is
difficult. Quantitative Fitness Analysis (QFA) is a high-throughput
experimental and computational methodology for quantifying the fitness of
microbial cultures. QFA can be used to compare between fitness observations for
different genotypes and thereby infer genetic interaction strengths. Current
""naive"" frequentist statistical approaches used in QFA do not model
between-genotype variation or difference in genotype variation under different
conditions. In this thesis, a Bayesian approach is introduced to evaluate
hierarchical models that better reflect the structure or design of QFA
experiments. First, a two-stage approach is presented: a hierarchical logistic
model is fitted to microbial culture growth curves and then a hierarchical
interaction model is fitted to fitness summaries inferred for each genotype.
Next, a one-stage Bayesian approach is presented: a joint hierarchical model
which does not require a univariate summary of fitness, used to pass
information between models. The new hierarchical approaches are then compared
using a dataset examining the effect of telomere defects on yeast. By better
describing the experimental structure, new evidence is found for genes and
complexes which interact with the telomere cap. Various extensions of these
models, including models for data transformation, batch effects, and
intrinsically stochastic growth models are also considered.
"
1405.7129,2016-08-30,Marginalization and Conditioning for LWF Chain Graphs,"  In this paper, we deal with the problem of marginalization over and
conditioning on two disjoint subsets of the node set of chain graphs (CGs) with
the LWF Markov property. For this purpose, we define the class of chain mixed
graphs (CMGs) with three types of edges and, for this class, provide a
separation criterion under which the class of CMGs is stable under
marginalization and conditioning and contains the class of LWF CGs as its
subclass. We provide a method for generating such graphs after marginalization
and conditioning for a given CMG or a given LWF CG. We then define and study
the class of anterial graphs, which is also stable under marginalization and
conditioning and contains LWF CGs, but has a simpler structure than CMGs.
"
1406.0758,2014-06-04,Pythagoras at the Bat,"  The Pythagorean formula is one of the most popular ways to measure the true
ability of a team. It is very easy to use, estimating a team's winning
percentage from the runs they score and allow. This data is readily available
on standings pages; no computationally intensive simulations are needed.
Normally accurate to within a few games per season, it allows teams to
determine how much a run is worth in different situations. This determination
helps solve some of the most important economic decisions a team faces: How
much is a player worth, which players should be pursued, and how much should
they be offered. We discuss the formula and these applications in detail, and
provide a theoretical justification, both for the formula as well as simpler
linear estimators of a team's winning percentage. The calculations and modeling
are discussed in detail, and when possible multiple proofs are given. We
analyze the 2012 season in detail, and see that the data for that and other
recent years support our modeling conjectures. We conclude with a discussion of
work in progress to generalize the formula and increase its predictive power
\emph{without} needing expensive simulations, though at the cost of requiring
play-by-play data.
"
1406.0913,2021-08-04,Generalized probabilities in statistical theories,"  In this review article we present different formal frameworks for the
description of generalized probabilities in statistical theories. We discuss
the particular cases of probabilities appearing in classical and quantum
mechanics, possible generalizations of the approaches of A. N. Kolmogorov and
R. T. Cox to non-commutative models, and the approach to generalized
probabilities based on convex sets.
"
1406.5004,2014-12-10,A mobile web for enhancing statistics and mathematics education,"  A freely available educational application (a mobile website) is presented.
This provides access to educational material and drilling on selected topics
within mathematics and statistics with an emphasis on tablets and mobile
phones. The application adapts to the student's performance, selecting from
easy to difficult questions, or older material etc. These adaptations are based
on statistical models and analyses of data from testing precursors of the
system within several courses, from calculus and introductory statistics
through multiple linear regression. The application can be used in both on-line
and off-line modes. The behavior of the application is determined by
parameters, the effects of which can be estimated statistically. Results
presented include analyses of how the internal algorithms relate to passing a
course and general incremental improvement in knowledge during a semester.
"
1406.5241,2014-06-23,"Using Google Scholar to predict self citation: A case study in Health
  Economics","  Metrics designed to quantify the influence of academics are increasingly used
and easily estimable, and perhaps the most popular is the h index. Metrics such
as this are however potentially impacted through excessive self citation. This
work explores the issue using a group of researchers working in a well defined
sub field of economics, namely Health Economics. It then employs self citation
identification software, and identifies the characteristics that best predict
self citation. This provides evidence regarding the scale of self citation in
the field, and the degree to which self citation impacts on inferences about
the relative influence of individual Health Economists. Using data from 545
Health Economists, it suggests self citation to be associated with the
geographical region and longevity of the Health Economist, with early career
researchers and researchers from mainland Europe and Australasia self citing
most frequently.
"
1406.6018,2017-09-27,"A brief history of long memory: Hurst, Mandelbrot and the road to ARFIMA","  Long memory plays an important role in many fields by determining the
behaviour and predictability of systems; for instance, climate, hydrology,
finance, networks and DNA sequencing. In particular, it is important to test if
a process is exhibiting long memory since that impacts the accuracy and
confidence with which one may predict future events on the basis of a small
amount of historical data. A major force in the development and study of long
memory was the late Benoit B. Mandelbrot. Here we discuss the original
motivation of the development of long memory and Mandelbrot's influence on this
fascinating field. We will also elucidate the sometimes contrasting approaches
to long memory in different scientific communities
"
1406.6085,2014-06-25,"Spectrum Estimation: A Unified Framework for Covariance Matrix
  Estimation and PCA in Large Dimensions","  Covariance matrix estimation and principal component analysis (PCA) are two
cornerstones of multivariate analysis. Classic textbook solutions perform
poorly when the dimension of the data is of a magnitude similar to the sample
size, or even larger. In such settings, there is a common remedy for both
statistical problems: nonlinear shrinkage of the eigenvalues of the sample
covariance matrix. The optimal nonlinear shrinkage formula depends on unknown
population quantities and is thus not available. It is, however, possible to
consistently estimate an oracle nonlinear shrinkage, which is motivated on
asymptotic grounds. A key tool to this end is consistent estimation of the set
of eigenvalues of the population covariance matrix (also known as the
spectrum), an interesting and challenging problem in its own right. Extensive
Monte Carlo simulations demonstrate that our methods have desirable
finite-sample properties and outperform previous proposals.
"
1407.0308,2014-07-02,"Using an Online Learning Environment to Teach an Undergraduate
  Statistics Course: the tutor-web","  A learning environment, the tutor-web (http://tutor-web.net), has been
developed and used for educational research. The system is accessible and free
to use for anyone having access to the Web. It is based on open source software
and the teaching material is licensed under the Creative Commons
Attribution-ShareAlike License. The system has been used for computer-assisted
education in statistics and mathematics. It offers a unique way to structure
and link together teaching material and includes interactive quizzes with the
primary purpose of increasing learning rather than mere evaluation.
  The system was used in a course on basic statistics in the University of
Iceland, spring 2013. A randomized trial was conducted to investigate the
difference in learning between students doing regular homework and students
using the system. The difference between the groups was not found to be
significant.
"
1407.2037,2014-07-29,"Which of the world's institutions employ the most highly cited
  researchers? An analysis of the data from highlycited.com","  A few weeks ago, Thomson Reuters published a list of the highly cited
researchers worldwide (highlycited.com). Since the data is freely available for
downloading and includes the names of the researchers' institutions, we
produced a ranking of the institutions on the basis of the number of highly
cited researchers per institution. This ranking is intended to be a helpful
amendment of other available institutional rankings.
"
1407.7172,2014-07-29,Tractable Measure of Component Overlap for Gaussian Mixture Models,"  The ability to quantify distinctness of a cluster structure is fundamental
for certain simulation studies, in particular for those comparing performance
of different classification algorithms. The intrinsic integral measure based on
the overlap of corresponding mixture components is often analytically
intractable. This is also the case for Gaussian mixture models with unequal
covariance matrices when space dimension $d > 1$. In this work we focus on
Gaussian mixture models and at the sample level we assume the class assignments
to be known. We derive a measure of component overlap based on eigenvalues of a
generalized eigenproblem that represents Fisher's discriminant task. We explain
rationale behind it and present simulation results that show how well it can
reflect the behavior of the integral measure in its linear approximation. The
analyzed coefficient possesses the advantage of being analytically tractable
and numerically computable even in complex setups.
"
1408.0963,2016-11-25,The Final Solutions of Monty Hall Problem and Three Prisoners Problem,"  Recently we proposed the linguistic interpretation of quantum mechanics
(called quantum and classical measurement theory, or quantum language), which
was characterized as a kind of metaphysical and linguistic turn of the
Copenhagen interpretation. This turn from physics to language does not only
extend quantum theory to classical systems but also yield the quantum
mechanical world view (i.e., the philosophy of quantum mechanics, in other
words, quantum philosophy).And we believe that this quantum language is the
most powerful language to describe science. The purpose of this paper is to
describe the Monty-Hall problem and the three prisoners problem in quantum
language. We of course believe that our proposal is the final solutions of the
two problems. Thus in this paper, we can answer the question: ""Why have
philosophers continued to stick to these problems?"" And the readers will find
that these problems are never elementary, and they can not be solved without
the deep understanding of ""probability"" and ""dualism"".
  KEY WORDS: Philosophy of probability, Fisher Maximum Likelihood Method,
Bayes' Method,The Principle of Equal (a priori) Probabilities
"
1408.1809,2022-12-20,Graphs for margins of Bayesian networks,"  Directed acyclic graph (DAG) models, also called Bayesian networks, impose
conditional independence constraints on a multivariate probability
distribution, and are widely used in probabilistic reasoning, machine learning
and causal inference. If latent variables are included in such a model, then
the set of possible marginal distributions over the remaining (observed)
variables is generally complex, and not represented by any DAG. Larger classes
of mixed graphical models, which use multiple edge types, have been introduced
to overcome this; however, these classes do not represent all the models which
can arise as margins of DAGs. In this paper we show that this is because
ordinary mixed graphs are fundamentally insufficiently rich to capture the
variety of marginal models.
  We introduce a new class of hyper-graphs, called mDAGs, and a latent
projection operation to obtain an mDAG from the margin of a DAG. We show that
each distinct marginal of a DAG model is represented by at least one mDAG, and
provide graphical results towards characterizing when two such marginal models
are the same. Finally we show that mDAGs correctly capture the marginal
structure of causally-interpreted DAGs under interventions on the observed
variables.
"
1408.4916,2014-09-16,The two envelopes paradox in non-Bayesian and Bayesian statistics,"  The purpose of this paper is to clarify the (non-Bayesian and Bayesian)
two-envelope problems in terms of quantum language (or, measurement theory),
which was recently proposed as a linguistic turn of quantum mechanics (with the
Copenhagen interpretation). The two envelopes paradox is only a kind of high
school student's probability puzzle, and it may be exaggerated to say that this
is an unsolved problem. However, since we are convinced that quantum language
is just statistics of the future, we believe that there is no clear answer
without the description by quantum language. In this sense, the readers are to
find that quantum language provides the final answer (i.e., the easiest and
deepest understanding) to the two envelope-problems in both non-Bayesian and
Bayesian statistics. Also, we add the discussion about St. Petersburg
two-envelope paradox.
"
1408.5720,2015-07-21,Invariant sums of random matrices and the onset of level repulsion,"  We compute analytically the joint probability density of eigenvalues and the
level spacing statistics for an ensemble of random matrices with interesting
features. It is invariant under the standard symmetry groups (orthogonal and
unitary) and yet the interaction between eigenvalues is not Vandermondian. The
ensemble contains real symmetric or complex hermitian matrices $\mathbf{S}$ of
the form $\mathbf{S}=\sum_{i=1}^M \langle \mathbf{O}_i
\mathbf{D}_i\mathbf{O}_i^{\mathrm{T}}\rangle$ or $\mathbf{S}=\sum_{i=1}^M
\langle \mathbf{U}_i \mathbf{D}_i\mathbf{U}_i^\dagger\rangle$ respectively. The
diagonal matrices
$\mathbf{D}_i=\mathrm{diag}\{\lambda_1^{(i)},\ldots,\lambda_N^{(i)}\}$ are
constructed from real eigenvalues drawn \emph{independently} from distributions
$p^{(i)}(x)$, while the matrices $\mathbf{O}_i$ and $\mathbf{U}_i$ are all
orthogonal or unitary. The average $\langle\cdot\rangle$ is simultaneously
performed over the symmetry group and the joint distribution of
$\{\lambda_j^{(i)}\}$. We focus on the limits i.) $N\to\infty$ and ii.)
$M\to\infty$, with $N=2$. In the limit i.), the resulting sum $\mathbf{S}$
develops level repulsion even though the original matrices do not feature it,
and classical RMT universality is restored asymptotically. In the limit ii.)
the spacing distribution attains scaling forms that are computed exactly: for
the orthogonal case, we recover the $\beta=1$ Wigner's surmise, while for the
unitary case an entirely new universal distribution is obtained. Our results
allow to probe analytically the microscopic statistics of the sum of random
matrices that become asymptotically free. We also give an interpretation of
this model in terms of radial random walks in a matrix space. The analytical
results are corroborated by numerical simulations.
"
1409.3803,2014-09-15,Revealing the Beauty behind the Sleeping Beauty Problem,"  A large number of essays address the Sleeping Beauty problem, which
undermines the validity of Bayesian inference and Bas Van Fraassen's
'Reflection Principle'. In this study a straightforward analysis of the problem
based on probability theory is presented. The key difference from previous
works is that apart from the random experiment imposed by the problem's
description, a different one is also considered, in order to negate the
confusion on the involved conditional probabilities. The results of the
analysis indicate that no inconsistency takes place, whereas both Bayesian
inference and 'Reflection Principle' are valid.
"
1409.4696,2015-05-21,Differentially Private Exponential Random Graphs,"  We propose methods to release and analyze synthetic graphs in order to
protect privacy of individual relationships captured by the social network.
Proposed techniques aim at fitting and estimating a wide class of exponential
random graph models (ERGMs) in a differentially private manner, and thus offer
rigorous privacy guarantees. More specifically, we use the randomized response
mechanism to release networks under $\epsilon$-edge differential privacy. To
maintain utility for statistical inference, treating the original graph as
missing, we propose a way to use likelihood based inference and Markov chain
Monte Carlo (MCMC) techniques to fit ERGMs to the produced synthetic networks.
We demonstrate the usefulness of the proposed techniques on a real data
example.
"
1409.5196,2016-03-01,How to read probability distributions as statements about process,"  Probability distributions can be read as simple expressions of information.
Each continuous probability distribution describes how information changes with
magnitude. Once one learns to read a probability distribution as a measurement
scale of information, opportunities arise to understand the processes that
generate the commonly observed patterns. Probability expressions may be parsed
into four components: the dissipation of all information, except the
preservation of average values, taken over the measurement scale that relates
changes in observed values to changes in information, and the transformation
from the underlying scale on which information dissipates to alternative scales
on which probability pattern may be expressed. Information invariances set the
commonly observed measurement scales and the relations between them. In
particular, a measurement scale for information is defined by its invariance to
specific transformations of underlying values into measurable outputs.
Essentially all common distributions can be understood within this simple
framework of information invariance and measurement scale.
"
1409.5501,2014-11-12,Tree Oriented Data Analysis,"  Complex data objects arise in many areas of modern science including
evolutionary biology, nueroscience, dynamics of gene expression and medical
imaging. Object oriented data analysis (OODA) is the statistical analysis of
datasets of complex objects. Data analysis of tree data objects is an exciting
research area with interesting questions and challenging problems. This thesis
focuses on tree oriented statistical methodologies, and algorithms for solving
related mathematical optimization problems.
  This research is motivated by the goal of analyzing a data set of images of
human brain arteries. The approach we take here is to use a novel
representation of brain artery systems as points in phylogenetic treespace. The
treespace property of unique global geodesics leads to a notion of geometric
center called a Fr\'echet mean. For a sample of data points, the Fr\'echet
function is the sum of squared distances from a point to the data points, and
the Fr\'echet mean is the minimizer of the Fr\'echet function.
  In this thesis we use properties of the Fr\'echet function to develop an
algorithmic system for computing Fr\'echet means. Properties of the Fr\'echet
function are also used to show a sticky law of large numbers which describes a
surprising stability of the topological tree structure of sample Fr\'echet
means at that of the population Fr\'echet mean. We also introduce
non-parametric regression of brain artery tree structure as a response variable
to age based on weighted Fr\'echet means.
"
1409.7419,2014-09-29,Identifying the number of clusters in discrete mixture models,"  Research on cluster analysis for categorical data continues to develop, with
new clustering algorithms being proposed. However, in this context, the
determination of the number of clusters is rarely addressed. In this paper, we
propose a new approach in which clustering of categorical data and the
estimation of the number of clusters is carried out simultaneously. Assuming
that the data originate from a finite mixture of multinomial distributions, we
develop a method to select the number of mixture components based on a minimum
message length (MML) criterion and implement a new expectation-maximization
(EM) algorithm to estimate all the model parameters. The proposed EM-MML
approach, rather than selecting one among a set of pre-estimated candidate
models (which requires running EM several times), seamlessly integrates
estimation and model selection in a single algorithm. The performance of the
proposed approach is compared with other well-known criteria (such as the
Bayesian information criterion-BIC), resorting to synthetic data and to two
real applications from the European Social Survey. The EM-MML computation time
is a clear advantage of the proposed method. Also, the real data solutions are
much more parsimonious than the solutions provided by competing methods, which
reduces the risk of model order overestimation and increases interpretability.
"
1409.7933,2014-09-30,Parametric Risk Parity,"  Any optimization algorithm based on the risk parity approach requires the
formulation of portfolio total risk in terms of marginal contributions. In this
paper we use the independence of the underlying factors in the market to derive
the centered moments required in the risk decomposition process when the
modified versions of Value at Risk and Expected Shortfall are considered.
  The choice of the Mixed Tempered Stable distribution seems adequate for
fitting skewed and heavy tailed distributions. The ensuing detailed description
of the optimization procedure is due to the existence of analytical higher
order moments. Better results are achieved in terms of out of sample
performance and greater diversification.
"
1410.0403,2014-10-03,"Computer experiments with functional inputs and scalar outputs by a
  norm-based approach","  A framework for designing and analyzing computer experiments is presented,
which is constructed for dealing with functional and real number inputs and
real number outputs. For designing experiments with both functional and real
number inputs a two stage approach is suggested. The first stage consists of
constructing a candidate set for each functional input and during the second
stage an optimal combination of the found candidate sets and a Latin hypercube
for the real number inputs is searched for. The resulting designs can be
considered to be generalizations of Latin hypercubes. GP models are explored as
metamodel. The functional inputs are incorporated into the kriging model by
applying norms in order to define distances between two functional inputs. In
order to make the calculation of these norms computationally feasible, the use
of B-splines is promoted.
"
1410.1107,2014-10-07,"Using Board Games and Mathematica to Teach the Fundamentals of Finite
  Stationary Markov Chains","  Markov chains are an important example for a course on stochastic processes
because simple board games can be used to illustrate the fundamental concepts.
For example, a looping board game (like Monopoly) consists of all recurrent
states, and a game where players win by reaching a final square (like Chutes
and Ladders) consists of all transient states except for the last one. With the
availability of computer algebra packages, these games can be analyzed. For
example, the mean times in transient states and the stationary probabilities
for recurrent states are easily computed. This article analyzes some simple
board games with Mathematica, and indicates how this can be extended to more
complex situations.
"
1410.1802,2014-10-08,"Piterbarg's max-discretisation theorem for stationary vector Gaussian
  processes observed on different grids","  In this paper we derive Piterbarg's max-discretisation theorem for two
different grids considering centered stationary vector Gaussian processes. So
far in the literature results in this direction have been derived for the joint
distribution of the maximum of Gaussian processes over $[0,T]$ and over a grid
$ \mathfrak{R}(\delta_1(T))=\{k\delta_1(T): k=0,1,\cdots\}$. In this paper we
extend recent findings by considering additionally the \bE{maximum} over
another grid $ \mathfrak{R}(\delta_2(T))$. We derive the joint limiting
distribution of maximum of stationary Gaussian vector processes for different
choices of such grids by letting $T\to \infty$.
"
1410.2759,2015-08-06,Network Analysis with the Enron Email Corpus,"  We use the Enron email corpus to study relationships in a network by applying
six different measures of centrality. Our results came out of an in-semester
undergraduate research seminar. The Enron corpus is well suited to statistical
analyses at all levels of undergraduate education. Through this note's focus on
centrality, students can explore the dependence of statistical models on
initial assumptions and the interplay between centrality measures and
hierarchical ranking, and they can use completed studies as springboards for
future research. The Enron corpus also presents opportunities for research into
many other areas of analysis, including social networks, clustering, and
natural language processing.
"
1410.3127,2020-07-21,"Data Science in Statistics Curricula: Preparing Students to ""Think with
  Data""","  A growing number of students are completing undergraduate degrees in
statistics and entering the workforce as data analysts. In these positions,
they are expected to understand how to utilize databases and other data
warehouses, scrape data from Internet sources, program solutions to complex
problems in multiple languages, and think algorithmically as well as
statistically. These data science topics have not traditionally been a major
component of undergraduate programs in statistics. Consequently, a curricular
shift is needed to address additional learning outcomes. The goal of this paper
is to motivate the importance of data science proficiency and to provide
examples and resources for instructors to implement data science in their own
statistics curricula. We provide case studies from seven institutions. These
varied approaches to teaching data science demonstrate curricular innovations
to address new needs. Also included here are examples of assignments designed
for courses that foster engagement of undergraduates with data and data
science.
"
1410.3929,2014-10-20,"Distributed Detection of a Random Process over a Multiple Access Channel
  under Energy and Bandwidth Constraints","  We analyze a binary hypothesis testing problem built on a wireless sensor
network (WSN) for detecting a stationary random process distributed both in
space and time with circularly-symmetric complex Gaussian distribution under
the Neyman-Pearson framework. Using an analog scheme, the sensors transmit
different linear combinations of their measurements through a multiple access
channel (MAC) to reach the fusion center (FC), whose task is to decide whether
the process is present or not. Considering an energy constraint on each node
transmission and a limited amount of channel uses, we compute the miss error
exponent of the proposed scheme using Large Deviation Theory (LDT) and show
that the proposed strategy is asymptotically optimal (when the number of
sensors approaches to infinity) among linear orthogonal schemes. We also show
that the proposed scheme obtains significant energy saving in the low
signal-to-noise ratio regime, which is the typical scenario of WSNs. Finally, a
Monte Carlo simulation of a 2-dimensional process in space validates the
analytical results.
"
1410.4515,2014-10-17,A Conversation with Howell Tong,"  The following conversation is partly based on an interview that took place in
the Hong Kong University of Science and Technology in July 2013.
"
1410.5772,2014-12-10,"Methods for the generation of normalized citation impact scores in
  bibliometrics: Which method best reflects the judgements of experts?","  Evaluative bibliometrics compares the citation impact of researchers,
research groups and institutions with each other across time scales and
disciplines. Both factors - discipline and period - have an influence on the
citation count which is independent of the quality of the publication.
Normalizing the citation impact of papers for these two factors started in the
mid-1980s. Since then, a range of different methods have been presented for
producing normalized citation impact scores. The current study uses a data set
of over 50,000 records to test which of the methods so far presented correlate
better with the assessment of papers by peers. The peer assessments come from
F1000Prime - a post-publication peer review system of the biomedical
literature. Of the normalized indicators, the current study involves not only
cited-side indicators, such as the mean normalized citation score, but also
citing-side indicators. As the results show, the correlations of the indicators
with the peer assessments all turn out to be very similar. Since F1000 focuses
on biomedicine, it is important that the results of this study are validated by
other studies based on datasets from other disciplines or (ideally) based on
multi-disciplinary datasets.
"
1410.6002,2014-10-30,Estimating the Tail Index by using Model Averaging,"  The ideas of model averaging are used to find weights in peak-over-threshold
problems using a possible range of thresholds. A range of the largest
observations are chosen and considered as possible thresholds, each time
performing estimation. Weights based on an information criterion for each
threshold are calculated. A weighted estimate of the threshold and shape
parameter can be calculated.
"
1410.7967,2023-07-19,"Technical Report: Compressive Temporal Higher Order Cyclostationary
  Statistics","  The application of nonlinear transformations to a cyclostationary signal for
the purpose of revealing hidden periodicities has proven to be useful for
applications requiring signal selectivity and noise tolerance. The fact that
the hidden periodicities, referred to as cyclic moments, are often compressible
in the Fourier domain motivates the use of compressive sensing (CS) as an
efficient acquisition protocol for capturing such signals. In this work, we
consider the class of Temporal Higher Order Cyclostationary Statistics (THOCS)
estimators when CS is used to acquire the cyclostationary signal assuming
compressible cyclic moments in the Fourier domain. We develop a theoretical
framework for estimating THOCS using the low-rate nonuniform sampling protocol
from CS and illustrate the performance of this framework using simulated data.
"
1410.8868,2014-11-03,"Precinct Size Matters - The Large Precinct Bias in US Presidential
  Elections","  Examination of precinct level data in US presidential elections reveals a
correlation of large precincts and increased fraction of Republican votes. The
large precinct bias is analyzed with respect to voter heterogeneity and voter
inconvenience as precinct size increases. The analysis shows that voter
inconvenience is a significant factor in election outcomes in certain states,
and may significantly disadvantage Democratic candidates.
"
1411.0539,2014-11-04,"A note on Bayesian logistic regression for spatial exponential family
  Gibbs point processes","  Recently, a very attractive logistic regression inference method for
exponential family Gibbs spatial point processes was introduced. We combined it
with the technique of quadratic tangential variational approximation and
derived a new Bayesian technique for analysing spatial point patterns. The
technique is described in detail, and demonstrated on numerical examples.
"
1411.2571,2014-11-11,"Parametric Order Constraints in Multinomial Processing Tree Models: An
  Extension of Knapp and Batchelder (2004)","  Multinomial processing tree (MPT) models are tools for disentangling the
contributions of latent cognitive processes in a given experimental paradigm.
The present note analyzes MPT models subject to order constraints on subsets of
its parameters. The constraints that we consider frequently arise in cases
where the response categories are ordered in some sense such as in
confidence-rating data, Likert scale data, where graded guessing tendencies or
response biases are created via base-rate or payoff manipulations, in the
analysis of contingency tables with order constraints, and in many other cases.
We show how to construct an MPT model without order constraints that is
statistically equivalent to the MPT model with order constraints. This new
closure result extends the mathematical analysis of the MPT class, and it
offers an approach to order-restricted inference that extends the approaches
discussed by Knapp and Batchelder (2004). The usefulness of the method is
illustrated by means of an analysis of an order-constrained version of the
two-high-threshold model for confidence ratings.
"
1411.2778,2015-07-01,"Testing Order Constraints: Qualitative Differences Between Bayes Factors
  and Normalized Maximum Likelihood","  We compared Bayes factors to normalized maximum likelihood for the simple
case of selecting between an order-constrained versus a full binomial model.
This comparison revealed two qualitative differences in testing order
constraints regarding data dependence and model preference.
"
1411.4750,2016-01-18,"Convergence rates of maximal deviation distribution for projection
  estimates of L\'evy densities","  In this paper, we consider projection estimates for L\'evy densities in
high-frequency setup. We give a unified treatment for different sets of basis
functions and focus on the asymptotic properties of the maximal deviation
distribution for these estimates. Our results are based on the idea to
reformulate the problems in terms of Gaussian processes of some special type
and to further analyze these Gaussian processes. In particular, we construct a
sequence of excursion sets, which guarantees the convergence of the deviation
distribution to the Gumbel distribution. We show that the rates of convergence
presented in previous articles on this topic are logarithmic and construct the
sequences of accompanying laws, which approximate the deviation distribution
with polynomial rate.
"
1411.4824,2014-11-19,Quantile of a Mixture,"  In this note, we give an explicit expression for the quantile of a mixture of
two random variables. We carefully examine all possible cases of discrete and
continuous variables with possibly unbounded support. The result is useful for
finding bounds on the Value-at-Risk of risky portfolios when only partial
information is available (Bernard and Vanduffel (2014)).
"
1411.5279,2014-11-20,"What Teachers Should Know about the Bootstrap: Resampling in the
  Undergraduate Statistics Curriculum","  I have three goals in this article: (1) To show the enormous potential of
bootstrapping and permutation tests to help students understand statistical
concepts including sampling distributions, standard errors, bias, confidence
intervals, null distributions, and P-values. (2) To dig deeper, understand why
these methods work and when they don't, things to watch out for, and how to
deal with these issues when teaching. (3) To change statistical practice---by
comparing these methods to common t tests and intervals, we see how inaccurate
the latter are; we confirm this with asymptotics. n >= 30 isn't enough---think
n >= 5000. Resampling provides diagnostics, and more accurate alternatives.
Sadly, the common bootstrap percentile interval badly under-covers in small
samples; there are better alternatives. The tone is informal, with a few
stories and jokes.
"
1412.1649,2014-12-05,A Class of Conjugate Priors Defined on the Unit Simplex,"  Dirichlet distribution and Dirichlet process as its infinite dimensional
generalization are primarily used conjugate prior of categorical and
multinomial distributions in Bayesian statistics. Extensions have been proposed
to broaden applications for different purposes. In this article, we explore a
class of prior distributions closely related to Dirichlet distribution
incorporating additional information on the data generating mechanism. Examples
are given to show potential use of the models.
"
1412.3773,2016-05-03,"Distinguishing cause from effect using observational data: methods and
  benchmarks","  The discovery of causal relationships from purely observational data is a
fundamental problem in science. The most elementary form of such a causal
discovery problem is to decide whether X causes Y or, alternatively, Y causes
X, given joint observations of two variables X, Y. An example is to decide
whether altitude causes temperature, or vice versa, given only joint
measurements of both variables. Even under the simplifying assumptions of no
confounding, no feedback loops, and no selection bias, such bivariate causal
discovery problems are challenging. Nevertheless, several approaches for
addressing those problems have been proposed in recent years. We review two
families of such methods: Additive Noise Methods (ANM) and Information
Geometric Causal Inference (IGCI). We present the benchmark CauseEffectPairs
that consists of data for 100 different cause-effect pairs selected from 37
datasets from various domains (e.g., meteorology, biology, medicine,
engineering, economy, etc.) and motivate our decisions regarding the ""ground
truth"" causal directions of all pairs. We evaluate the performance of several
bivariate causal discovery methods on these real-world benchmark data and in
addition on artificially simulated data. Our empirical results on real-world
data indicate that certain methods are indeed able to distinguish cause from
effect using only purely observational data, although more benchmark data would
be needed to obtain statistically significant conclusions. One of the best
performing methods overall is the additive-noise method originally proposed by
Hoyer et al. (2009), which obtains an accuracy of 63+-10 % and an AUC of
0.74+-0.05 on the real-world benchmark. As the main theoretical contribution of
this work we prove the consistency of that method.
"
1412.5294,2015-10-28,"Generalized Labeled Multi-Bernoulli Approximation of Multi-Object
  Densities","  In multi-object inference, the multi-object probability density captures the
uncertainty in the number and the states of the objects as well as the
statistical dependence between the objects. Exact computation of the
multi-object density is generally intractable and tractable implementations
usually require statistical independence assumptions between objects. In this
paper we propose a tractable multi-object density approximation that can
capture statistical dependence between objects. In particular, we derive a
tractable Generalized Labeled Multi-Bernoulli (GLMB) density that matches the
cardinality distribution and the first moment of the labeled multi-object
distribution of interest. It is also shown that the proposed approximation
minimizes the Kullback-Leibler divergence over a special tractable class of
GLMB densities. Based on the proposed GLMB approximation we further demonstrate
a tractable multi-object tracking algorithm for generic measurement models.
Simulation results for a multi-object Track-Before-Detect example using radar
measurements in low signal-to-noise ratio (SNR) scenarios verify the
applicability of the proposed approach.
"
1412.5936,2015-10-14,"Nonparametric estimation of the division rate of an age dependent
  branching process","  We study the nonparametric estimation of the branching rate $B(x)$ of a
supercritical Bellman-Harris population: a particle with age $x$ has a random
lifetime governed by $B(x)$; at its death time, it gives rise to $k \geq 2$
children with lifetimes governed by the same division rate and so on. We
observe in continuous time the process over $[0,T]$. Asymptotics are taken as
$T \rightarrow \infty$; the data are stochastically dependent and one has to
face simultaneously censoring, bias selection and non-ancillarity of the number
of observations. In this setting, under appropriate ergodicity properties, we
construct a kernel-based estimator of $B(x)$ that achieves the rate of
convergence $\exp(-\lambda_B \frac{\beta}{2\beta+1}T)$, where $\lambda_B$ is
the Malthus parameter and $\beta >0$ is the smoothness of the function $B(x)$
in a vicinity of $x$. We prove that this rate is optimal in a minimax sense and
we relate it explicitly to classical nonparametric models such as density
estimation observed on an appropriate (parameter dependent) scale. We also shed
some light on the fact that estimation with kernel estimators based on data
alive at time $T$ only is not sufficient to obtain optimal rates of
convergence, a phenomenon which is specific to nonparametric estimation and
that has been observed in other related growth-fragmentation models.
"
1412.7261,2014-12-24,"From Curriculum Guidelines to Learning Objectives: A Survey of Five
  Statistics Programs","  The 2000 ASA Guidelines for Undergraduate Statistics majors aimed to provide
guidance to programs with undergraduate degrees in statistics as to the content
and skills that statistics majors should be learning. With new guidelines
forthcoming, it is important to help programs develop an assessment cycle of
evaluation. How do we know the students are learning what we want them to
learn? How do we improve the program over time? The first step in this process
is to translate the broader Guidelines into institution-specific measurable
learning outcomes. This paper provides examples of how five programs did so for
the 2000 Guidelines. We hope they serve as illustrative examples for programs
moving forward with the new guidelines.
"
1412.7831,2014-12-30,Asymptotics of the convex hull of spherical samples,"  In this paper we consider the convex hull of a spherically symmetric sample
in $R^d$. Our main contributions are some new asymptotic results for the
expectation of the number of vertices, number of facets, area and the volume of
the convex hull assuming that the marginal distributions are in the Gumbel
max-domain of attraction. Further, we briefly discuss two other models assuming
that the marginal distributions are regularly varying or $O$-regularly varying.
"
1501.00599,2015-01-06,On testing More IFRA Ordering-II,"  Suppose F and G are two life distribution functions. It is said that F is
more IFRA than G (written by F<_* G) if G^(-1) F(x) is starshaped on (0,infty).
In this paper, the problem of testing H_0:F=_* G against H_1:F<_* G and F
\neq_* G is considered in both cases when G is known and when G is unknown. We
propose a new test based on U-statistics and obtain the asymptotic distribution
of the test statistics. The new test is compared with some well known tests in
the literature. In addition, we apply our test to a real data set in the
context of reliability.
"
1501.00818,2021-02-02,"Forecasting day ahead electricity spot prices: The impact of the EXAA to
  other European electricity markets","  In our paper we analyze the relationship between the day-ahead electricity
price of the Energy Exchange Austria (EXAA) and other day-ahead electricity
prices in Europe. We focus on markets, which settle their prices after the
EXAA, which enables traders to include the EXAA price into their calculations.
For each market we employ econometric models to incorporate the EXAA price and
compare them with their counterparts without the price of the Austrian
exchange. By employing a forecasting study, we find that electricity price
models can be improved when EXAA prices are considered.
"
1501.01908,2015-01-09,The Compass for Statistical Researchers,"  We have hiked many miles alongside several professors as we traversed our
statistical path -- a regime switching trail which changed direction following
a class on the foundations of our discipline. As we play the game of research
in that limbo between student and academic, one thing among Prof. Bernardi's
teachings has never been more clear: to draw a route in the research map you
not only need to know your destination, but you must also understand where you
are and how you arrived there.
"
1501.03215,2015-01-15,"Creating, Automating, and Assessing Online Homework in Introductory
  Statistics and Mathematics Classes","  Although textbook publishers offer course management systems, they do so to
promote brand loyalty, and while an open source tool such as WeBWorK is
promising, it requires administrative and IT buy-in. So supported in part by a
College Access Challenge Grant from the Department of Education, we
collaborated with other instructors to create online homework sets for three
classes: Elementary Algebra, Intermediate Algebra, and Statistics for
Behavioral Sciences I. After experimentation, some of these question pools are
now created by Mathematica programs that can generate data sets from specified
distributions, generate random polynomials that factor in a given way, create
image files of histograms, scatterplots, and so forth. These programs produce
files that can be read by the software package, Respondus, which then uploads
the questions into Blackboard Learn, the course management system used by the
Connecticut State University system. Finally, we summarize five classes worth
of student performance data along with lessons learned while working on this
project.
"
1501.03811,2015-01-19,The Problem Of Grue Isn't,"  The so-called problem of grue was introduced by Nelson Goodman in 1954 as a
""riddle"" about induction, a riddle which has been widely thought to cast doubt
on the validity and rationality of induction. That unnecessary doubt in turn is
partly responsible for the reluctance to adopt the view that probability is
part of logic. Several authors have pointed out deficiencies in grue;
nevertheless, the ""problem"" still excites. Here, adapted from Groarke, is
presented the basis of grue, along with another simple demonstration that the
""problem"" makes no sense and is brought about by a misunderstanding of
causation.
"
1501.05019,2016-08-24,Robust Hypothesis Testing with $\alpha$-Divergence,"  A robust minimax test for two composite hypotheses, which are determined by
the neighborhoods of two nominal distributions with respect to a set of
distances - called $\alpha-$divergence distances, is proposed. Sion's minimax
theorem is adopted to characterize the saddle value condition. Least favorable
distributions, the robust decision rule and the robust likelihood ratio test
are derived. If the nominal probability distributions satisfy a symmetry
condition, the design procedure is shown to be simplified considerably. The
parameters controlling the degree of robustness are bounded from above and the
bounds are shown to be resulting from a solution of a set of equations. The
simulations performed evaluate and exemplify the theoretical derivations.
"
1502.00318,2020-07-21,"Setting the stage for data science: integration of data management
  skills in introductory and second courses in statistics","  Many have argued that statistics students need additional facility to express
statistical computations. By introducing students to commonplace tools for data
management, visualization, and reproducible analysis in data science and
applying these to real-world scenarios, we prepare them to think statistically.
In an era of increasingly big data, it is imperative that students develop
data-related capacities, beginning with the introductory course. We believe
that the integration of these precursors to data science into our
curricula-early and often-will help statisticians be part of the dialogue
regarding ""Big Data"" and ""Big Questions"".
"
1502.02555,2015-02-10,What are the true clusters?,"  Constructivist philosophy and Hasok Chang's active scientific realism are
used to argue that the idea of ""truth"" in cluster analysis depends on the
context and the clustering aims. Different characteristics of clusterings are
required in different situations. Researchers should be explicit about on what
requirements and what idea of ""true clusters"" their research is based, because
clustering becomes scientific not through uniqueness but through transparent
and open communication. The idea of ""natural kinds"" is a human construct, but
it highlights the human experience that the reality outside the observer's
control seems to make certain distinctions between categories inevitable.
Various desirable characteristics of clusterings and various approaches to
define a context-dependent truth are listed, and I discuss what impact these
ideas can have on the comparison of clustering methods, and the choice of a
clustering methods and related decisions in practice.
"
1502.02708,2015-08-12,"A comparative review of generalizations of the Gumbel extreme value
  distribution with an application to wind speed data","  The generalized extreme value distribution and its particular case, the
Gumbel extreme value distribution, are widely applied for extreme value
analysis. The Gumbel distribution has certain drawbacks because it is a
non-heavy-tailed distribution and is characterized by constant skewness and
kurtosis. The generalized extreme value distribution is frequently used in this
context because it encompasses the three possible limiting distributions for a
normalized maximum of infinite samples of independent and identically
distributed observations. However, the generalized extreme value distribution
might not be a suitable model when each observed maximum does not come from a
large number of observations. Hence, other forms of generalizations of the
Gumbel distribution might be preferable. Our goal is to collect in the present
literature the distributions that contain the Gumbel distribution embedded in
them and to identify those that have flexible skewness and kurtosis, are
heavy-tailed and could be competitive with the generalized extreme value
distribution. The generalizations of the Gumbel distribution are described and
compared using an application to a wind speed data set and Monte Carlo
simulations. We show that some distributions suffer from overparameterization
and coincide with other generalized Gumbel distributions with a smaller number
of parameters, i.e., are non-identifiable. Our study suggests that the
generalized extreme value distribution and a mixture of two extreme value
distributions should be considered in practical applications.
"
1502.04047,2015-03-02,"On Statistical Analysis of the Pattern of Evolution of Perceived
  Emotions Induced by Hindustani Music- A Study Based on Listener Responses","  The objective of this study is to find the underlying pattern of how
perception of emotions has evolved in India. Here Hindustani Music has been
used as a reference frame for tracking the changing perception of emotions. It
has been found that different emotions perceived from Hindustani Music form a
particular sequential pattern when their corresponding pitch periods are
analyzed using the standard deviations and mean successive squared
differences.This sequential pattern of emotions coincides with their
corresponding sequential pattern of tempos or average number of steady states.
On the basis of this result we further found that the range of perception of
emotions has diminished significantly these days compared to what it was
before. The proportion of responses for the perceived emotions like Anger,
Serenity, Romantic and Sorrow has also decreased to a great extent than what it
was previously. The proportion of responses for the perceived emotion Anxiety
has increased phenomenally. Both standard deviation and mean successive squared
difference are two very good measures in tracking the changing perception of
emotions. The overall pattern of the change of perceived emotions has
corresponded to the psychological and sociological change of human life.
"
1503.00481,2015-03-03,"A Reputation Economy: Results from an Empirical Survey on Academic Data
  Sharing","  Academic data sharing is a way for researchers to collaborate and thereby
meet the needs of an increasingly complex research landscape. It enables
researchers to verify results and to pursuit new research questions with ""old""
data. It is therefore not surprising that data sharing is advocated by funding
agencies, journals, and researchers alike. We surveyed 2661 individual academic
researchers across all disciplines on their dealings with data, their
publication practices, and motives for sharing or withholding research data.
The results for 1564 valid responses show that researchers across disciplines
recognise the benefit of secondary research data for their own work and for
scientific progress as a whole-still they only practice it in moderation. An
explanation for this evidence could be an academic system that is not driven by
monetary incentives, nor the desire for scientific progress, but by individual
reputation-expressed in (high ranked journal) publications. We label this
system a Reputation Economy. This special economy explains our findings that
show that researchers have a nuanced idea how to provide adequate formal
recognition for making data available to others-namely data citations. We
conclude that data sharing will only be widely adopted among research
professionals if sharing pays in form of reputation. Thus, policy measures that
intend to foster research collaboration need to understand academia as a
reputation economy. Successful measures must value intermediate products, such
as research data, more highly than it is the case now.
"
1503.00781,2015-03-04,Teaching and Learning Data Visualization: Ideas and Assignments,"  This article discusses how to make statistical graphics a more prominent
element of the undergraduate statistics curricula. The focus is on several
different types of assignments that exemplify how to incorporate graphics into
a course in a pedagogically meaningful way. These assignments include having
students deconstruct and reconstruct plots, copy masterful graphs, create
one-minute visual revelations, convert tables into `pictures', and develop
interactive visualizations with, e.g., the virtual earth as a plotting canvas.
In addition to describing the goals and details of each assignment, we also
discuss the broader topic of graphics and key concepts that we think warrant
inclusion in the statistics curricula. We advocate that more attention needs to
be paid to this fundamental field of statistics at all levels, from
introductory undergraduate through graduate level courses. With the rapid rise
of tools to visualize data, e.g., Google trends, GapMinder, ManyEyes, and
Tableau, and the increased use of graphics in the media, understanding the
principles of good statistical graphics, and having the ability to create
informative visualizations is an ever more important aspect of statistics
education.
"
1503.02188,2020-07-21,"Challenges and opportunities for statistics and statistical education:
  looking back, looking forward","  The 175th anniversary of the ASA provides an opportunity to look back into
the past and peer into the future. What led our forebears to found the
association? What commonalities do we still see? What insights might we glean
from their experiences and observations? I will use the anniversary as a chance
to reflect on where we are now and where we are headed in terms of statistical
education amidst the growth of data science. Statistics is the science of
learning from data. By fostering more multivariable thinking, building
data-related skills, and developing simulation-based problem solving, we can
help to ensure that statisticians are fully engaged in data science and the
analysis of the abundance of data now available to us.
"
1503.02780,2015-08-27,"Replication, Communication, and the Population Dynamics of Scientific
  Discovery","  Many published research results are false, and controversy continues over the
roles of replication and publication policy in improving the reliability of
research. Addressing these problems is frustrated by the lack of a formal
framework that jointly represents hypothesis formation, replication,
publication bias, and variation in research quality. We develop a mathematical
model of scientific discovery that combines all of these elements. This model
provides both a dynamic model of research as well as a formal framework for
reasoning about the normative structure of science. We show that replication
may serve as a ratchet that gradually separates true hypotheses from false, but
the same factors that make initial findings unreliable also make replications
unreliable. The most important factors in improving the reliability of research
are the rate of false positives and the base rate of true hypotheses, and we
offer suggestions for addressing each. Our results also bring clarity to verbal
debates about the communication of research. Surprisingly, publication bias is
not always an obstacle, but instead may have positive impacts---suppression of
negative novel findings is often beneficial. We also find that communication of
negative replications may aid true discovery even when attempts to replicate
have diminished power. The model speaks constructively to ongoing debates about
the design and conduct of science, focusing analysis and discussion on precise,
internally consistent models, as well as highlighting the importance of
population dynamics.
"
1503.04620,2016-01-20,"Two symmetry breaking mechanisms for the development of orientation
  selectivity in a neural system","  Orientation selectivity is a remarkable feature of the neurons located in the
primary visual cortex. Provided that the visual neurons acquire orientation
selectivity through activity-dependent Hebbian learning, the development
process could be understood as a kind of symmetry breaking phenomenon in the
view of physics. The key mechanisms of the development process are examined
here in a neural system. Found is that there are at least two different
mechanisms which lead to the development of orientation selectivity through
breaking the radial symmetry in receptive fields. The first, a simultaneous
symmetry breaking mechanism, bases on the competition between neighboring
neurons, and the second, a spontaneous one, bases on the nonlinearity in
interactions. It turns out that only the second mechanism leads to the
formation of a columnar pattern which characteristics accord with those
observed in an animal experiment.
"
1503.05570,2015-03-20,A Data Science Course for Undergraduates: Thinking with Data,"  Data science is an emerging interdisciplinary field that combines elements of
mathematics, statistics, computer science, and knowledge in a particular
application domain for the purpose of extracting meaningful information from
the increasingly sophisticated array of data available in many settings. These
data tend to be non-traditional, in the sense that they are often live, large,
complex, and/or messy. A first course in statistics at the undergraduate level
typically introduces students with a variety of techniques to analyze small,
neat, and clean data sets. However, whether they pursue more formal training in
statistics or not, many of these students will end up working with data that is
considerably more complex, and will need facility with statistical computing
techniques. More importantly, these students require a framework for thinking
structurally about data. We describe an undergraduate course in a liberal arts
environment that provides students with the tools necessary to apply data
science. The course emphasizes modern, practical, and useful skills that cover
the full data analysis spectrum, from asking an interesting question to
acquiring, managing, manipulating, processing, querying, analyzing, and
visualizing data, as well communicating findings in written, graphical, and
oral forms.
"
1503.06201,2015-03-23,Data Science as a New Frontier for Design,"  The purpose of this paper is to contribute to the challenge of transferring
know-how, theories and methods from design research to the design processes in
information science and technologies. More specifically, we shall consider a
domain, namely data-science, that is becoming rapidly a globally invested
research and development axis with strong imperatives for innovation given the
data deluge we are currently facing. We argue that, in order to rise to the
data-related challenges that the society is facing, data-science initiatives
should ensure a renewal of traditional research methodologies that are still
largely based on trial-error processes depending on the talent and insights of
a single (or a restricted group of) researchers. It is our claim that design
theories and methods can provide, at least to some extent, the much-needed
framework. We will use a worldwide data-science challenge organized to study a
technical problem in physics, namely the detection of Higgs boson, as a use
case to demonstrate some of the ways in which design theory and methods can
help in analyzing and shaping the innovation dynamics in such projects.
"
1503.08019,2016-03-25,"Optimality of Fast Matching Algorithms for Random Networks with
  Applications to Structural Controllability","  Network control refers to a very large and diverse set of problems including
controllability of linear time-invariant dynamical systems, where the objective
is to select an appropriate input to steer the network to a desired state.
There are many notions of controllability, one of them being structural
controllability, which is intimately connected to finding maximum matchings on
the underlying network topology. In this work, we study fast, scalable
algorithms for finding maximum matchings for a large class of random networks.
First, we illustrate that degree distribution random networks are realistic
models for real networks in terms of structural controllability. Subsequently,
we analyze a popular, fast and practical heuristic due to Karp and Sipser as
well as a simplification of it. For both heuristics, we establish asymptotic
optimality and provide results concerning the asymptotic size of maximum
matchings for an extensive class of random networks.
"
1503.08278,2015-03-31,Modeling population structure under hierarchical Dirichlet processes,"  We propose a Bayesian nonparametric model to infer population admixture,
extending the Hierarchical Dirichlet Process to allow for correlation between
loci due to Linkage Disequilibrium. Given multilocus genotype data from a
sample of individuals, the model allows inferring classifying individuals as
unadmixed or admixed, inferring the number of subpopulations ancestral to an
admixed population and the population of origin of chromosomal regions. Our
model does not assume any specific mutation process and can be applied to most
of the commonly used genetic markers. We present a MCMC algorithm to perform
posterior inference from the model and discuss methods to summarise the MCMC
output for the analysis of population admixture. We demonstrate the performance
of the proposed model in simulations and in a real application, using genetic
data from the EDAR gene, which is considered to be ancestry-informative due to
well-known variations in allele frequency as well as phenotypic effects across
ancestry. The structure analysis of this dataset leads to the identification of
a rare haplotype in Europeans.
"
1503.08653,2015-03-31,Exponentiated Extended Weibull-Power Series Class of Distributions,"  In this paper, we introduce a new class of distributions by compounding the
exponentiated extended Weibull family and power series family. This
distribution contains several lifetime models such as the complementary
extended Weibull-power series, generalized exponential-power series,
generalized linear failure rate-power series, exponentiated Weibull-power
series, generalized modified Weibull-power series, generalized Gompertz-power
series and exponentiated extended Weibull distributions as special cases. We
obtain several properties of this new class of distributions such as Shannon
entropy, mean residual life, hazard rate function, quantiles and moments. The
maximum likelihood estimation procedure via a EM-algorithm is presented.
"
1503.09072,2019-10-25,Failure and Uses of Jaynes' Principle of Transformation Groups,"  Bertand's paradox is a fundamental problem in probability that casts doubt on
the applicability of the indifference principle by showing that it may yield
contradictory results, depending on the meaning assigned to ""randomness"".
Jaynes claimed that symmetry requirements (the principle of transformation
groups) solve the paradox by selecting a unique solution to the problem. I show
that this is not the case and that every variant obtained from the principle of
indifference can also be obtained from Jaynes' principle of transformation
groups. This is because the same symmetries can be mathematically implemented
in different ways, depending on the procedure of random selection that one
uses. I describe a simple experiment that supports a result from symmetry
arguments, but the solution is different from Jaynes'. Jaynes' method is thus
best seen as a tool to obtain probability distributions when the principle of
indifference is inconvenient, but it cannot resolve ambiguities inherent in the
use of that principle and still depends on explicitly defining the selection
procedure.
"
1504.01361,2018-02-12,"Bertrand `paradox' reloaded (with details on transformations of
  variables, an introduction to Monte Carlo simulation and an inferential
  variation of the problem)","  This note is mainly to point out, if needed, that uncertainty about models
and their parameters has little to do with a `paradox'. The proposed `solution'
is to formulate practical questions instead of seeking refuge into abstract
principles. (And, in order to be concrete, some details on how to calculate the
probability density functions of the chord lengths are provided, together with
some comments on simulations and an appendix on the inferential aspects of the
problem.)
"
1504.01950,2015-04-09,"Le Her and Other Problems in Probability Discussed by Bernoulli,
  Montmort and Waldegrave","  Part V of the second edition of Pierre R\'{e}mond de Montmort's Essay
d'analyse sur les jeux de hazard published in 1713 contains correspondence on
probability problems between Montmort and Nicolaus Bernoulli. This
correspondence begins in 1710. The last published letter, dated November 15,
1713, is from Montmort to Nicolaus Bernoulli. There is some discussion of the
strategy of play in the card game Le Her and a bit of news that Montmort's
friend Waldegrave in Paris was going to take care of the printing of the book.
From earlier correspondence between Bernoulli and Montmort, it is apparent that
Waldegrave had also analyzed Le Her and had come up with a mixed strategy as a
solution. He had also suggested working on the ""problem of the pool,"" or what
is often called Waldegrave's problem. The Universit\""{a}tsbibliothek Basel
contains an additional forty-two letters between Bernoulli and Montmort written
after 1713, as well as two letters between Bernoulli and Waldegrave. The
letters are all in French, and here we provide translations of key passages.
The trio continued to discuss probability problems, particularly Le Her which
was still under discussion when the Essay d'analyse went to print. We describe
the probability content of this body of correspondence and put it in its
historical context. We also provide a proper identification of Waldegrave based
on manuscripts in the Archives nationales de France in Paris.
"
1504.02057,2015-04-09,"A Singular Value Decomposition-based Factorization and Parsimonious
  Component Model of Demographic Quantities Correlated by Age: Predicting
  Complete Demographic Age Schedules with Few Parameters","  BACKGROUND. Formal demography has a long history of building simple models of
age schedules of demographic quantities, e.g. mortality and fertility rates.
These are widely used in demographic methods to manipulate whole age schedules
using few parameters.
  OBJECTIVE. The Singular Value Decomposition (SVD) factorizes a matrix into
three matrices with useful properties including the ability to reconstruct the
original matrix using many fewer, simple matrices. This work demonstrates how
these properties can be exploited to build parsimonious models of whole age
schedules of demographic quantities that can be further parameterized in terms
of arbitrary covariates.
  METHODS. The SVD is presented and explained in detail with attention to
developing an intuitive understanding. The SVD is used to construct a general,
component model of demographic age schedules, and that model is demonstrated
with age-specific mortality and fertility rates. Finally, the model is used (1)
to predict age-specific mortality using HIV indicators and summary measures of
age-specific mortality, and (2) to predict age-specific fertility using the
total fertility rate (TFR).
  RESULTS. The component model of age-specific mortality and fertility rates
succeeds in reproducing the data with two inputs, and acting through those two
inputs, various covariates are able to accurately predict full age schedules.
  CONCLUSIONS. The SVD is potentially useful as a way to summarize, smooth and
model age-specific demographic quantities. The component model is a general
method of relating covariates to whole age schedules.
  COMMENTS. The focus of this work is the SVD and the component model. The
applications are for illustrative purposes only.
"
1504.02124,2017-05-29,"Hyak Mortality Monitoring System: Innovative Sampling and Estimation
  Methods - Proof of Concept by Simulation","  Traditionally health statistics are derived from civil and/or vital
registration. Civil registration in low-income countries varies from partial
coverage to essentially nothing at all. Consequently the state of the art for
public health information in low-income countries is efforts to combine or
triangulate data from different sources to produce a more complete picture
across both time and space - data amalgamation. Data sources amenable to this
approach include sample surveys, sample registration systems, health and
demographic surveillance systems, administrative records, census records,
health facility records and others.
  We propose a new statistical framework for gathering health and population
data - Hyak - that leverages the benefits of sampling and longitudinal,
prospective surveillance to create a cheap, accurate, sustainable monitoring
platform. Hyak has three fundamental components:
  1) Data Amalgamation: a sampling and surveillance component that organizes
two or more data collection systems to work together: a) data from HDSS with
frequent, intense, linked, prospective follow-up and b) data from sample
surveys conducted in large areas surrounding the Health and Demographic
Surveillance System sites using informed sampling so as to capture as many
events as possible;
  2) Cause of Death: verbal autopsy to characterize the distribution of deaths
by cause at the population level; and
  3) SES: measurement of socioeconomic status in order to characterize poverty
and wealth.
  We conduct a simulation study of the informed sampling component of Hyak
based on the Agincourt HDSS site in South Africa. Compared to traditional
cluster sampling, Hyak's informed sampling captures more deaths, and when
combined with an estimation model that includes spatial smoothing, produces
estimates mortality that have lower variance and small bias.
"
1504.02129,2015-04-10,"InSilicoVA: A Method to Automate Cause of Death Assignment for Verbal
  Autopsy","  Verbal autopsies (VA) are widely used to provide cause-specific mortality
estimates in developing world settings where vital registration does not
function well. VAs assign cause(s) to a death by using information describing
the events leading up to the death, provided by care givers. Typically
physicians read VA interviews and assign causes using their expert knowledge.
Physician coding is often slow, and individual physicians bring bias to the
coding process that results in non-comparable cause assignments. These problems
significantly limit the utility of physician-coded VAs. A solution to both is
to use an algorithmic approach that formalizes the cause-assignment process.
This ensures that assigned causes are comparable and requires many fewer
person-hours so that cause assignment can be conducted quickly without
disrupting the normal work of physicians. Peter Byass' InterVA method is the
most widely used algorithmic approach to VA coding and is aligned with the WHO
2012 standard VA questionnaire.
  The statistical model underpinning InterVA can be improved; uncertainty needs
to be quantified, and the link between the population-level CSMFs and the
individual-level cause assignments needs to be statistically rigorous.
Addressing these theoretical concerns provides an opportunity to create new
software using modern languages that can run on multiple platforms and will be
widely shared. Building on the overall framework pioneered by InterVA, our work
creates a statistical model for automated VA cause assignment.
"
1504.03089,2015-04-14,A Conversation with Nancy Flournoy,"  Nancy Flournoy was born in Long Beach, California, on May 4, 1947. After
graduating from Polytechnic School in Pasadena in 1965, she earned a B.S.
(1969) and M.S. (1971) in biostatistics from UCLA. Between her bachelors and
masters degrees, she worked as a Statistician I for Regional Medical Programs
at UCLA. After receiving her master's degree, she spend three years at the
Southwest Laboratory for Education Research and Development in Seal Beach,
California. Flournoy joined the Seattle team pioneering bone marrow
transplantation in 1973. She moved with the transplant team into the newly
formed Fred Hutchinson Cancer Research Center in 1975 as Director of Clinical
Statistics, where she supervised a group responsible for the design and
analysis of about 80 simultaneous clinical trials. To support the Clinical
Division, she supervised the development of an interdisciplinary shared data
software system. She recruited Leonard B. Hearne to create this database
management system in 1975 (and married him in 1978). While at the Cancer
Center, she was also at the University of Washington, where she received her
doctorate in biomathematics in 1982. She became the first female director of
the program in statistics at the National Science Foundation (NSF) in 1986. She
received service awards from the NSF in 1988 and the National Institute of
Statistical Science in 2006 for facilitating interdisciplinary research.
Flournoy joined the Department of Mathematics and Statistics at American
University in 1988. She moved as department chair to the University of Missouri
in 2002, where she became Curators' Distinguished Professor in 2012.
"
1504.03131,2015-04-14,A Conversation with Richard A. Olshen,"  Richard Olshen was born in Portland, Oregon, on May 17, 1942. Richard spent
his early years in Chevy Chase, Maryland, but has lived most of his life in
California. He received an A.B. in Statistics at the University of California,
Berkeley, in 1963, and a Ph.D. in Statistics from Yale University in 1966,
writing his dissertation under the direction of Jimmie Savage and Frank
Anscombe. He served as Research Staff Statistician and Lecturer at Yale in
1966-1967. Richard accepted a faculty appointment at Stanford University in
1967, and has held tenured faculty positions at the University of Michigan
(1972-1975), the University of California, San Diego (1975-1989), and Stanford
University (since 1989). At Stanford, he is Professor of Health Research and
Policy (Biostatistics), Chief of the Division of Biostatistics (since 1998) and
Professor (by courtesy) of Electrical Engineering and of Statistics. At various
times, he has had visiting faculty positions at Columbia, Harvard, MIT,
Stanford and the Hebrew University. Richard's research interests are in
statistics and mathematics and their applications to medicine and biology. Much
of his work has concerned binary tree-structured algorithms for classification,
regression, survival analysis and clustering. Those for classification and
survival analysis have been used with success in computer-aided diagnosis and
prognosis, especially in cardiology, oncology and toxicology. He coauthored the
1984 book Classification and Regression Trees (with Leo Brieman, Jerome
Friedman and Charles Stone) which gives motivation, algorithms, various
examples and mathematical theory for what have come to be known as CART
algorithms. The approaches to tree-structured clustering have been applied to
problems in digital radiography (with Stanford EE Professor Robert Gray) and to
HIV genetics, the latter work including studies on single nucleotide
polymorphisms, which has helped to shed light on the presence of hypertension
in certain subpopulations of women.
"
1504.07336,2015-04-29,Information content of partially rank-ordered set samples,"  Partially rank-ordered set (PROS) sampling is a generalization of ranked set
sampling in which rankers are not required to fully rank the sampling units in
each set, hence having more flexibility to perform the necessary judgemental
ranking process. The PROS sampling has a wide range of applications in
different fields ranging from environmental and ecological studies to medical
research and it has been shown to be superior over ranked set sampling and
simple random sampling for estimating the population mean. In this paper, we
study the Fisher information content and uncertainty structure of the PROS
samples and compare them with those of simple random sample (SRS) and ranked
set sample (RSS) counterparts of the same size from the underlying population.
We study the uncertainty structure in terms of the Shannon entropy, Renyi
entropy and Kullback-Leibler (KL) discrimination measures. Several examples
including the FI of PROS samples from the location-scale family of
distributions as well as a regression model are discussed.
"
1505.06354,2018-06-13,Online Updating of Statistical Inference in the Big Data Setting,"  We present statistical methods for big data arising from online analytical
processing, where large amounts of data arrive in streams and require fast
analysis without storage/access to the historical data. In particular, we
develop iterative estimating algorithms and statistical inferences for linear
models and estimating equations that update as new data arrive. These
algorithms are computationally efficient, minimally storage-intensive, and
allow for possible rank deficiencies in the subset design matrices due to
rare-event covariates. Within the linear model setting, the proposed
online-updating framework leads to predictive residual tests that can be used
to assess the goodness-of-fit of the hypothesized model. We also propose a new
online-updating estimator under the estimating equation setting. Theoretical
properties of the goodness-of-fit tests and proposed estimators are examined in
detail. In simulation studies and real data applications, our estimator
compares favorably with competing approaches under the estimating equation
setting.
"
1505.07087,2015-05-27,"Propagation of Uncertainty in Risk Analysis and Safety Integrity Level
  Composition","  In many risk analyses the results are only given as mean values and often the
input data are also mean values. However the required accuracy of the result is
often an interval of values e. g. for the derivation of a Safety Integrity
Level (SIL). In this paper we reason what should be the accuracy of the input
data of risk analyses if a particular certainty of the result is demanded. Also
the backside of the coin, the SIL composition is discussed. The results show
that common methods for risk analysis are faulty and that SIL allocation by a
kind of SIL calculus seems infeasible without additional requirements on the
composed components. A justification of a common practice for parameter scaling
in well-constructed semi-quantitative risk analysis is also provided.
"
1506.04131,2015-06-15,"Two Challenges of Stealthy Hypervisors Detection: Time Cheating and Data
  Fluctuations","  Hardware virtualization technologies play a significant role in cyber
security. On the one hand these technologies enhance security levels, by
designing a trusted operating system. On the other hand these technologies can
be taken up into modern malware which is rather hard to detect. None of the
existing methods is able to efficiently detect a hypervisor in the face of
countermeasures such as time cheating, temporary self uninstalling, memory
hiding etc. New hypervisor detection methods which will be described in this
paper can detect a hypervisor under these countermeasures and even count
several nested ones. These novel approaches rely on the new statistical
analysis of time discrepancies by examination of a set of instructions, which
are unconditionally intercepted by a hypervisor. Reliability was achieved
through the comprehensive analysis of the collected data despite its
fluctuation. These offered methods were comprehensively assessed in both Intel
and AMD CPUs.
"
1506.04570,2015-06-16,The Two-envelope Problem: An Informed Choice,"  The host of a game presents two indistinguishable envelopes to an agent. One
of the envelopes is randomly selected and allocated to the agent. The agent is
informed that the monetary content of one of the envelopes is twice that of the
other. The dilemma is under which conditions it would be beneficial to switch
the allocated envelope for the complementary one. The objective of his or her
envelope-switching strategy is to determine the benefit of switching the
allocated envelope and its content for the expected content of the
complementary envelope.
  The agent, upon revealing the content of the allocated envelope, must
consider the events that are likely to have taken place as a result of the
host's activities. The preceding approach is in stark contrast to considering
the agent's reasoning for a particular outcome that seeks to derive a strategy
based on the relative contents of the presented envelopes. However, it is the
former reasoning that seeks to identify what the initial amounts could have
been, as a result of the observed amount, that facilitates the identification
of an appropriate switching strategy.
  Knowledge of the content and allocation process is essential for the agent to
derive a successful switching strategy, as is the distribution function from
which the host sampled the initial amount that is assigned to the first
envelope.
  For every play of the game, once the agent is afforded the opportunity of
sighting the content of the randomly allocated envelope, he or she can
determine the expected benefit of switching.
"
1506.05557,2016-08-15,Exponential Quantum Tsallis Havrda Charvat Entropy of Type Alpha,"  Entropy is a key measure in studies related to information theory and its
many applications. Campbell of the first time recognized that exponential of
Shannons entropy is just the size of the sample space when the distribution is
uniform. In this paper, we introduce a quantity which is called exponential
Tsallis Havrda Charvat entropy and discuss its some properties. Further, we
gave the application of exponential Tsallis Havrda Charvat entropy in quantum
information theory which is called exponential quantum Tsallis Havrda Charvat
entropy with its some major properties such as non-negative, concavity and
continuity. It is found that projective measurement will not decrease the
quantum entropy of a quantum state and at the end of the paper gave an upper
bound on the quantum exponential entropy in terms of ensembles of pure state.
"
1507.01404,2021-08-17,"A new Universal Resample Stable Bootstrap-based Stopping Criterion in
  PLS Components Construction","  We develop a new robust stopping criterion in Partial Least Squares
Regressions (PLSR) components construction characterised by a high level of
stability. This new criterion is defined as a universal one since it is
suitable both for PLSR and its extension to Generalized Linear Regressions
(PLSGLR). This criterion is based on a non-parametric bootstrap process and has
to be computed algorithmically. It allows to test each successive components on
a preset significant level alpha. In order to assess its performances and
robustness with respect to different noise levels, we perform intensive
datasets simulations, with a preset and known number of components to extract,
both in the case n>p (n being the number of subjects and p the number of
original predictors), and for datasets with n<p. We then use t-tests to compare
the performance of our approach to some others classical criteria. The property
of robustness is particularly tested through resampling processes on a real
allelotyping dataset. Our conclusion is that our criterion presents also better
global predictive performances, both in the PLSR and PLSGLR (Logistic and
Poisson) frameworks.
"
1507.02537,2016-03-09,"Modeling asymptotically independent spatial extremes based on Laplace
  random fields","  We tackle the modeling of threshold exceedances in asymptotically independent
stochastic processes by constructions based on Laplace random fields. These are
defined as Gaussian random fields scaled with a stochastic variable following
an exponential distribution. This framework yields useful asymptotic properties
while remaining statistically convenient. Univariate distribution tails are of
the half exponential type and are part of the limiting generalized Pareto
distributions for threshold exceedances. After normalizing marginal tail
distributions in data, a standard Laplace field can be used to capture spatial
dependence among extremes. Asymptotic properties of Laplace fields are explored
and compared to the classical framework of asymptotic dependence. Multivariate
joint tail decay rates for Laplace fields are slower than for Gaussian fields
with the same covariance structure; hence they provide more conservative
estimates of very extreme joint risks while maintaining asymptotic
independence. Statistical inference is illustrated on extreme wind gusts in the
Netherlands where a comparison to the Gaussian dependence model shows a better
goodness-of-fit in terms of Akaike's criterion. In this specific application we
fit the well-adapted Weibull distribution as univariate tail model, such that
the normalization of univariate tail distributions can be done through a simple
power transformation of data.
"
1507.05057,2015-07-20,Visualizing Probabilistic Proof,"  The author revisits the Blue Bus Problem, a famous thought-experiment in law
involving probabilistic proof, and presents simple Bayesian solutions to
different versions of the blue bus hypothetical. In addition, the author
expresses his solutions in standard and visual formats, i.e. in terms of
probabilities and natural frequencies.
"
1507.05346,2015-07-21,"Mere Renovation is Too Little Too Late: We Need to Rethink Our
  Undergraduate Curriculum from the Ground Up","  The last half-dozen years have seen The American Statistician publish
well-argued and provocative calls to change our thinking about statistics and
how we teach it, among them Brown and Kass (2009), Nolan and Temple-Lang
(2010), and Legler et al. (2010). Within this past year, the ASA has issued a
new and comprehensive set of guidelines for undergraduate programs (ASA 2014).
Accepting (and applauding) all this as background, the current article argues
the need to rethink our curriculum from the ground up, and offers five
principles and two caveats intended to help us along the path toward a new
synthesis. These principles and caveats rest on my sense of three parallel
evolutions: the convergence of trends in the roles of mathematics, computation,
and context within statistics education. These ongoing changes, together with
the articles cited above and the seminal provocation by Leo Breiman (2001) call
for a deep rethinking of what we teach to undergraduates. In particular,
following Brown and Kass, we should put priority on two goals, to make
fundamental concepts accessible and to minimize prerequisites to research.
"
1507.05982,2015-07-23,On locating statistics in the world of finding out,"  This paper attempts to situate statistics in relation to qualitative research
methods and other means of ""finding out"". It compares and contrasts aspects of
qualitative research methods and statistical inquiry and attempts to answer the
question of whether and how elements of qualitative research methods should be
included in statistics teaching.
"
1507.06411,2015-07-24,Arbitrariness of peer review: A Bayesian analysis of the NIPS experiment,"  The principle of peer review is central to the evaluation of research, by
ensuring that only high-quality items are funded or published. But peer review
has also received criticism, as the selection of reviewers may introduce biases
in the system. In 2014, the organizers of the ``Neural Information Processing
Systems\rq\rq{} conference conducted an experiment in which $10\%$ of submitted
manuscripts (166 items) went through the review process twice. Arbitrariness
was measured as the conditional probability for an accepted submission to get
rejected if examined by the second committee. This number was equal to $60\%$,
for a total acceptance rate equal to $22.5\%$. Here we present a Bayesian
analysis of those two numbers, by introducing a hidden parameter which measures
the probability that a submission meets basic quality criteria. The standard
quality criteria usually include novelty, clarity, reproducibility, correctness
and no form of misconduct, and are met by a large proportions of submitted
items. The Bayesian estimate for the hidden parameter was equal to $56\%$
($95\%$CI: $ I = (0.34, 0.83)$), and had a clear interpretation. The result
suggested the total acceptance rate should be increased in order to decrease
arbitrariness estimates in future review processes.
"
1507.07244,2015-07-30,"The Crisis Of Evidence: Why Probability And Statistics Cannot Discover
  Cause","  Probability models are only useful at explaining the uncertainty of what we
do not know, and should never be used to say what we already know. Probability
and statistical models are useless at discerning cause. Classical statistical
procedures, in both their frequentist and Bayesian implementations are, falsely
imply they can speak about cause. No hypothesis test, or Bayes factor, should
ever be used again. Even assuming we know the cause or partial cause for some
set of observations, reporting via relative risk exagerates the certainty we
have in the future, often by a lot. This over-certainty is made much worse when
parametetric and not predictive methods are used. Unfortunately, predictive
methods are rarely used; and even when they are, cause must still be an
assumption, meaning (again) certainty in our scientific pronouncements is too
high.
"
1507.07902,2015-07-29,Robust Estimation in Stochastic Frontier Models,"  This study proposes a robust estimator for stochastic frontier models by
integrating the idea of Basu et al. [1998, Biometrika 85, 549-559] into such
models. We verify that the suggested estimator is strongly consistent and
asymptotic normal under regularity conditions and investigate robust
properties. We use a simulation study to demonstrate that the estimator has
strong robust properties with little loss in asymptotic efficiency relative to
the maximum likelihood estimator. A real data analysis is performed for
illustrating the use of the estimator.
"
1507.07905,2015-09-25,The XL-mHG Test For Enrichment: A Technical Report,"  The minimum hypergeometric test (mHG) is a powerful nonparametric hypothesis
test to detect enrichment in ranked binary lists. Here, I provide a detailed
review of its definition, as well as the algorithms used in its implementation,
which enable the efficient computation of an exact p-value. I then introduce a
generalization of the mHG, termed XL-mHG, which provides additional control
over the type of enrichment tested, and describe the precise algorithmic
modifications necessary to compute its test statistic and p-value. The XL-mHG
algorithm is a building block of GO-PCA, a recently proposed method for the
exploratory analysis of gene expression data using prior knowledge.
"
1507.08500,2015-07-31,A Conversation with Robert C. Elston,"  Robert C. Elston was born on February 4, 1932, in London, England. He went to
Cambridge University to study natural science from 1952-1956 and obtained B.A.,
M.A. and Diploma in Agriculture (Dip Ag). He came to the US at age 24 to study
animal breeding at Cornell University and received his Ph.D. in 1959. From
1959-1960, he was a post-doctoral fellow in biostatistics at University of
North Carolina (UNC), Chapel Hill, where he studied mathematical statistics. He
then rose through the academic ranks in the department of biostatistics at UNC,
becoming a full professor in 1969. From 1979-1995, he was a professor and head
of the Department of Biometry and Genetics at Louisiana State University
Medical Center in New Orleans. In 1995, he moved to Case Western Reserve
University where he is a professor of epidemiology and biostatistics and served
as chairman from 2008 to 2014. Between 1966 and 2013, he directed 42 Ph.D.
students and mentored over 40 post-doctoral fellows. If one regards him as a
founder of a pedigree in research in genetic epidemiology, it was estimated in
2007 that there were more than 500 progeny. Among his many honors are a NIH
Research Career Development Award (1966-1976), the Leadership Award from
International Society of Human Genetics (1995), William Allan Award from
American Society of Human Genetics (1996), NIH MERIT Award (1998) and the
Marvin Zelen Leadership Award, Harvard University (2004). He is a Fellow of the
American Statistical Association and the Institute of Mathematical Statistics
as well as a Fellow of the Ohio Academy of Science. A leader in research in
genetic epidemiology for over 40 years, he has published over 600 research
articles in biostatistics, genetic epidemiology and applications. He has also
coauthored and edited 9 books in biostatistics, population genetics and methods
for the analysis of genetic data.
"
1507.08502,2015-07-31,A Conversation with Jerry Friedman,"  Jerome H. Friedman was born in Yreka, California, USA, on December 29, 1939.
He received his high school education at Yreka High School, then spent two
years at Chico State College before transferring to the University of
California at Berkeley in 1959. He completed an undergraduate degree in physics
in 1962 and a Ph.D. in high-energy particle physics in 1968 and was a
post-doctoral research physicist at the Lawrence Berkeley Laboratory during
1968-1972. In 1972, he moved to Stanford Linear Accelerator Center (SLAC) as
head of the Computation Research Group, retaining this position until 2006. In
1981, he was appointed half time as Professor in the Department of Statistics,
Stanford University, remaining half time with his SLAC appointment. He has held
visiting appointments at CSIRO in Sydney, CERN and the Department of Statistics
at Berkeley, and has had a very active career as a commercial consultant. Jerry
became Professor Emeritus in the Department of Statistics in 2007. Apart from
some 30 publications in high-energy physics early in his career, Jerry has
published over 70 research articles and books in statistics and computer
science, including co-authoring the pioneering books Classification and
Regression Trees and The Elements of Statistical Learning. Many of his
publications have hundreds if not thousands of citations (e.g., the CART book
has over 21,000). Much of his software is incorporated in commercial products,
including at least one popular search engine. Many of his methods and
algorithms are essential inclusions in modern statistical and data mining
packages. Honors include the following: the Rietz Lecture (1999) and the Wald
Lectures (2009); election to the American Academy of Arts and Sciences (2005)
and the US National Academy of Sciences (2010); a Fellow of the American
Statistical Association; Paper of the Year (JASA 1980, 1985; Technometrics
1998, 1992); Statistician of the Year (ASA, Chicago Chapter, 1999); ACM Data
Mining Lifetime Innovation Award (2002), Emanuel & Carol Parzen Award for
Statistical Innovation (2004); Noether Senior Lecturer (American Statistical
Association, 2010); and the IEEE Computer Society Data Mining Research
Contribution Award (2012).
"
1507.08934,2015-08-03,"A Framework for Infusing Authentic Data Experiences Within Statistics
  Courses","  Working with complex data is one of the important updates to the 2014 ASA
Curriculum Guidelines for Undergraduate Programs in Statistical Science.
Infusing 'authentic data experiences' within courses allow students
opportunities to learn and practice data skills as they prepare a dataset for
analysis. While more modest in scope than a senior-level culminating
experience, authentic data experiences provide an opportunity to demonstrate
connections between data skills and statistical skills. The result is more
practice of data skills for undergraduate statisticians.
"
1508.00543,2015-08-04,"Combating anti-statistical thinking using simulation-based methods
  throughout the undergraduate curriculum","  The use of simulation-based methods for introducing inference is growing in
popularity for the Stat 101 course, due in part to increasing evidence of the
methods ability to improve students' statistical thinking. This impact comes
from simulation-based methods (a) clearly presenting the overarching logic of
inference, (b) strengthening ties between statistics and probability or
mathematical concepts, (c) encouraging a focus on the entire research process,
(d) facilitating student thinking about advanced statistical concepts, (e)
allowing more time to explore, do, and talk about real research and messy data,
and (f) acting as a firmer foundation on which to build statistical intuition.
Thus, we argue that simulation-based inference should be an entry point to an
undergraduate statistics program for all students, and that simulation-based
inference should be used throughout all undergraduate statistics courses. In
order to achieve this goal and fully recognize the benefits of simulation-based
inference on the undergraduate statistics program we will need to break free of
historical forces tying undergraduate statistics curricula to mathematics,
consider radical and innovative new pedagogical approaches in our courses,
fully implement assessment-driven content innovations, and embrace computation
throughout the curriculum.
"
1508.02083,2015-10-28,"Studies on properties and estimation problems for modified extension of
  exponential distribution","  The present paper considers modified extension of the exponential
distribution with three parameters. We study the main properties of this new
distribution, with special emphasis on its median, mode and moments function
and some characteristics related to reliability studies. For Modified-
extension exponential distribution (MEXED) we have obtained the Bayes
Estimators of scale and shape parameters using Lindley's approximation
(L-approximation) under squared error loss function. But, through this
approximation technique it is not possible to compute the interval estimates of
the parameters. Therefore, we also propose Gibbs sampling method to generate
sample from the posterior distribution. On the basis of generated posterior
sample we computed the Bayes estimates of the unknown parameters and
constructed 95 % highest posterior density credible intervals. A Monte Carlo
simulation study is carried out to compare the performance of Bayes estimators
with the corresponding classical estimators in terms of their simulated risk. A
real data set has been considered for illustrative purpose of the study.
"
1508.02384,2015-08-12,"The Third Way Of Probability & Statistics: Beyond Testing and Estimation
  To Importance, Relevance, and Skill","  There is a third way of implementing probability models and practicing. This
is to answer questions put in terms of observables. This eliminates frequentist
hypothesis testing and Bayes factors and it also eliminates parameter
estimation. The Third Way is the logical probability approach, which is to make
statements $\Pr(Y \in y | X,D,M)$ about observables of interest $Y$ taking
values $y$, given probative data $X$, past observations (when present) $D$ and
some model (possibly deduced) $M$. Significance and the false idea that
probability models show causality are no more, and in their place are
importance and relevance. Models are built keeping on information that is
relevant and important to a decision maker (and not a statistician). All models
are stated in publicly verifiable fashion, as predictions. All models must
undergo a verification process before any trust is put into them.
"
1508.03808,2016-03-21,"Quantifying information transfer and mediation along causal pathways in
  complex systems","  Measures of information transfer have become a popular approach to analyze
interactions in complex systems such as the Earth or the human brain from
measured time series. Recent work has focused on causal definitions of
information transfer excluding effects of common drivers and indirect
influences. While the former clearly constitutes a spurious causality, the aim
of the present article is to develop measures quantifying different notions of
the strength of information transfer along indirect causal paths, based on
first reconstructing the multivariate causal network (\emph{Tigramite}
approach). Another class of novel measures quantifies to what extent different
intermediate processes on causal paths contribute to an interaction mechanism
to determine pathways of causal information transfer. A rigorous mathematical
framework allows for a clear information-theoretic interpretation that can also
be related to the underlying dynamics as proven for certain classes of
processes. Generally, however, estimates of information transfer remain hard to
interpret for nonlinearly intertwined complex systems. But, if experiments or
mathematical models are not available, measuring pathways of information
transfer within the causal dependency structure allows at least for an
abstraction of the dynamics. The measures are illustrated on a climatological
example to disentangle pathways of atmospheric flow over Europe.
"
1508.05453,2015-08-25,Beyond subjective and objective in statistics,"  We argue that the words ""objectivity"" and ""subjectivity"" in statistics
discourse are used in a mostly unhelpful way, and we propose to replace each of
them with broader collections of attributes, with objectivity replaced by
transparency, consensus, impartiality, and correspondence to observable
reality, and subjectivity replaced by awareness of multiple perspectives and
context dependence. The advantage of these reformulations is that the
replacement terms do not oppose each other. Instead of debating over whether a
given statistical method is subjective or objective (or normatively debating
the relative merits of subjectivity and objectivity in statistical practice),
we can recognize desirable attributes such as transparency and acknowledgment
of multiple perspectives as complementary goals. We demonstrate the
implications of our proposal with recent applied examples from pharmacology,
election polling, and socioeconomic stratification.
"
1508.05541,2015-08-25,"Explorations in Statistics Research: An Approach to Expose
  Undergraduates to Authentic Data Analysis","  The Explorations in Statistics Research workshop is a one-week NSF-funded
summer program that introduces undergraduate students to current research
problems in applied statistics. The goal of the workshop is to expose students
to exciting, modern applied statistical research and practice, with the
ultimate aim of interesting them in seeking more training in statistics at the
undergraduate and graduate levels. The program is explicitly designed to engage
students in the connections between authentic domain problems and the
statistical ideas and approaches needed to address these problems, which is an
important aspect of statistical thinking that is difficult to teach and
sometimes lacking in our methodological courses and programs. Over the past
nine years, we ran the workshop six times and a similar program in the sciences
two times. We describe the program, summarize feedback from participants, and
identify the key features to its success. We abstract these features and
provide a set of recommendations for how faculty can incorporate important
elements into their regular courses.
"
1508.05914,2015-08-25,"Extended Dynamic Generalized Linear Models: the two-parameter
  exponential family","  We develop a Bayesian framework for estimation and prediction of dynamic
models for observations from the two-parameter exponential family. Different
link functions are introduced to model both the mean and the precision in the
exponential family allowing the introduction of covariates and time series
components. We explore conjugacy and analytical approximations under the class
of partial specified models to keep the computation fast. The algorithm of
West, Harrison and Migon (1985) is extended to cope with the two-parameter
exponential family models. The methodological novelties are illustrated with
two applications to real data. The first, considers unemployment rates in
Brazil and the second some macroeconomic variables for the United Kingdom.
"
1508.05973,2015-11-06,"A Review of Nonparametric Hypothesis Tests of Isotropy Properties in
  Spatial Data","  An important aspect of modeling spatially-referenced data is appropriately
specifying the covariance function of the random field. A practitioner working
with spatial data is presented a number of choices regarding the structure of
the dependence between observations. One of these choices is determining
whether or not an isotropic covariance function is appropriate. Isotropy
implies that spatial dependence does not depend on the direction of the spatial
separation between sampling locations. Misspecification of isotropy properties
(directional dependence) can lead to misleading inferences, e.g., inaccurate
predictions and parameter estimates. A researcher may use graphical
diagnostics, such as directional sample variograms, to decide whether the
assumption of isotropy is reasonable. These graphical techniques can be
difficult to assess, open to subjective interpretations, and misleading.
Hypothesis tests of the assumption of isotropy may be more desirable. To this
end, a number of tests of directional dependence have been developed using both
the spatial and spectral representations of random fields. We provide an
overview of nonparametric methods available to test the hypotheses of isotropy
and symmetry in spatial data. We summarize test properties, discuss important
considerations and recommendations in choosing and implementing a test, compare
several of the methods via a simulation study, and propose a number of open
research questions. Several of the reviewed methods can be implemented in R
using our package spTest, available on CRAN.
"
1509.01365,2015-09-07,"Publication Bias in Meta-Analysis: Confidence Intervals for Rosenthal's
  Fail-Safe Number","  The purpose of the present paper is to assess the efficacy of confidence
intervals for Rosenthal's fail-safe number. Although Rosenthal's estimator is
highly used by researchers, its statistical properties are largely unexplored.
First of all, we developed statistical theory which allowed us to produce
confidence intervals for Rosenthal's fail-safe number.This was produced by
discerning whether the number of studies analysed in a meta-analysis is fixed
or random. Each case produces different variance estimators. For a given number
of studies and a given distribution, we provided five variance estimators.
Confidence intervals are examined with a normal approximation and a
nonparametric bootstrap. The accuracy of the different confidence interval
estimates was then tested by methods of simulation under different
distributional assumptions. The half normal distribution variance estimator has
the best probability coverage. Finally, we provide a table of lower confidence
intervals for Rosenthal's estimator.
"
1509.03056,2015-09-11,A Conversation with Professor Tadeusz Cali\'{n}ski,"  Tadeusz Cali\'{n}ski was born in Pozna\'{n}, Poland in 1928. Despite the
absence of formal secondary eduction for Poles during the Second World War, he
entered the University of Pozna\'{n} in 1948, initially studying agronomy and
in later years mathematics. From 1953 to 1988 he taught statistics, biometry
and experimental design at the Agricultural University of Pozna\'{n}. During
this period he founded and developed the Pozna\'{n} inter-university school of
mathematical statistics and biometry, which has become one of the most
important schools of this type in Poland and beyond. He has supervised 24 Ph.D.
students, many of whom are currently professors at a variety of universities.
He is now Professor Emeritus. Among many awards, in 1995 Professor Cali\'{n}ski
received the Order of Polonia Restituta for his outstanding achievements in the
fields of Education and Science. In 2012 the Polish Statistical Society awarded
him The Jerzy Sp{\l}awa-Neyman Medal for his contribution to the development of
research in statistics in Poland. Professor Cali\'{n}ski in addition has
Doctoral Degrees honoris causa from the Agricultural University of Pozna\'{n}
and the Warsaw University of Life Sciences. His research interests include
mathematical statistics and biometry, with applications to agriculture, natural
sciences, biology and genetics. He has published over 140 articles in
scientific journals as well as, with Sanpei Kageyama, two important books on
the randomization approach to the design and analysis of experiments. He has
been extremely active and successful in initiating and contributing to fruitful
international research cooperation between Polish statisticians and
biometricians and their colleagues in various countries, particularly in the
Netherlands, France, Italy, Great Britain, Germany, Japan and Portugal. The
conversations in addition cover the history of biometry and experimental design
in Poland and the early influence of British statisticians.
"
1509.03068,2015-09-11,A Conversation with Alan Gelfand,"  Alan E. Gelfand was born April 17, 1945, in the Bronx, New York. He attended
public grade schools and did his undergraduate work at what was then called
City College of New York (CCNY, now CUNY), excelling at mathematics. He then
surprised and saddened his mother by going all the way across the country to
Stanford to graduate school, where he completed his dissertation in 1969 under
the direction of Professor Herbert Solomon, making him an academic grandson of
Herman Rubin and Harold Hotelling. Alan then accepted a faculty position at the
University of Connecticut (UConn) where he was promoted to tenured associate
professor in 1975 and to full professor in 1980. A few years later he became
interested in decision theory, then empirical Bayes, which eventually led to
the publication of Gelfand and Smith [J. Amer. Statist. Assoc. 85 (1990)
398-409], the paper that introduced the Gibbs sampler to most statisticians and
revolutionized Bayesian computing. In the mid-1990s, Alan's interests turned
strongly to spatial statistics, leading to fundamental contributions in
spatially-varying coefficient models, coregionalization, and spatial boundary
analysis (wombling). He spent 33 years on the faculty at UConn, retiring in
2002 to become the James B. Duke Professor of Statistics and Decision Sciences
at Duke University, serving as chair from 2007-2012. At Duke, he has continued
his work in spatial methodology while increasing his impact in the
environmental sciences. To date, he has published over 260 papers and 6 books;
he has also supervised 36 Ph.D. dissertations and 10 postdocs. This interview
was done just prior to a conference of his family, academic descendants, and
colleagues to celebrate his 70th birthday and his contributions to statistics
which took place on April 19-22, 2015 at Duke University.
"
1509.03346,2015-09-14,A multi-dimensional stream and its signature representation,"  The signature of a path is an essential object in the theory of rough paths.
The signature representation of the data stream can recover standard
statistics, e.g. the moments of the data stream. The classification of random
walks indicates the advantages of using the signature of a stream as the
feature set for machine learning.
"
1509.06721,2015-09-23,Designed Sampling from Large Databases for Controlled Trials,"  The increasing prevalence of rich sources of data and the availability of
electronic medical record databases and electronic registries opens tremendous
opportunities for enhancing medical research. For example, controlled trials
are ubiquitously used to investigate the effect of a medical treatment, perhaps
dependent on a set of patient covariates, and traditional approaches have
relied primarily on randomized patient sampling and allocation to treatment and
control group. However, when covariate data for a large cohort group of
patients have already been collected and are available in a database, one can
potentially design a treatment/control sample and allocation that provides far
better estimates of the covariate-dependent effects of the treatment. In this
paper, we develop a new approach that uses optimal design of experiments (DOE)
concepts to accomplish this objective. The approach selects the patients for
the treatment and control samples upfront, based on their covariate values, in
a manner that optimizes the information content in the data. For the optimal
sample selection, we develop simple guidelines and an optimization algorithm
that provides solutions that are substantially better than random sampling.
Moreover, our approach causes no sampling bias in the estimated effects, for
the same reason that DOE principles do not bias estimated effects. We test our
method with a simulation study based on a testbed data set containing
information on the effect of statins on low-density lipoprotein (LDL)
cholesterol.
"
1509.07804,2015-12-23,From Statistician to Data Scientist,"  According to a recent report from the European Commission, the world
generates every minute 1.7 million of billions of data bytes, the equivalent of
360,000 DVDs, and companies that build their decision-making processes by
exploiting these data increase their productivity. The treatment and
valorization of massive data has consequences on the employment of graduate
students in statistics. Which additional skills do students trained in
statistics need to acquire to become data scientists ? How to evolve training
so that future graduates can adapt to rapid changes in this area, without
neglecting traditional jobs and the fundamental and lasting foundation for the
training? After considering the notion of big data and questioning the
emergence of a ""new"" science: Data Science, we present the current developments
in the training of engineers in Mathematical and Modeling at INSA Toulouse.
"
1509.08307,2015-09-29,An Outline of the Bayesian Decision Theory,"  In this paper we give an outline on the Bayesian Decision Theory.
"
1510.00292,2017-08-02,"On Classification Issues within Ensemble-Based Complex System Simulation
  Tasks","  Contemporary tasks of complex system simulation are often related to the
issue of uncertainty management. It comes from the lack of information or
knowledge about the simulated system as well as from restrictions of the model
set being used. One of the powerful tools for the uncertainty management is
ensemble-based simulation, which uses variation in input or output data, model
parameters, or available versions of models to improve the simulation
performance. Furthermore the system of models for complex system simulation
(especially in case of hiring ensemble-based approach) can be considered as a
complex system. As a result, the identification of the complex model's
structure and parameters provide additional sources of uncertainty to be
managed. Within the presented work we are developing a conceptual and
technological approach to manage the ensemble-based simulation taking into
account changing states of both simulated system and system of models within
the ensemble-based approach. The states of these systems are considered as a
subject of classification with consequent inference of better strategies for
ensemble evolution over the simulation time and ensemble aggregation. Here the
ensemble evolution enables implementation of dynamic reactive solutions which
can automatically conform to the changing states of both systems. The ensemble
aggregation can be considered within a scope of averaging (regression way) or
selection (classification way, which complement the classification mentioned
earlier) approach. The technological basis for such approach includes
ensemble-based simulation techniques using domain-specific software combined
within a composite application; data science approaches for analysis of
available datasets (simulation data, observations, situation assessment etc.);
and machine learning algorithms for classes identification, ensemble management
and knowledge acquisition.
"
1510.01993,2016-09-21,"Sensor Selection for Target Tracking in Wireless Sensor Networks with
  Uncertainty","  In this paper, we propose a multiobjective optimization framework for the
sensor selection problem in uncertain Wireless Sensor Networks (WSNs). The
uncertainties of the WSNs result in a set of sensor observations with
insufficient information about the target. We propose a novel mutual
information upper bound (MIUB) based sensor selection scheme, which has low
computational complexity, same as the Fisher information (FI) based sensor
selection scheme, and gives estimation performance similar to the mutual
information (MI) based sensor selection scheme. Without knowing the number of
sensors to be selected a priori, the multiobjective optimization problem (MOP)
gives a set of sensor selection strategies that reveal different trade-offs
between two conflicting objectives: minimization of the number of selected
sensors and minimization of the gap between the performance metric (MIUB and
FI) when all the sensors transmit measurements and when only the selected
sensors transmit their measurements based on the sensor selection strategy.
Illustrative numerical results that provide valuable insights are presented.
"
1511.00634,2018-08-02,A Simple and Adaptive Dispersion Regression Model for Count Data,"  Regression for count data is widely performed by models such as Poisson,
negative binomial (NB) and zero-inflated regression. A challenge often faced by
practitioners is the selection of the right model to take into account
dispersion, which typically occurs in count datasets. It is highly desirable to
have a unified model that can automatically adapt to the underlying dispersion
and that can be easily implemented in practice. In this paper, a discrete
Weibull regression model is shown to be able to adapt in a simple way to
different types of dispersions relative to Poisson regression: overdispersion,
underdispersion and covariate-specific dispersion. Maximum likelihood can be
used for efficient parameter estimation. The description of the model,
parameter inference and model diagnostics is accompanied by simulated and real
data analyses.
"
1511.04351,2016-11-02,"A Scalable Framework for NBA Player and Team Comparisons Using Player
  Tracking Data","  The release of NBA player tracking data greatly enhances the granularity and
dimensionality of basketball statistics used to evaluate and compare player
performance. However, the high dimensionality of this new data source can be
troublesome as it demands more computational resources and reduces the ability
to easily interpret findings. Therefore, we must find a way to reduce the
dimensionality of the data while retaining the ability to differentiate and
compare player performance.
  In this paper, Principal Component Analysis (PCA) is used to identify four
principal components that account for 68% of the variation in player tracking
data from the 2013-2014 regular season and intuitive interpretations of these
new dimensions are developed by examining the statistics that influence them
the most. In this new high variance, low dimensional space, you can easily
compare statistical profiles across any or all of the principal component
dimensions to evaluate characteristics that make certain players and teams
similar or unique. A simple measure of similarity between two player or team
statistical profiles based on the four principal component scores is also
constructed. The Statistical Diversity Index (SDI) allows for quick and
intuitive comparisons using the entirety of the player tracking data. As new
statistics emerge, this framework is scalable as it can incorporate existing
and new data sources by reconstructing the principal component dimensions and
SDI for improved comparisons. Using principal component scores and SDI, several
use cases are presented for improved personnel management.
"
1511.06013,2015-11-20,Statistical Engineering: An Idea Whose Time Has Come?,"  Several authors, including the American Statistician (ASA), have noted the
challenges facing statisticians when attacking large, complex, unstructured
problems, as opposed to well-defined textbook problems. Clearly, the standard
paradigm of selecting the one ""correct"" statistical method for such problems is
not sufficient; a new paradigm is needed. Statistical engineering has been
proposed as a discipline that can provide a viable paradigm to attack such
problems, used in conjunction with sound statistical science. Of course, in
order to develop as a true discipline, statistical engineering needs a
well-developed theory, not just a formal definition and successful case
studies. This article documents and disseminates the current state of the
underlying theory of statistical engineering. Our purpose is to provide a
vehicle for applied statisticians to further enhance the practice of
statistics, and for academics so interested to continue development of the
underlying theory of statistical engineering.
"
1511.08180,2016-11-28,J. B. S. Haldane's Contribution to the Bayes Factor Hypothesis Test,"  This article brings attention to some historical developments that gave rise
to the Bayes factor for testing a point null hypothesis against a composite
alternative. In line with current thinking, we find that the conceptual
innovation - to assign prior mass to a general law - is due to a series of
three articles by Dorothy Wrinch and Sir Harold Jeffreys (1919, 1921, 1923).
However, our historical investigation also suggests that in 1932 J. B. S.
Haldane made an important contribution to the development of the Bayes factor
by proposing the use of a mixture prior comprising a point mass and a
continuous probability density. Jeffreys was aware of Haldane's work and it may
have inspired him to pursue a more concrete statistical implementation for his
conceptual ideas. It thus appears that Haldane may have played a much bigger
role in the statistical development of the Bayes factor than has hitherto been
assumed.
"
1512.02688,2015-12-10,"Modelling Hospital length of stay using convolutive mixtures
  distributions","  Length of hospital stay (LOS) is an important indicator of the hospital
activity and management of health care. The skewness in the distribution of LOS
poses problems in statistical modelling because it fails to adequately follow
the usual traditional distribution such as the log-normal distribution. The aim
of this work is to model the variable LOS using the convolution of two
distributions; a technique well known in the signal processing community. The
specificity of that model is that the variable of interest is considered to be
the resulting sum of two random variables with different distributions. One of
the variables will feature the patient-related factors in terms their need to
recover from their admission condition, while the other models the hospital
management process such as the discharging process. Two estimation procedures
are proposed. One is the classical maximum likelihood, while the other relates
to the expectation maximisation algorithm. We will present some results
obtained by applying this model to a set of real data from a group of hospitals
in Victoria (Australia).
"
1512.03533,2015-12-14,A Conversation with Nan Laird,"  Nan McKenzie Laird is the Harvey V. Fineberg Professor of Biostatistics at
the Harvard T. H. Chan School of Public Health. She has made fundamental
contributions to statistical methods for longitudinal data analysis, missing
data and meta-analysis. In addition, she is widely known for her work in
statistical genetics and in statistical methods for psychiatric epidemiology.
Her 1977 paper with Dempster and Rubin on the EM algorithm is among the top 100
most highly cited papers in science [Nature 524 (2014) 550-553]. Her applied
work on medical practice errors is widely cited among the medical malpractice
community. Nan was born in Gainesville, Florida, in 1943. Shortly thereafter,
her parents Angus McKenzie Laird and Myra Adelia Doyle, moved to Tallahassee,
Florida, with Nan and her sister Victoria Mell. Nan started college at Rice
University in 1961, but then transferred to the University of Georgia where she
received a B.S. in Statistics in 1969 and was elected to Phi Beta Kappa. After
graduation Nan worked at the Massachusetts Institute of Technology Draper
Laboratories where she worked on Kalman filtering for the Apollo Man to the
Moon Program. She enrolled in the Statistics Department at Harvard University
in 1971 and received her Ph.D. in 1975. She joined the faculty of Harvard
School of Public Health upon receiving her Ph.D., and remains there as research
professor, after her retirement in 2015. The interview was conducted in Boston,
Massachusetts, in July 2014. A link to Nan's full CV can be found at
\surlwww.hsph.harvard.edu/nan-laird/.
"
1512.04858,2016-11-07,"Solution for the Indefinite Integral of the Standard Normal Probability
  Density Function","  Conventional wisdom assumes that the indefinite integral of the probability
density function for the standard normal distribution cannot be expressed in
finite elementary terms. While this is true, there is an expression for this
anti-derivative in infinite elementary terms that, when being differentiated,
directly yields the standard normal density function. We derive this function
using infinite partial integration and review its relation to the cumulative
distribution function for the standard normal distribution and the error
function.
"
1512.05307,2015-12-17,"Implicit Regression: Detecting Constants and Inverse Relationships with
  Bivariate Random Error","  In 2011, Wooten introduced Non-Response Analysis the founding theory in
Implicit Regression where Implicit Regression treats the variables implicitly
as codependent variables and not as an explicit function with dependent or
independent variables as in standard regression. The motivation of this paper
is to introduce methods of implicit regression to determine the constant nature
of a variable or the interactive term, and address inverse relationship among
measured variables with random error present in both directions.
"
1512.08773,2015-12-31,"Hot Hands, Streaks and Coin-flips: Numerical Nonsense in the New York
  Times","  The existence of ""Hot Hands"" and ""Streaks"" in sports and gambling is hotly
debated, but there is no uncertainty about the recent batting-average of the
New York Times: it is now two-for-two in mangling and misunderstanding
elementary concepts in probability and statistics; and mixing up the key points
in a recent paper that re-examines earlier work on the statistics of streaks.
In so doing, it's high-visibility articles have added to the general-public's
confusion about probability, making it seem mysterious and paradoxical when it
needn't be. However, those articles make excellent case studies on how to get
it wrong, and for discussions in high-school and college classes focusing on
quantitative reasoning, data analysis, probability and statistics. What I have
written here is intended for that audience.
"
1512.08819,2015-12-31,Joint limiting laws for high-dimensional independence tests,"  Testing independence is of significant interest in many important areas of
large-scale inference. Using extreme-value form statistics to test against
sparse alternatives and using quadratic form statistics to test against dense
alternatives are two important testing procedures for high-dimensional
independence. However, quadratic form statistics suffer from low power against
sparse alternatives, and extreme-value form statistics suffer from low power
against dense alternatives with small disturbances and may have size
distortions due to its slow convergence. For real-world applications, it is
important to derive powerful testing procedures against more general
alternatives. Based on intermediate limiting distributions, we derive
(model-free) joint limiting laws of extreme-value form and quadratic form
statistics, and surprisingly, we prove that they are asymptotically
independent. Given such asymptotic independencies, we propose (model-free)
testing procedures to boost the power against general alternatives and also
retain the correct asymptotic size. Under the high-dimensional setting, we
derive the closed-form limiting null distributions, and obtain their explicit
rates of uniform convergence. We prove their consistent statistical powers
against general alternatives. We demonstrate the performance of our proposed
test statistics in simulation studies. Our work provides very helpful insights
to high-dimensional independence tests, and fills an important gap.
"
1512.09016,2017-02-03,Pairwise Markov properties for regression graphs,"  With a sequence of regressions, one may generate joint probability
distributions. One starts with a joint, marginal distribution of context
variables having possibly a concentration graph structure and continues with an
ordered sequence of conditional distributions, named regressions in joint
responses. The involved random variables may be discrete, continuous or of both
types. Such a generating process specifies for each response a conditioning set
which contains just its regressor variables and it leads to at least one valid
ordering of all nodes in the corresponding regression graph which has three
types of edge; one for undirected dependences among context variables, another
for undirected dependences among joint responses and one for any directed
dependence of a response on a regressor variable. For this regression graph,
there are several definitions of pairwise Markov properties, where each
interprets the conditional independence associated with a missing edge in the
graph in a different way. We explain how these properties arise, prove their
equivalence for compositional graphoids and point at the equivalence of each
one of them to the global Markov property.
"
1601.00665,2016-01-06,"Impugning Randomness, Convincingly","  John organized a state lottery and his wife won the main prize. You may feel
that the event of her winning wasn't particularly random, but how would you
argue that in a fair court of law? Traditional probability theory does not even
have the notion of random events. Algorithmic information theory does, but it
is not applicable to real-world scenarios like the lottery one. We attempt to
rectify that.
"
1601.08099,2016-02-15,"Chaos in Fractionally Integrated Generalized Autoregressive Conditional
  Heteroskedastic Processes","  Fractionally integrated generalized autoregressive conditional
heteroskedasticity (FIGARCH) arises in modeling of financial time series.
FIGARCH is essentially governed by a system of nonlinear stochastic difference
equations ${u_t}$ = ${z_t}$ $(1-\sum\limits_{j=1}^q \beta_j L^j)\sigma_{t}^2 =
\omega+(1-\sum\limits_{j=1}^q \beta_j L^j - (\sum\limits_{k=1}^p \varphi_k L^k)
(1-L)^d) u_t^2$, where $\omega\in$ R, and $\beta_j\in$ R are constant
parameters, $\{u_t\}_{{t\in}^+}$ and $\{\sigma_t\}_{{t\in}^+}$ are the discrete
time real valued stochastic processes which represent FIGARCH (p,d,q) and
stochastic volatility, respectively. Moreover, L is the backward shift
operator, i.e. $L^d u_t \equiv u_{t-d}$ (d is the fractional differencing
parameter 0$<$d$<$1).
  In this work, we have studied the chaoticity properties of FIGARCH (p,d,q)
processes by computing mutual information, correlation dimensions, FNNs (False
Nearest Neighbour), the Lyapunov exponents, and for both the stochastic
difference equation given above and for the financial time series. We have
observed that maximal Lyapunov exponents are negative, therefore, it can be
suggested that FIGARCH (p,d,q) is not deterministic chaotic process.
"
1602.01347,2018-10-23,"Bayesian prediction for physical models with application to the
  optimization of the synthesis of pharmaceutical products using chemical
  kinetics","  Quality control in industrial processes is increasingly making use of prior
scientific knowledge, often encoded in physical models that require numerical
approximation. Statistical prediction, and subsequent optimization, is key to
ensuring the process output meets a specification target. However, the
numerical expense of approximating the models poses computational challenges to
the identification of combinations of the process factors where there is
confidence in the quality of the response. Recent work in Bayesian computation
and statistical approximation (emulation) of expensive computational models is
exploited to develop a novel strategy for optimizing the posterior probability
of a process meeting specification. The ensuing methodology is motivated by,
and demonstrated on, a chemical synthesis process to manufacture a
pharmaceutical product, within which an initial set of substances evolve
according to chemical reactions, under certain process conditions, into a
series of new substances. One of these substances is a target pharmaceutical
product and two are unwanted by-products. The aim is to determine the
combinations of process conditions and amounts of initial substances that
maximize the probability of obtaining sufficient target pharmaceutical product
whilst ensuring unwanted by-products do not exceed a given level. The
relationship between the factors and amounts of substances of interest is
theoretically described by the solution to a system of ordinary differential
equations incorporating temperature dependence. Using data from a small
experiment, it is shown how the methodology can approximate the multivariate
posterior predictive distribution of the pharmaceutical target and by-products,
and therefore identify suitable operating values. Materials to replicate the
analysis can be found at www.github.com/amo105/chemicalkinetics.
"
1602.01370,2016-02-08,Sensory evaluation of commercial coffee brands in Colombia,"  Colombian coffee farmers have traditionally focused their efforts on
activities including seeding, planting and drying. Strategic issues to
successfully compete in the industry, such as branding, marketing and consumer
research, have been neglected. In this research, we apply a type of sensory
analysis, based on several statistical techniques used to investigate the key
features of ten different brands of Colombian coffee. A panel composed of 32
judges investigated nine different attributes related to flavour, fragrance,
sweetness and acidity, among others. The last section presents the conclusions
reached regarding customer preference and brands profiles.
"
1602.01815,2016-06-14,"Exploiting plume structure to decode gas source distance using
  metal-oxide gas sensors","  Estimating the distance of a gas source is important in many applications of
chemical sensing, like e.g. environmental monitoring, or chemically-guided
robot navigation. If an estimation of the gas concentration at the source is
available, source proximity can be estimated from the time-averaged gas
concentration at the sensing site. However, in turbulent environments, where
fast concentration fluctuations dominate, comparably long measurements are
required to obtain a reliable estimate. A lesser known feature that can be
exploited for distance estimation in a turbulent environment lies in the
relationship between source proximity and the temporal variance of the local
gas concentration - the farther the source, the more intermittent are gas
encounters. However, exploiting this feature requires measurement of changes in
gas concentration on a comparably fast time scale, that have up to now only
been achieved using photo-ionisation detectors. Here, we demonstrate that by
appropriate signal processing, off-the-shelf metal-oxide sensors are capable of
extracting rapidly fluctuating features of gas plumes that strongly correlate
with source distance. We show that with a straightforward analysis method it is
possible to decode events of large, consistent changes in the measured signal,
so-called 'bouts'. The frequency of these bouts predicts the distance of a gas
source in wind-tunnel experiments with good accuracy. In addition, we found
that the variance of bout counts indicates cross-wind offset to the centreline
of the gas plume. Our results offer an alternative approach to estimating gas
source proximity that is largely independent of gas concentration, using
off-the-shelf metal-oxide sensors. The analysis method we employ demands very
few computational resources and is suitable for low-power microcontrollers.
"
1602.03434,2016-02-11,"Impact of degree truncation on the spread of a contagious process on
  networks","  Understanding how person-to-person contagious processes spread through a
population requires accurate information on connections between population
members. However, such connectivity data, when collected via interview, is
often incomplete due to partial recall, respondent fatigue or study design,
e.g., fixed choice designs (FCD) truncate out-degree by limiting the number of
contacts each respondent can report. Past research has shown how FCD truncation
affects network properties, but its implications for predicted speed and size
of spreading processes remain largely unexplored. To study the impact of degree
truncation on spreading processes, we generated collections of synthetic
networks containing specific properties (degree distribution,
degree-assortativity, clustering), and also used empirical social network data
from 75 villages in Karnataka, India. We simulated FCD using various truncation
thresholds and ran a susceptible-infectious-recovered (SIR) process on each
network. We found that spreading processes propagated on truncated networks
resulted in slower and smaller epidemics, with a sudden decrease in prediction
accuracy at a level of truncation that varied by network type. Our results have
implications beyond FCD to truncation due to any limited sampling from a larger
network. We conclude that knowledge of network structure is important for
understanding the accuracy of predictions of process spread on degree truncated
networks.
"
1602.03926,2016-05-11,"Modelling the level of adoption of analytical tools; An implementation
  of multi-criteria evidential reasoning","  In the future, competitive advantages will be given to organisations that can
extract valuable information from massive data and make better decisions. In
most cases, this data comes from multiple sources. Therefore, the challenge is
to aggregate them into a common framework in order to make them meaningful and
useful. This paper will first review the most important multi-criteria decision
analysis methods (MCDA) existing in current literature. We will offer a novel,
practical and consistent methodology based on a type of MCDA, to aggregate data
from two different sources into a common framework. Two datasets that are
different in nature but related to the same topic are aggregated to a common
scale by implementing a set of transformation rules. This allows us to generate
appropriate evidence for assessing and finally prioritising the level of
adoption of analytical tools in four types of companies. A numerical example is
provided to clarify the form for implementing this methodology. A six-step
process is offered as a guideline to assist engineers, researchers or
practitioners interested in replicating this methodology in any situation where
there is a need to aggregate and transform multiple source data.
"
1602.04091,2016-02-15,Interactive graphics for functional data analyses,"  Although there are established graphics that accompany the most common
functional data analyses, generating these graphics for each dataset and
analysis can be cumbersome and time consuming. Often, the barriers to
visualization inhibit useful exploratory data analyses and prevent the
development of intuition for a method and its application to a particular
dataset. The refund.shiny package was developed to address these issues for
several of the most common functional data analyses. After conducting an
analysis, the plot_shiny() function is used to generate an interactive
visualization environment that contains several distinct graphics, many of
which are updated in response to user input. These visualizations reduce the
burden of exploratory analyses and can serve as a useful tool for the
communication of results to non-statisticians.
"
1602.06696,2016-02-23,A note on basis dimension selection in generalized additive modelling,"  Two new approaches for checking the dimension of the basis functions when
using penalized regression smoothers are presented. The first approach is a
test for adequacy of the basis dimension based on an estimate of the residual
variance calculated by differencing residuals that are neighbours according to
the smooth covariates. The second approach is based on estimated degrees of
freedom for a smooth of the model residuals with respect to the model
covariates. In comparison with basis dimension selection algorithms based on
smoothness selection criterion (GCV, AIC, REML) the above procedures are
computationally efficient enough for routine use as part of model checking.
"
1602.07836,2016-03-01,A Bayesian baseline for belief in uncommon events,"  The plausibility of uncommon events and miracles based on testimony of such
an event has been much discussed. When analyzing the probabilities involved, it
has mostly been assumed that the common events can be taken as data in the
calculations. However, we usually have only testimonies for the common events.
While this difference does not have a significant effect on the inductive part
of the inference, it has a large influence on how one should view the
reliability of testimonies. In this work, a full Bayesian solution is given for
the more realistic case, where one has a large number of testimonies for a
common event and one testimony for an uncommon event. It is seen that, in order
for there to be a large amount of testimonies for a common event, the
testimonies will probably be quite reliable. For this reason, because the
testimonies are quite reliable based on the testimonies for the common events,
the probability for the uncommon event, given a testimony for it, is also
higher. Hence, one should be more open-minded when considering the plausibility
of uncommon events.
"
1603.00544,2016-05-31,On the capacity of information processing systems,"  We propose and analyze a family of information processing systems, where a
finite set of experts or servers are employed to extract information about a
stream of incoming jobs. Each job is associated with a hidden label drawn from
some prior distribution. An inspection by an expert produces a noisy outcome
that depends both on the job's hidden label and the type of the expert, and
occupies the expert for a finite time duration. A decision maker's task is to
dynamically assign inspections so that the resulting outcomes can be used to
accurately recover the labels of all jobs, while keeping the system stable.
Among our chief motivations are applications in crowd-sourcing, diagnostics,
and experiment designs, where one wishes to efficiently learn the nature of a
large number of items, using a finite pool of computational resources or human
agents.
  We focus on the capacity of such an information processing system. Given a
level of accuracy guarantee, we ask how many experts are needed in order to
stabilize the system, and through what inspection architecture. Our main result
provides an adaptive inspection policy that is asymptotically optimal in the
following sense: the ratio between the required number of experts under our
policy and the theoretical optimal converges to one, as the probability of
error in label recovery tends to zero.
"
1603.00738,2017-09-27,"Mandelbrot's 1/f fractional renewal models of 1963-67: The non-ergodic
  missing link between change points and long range dependence","  The problem of 1/f noise has been with us for about a century. Because it is
so often framed in Fourier spectral language, the most famous solutions have
tended to be the stationary long range dependent (LRD) models such as
Mandelbrot's fractional Gaussian noise. In view of the increasing importance to
physics of non-ergodic fractional renewal models, I present preliminary results
of my research into the history of Mandelbrot's very little known work in that
area from 1963-67. I speculate about how the lack of awareness of this work in
the physics and statistics communities may have affected the development of
complexity science, and I discuss the differences between the Hurst effect, 1/f
noise and LRD, concepts which are often treated as equivalent.
"
1603.02373,2018-03-05,On point processes in multitarget tracking,"  The finite-set statistics (FISST) approach to multitarget tracking was
introduced in the mid-1990s. Its current extended form dates from 2001. In
2008, an ""elementary"" alternative to FISST was proposed, based on ""finite point
processes"" rather than RFS's. This was accompanied by single-sensor and
multisensor versions of a claimed generalization of the PHD filter, the
""iFilter."" Then in 2013 in the Journal of Advances in Information Fusion (JAIF)
and elsewhere, the same author went on to claim that the FISST
p.g.fl./functional derivative approach is actually ""due to"" (a ""corollary"" of)
a 50-year-old pure-mathematics paper by Moyal; and described a ""point process""
p.g.fl./functional derivative approach to multitarget tracking supposedly based
on it. In this paper it is shown that: (1)non-RFS point processes are a
phenomenologically erroneous foundation for multitarget tracking; (2) nearly
every equation, concept, discussion, derivation, and methodology in the JAIF
paper originally appeared in FISST publications, without being so attributed;
(3) FISST cannot possibly be ""due to Moyal""; (4) the ""point process"" approach
described in JAIF differs from FISST only in regard to terminology and
notation, and thus in this sense appears to be an obscured, phenomenologically
erroneous, and improperly attributed copy of FISST. It is also shown that the
derivations of the single-sensor and multisensory iFilter appear to have had
major errors, as did a subsequent recasting of the multisensor iFilter as a
""traffic mapping filter.""
"
1603.04912,2018-01-08,Dynamic Data in the Statistics Classroom,"  The call for using real data in the classroom has long meant using datasets
which are culled, cleaned, and wrangled prior to any student working with the
observations. However, an important part of teaching statistics should include
actually retrieving data from the Internet. Nowadays, there are many different
sources of data that are continually updated by the organization hosting the
data website. The R tools to download such dynamic data have improved in such a
way to make accessing the data possible even in an introductory statistics
class. We provide five full analyses on dynamic data as well as an additional
nine sources of dynamic data that can be brought into the classroom. The goal
of our work is to demonstrate that using dynamic data can have a short learning
curve, even for introductory students or faculty unfamiliar with the landscape.
The examples provided are unlikely to create expert data scrapers, but they
should help motivate students and faculty toward more engaged use of online
data sources.
"
1603.07408,2016-03-25,"Fisher, Neyman-Pearson or NHST? A Tutorial for Teaching Data Testing","  Despite frequent calls for the overhaul of null hypothesis significance
testing (NHST), this controversial procedure remains ubiquitous in behavioral,
social and biomedical teaching and research. Little change seems possible once
the procedure becomes well ingrained in the minds and current practice of
researchers; thus, the optimal opportunity for such change is at the time the
procedure is taught, be this at undergraduate or at postgraduate levels. This
paper presents a tutorial for the teaching of data testing procedures, often
referred to as hypothesis testing theories. The first procedure introduced is
the approach to data testing followed by Fisher (tests of significance); the
second is the approach followed by Neyman and Pearson (tests of acceptance);
the final procedure is the incongruent combination of the previous two theories
into the current approach (NSHT). For those researchers sticking with the
latter, two compromise solutions on how to improve NHST conclude the tutorial.
"
1603.08830,2018-12-27,"Reduced Perplexity: A simplified perspective on assessing probabilistic
  forecasts","  A simple, intuitive approach to the assessment of probabilistic inferences is
introduced. The Shannon information metrics are translated to the probability
domain. The translation shows that the negative logarithmic score and the
geometric mean are equivalent measures of the accuracy of a probabilistic
inference. Thus there is both a quantitative reduction in perplexity, which is
the inverse of the geometric mean of the probabilities, as good inference
algorithms reduce the uncertainty and a qualitative reduction due to the
increased clarity between the original set of probabilistic forecasts and their
central tendency, the geometric mean. Further insight is provided by showing
that the R\'enyi and Tsallis entropy functions translated to the probability
domain are both the weighted generalized mean of the distribution. The
generalized mean of probabilistic forecasts forms a spectrum of performance
metrics referred to as a Risk Profile. The arithmetic mean is used to measure
the decisiveness, while the -2/3 mean is used to measure the robustness.
"
1603.09406,2017-04-26,Risk contagion under regular variation and asymptotic tail independence,"  Risk contagion concerns any entity dealing with large scale risks. Suppose
(X,Y) denotes a risk vector pertaining to two components in some system. A
relevant measurement of risk contagion would be to quantify the amount of
influence of high values of Y on X. This can be measured in a variety of ways.
In this paper, we study two such measures: the quantity E[max(X-t,0)|Y > t]
called Marginal Mean Excess (MME) as well as the related quantity E[X|Y > t]
called Marginal Expected Shortfall (MES). Both quantities are indicators of
risk contagion and useful in various applications ranging from finance,
insurance and systemic risk to environmental and climate risk. We work under
the assumptions of multivariate regular variation, hidden regular variation and
asymptotic tail independence for the risk vector (X,Y). Many broad and useful
model classes satisfy these assumptions. We present several examples and derive
the asymptotic behavior of both MME and MES as the threshold t tends to
infinity. We observe that although we assume asymptotic tail independence in
the models, MME and MES converge to 1 under very general conditions; this
reflects that the underlying weak dependence in the model still remains
significant. Besides the consistency of the empirical estimators, we introduce
an extrapolation method based on extreme value theory to estimate both MME and
MES for high thresholds t where little data are available. We show that these
estimators are consistent and illustrate our methodology in both simulated and
real data sets.
"
1603.09453,2016-04-01,An overview and perspective on social network monitoring,"  In this expository paper we give an overview of some statistical methods for
the monitoring of social networks. We discuss the advantages and limitations of
various methods as well as some relevant issues. One of our primary
contributions is to give the relationships between network monitoring methods
and monitoring methods in engineering statistics and public health
surveillance. We encourage researchers in the industrial process monitoring
area to work on developing and comparing the performance of social network
monitoring methods. We also discuss some of the issues in social network
monitoring and give a number of research ideas.
"
1604.01455,2019-01-24,Picking Winners in Daily Fantasy Sports Using Integer Programming,"  We consider the problem of selecting a portfolio of entries of fixed
cardinality for contests with top-heavy payoff structures, i.e. most of the
winnings go to the top-ranked entries. This framework is general and can be
used to model a variety of problems, such as movie studios selecting movies to
produce, venture capital firms picking start-up companies to invest in, or
individuals selecting lineups for daily fantasy sports contests, which is the
example we focus on here. We model the portfolio selection task as a
combinatorial optimization problem with a submodular objective function, which
is given by the probability of at least one entry winning. We then show that
this probability can be approximated using only pairwise marginal probabilities
of the entries winning when there is a certain structure on their joint
distribution. We consider a model where the entries are jointly Gaussian random
variables and present a closed form approximation to the objective function.
Building on this, we then consider a scenario where the entries are given by
sums of constrained resources and present an integer programming formulation to
construct the entries. Our formulation uses principles based on our theoretical
analysis to construct entries: we maximize the expected score of an entry
subject to a lower bound on its variance and an upper bound on its correlation
with previously constructed entries. To demonstrate the effectiveness of our
integer programming approach, we apply it to daily fantasy sports contests that
have top-heavy payoff structures. We find that our approach performs well in
practice. Using our integer programming approach, we are able to rank in the
top-ten multiple times in hockey and baseball contests with thousands of
competing entries. Our approach can easily be extended to other problems with
constrained resources and a top-heavy payoff structure.
"
1604.01844,2016-04-08,Statistical sensitiveness for science,"  Research often necessitates of samples, yet obtaining large enough samples is
not always possible. When it is, the researcher may use one of two methods for
deciding upon the required sample size: rules-of-thumb, quick yet uncertain,
and estimations for power, mathematically precise yet with the potential to
overestimate or underestimate sample sizes when effect sizes are unknown.
Misestimated sample sizes have negative repercussions in the form of increased
costs, abandoned projects or abandoned publication of non-significant results.
Here I describe a procedure for estimating sample sizes adequate for the
testing approach which is most common in the behavioural, social, and
biomedical sciences, that of tests of significance developed by Fisher. The
procedure focuses on a desired minimum effect size for the research at hand and
finds the minimum sample size required for capturing such effect size as a
statistically significant result. In a similar fashion than power analyses,
sensitiveness analyses can also be extended to finding the minimum effect for a
given sample size a priori as well as to calculating sensitiveness a
posteriori. The article provides a full tutorial for carrying out a
sensitiveness analysis, as well as empirical support via simulation
"
1604.02221,2017-03-09,Box-Cox symmetric distributions and applications to nutritional data,"  We introduce the Box-Cox symmetric class of distributions, which is useful
for modeling positively skewed, possibly heavy-tailed, data. The new class of
distributions includes the Box-Cox t, Box-Cox Cole-Gree, Box-Cox power
exponential distributions, and the class of the log-symmetric distributions as
special cases. It provides easy parameter interpretation, which makes it
convenient for regression modeling purposes. Additionally, it provides enough
flexibility to handle outliers. The usefulness of the Box-Cox symmetric models
is illustrated in applications to nutritional data.
"
1604.05224,2017-02-06,BFDA: A Matlab Toolbox for Bayesian Functional Data Analysis,"  We provide a MATLAB toolbox, BFDA, that implements a Bayesian hierarchical
model to smooth multiple functional data with the assumptions of the same
underlying Gaussian process distribution, a Gaussian process prior for the mean
function, and an Inverse-Wishart process prior for the covariance function.
This model-based approach can borrow strength from all functional data to
increase the smoothing accuracy, as well as estimate the mean-covariance
functions simultaneously. An option of approximating the Bayesian inference
process using cubic B-spline basis functions is integrated in BFDA, which
allows for efficiently dealing with high-dimensional functional data. Examples
of using BFDA in various scenarios and conducting follow-up functional
regression are provided. The advantages of BFDA include: (1) Simultaneously
smooths multiple functional data and estimates the mean-covariance functions in
a nonparametric way; (2) flexibly deals with sparse and high-dimensional
functional data with stationary and nonstationary covariance functions, and
without the requirement of common observation grids; (3) provides accurately
smoothed functional data for follow-up analysis.
"
1604.05418,2016-04-20,"Its All on the Square- The Importance of the Sum of Squares and Making
  the General Linear Model Simple","  Statistics is one of the most valuable of disciplines. Science is based on
proof and it alone produces results, other approaches are not, and do not.
Statistics is the only acceptable language of proof in science. Yet statistics
is difficult to understand for a large percentage of those who will be
evaluating and even doing research. Reasons for this difficulty may be that
statistics operates counter to the way people think, as well as the widespread
phobia of numeracy. Adding to the difficulty is that undergraduate textbooks
tend to make statistical tests seem to be an unorganized conglomeration of
unrelated procedures, and this leads to a failure of students to understand
that all of the parametric procedures they are studying in an introductory
course are ultimately doing the same thing and stem from common sources. In
statistics, precisely because the material is complex, the presentation must be
simple! This article endeavors to do just that.
"
1604.07391,2016-04-27,Ubiquity of Benfords law and emergence of the reciprocal distribution,"  We apply the Law of Total Probability to the construction of scale-invariant
probability distribution functions (pdfs), and require that probability
measures be dimensionless and unitless under a continuous change of scales. If
the scale-change distribution function is scale invariant then the constructed
distribution will also be scale invariant. Repeated application of this
construction on an arbitrary set of (normalizable) pdfs results again in
scale-invariant distributions. The invariant function of this procedure is
given uniquely by the reciprocal distribution, suggesting a kind of
universality. We separately demonstrate that the reciprocal distribution
results uniquely from requiring maximum entropy for size-class distributions
with uniform bin sizes.
"
1604.07397,2016-04-27,Teaching Data Science,"  We describe an introductory data science course, entitled Introduction to
Data Science, offered at the University of Illinois at Urbana-Champaign. The
course introduced general programming concepts by using the Python programming
language with an emphasis on data preparation, processing, and presentation.
The course had no prerequisites, and students were not expected to have any
programming experience. This introductory course was designed to cover a wide
range of topics, from the nature of data, to storage, to visualization, to
probability and statistical analysis, to cloud and high performance computing,
without becoming overly focused on any one subject. We conclude this article
with a discussion of lessons learned and our plans to develop new data science
courses.
"
1605.04391,2017-07-12,"Bayesian Lower Bounds for Dense or Sparse (Outlier) Noise in the RMT
  Framework","  Robust estimation is an important and timely research subject. In this paper,
we investigate performance lower bounds on the mean-square-error (MSE) of any
estimator for the Bayesian linear model, corrupted by a noise distributed
according to an i.i.d. Student's t-distribution. This class of prior
parametrized by its degree of freedom is relevant to modelize either dense or
sparse (accounting for outliers) noise. Using the hierarchical Normal-Gamma
representation of the Student's t-distribution, the Van Trees' Bayesian
Cram\'er-Rao bound (BCRB) on the amplitude parameters is derived. Furthermore,
the random matrix theory (RMT) framework is assumed, i.e., the number of
measurements and the number of unknown parameters grow jointly to infinity with
an asymptotic finite ratio. Using some powerful results from the RMT,
closed-form expressions of the BCRB are derived and studied. Finally, we
propose a framework to fairly compare two models corrupted by noises with
different degrees of freedom for a fixed common target signal-to-noise ratio
(SNR). In particular, we focus our effort on the comparison of the BCRBs
associated with two models corrupted by a sparse noise promoting outliers and a
dense (Gaussian) noise, respectively.
"
1605.05069,2016-05-18,Sobol' indices for problems defined in non-rectangular domains,"  A novel theoretical and numerical framework for the estimation of Sobol
sensitivity indices for models in which inputs are confined to a
non-rectangular domain (e.g., in presence of inequality constraints) is
developed. Two numerical methods, namely the quadrature integration method
which may be very efficient for problems of low and medium dimensionality and
the MC/QMC estimators based on the acceptance-rejection sampling method are
proposed for the numerical estimation of Sobol sensitivity indices. Several
model test functions with constraints are considered for which analytical
solutions for Sobol sensitivity indices were found. These solutions were used
as benchmarks for verifying numerical estimates. The method is shown to be
general and efficient.
"
1605.05814,2016-05-20,Some Mathematical Aspects of Price Optimisation,"  Calculation of an optimal tariff is a principal challenge for pricing
actuaries. In this contribution we are concerned with the renewal insurance
business discussing various mathematical aspects of calculation of an optimal
renewal tariff. Our motivation comes from two important actuarial tasks, namely
a) construction of an optimal renewal tariff subject to business and technical
constraints, and b) determination of an optimal allocation of certain premium
loadings. We consider both continuous and discrete optimisation and then
present several algorithmic sub-optimal solutions. Additionally, we explore
some simulation techniques. Several illustrative examples show both the
complexity and the importance of the optimisation approach.
"
1605.08753,2016-05-30,"Fairly Random: The Impact of Winning the Toss on the Probability of
  Winning","  In a competitive sport, every little thing matters. Yet, many sports leave
some large levers out of the reach of the teams, and in the hands of fate. In
cricket, world's second most popular sport by some measures, one such
lever---the toss---has been subject to much recent attention. Using a large
novel dataset of 44,224 cricket matches, we estimate the impact of winning the
toss on the probability of winning. The data suggest that winning the toss
increases the chance of winning by a small ($\sim$ 2.8\%) but significant
margin. The advantage varies heftily and systematically, by how closely matched
the competing teams are, and by playing conditions---tautologically, winning
the toss in conditions where the toss grants a greater advantage, for e.g., in
day and night matches, has a larger impact on the probability of winning.
"
1605.09081,2016-10-30,Understanding Convolutional Neural Networks,"  Convoulutional Neural Networks (CNNs) exhibit extraordinary performance on a
variety of machine learning tasks. However, their mathematical properties and
behavior are quite poorly understood. There is some work, in the form of a
framework, for analyzing the operations that they perform. The goal of this
project is to present key results from this theory, and provide intuition for
why CNNs work.
"
1606.00546,2016-06-03,"Forecasting wind power - Modeling periodic and non-linear effects under
  conditional heteroscedasticity","  In this article we present an approach that enables joint wind speed and wind
power forecasts for a wind park. We combine a multivariate seasonal time
varying threshold autoregressive moving average (TVARMA) model with a power
threshold generalized autoregressive conditional heteroscedastic (power-TGARCH)
model. The modeling framework incorporates diurnal and annual periodicity
modeling by periodic B-splines, conditional heteroscedasticity and a complex
autoregressive structure with non-linear impacts. In contrast to usually
time-consuming estimation approaches as likelihood estimation, we apply a
high-dimensional shrinkage technique. We utilize an iteratively re-weighted
least absolute shrinkage and selection operator (lasso) technique. It allows
for conditional heteroscedasticity, provides fast computing times and
guarantees a parsimonious and regularized specification, even though the
parameter space may be vast. We are able to show that our approach provides
accurate forecasts of wind power at a turbine-specific level for forecasting
horizons of up to 48 h (short- to medium-term forecasts).
"
1606.00770,2016-06-03,"Different numerical estimators for main effect global sensitivity
  indices","  The variance-based method of global sensitivity indices based on Sobol
sensitivity indices became very popular among practitioners due to its easiness
of interpretation. For complex practical problems computation of Sobol indices
generally requires a large number of function evaluations to achieve reasonable
convergence. Four different direct formulas for computing Sobol main effect
sensitivity indices are compared on a set of test problems for which there are
analytical results. These formulas are based on high-dimensional integrals
which are evaluated using MC and QMC techniques. Direct formulas are also
compared with a different approach based on the so-called double loop
reordering formula. It is found that the double loop reordering (DLR) approach
shows a superior performance among all methods both for models with independent
and dependent variables.
"
1606.01183,2016-06-06,Peter Hall's work on high-dimensional data and classification,"  In this article, I summarise Peter Hall's contributions to high-dimensional
data, including their geometric representations and variable selection methods
based on ranking. I also discuss his work on classification problems,
concluding with some personal reflections on my own interactions with him.
"
1606.01202,2016-06-06,When Does a Boltzmannian Equilibrium Exist?,"  The received wisdom in statistical mechanics is that isolated systems, when
left to themselves, approach equilibrium. But under what circumstances does an
equilibrium state exist and an approach to equilibrium take place? In this
paper we address these questions from the vantage point of the long-run
fraction of time definition of Boltzmannian equilibrium that we developed in
two recent papers (Werndl and Frigg 2015a, 2015b). After a short summary of
Boltzmannian statistical mechanics and our definition of equilibrium, we state
an existence theorem which provides general criteria for the existence of an
equilibrium state. We first illustrate how the theorem works with a toy
example, which allows us to illustrate the various elements of the theorem in a
simple setting. After commenting on the ergodic programme, we discuss
equilibria in a number of different gas systems: the ideal gas, the dilute gas,
the Kac gas, the stadium gas, the mushroom gas and the multi-mushroom gas. In
the conclusion we briefly summarise the main points and highlight open
questions.
"
1606.02352,2017-07-14,A statistical inference course based on p-values,"  Introductory statistical inference texts and courses treat the point
estimation, hypothesis testing, and interval estimation problems separately,
with primary emphasis on large-sample approximations. Here I present an
alternative approach to teaching this course, built around p-values,
emphasizing provably valid inference for all sample sizes. Details about
computation and marginalization are also provided, with several illustrative
examples, along with a course outline.
"
1606.03189,2016-06-13,Bringing Order to the Chaos in the Brickyard,"  An allegory published in 1963 titled Chaos in the Brickyard spoke to the
decline in the quality of research. In the intervening time greater awareness
of the issues and actions to improve research endeavors have emerged. Still,
problems persist. This paper is intended to clarify some of the challenges,
particularly with respect to quantitative research, then suggest ways to
improve the quality of published research. The paper highlights where feasible
refinements in analytical techniques can be made and provides a guide to
fundamental principles related to data analysis in research.
"
1606.05598,2016-08-01,Identifiability and testability in GRT with Individual Differences,"  Silbert and Thomas (2013) showed that failures of decisional separability are
not, in general, identifiable in fully parameterized $2 \times 2$ Gaussian GRT
models. A recent extension of $2 \times 2$ GRT models (GRTwIND) was developed
to solve this problem and a conceptually similar problem with the simultaneous
identifiability of means and marginal variances in GRT models. Central to the
ability of GRTwIND to solve these problems is the assumption of universal
perception, which consists of shared perceptual distributions modified by
attentional and global scaling parameters (Soto et al., 2015). If universal
perception is valid, GRTwIND solves both issues. In this paper, we show that
GRTwIND with universal perception and subject-specific failures of decisional
separability is mathematically, and thereby empirically, equivalent to a model
with decisional separability and failure of universal perception. We then
provide a formal proof of the fact that means and marginal variances are not,
in general, simultaneously identifiable in $2 \times 2$ GRT models, including
GRTwIND. These results can be taken to delineate precisely what the assumption
of universal perception must consist of. Based on these results and related
recent mathematical developments in the GRT framework, we propose that, in
addition to requiring a fixed subset of parameters to determine the location
and scale of any given GRT model, some subset of parameters must be set in GRT
models to fix the orthogonality of the modeled perceptual dimensions, a central
conceptual underpinning of the GRT framework. We conclude with a discussion of
perceptual primacy and its relationship to universal perception.
"
1606.09017,2016-06-30,Consider avoiding the .05 significance level,"  It is suggested that some shortcomings of Null Hypothesis Significance
Testing (NHST), viewed from the perspective of Bayesian statistics, turn benign
once the traditional threshold p value of .05 is substituted by a sufficiently
smaller value. To illustrate, the posterior probability of H0 stating P=.5,
given data that just render it rejected by NHST with a p value of .05 (and a
uniform prior), is shown here to be not much smaller than .50 for most values
of N below 100 (and even exceeds .50 for N>=100); in contrast, with a p value
of .001 posterior probability does not exceed .06 for N<=100 (neither .25 for
N<9000). Yet more interesting, posterior probability becomes quite independent
of N with a p value of .0001, hence practically satisfying the alpha postulate
- set by Cornfield (1966) as the condition for p value being a measure of
evidence in itself. In view of the low prospect that most researchers will soon
convert to use Bayesian statistics in any form, we thus suggest that
researchers who elect the conservative option of resorting to NHST be
encouraged to avoid as much as possible using a p value of .05 as a threshold
for rejecting H0. The analysis presented here may be used to discuss afresh
which level of threshold p value seems to be a reasonable, practical
substitute.
"
1607.00021,2016-07-04,The Simulator: An Engine to Streamline Simulations,"  The simulator is an R package that streamlines the process of performing
simulations by creating a common infrastructure that can be easily used and
reused across projects. Methodological statisticians routinely write
simulations to compare their methods to preexisting ones. While developing
ideas, there is a temptation to write ""quick and dirty"" simulations to try out
ideas. This approach of rapid prototyping is useful but can sometimes backfire
if bugs are introduced. Using the simulator allows one to remove the ""dirty""
without sacrificing the ""quick."" Coding is quick because the statistician
focuses exclusively on those aspects of the simulation that are specific to the
particular paper being written. Code written with the simulator is succinct,
highly readable, and easily shared with others. The modular nature of
simulations written with the simulator promotes code reusability, which saves
time and facilitates reproducibility. The syntax of the simulator leads to
simulation code that is easily human-readable. Other benefits of using the
simulator include the ability to ""step in"" to a simulation and change one
aspect without having to rerun the entire simulation from scratch, the
straightforward integration of parallel computing into simulations, and the
ability to rapidly generate plots, tables, and reports with minimal effort.
"
1607.00858,2016-07-05,Embracing Data Science,"  Statistics is running the risk of appearing irrelevant to today's
undergraduate students. Today's undergraduate students are familiar with data
science projects and they judge statistics against what they have seen.
Statistics, especially at the introductory level, should take inspiration from
data science so that the discipline is not seen as somehow lesser than data
science. This article provides a brief overview of data science, outlines ideas
for how introductory courses could take inspiration from data science, and
provides a reference to materials for developing stand-alone data science
courses.
"
1607.04209,2016-07-15,Dynamic Question Ordering in Online Surveys,"  Online surveys have the potential to support adaptive questions, where later
questions depend on earlier responses. Past work has taken a rule-based
approach, uniformly across all respondents. We envision a richer interpretation
of adaptive questions, which we call dynamic question ordering (DQO), where
question order is personalized. Such an approach could increase engagement, and
therefore response rate, as well as imputation quality. We present a DQO
framework to improve survey completion and imputation. In the general
survey-taking setting, we want to maximize survey completion, and so we focus
on ordering questions to engage the respondent and collect hopefully all
information, or at least the information that most characterizes the
respondent, for accurate imputations. In another scenario, our goal is to
provide a personalized prediction. Since it is possible to give reasonable
predictions with only a subset of questions, we are not concerned with
motivating users to answer all questions. Instead, we want to order questions
to get information that reduces prediction uncertainty, while not being too
burdensome. We illustrate this framework with an example of providing energy
estimates to prospective tenants. We also discuss DQO for national surveys and
consider connections between our statistics-based question-ordering approach
and cognitive survey methodology.
"
1607.04807,2016-11-08,Progress on a Conjecture Regarding the Triangular Distribution,"  Triangular distributions are a well-known class of distributions that are
often used as an elementary example of a probability model. Maximum likelihood
estimation of the mode parameter of the triangular distribution over the unit
interval can be performed via an order statistics-based method. It had been
conjectured that such a method can be conducted using only a constant number of
likelihood function evaluations, on average, as the sample size becomes large.
We prove two theorems that validate this conjecture. Graphical and numerical
results are presented to supplement our proofs.
"
1607.05150,2016-07-19,"Statistical Methods in Topological Data Analysis for Complex,
  High-Dimensional Data","  The utilization of statistical methods an their applications within the new
field of study known as Topological Data Analysis has has tremendous potential
for broadening our exploration and understanding of complex, high-dimensional
data spaces. This paper provides an introductory overview of the mathematical
underpinnings of Topological Data Analysis, the workflow to convert samples of
data to topological summary statistics, and some of the statistical methods
developed for performing inference on these topological summary statistics. The
intention of this non-technical overview is to motivate statisticians who are
interested in learning more about the subject.
"
1607.05952,2017-12-12,Data-driven generation of spatio-temporal routines in human mobility,"  The generation of realistic spatio-temporal trajectories of human mobility is
of fundamental importance in a wide range of applications, such as the
developing of protocols for mobile ad-hoc networks or what-if analysis in urban
ecosystems. Current generative algorithms fail in accurately reproducing the
individuals' recurrent schedules and at the same time in accounting for the
possibility that individuals may break the routine during periods of variable
duration. In this article we present DITRAS (DIary-based TRAjectory Simulator),
a framework to simulate the spatio-temporal patterns of human mobility. DITRAS
operates in two steps: the generation of a mobility diary and the translation
of the mobility diary into a mobility trajectory. We propose a data-driven
algorithm which constructs a diary generator from real data, capturing the
tendency of individuals to follow or break their routine. We also propose a
trajectory generator based on the concept of preferential exploration and
preferential return. We instantiate DITRAS with the proposed diary and
trajectory generators and compare the resulting algorithm with real data and
synthetic data produced by other generative algorithms, built by instantiating
DITRAS with several combinations of diary and trajectory generators. We show
that the proposed algorithm reproduces the statistical properties of real
trajectories in the most accurate way, making a step forward the understanding
of the origin of the spatio-temporal patterns of human mobility.
"
1608.02533,2016-10-24,Designing Modular Software: A Case Study in Introductory Statistics,"  Modular programming is a development paradigm that emphasizes self-contained,
flexible, and independent pieces of functionality. This practice allows new
features to be seamlessly added when desired, and unwanted features to be
removed, thus simplifying the user-facing view of the software. The recent rise
of web-based software applications has presented new challenges for designing
an extensible, modular software system. In this paper, we outline a framework
for designing such a system, with a focus on reproducibility of the results. We
present as a case study a Shiny-based web application called intRo, that allows
the user to perform basic data analyses and statistical routines. Finally, we
highlight some challenges we encountered, and how to address them, when
combining modular programming concepts with reactive programming as used by
Shiny.
"
1608.04843,2019-09-12,Putting Down Roots: A Graphical Exploration of Community Attachment,"  In this paper, we explore the relationships that individuals have with their
communities. This work was prepared as part of the ASA Data Expo '13 sponsored
by the Graphics Section and the Computing Section, using data provided by the
Knight Foundation Soul of the Community survey. The Knight Foundation in
cooperation with Gallup surveyed 43,000 people over three years in 26
communities across the United States with the intention of understanding the
association between community attributes and the degree of attachment people
feel towards their community. These include the different facets of both urban
and rural communities, the impact of quality education, and the trend in the
perceived economic conditions of a community over time. The goal of our work is
to facilitate understanding of why people feel attachment to their communities
through the use of an interactive and web-based visualization. We will explain
the development and use of web-based interactive graphics, including an
overview of the R package Shiny and the JavaScript library D3, focusing on the
choices made in producing the visualizations and technical aspects of how they
were created. Then we describe the stories about community attachment that
unfolded from our analysis.
"
1608.05810,2017-07-12,Unifying Markov Properties for Graphical Models,"  Several types of graphs with different conditional independence
interpretations --- also known as Markov properties --- have been proposed and
used in graphical models. In this paper we unify these Markov properties by
introducing a class of graphs with four types of edges --- lines, arrows, arcs,
and dotted lines --- and a single separation criterion. We show that
independence structures defined by this class specialize to each of the
previously defined cases, when suitable subclasses of graphs are considered. In
addition, we define a pairwise Markov property for the subclass of chain mixed
graphs which includes chain graphs with the LWF interpretation, as well as
summary graphs (and consequently ancestral graphs). We prove the equivalence of
this pairwise Markov property to the global Markov property for compositional
graphoid independence models.
"
1608.06330,2017-07-27,"Revisiting nested group testing procedures: new results, comparisons,
  and robustness","  Group testing has its origin in the identification of syphilis in the US army
during World War II. Much of the theoretical framework of group testing was
developed starting in the late 1950s, with continued work into the 1990s.
Recently, with the advent of new laboratory and genetic technologies, there has
been an increasing interest in group testing designs for cost saving purposes.
In this paper, we compare different nested designs, including Dorfman, Sterrett
and an optimal nested procedure obtained through dynamic programming. To
elucidate these comparisons, we develop closed-form expressions for the optimal
Sterrett procedure and provide a concise review of the prior literature for
other commonly used procedures. We consider designs where the prevalence of
disease is known as well as investigate the robustness of these procedures when
it is incorrectly assumed. This article provides a technical presentation that
will be of interest to researchers as well as from a pedagogical perspective.
Supplementary material for this article is available online.
"
1609.00494,2020-01-29,Publication bias and the canonization of false facts,"  In the process of scientific inquiry, certain claims accumulate enough
support to be established as facts. Unfortunately, not every claim accorded the
status of fact turns out to be true. In this paper, we model the dynamic
process by which claims are canonized as fact through repeated experimental
confirmation. The community's confidence in a claim constitutes a Markov
process: each successive published result shifts the degree of belief, until
sufficient evidence accumulates to accept the claim as fact or to reject it as
false. In our model, publication bias --- in which positive results are
published preferentially over negative ones --- influences the distribution of
published results. We find that when readers do not know the degree of
publication bias and thus cannot condition on it, false claims often can be
canonized as facts. Unless a sufficient fraction of negative results are
published, the scientific process will do a poor job at discriminating false
from true claims. This problem is exacerbated when scientists engage in
p-hacking, data dredging, and other behaviors that increase the rate at which
false positives are published. If negative results become easier to publish as
a claim approaches acceptance as a fact, however, true and false claims can be
more readily distinguished. To the degree that the model accurately represents
current scholarly practice, there will be serious concern about the validity of
purported facts in some areas of scientific research.
"
1609.00711,2020-10-20,"Generalized Spatial and Spatiotemporal Autoregressive Conditional
  Heteroscedasticity","  In this paper, we introduce a new spatial model that incorporates
heteroscedastic variance depending on neighboring locations. The proposed
process is regarded as the spatial equivalent to the temporal autoregressive
conditional heteroscedasticity (ARCH) model. We show additionally how the
introduced spatial ARCH model can be used in spatiotemporal settings. In
contrast to the temporal ARCH model, in which the distribution is known given
the full information set of the prior periods, the distribution is not
straightforward in the spatial and spatiotemporal setting. However, it is
possible to estimate the parameters of the model using the maximum-likelihood
approach. Via Monte Carlo simulations, we demonstrate the performance of the
estimator for a specific spatial weighting matrix. Moreover, we combine the
known spatial autoregressive model with the spatial ARCH model assuming
heteroscedastic errors. Eventually, the proposed autoregressive process is
illustrated using an empirical example. Specifically, we model lung cancer
mortality in 3108 U.S. counties and compare the introduced model with two
benchmark approaches.
"
1609.04383,2016-09-15,"Probabilistic Population Projections for Countries with Generalized
  HIV/AIDS Epidemics","  The United Nations (UN) issued official probabilistic population projections
for all countries to 2100 in July 2015. This was done by simulating future
levels of total fertility and life expectancy from Bayesian hierarchical
models, and combining the results using a standard cohort-component projection
method. The 40 countries with generalized HIV/AIDS epidemics were treated
differently from others, in that the projections used the highly multistate
Spectrum/EPP model, a complex 15-compartment model that was designed for
short-term projections of quantities relevant to policy for the epidemic. Here
we propose a simpler approach that is more compatible with the existing UN
probabilistic projection methodology for other countries. Changes in life
expectancy are projected probabilistically using a simple time series
regression model on current life expectancy, HIV prevalence and ART coverage.
These are then converted to age- and sex-specific mortality rates using a new
family of model life tables designed for countries with HIV/AIDS epidemics that
reproduces the characteristic hump in middle adult mortality. These are then
input to the standard cohort-component method, as for other countries. The
method performed well in an out-of-sample cross-validation experiment. It gives
similar population projections to Spectrum/EPP in the short run, while being
simpler and avoiding multistate modeling.
"
1609.04478,2017-04-17,Sterrett Procedure for the Generalized Group Testing Problem,"  Group testing is a useful method that has broad applications in medicine,
engineering, and even in airport security control. Consider a finite population
of $N$ items, where item $i$ has a probability $p_i$ to be defective. The goal
is to identify all items by means of group testing. This is the generalized
group testing problem. The optimum procedure, with respect to the expected
total number of tests, is unknown even in case when all $p_i$ are equal.
\cite{H1975} proved that an ordered partition (with respect to $p_i$) is the
optimal for the Dorfman procedure (procedure $D$), and obtained an optimum
solution (i.e., found an optimal partition) by dynamic programming. In this
paper, we investigate the Sterrett procedure (procedure $S$). We provide close
form expression for the expected total number of tests, which allows us to find
the optimum arrangement of the items in the particular group. We also show that
an ordered partition is not optimal for the procedure $S$ or even for a
slightly modified Dorfman procedure (procedure $D^{\prime}$). This discovery
implies that finding an optimal procedure $S$ appears to be a hard
computational problem. However, by using an optimal ordered partition for all
procedures, we show that procedure $D^{\prime}$ is uniformly better than
procedure $D$, and based on numerical comparisons, procedure $S$ is uniformly
and significantly better than procedures $D$ and $D^{\prime}$.
"
1609.05551,2019-06-18,Graphical Models for Discrete and Continuous Data,"  We introduce a general framework for undirected graphical models. It
generalizes Gaussian graphical models to a wide range of continuous, discrete,
and combinations of different types of data. The models in the framework,
called exponential trace models, are amenable to estimation based on maximum
likelihood. We introduce a sampling-based approximation algorithm for computing
the maximum likelihood estimator, and we apply this pipeline to learn
simultaneous neural activities from spike data.
"
1610.00048,2016-10-04,"Spatial Temporal Exponential-Family Point Process Models for the
  Evolution of Social Systems","  We develop a class of exponential-family point processes based on a latent
social space to model the coevolution of social structure and behavior over
time. Temporal dynamics are modeled as a discrete Markov process specified
through individual transition distributions for each actor in the system at a
given time. We prove that these distributions have an analytic closed form
under certain conditions and use the result to develop likelihood-based
inference. We provide a computational framework to enable both simulation and
inference in practice. Finally, we demonstrate the value of these models by
analyzing alcohol and drug use over time in the context of adolescent
friendship networks.
"
1610.00290,2016-10-04,"Conditional Visualization for Statistical Models: An Introduction to the
  condvis Package in R","  The condvis package is for interactive visualization of sections in data
space, showing fitted models on the section, and observed data near the
section. The primary goal is the interpretation of complex models, and showing
how the observed data support the fitted model. There is a video accompaniment
to this paper available at https://www.youtube.com/watch?v=rKFq7xwgdX0. This is
a preprint version of an article to appear in the Journal of Statistical
Software.
"
1610.01537,2016-10-07,Scale and curvature effects in principal geodesic analysis,"  There is growing interest in using the close connection between differential
geometry and statistics to model smooth manifold-valued data. In particular,
much work has been done recently to generalize principal component analysis
(PCA), the method of dimension reduction in linear spaces, to Riemannian
manifolds. One such generalization is known as principal geodesic analysis
(PGA). This paper, in a novel fashion, obtains Taylor expansions in scaling
parameters introduced in the domain of objective functions in PGA. It is shown
this technique not only leads to better closed-form approximations of PGA but
also reveals the effects that scale, curvature and the distribution of data
have on solutions to PGA and on their differences to first-order tangent space
approximations. This approach should be able to be applied not only to PGA but
also to other generalizations of PCA and more generally to other intrinsic
statistics on Riemannian manifolds.
"
1610.02608,2018-01-03,Research and Education in Computational Science and Engineering,"  Over the past two decades the field of computational science and engineering
(CSE) has penetrated both basic and applied research in academia, industry, and
laboratories to advance discovery, optimize systems, support decision-makers,
and educate the scientific and engineering workforce. Informed by centuries of
theory and experiment, CSE performs computational experiments to answer
questions that neither theory nor experiment alone is equipped to answer. CSE
provides scientists and engineers of all persuasions with algorithmic
inventions and software systems that transcend disciplines and scales. Carried
on a wave of digital technology, CSE brings the power of parallelism to bear on
troves of data. Mathematics-based advanced computing has become a prevalent
means of discovery and innovation in essentially all areas of science,
engineering, technology, and society; and the CSE community is at the core of
this transformation. However, a combination of disruptive
developments---including the architectural complexity of extreme-scale
computing, the data revolution that engulfs the planet, and the specialization
required to follow the applications to new frontiers---is redefining the scope
and reach of the CSE endeavor. This report describes the rapid expansion of CSE
and the challenges to sustaining its bold advances. The report also presents
strategies and directions for CSE research and education for the next decade.
"
1610.05733,2016-10-19,A Devastating Example for the Halfer Rule,"  How should we update de dicto beliefs in the face of de se evidence? The
Sleeping Beauty problem divides philosophers into two camps, halfers and
thirders. But there is some disagreement among halfers about how their position
should generalize to other examples. A full generalization is not always given;
one notable exception is the Halfer Rule, under which the agent updates her
uncentered beliefs based on only the uncentered part of her evidence. In this
brief article, I provide a simple example for which the Halfer Rule prescribes
credences that, I argue, cannot be reasonably held by anyone. In particular,
these credences constitute an egregious violation of the Reflection Principle.
I then discuss the consequences for halfing in general.
"
1610.09184,2017-03-01,Eigenvector statistics of the product of Ginibre matrices,"  We develop a method to calculate left-right eigenvector correlations of the
product of $m$ independent $N\times N$ complex Ginibre matrices. For
illustration, we present explicit analytical results for the vector overlap for
a couple of examples for small $m$ and $N$. We conjecture that the integrated
overlap between left and right eigenvectors is given by the formula $O = 1 +
(m/2)(N-1)$ and support this conjecture by analytical and numerical
calculations. We derive an analytical expression for the limiting correlation
density as $N\rightarrow \infty$ for the product of Ginibre matrices as well as
for the product of elliptic matrices. In the latter case, we find that the
correlation function is independent of the eccentricities of the elliptic laws.
"
1611.03072,2016-11-10,Apocalypse Now? Reviving the Doomsday Argument,"  Whether the fate of our species can be forecast from its past has been the
topic of considerable controversy. One refutation of the so-called Doomsday
Argument is based on the premise that we are more likely to exist in a universe
containing a greater number of observers. Here we present a Bayesian
reformulation of the Doomsday Argument which is immune to this effect. By
marginalising over the spatial configuration of observers, we find that any
preference for a larger total number of observers has no impact on the inferred
local number. Our results remain unchanged when we adopt either the
Self-Indexing Assumption (SIA) or the Self-Sampling Assumption (SSA).
Furthermore the median value of our posterior distribution is found to be in
agreement with the frequentist forecast. Humanity's prognosis for the coming
century is well approximated by a global catastrophic risk of 0.2% per year.
"
1611.03073,2017-04-26,Causal influence in linear response models,"  The intuition of causation is so fundamental that almost every research study
in life sciences refers to this concept. However a widely accepted formal
definition of causal influence between observables is still missing. In the
framework of linear Langevin networks without feedbacks (linear response
models) we developed a measure of causal influence based on a decomposition of
information flows over time. We discuss its main properties and compare it with
other information measures like the Transfer Entropy. Finally we outline some
difficulties of the extension to a general definition of causal influence for
complex systems.
"
1611.03974,2018-03-05,On approximations via convolution-defined mixture models,"  An often-cited fact regarding mixing or mixture distributions is that their
density functions are able to approximate the density function of any unknown
distribution to arbitrary degrees of accuracy, provided that the mixing or
mixture distribution is sufficiently complex. This fact is often not made
concrete. We investigate and review theorems that provide approximation bounds
for mixing distributions. Connections between the approximation bounds of
mixing distributions and estimation bounds for the maximum likelihood estimator
of finite mixtures of location- scale distributions are reviewed.
"
1611.06168,2016-11-21,On $p$-values,"  Models are consistently treated as approximations and all procedures are
consistent with this. They do not treat the model as being true. In this
context $p$-values are one measure of approximation, a small $p$-value
indicating a poor approximation. Approximation regions are defined and
distinguished from confidence regions.
"
1611.06545,2016-11-22,Stop the tests: Opinion bias and statistical tests,"  When statisticians quarrel about hypothesis testing, the debate usually focus
on which method is the correct one. The fundamental question of whether we
should test hypothesis at all tends to be forgotten. This lack of debate has
its roots on our desire to have ideas we believe and defend. But cognitive
experiments have been showing that, when we do choose ideas, we become prey to
a large number of biases. Several of our biases can be grouped together in a
single description, an opinion bias. This opinion bias is nothing more than our
desire to believe in something and to defend it. Also, despite our feelings,
believing has no solid logical or philosophical grounds. In this paper, I will
show that if we combine the fact that even logic can never prove an idea right
or wrong and the problems our brains cause when we pick ideas, hypothesis
testing and its terminology are a recipe for disaster. Testing should have no
place when we are thinking about hypothesis.
"
1611.08118,2016-11-28,"BayesVarSel: Bayesian Testing, Variable Selection and model averaging in
  Linear Models using R","  This paper introduces the R package BayesVarSel which implements objective
Bayesian methodology for hypothesis testing and variable selection in linear
models. The package computes posterior probabilities of the competing
hypotheses/models and provides a suite of tools, specifically proposed in the
literature, to properly summarize the results. Additionally, \ourpack\ is armed
with functions to compute several types of model averaging estimations and
predictions with weights given by the posterior probabilities. BayesVarSel
contains exact algorithms to perform fast computations in problems of small to
moderate size and heuristic sampling methods to solve large problems. The
software is intended to appeal to a broad spectrum of users, so the interface
has been carefully designed to be highly intuititive and is inspired by the
well-known lm function. The issue of prior inputs is carefully addressed. In
the default usage (fully automatic for the user)BayesVarSel implements the
criteria-based priors proposed by Bayarri et al (2012), but the advanced user
has the possibility of using several other popular priors in the literature.
The package is available through the Comprehensive R Archive Network, CRAN. We
illustrate the use of BayesVarSel with several data examples.
"
1611.08942,2016-12-16,The BIN_COUNTS Constraint: Filtering and Applications,"  We introduce the BIN_COUNTS constraint, which deals with the problem of
counting the number of decision variables in a set which are assigned values
that lie in given bins. We illustrate a decomposition and a filtering algorithm
that achieves generalised arc consistency. We contrast the filtering power of
these two approaches and we discuss a number of applications. We show that
BIN_COUNTS can be employed to develop a decomposition for the $\chi^2$ test
constraint, a new statistical constraint that we introduce in this work. We
also show how this new constraint can be employed in the context of the
Balanced Academic Curriculum Problem and of the Balanced Nursing Workload
Problem. For both these problems we carry out numerical studies involving our
reformulations. Finally, we present a further application of the $\chi^2$ test
constraint in the context of confidence interval analysis.
"
1612.01619,2021-10-11,mBART: Multidimensional Monotone BART,"  For the discovery of regression relationships between Y and a large set of p
potential predictors x 1 , . . . , x p , the flexible nonparametric nature of
BART (Bayesian Additive Regression Trees) allows for a much richer set of
possibilities than restrictive parametric approaches. However, subject matter
considerations sometimes warrant a minimal assumption of monotonicity in at
least some of the predictors. For such contexts, we introduce mBART, a
constrained version of BART that can flexibly incorporate monotonicity in any
predesignated subset of predictors using a multivariate basis of monotone
trees, while avoiding the further confines of a full parametric form. For such
monotone relationships, mBART provides (i) function estimates that are smoother
and more interpretable, (ii) better out-of-sample predictive performance, and
(iii) less post-data uncertainty. While many key aspects of the unconstrained
BART model carry over directly to mBART, the introduction of monotonicity
constraints necessitates a fundamental rethinking of how the model is
implemented. In particular, the original BART Markov Chain Monte Carlo
algorithm relied on a conditional conjugacy that is no longer available in a
monotonically constrained space. Various simulated and real examples
demonstrate the wide ranging potential of mBART.
"
1612.02252,2016-12-08,Building Communication Skills in a Theoretical Statistics Course,"  The traditional theoretical statistics course which develops the theoretical
underpinnings of the discipline (usually following a probability course) is
undergoing near-continuous revision in the statistics community. In particular,
recent versions of this course have incorporated more and more computation. We
take a look at a different aspect of the revision - building student
communication skills in the course, in both written and verbal forms, to allow
students to demonstrate their ability to explain statistical concepts. Two
separate projects are discussed, both of which were engaged in by a class of
size 17 in Spring 2015. The first project had a computational aspect (performed
using R), a statistical theory component, and a writing component, and was
based on the historical German tank problem. The second project involved a
class presentation and written report summarizing, critiquing, and/or
explaining an article selected from The American Statistician.
"
1612.05292,2018-02-07,"Probability, propensity and probabilities of propensities (and of
  probabilities)","  The process of doing Science in condition of uncertainty is illustrated with
a toy experiment in which the inferential and the forecasting aspects are both
present. The fundamental aspects of probabilistic reasoning, also relevant in
real life applications, arise quite naturally and the resulting discussion
among non-ideologized, free-minded people offers an opportunity for
clarifications.
"
1612.07140,2017-05-16,A Guide to Teaching Data Science,"  Demand for data science education is surging and traditional courses offered
by statistics departments are not meeting the needs of those seeking training.
This has led to a number of opinion pieces advocating for an update to the
Statistics curriculum. The unifying recommendation is computing should play a
more prominent role. We strongly agree with this recommendation, but advocate
the main priority is to bring applications to the forefront as proposed by
Nolan and Speed (1999). We also argue that the individuals tasked with
developing data science courses should not only have statistical training, but
also have experience analyzing data with the main objective of solving
real-world problems. Here, we share a set of general principles and offer a
detailed guide derived from our successful experience developing and teaching a
graduate-level, introductory data science course centered entirely on case
studies. We argue for the importance of statistical thinking, as defined by
Wild and Pfannkuck (1999) and describe how our approach teaches students three
key skills needed to succeed in data science, which we refer to as creating,
connecting, and computing. This guide can also be used for statisticians
wanting to gain more practical knowledge about data science before embarking on
teaching an introductory course.
"
1612.07542,2016-12-23,A Graph Downsampling Technique Based On Graph Fourier Transform,"  In this paper, we provide a Graph Fourier Transform based approach to
downsample signals on graphs. For bandlimited signals on a graph, a test is
provided to identify whether signal reconstruction is possible from the given
downsampled signal. Moreover, if the signal is not bandlimited, we provide a
quality measure for comparing different downsampling schemes. Using this
quality measure, we propose a greedy downsampling algorithm. Most of the
prevailing approaches consider undirected graphs, and exploit the topological
properties of the graph in order to downsample the grid, while the proposed
method exploits spectral properties of graph signals, and is applicable to
directed graphs, undirected graphs, and graphs with negative edge-weights. We
provide several experiments demonstrating our downsampling scheme, and compare
our quality measure with measures like normalized cuts.
"
1701.02383,2017-01-11,SPEW: Synthetic Populations and Ecosystems of the World,"  Agent-based models (ABMs) simulate interactions between autonomous agents in
constrained environments over time. ABMs are often used for modeling the spread
of infectious diseases. In order to simulate disease outbreaks or other
phenomena, ABMs rely on ""synthetic ecosystems,"" or information about agents and
their environments that is representative of the real world. Previous
approaches for generating synthetic ecosystems have some limitations: they are
not open-source, cannot be adapted to new or updated input data sources, and do
not allow for alternative methods for sampling agent characteristics and
locations. We introduce a general framework for generating Synthetic
Populations and Ecosystems of the World (SPEW), implemented as an open-source R
package. SPEW allows researchers to choose from a variety of sampling methods
for agent characteristics and locations when generating synthetic ecosystems
for any geographic region. SPEW can produce synthetic ecosystems for any agent
(e.g. humans, mosquitoes, etc), provided that appropriate data is available. We
analyze the accuracy and computational efficiency of SPEW given different
sampling methods for agent characteristics and locations and provide a suite of
diagnostics to screen our synthetic ecosystems. SPEW has generated over five
billion human agents across approximately 100,000 geographic regions in about
70 countries, available online.
"
1701.04583,2017-10-04,PUMA criterion = MODE criterion,"  We show that the recently proposed (enhanced) PUMA estimator for array
processing minimizes the same criterion function as the well-established MODE
estimator. (PUMA = principal-singular-vector utilization for modal analysis,
MODE = method of direction estimation.)
"
1701.08290,2017-01-31,"HyperTools: A Python toolbox for visualizing and manipulating
  high-dimensional data","  Data visualizations can reveal trends and patterns that are not otherwise
obvious from the raw data or summary statistics. While visualizing
low-dimensional data is relatively straightforward (for example, plotting the
change in a variable over time as (x,y) coordinates on a graph), it is not
always obvious how to visualize high-dimensional datasets in a similarly
intuitive way. Here we present HypeTools, a Python toolbox for visualizing and
manipulating large, high-dimensional datasets. Our primary approach is to use
dimensionality reduction techniques (Pearson, 1901; Tipping & Bishop, 1999) to
embed high-dimensional datasets in a lower-dimensional space, and plot the data
using a simple (yet powerful) API with many options for data manipulation [e.g.
hyperalignment (Haxby et al., 2011), clustering, normalizing, etc.] and plot
styling. The toolbox is designed around the notion of data trajectories and
point clouds. Just as the position of an object moving through space can be
visualized as a 3D trajectory, HyperTools uses dimensionality reduction
algorithms to create similar 2D and 3D trajectories for time series of
high-dimensional observations. The trajectories may be plotted as interactive
static plots or visualized as animations. These same dimensionality reduction
and alignment algorithms can also reveal structure in static datasets (e.g.
collections of observations or attributes). We present several examples
showcasing how using our toolbox to explore data through trajectories and
low-dimensional embeddings can reveal deep insights into datasets across a wide
variety of domains.
"
1701.08366,2018-01-30,Faithfulness of Probability Distributions and Graphs,"  A main question in graphical models and causal inference is whether, given a
probability distribution $P$ (which is usually an underlying distribution of
data), there is a graph (or graphs) to which $P$ is faithful. The main goal of
this paper is to provide a theoretical answer to this problem. We work with
general independence models, which contain probabilistic independence models as
a special case. We exploit a generalization of ordering, called preordering, of
the nodes of (mixed) graphs. This allows us to provide sufficient conditions
for a given independence model to be Markov to a graph with the minimum
possible number of edges, and more importantly, necessary and sufficient
conditions for a given probability distribution to be faithful to a graph. We
present our results for the general case of mixed graphs, but specialize the
definitions and results to the better-known subclasses of undirected
(concentration) and bidirected (covariance) graphs as well as directed acyclic
graphs.
"
1701.08438,2020-07-21,"Using a ""Study of Studies"" to help statistics students assess research
  findings","  One learning goal of the introductory statistics course is to develop the
ability to make sense of research findings in published papers. The Atlantic
magazine regularly publishes a feature called ""Study of Studies"" that
summarizes multiple articles published in a particular domain. We describe a
classroom activity to develop this capacity using the ""Study of Studies."" In
this activity, students read capsule summaries of twelve research papers
related to restaurants and dining that was published in April 2015. The
selected papers report on topics such as how seating arrangement, server
posture, plate color and size, and the use of background music relate to
revenue, ambiance, and perceived food quality. The students are assigned one of
the twelve papers to read and critique as part of a small group. Their group
critiques are shared with the class and the instructor.
  A pilot study was conducted during the 2015-2016 academic year at Amherst
College. Students noted that key details were not included in the published
summary. They were generally skeptical of the published conclusions. The
students often provided additional summarization of information from the
journal articles that better describe the results. By independently assessing
and comparing the original study conclusions with the capsule summary in the
""Study of Studies,"" students can practice developing judgment and assessing the
validity of statistical results.
"
1701.08452,2020-07-21,"Enriching students' conceptual understanding of confidence intervals: An
  interactive trivia-based classroom activity","  Confidence intervals provide a way to determine plausible values for a
population parameter. They are omnipresent in research articles involving
statistical analyses. Appropriately, a key statistical literacy learning
objective is the ability to interpret and understand confidence intervals in a
wide range of settings. As instructors, we devote a considerable amount of time
and effort to ensure that students master this topic in introductory courses
and beyond. Yet, studies continue to find that confidence intervals are
commonly misinterpreted and that even experts have trouble calibrating their
individual confidence levels. In this article, we present a ten-minute trivia
game-based activity that addresses these misconceptions by exposing students to
confidence intervals from a personal perspective. We describe how the activity
can be integrated into a statistics course as a one-time activity or with
repetition at intervals throughout a course, discuss results of using the
activity in class, and present possible extensions.
"
1702.00308,2023-01-11,Approximate Variational Estimation for a Model of Network Formation,"  We develop approximate estimation methods for exponential random graph models
(ERGMs), whose likelihood is proportional to an intractable normalizing
constant. The usual approach approximates this constant with Monte Carlo
simulations, however convergence may be exponentially slow. We propose a
deterministic method, based on a variational mean-field approximation of the
ERGM's normalizing constant. We compute lower and upper bounds for the
approximation error for any network size, adapting nonlinear large deviations
results. This translates into bounds on the distance between true likelihood
and mean-field likelihood. Monte Carlo simulations suggest that in practice our
deterministic method performs better than our conservative theoretical
approximation bounds imply, for a large class of models.
"
1702.00823,2017-02-06,Nonparametric Spherical Regression Using Diffeomorphic Mappings,"  Spherical regression explores relationships between variables on spherical
domains. We develop a nonparametric model that uses a diffeomorphic map from a
sphere to itself. The restriction of this mapping to diffeomorphisms is natural
in several settings. The model is estimated in a penalized maximum-likelihood
framework using gradient-based optimization. Towards that goal, we specify a
first-order roughness penalty using the Jacobian of diffeomorphisms. We compare
the prediction performance of the proposed model with state-of-the-art methods
using simulated and real data involving cloud deformations, wind directions,
and vector-cardiograms. This model is found to outperform others in capturing
relationships between spherical variables.
"
1702.02432,2018-04-12,"MH370 Burst Frequency Offset Analysis and Implications on Descent Rate
  at End-of-Flight","  Malaysian Airlines flight MH370 veered off course unexpectedly during a
scheduled trip from Kuala Lumpur to Beijing on the 7th of March 2014. MH370 was
tracked via military radar into the Malacca Straits and, after disappearing
from radar, was subsequently believed to have turned south towards the southern
Indian Ocean before crashing approximately 6 hours later. This article
discusses specifically the analysis of burst frequency offset (BFO) metadata
from the SATCOM messages. It is shown that the BFOs corresponding to the last
two SATCOM messages from the plane at 00:19:29Z and 00:19:37Z 8th March 2014
suggest that flight MH370 was rapidly descending and accelerating downwards
when message exchange with the ground station ceased.
"
1702.05340,2017-02-20,"Combinatorics of Distance Covariance: Inclusion-Minimal Maximizers of
  Quasi-Concave Set Functions for Diverse Variable Selection","  In this paper we show that the negative sample distance covariance function
is a quasi-concave set function of samples of random variables that are not
statistically independent. We use these properties to propose greedy algorithms
to combinatorially optimize some diversity (low statistical dependence)
promoting functions of distance covariance. Our greedy algorithm obtains all
the inclusion-minimal maximizers of this diversity promoting objective.
Inclusion-minimal maximizers are multiple solution sets of globally optimal
maximizers that are not a proper subset of any other maximizing set in the
solution set. We present results upon applying this approach to obtain diverse
features (covariates/variables/predictors) in a feature selection setting for
regression (or classification) problems. We also combine our diverse feature
selection algorithm with a distance covariance based relevant feature selection
algorithm of [7] to produce subsets of covariates that are both relevant yet
ordered in non-increasing levels of diversity of these subsets.
"
1702.06176,2017-06-02,MOLIERE: Automatic Biomedical Hypothesis Generation System,"  Hypothesis generation is becoming a crucial time-saving technique which
allows biomedical researchers to quickly discover implicit connections between
important concepts. Typically, these systems operate on domain-specific
fractions of public medical data. MOLIERE, in contrast, utilizes information
from over 24.5 million documents. At the heart of our approach lies a
multi-modal and multi-relational network of biomedical objects extracted from
several heterogeneous datasets from the National Center for Biotechnology
Information (NCBI). These objects include but are not limited to scientific
papers, keywords, genes, proteins, diseases, and diagnoses. We model hypotheses
using Latent Dirichlet Allocation applied on abstracts found near shortest
paths discovered within this network, and demonstrate the effectiveness of
MOLIERE by performing hypothesis generation on historical data. Our network,
implementation, and resulting data are all publicly available for the broad
scientific community.
"
1702.07074,2017-02-24,"Social Big Data Analytics of Consumer Choices: A Two Sided Online
  Platform Perspective","  This dissertation examines three distinct big data analytics problems related
to the social aspects of consumers' choices. The main goal of this line of
research is to help two sided platform firms to target their marketing policies
given the great heterogeneity among their customers. In three essays, I
combined structural modeling and machine learning approaches to first
understand customers' responses to intrinsic and extrinsic factors, using
unique data sets I scraped from the web, and then explore methods to optimize
two sided platforms' firms' reactions accordingly. The first essay examines
""social learning"" in the mobile app store context, controlling for intrinsic
value of hedonic and utilitarian mobile apps, price, advertising, and number of
options available. The second essay investigates bidders' anticipated winner
and loser regret in the context of the eBay online auction platform. Using a
large data set from eBay and empirical Bayesian estimation method, I quantify
the bidders' anticipation of regret in various product categories, and
investigate the role of experience in explaining the bidders' regret and
learning behaviors. The third essay investigates the effects of Gamification
incentive mechanisms in an online platform for user generated content. I use an
ensemble method over LDA, mixed normal and k-mean clustering methods to segment
users into competitors, collaborators, achievers, explorers and uninterested
users. These findings help the Gamification platform to target its users. The
simulation counterfactual analysis suggests that a two sided platform can
increase the number of user contributions, by making earning badges more
difficult.
"
1702.08261,2017-02-28,J.B.S. Haldane Could Have Done Better,"  In a review on the contribution of J.B.S. Haldane to the development of the
Bayes factor hypothesis test (arXiv:1511.08180), Etz and Wagenmakers focus on
Haldane's proposition of a mixture prior in a genetic example (Haldane 1932, A
note on inverse probability. Mathematical Proceedings of the Cambridge
Philosophical Society, 28, 55-61.). As Haldane never followed up on these
ideas, it is difficult to gauge his motivation and intentions. I argue that
contrary to Haldane's stated intention of replacing flat priors with more
reasonable assumptions, he actually chose in this example an unreasonable flat
prior. Considering the information available to Haldane, I derive a superior
prior and compare to Haldane's flat prior. Haldane's main intent with his
article seems to have been to explore the different parameter regions of the
binomial and the conjugate beta. Furthermore, I agree with Etz and Wagenmakers
that Haldane serendipitously adopted a mixture prior comprising a point mass
and smooth distribution in his genetic example.
"
1703.00352,2017-12-06,Do Reichenbachian Common Cause Systems of Arbitrary Finite Size Exist?,"  The principle of common cause asserts that positive correlations between
causally unrelated events ought to be explained through the action of some
shared causal factors. Reichenbachian common cause systems are probabilistic
structures aimed at accounting for cases where correlations of the aforesaid
sort cannot be explained through the action of a single common cause. The
existence of Reichenbachian common cause systems of arbitrary finite size for
each pair of non-causally correlated events was allegedly demonstrated by
Hofer-Szab\'o and R\'edei in 2006. This paper shows that their proof is
logically deficient, and we propose an improved proof.
"
1703.03043,2017-12-06,Bootstrap with Clustering in Two or More Dimensions,"  We propose a bootstrap procedure for data that may exhibit clustering in two
or more dimensions. We use insights from the theory of generalized U-statistics
to analyze the large-sample properties of statistics that are sample averages
from the observations pooled across clusters. The asymptotic distribution of
these statistics may be non-standard if there is no clustering in means. We
show that the proposed bootstrap procedure is (a) point-wise consistent for any
fixed data-generating process (DGP), (b) uniformly consistent if we exclude the
case of clustering without clustering in means, and (c) provides refinements
for any DGP such that the limiting distribution is Gaussian.
"
1703.04185,2018-07-30,Estimating the Probability that a Function Observed with Noise is Convex,"  Consider a real-valued function that can only be observed with stochastic
noise at a finite set of design points within a Euclidean space. We wish to
determine whether there exists a convex function that goes through the true
function values at the design points. We develop an asymptotically consistent
Bayesian sequential sampling procedure that estimates the posterior probability
of this being true. In each iteration, the posterior probability is estimated
using Monte Carlo simulation. We offer three variance reduction methods --
change of measure, acceptance-rejection, and conditional Monte Carlo. Numerical
experiments suggest that the conditional Monte Carlo method should be
preferred.
"
1703.04467,2024-01-24,"Gaussian spatial regression using the spmoran package: case study
  examples","  This study demonstrates how to use the ""spmoran"" package implementing
scalable spatial regression models for Gaussian and non-Gaussian data.
Implemented models include spatially varying coefficient models, models with
group effects, spatial unconditional quantile regression model, and low rank
spatial econometric models. All of these models are estimated in a
computationally efficient manner for large samples. Moran eigenvectors are used
to an approximate Gaussian process (GP) modeling that is interpretable in terms
of the Moran coefficient. The GP is used for modeling the spatial processes in
residuals and regression coefficients. The sample codes are available from
https://github.com/dmuraka/spmoran. While this vignette mainly focuses on
Gaussian data modeling, another vignette focusing on non-Gaussian data
including count regression is also available from the same GitHub page.
"
1703.06109,2017-03-20,Generalised Reichenbachian Common Cause Systems,"  The principle of the common cause claims that if an improbable coincidence
has occurred, there must exist a common cause. This is generally taken to mean
that positive correlations between non-causally related events should disappear
when conditioning on the action of some underlying common cause. The extended
interpretation of the principle, by contrast, urges that common causes should
be called for in order to explain positive deviations between the estimated
correlation of two events and the expected value of their correlation. The aim
of this paper is to provide the extended reading of the principle with a
general probabilistic model, capturing the simultaneous action of a system of
multiple common causes. To this end, two distinct models are elaborated, and
the necessary and sufficient conditions for their existence are determined.
"
1703.08648,2018-05-22,"The new concepts of measurement error's regularities and effect
  characteristics","  In several literatures, the authors give a new thinking of measurement theory
system based on error non-classification philosophy, which completely
overthrows the existing measurement concept system of precision, trueness and
accuracy. In this paper, by focusing on the issues of error's regularities and
effect characteristics, the authors will do a thematic interpretation, and
prove that the error's regularities actually come from different cognitive
perspectives, are also unable to be used for classifying errors, and that the
error's effect characteristics actually depend on artificial condition rules of
repeated measurement, and are still unable to be used for classifying errors.
Thus, from the perspectives of error's regularities and effect characteristics,
the existing error classification philosophy is still incorrect; and an
uncertainty concept system, which must be interpreted by the error
non-classification philosophy, naturally becomes the only way out of
measurement theory.
"
1703.10117,2017-10-26,Data-Mining Research in Education,"  As an interdisciplinary discipline, data mining (DM) is popular in education
area especially when examining students' learning performances. It focuses on
analyzing educational related data to develop models for improving learners'
learning experiences and enhancing institutional effectiveness. Therefore, DM
does help education institutions provide high-quality education for its
learners. Applying data mining in education also known as educational data
mining (EDM), which enables to better understand how students learn and
identify how improve educational outcomes. Present paper is designed to justify
the capabilities of data mining approaches in the filed of education. The
latest trends on EDM research are introduced in this review. Several specific
algorithms, methods, applications and gaps in the current literature and future
insights are discussed here.
"
1704.00609,2017-04-04,What is the best fractional derivative to fit data?,"  The aim of this work is to show, based on concrete data observation, that the
choice of the fractional derivative when modelling a problem is relevant for
the accuracy of a method. Using the least squares fitting technique, we
determine the order of the fractional differential equation that better
describes the experimental data, for different types of fractional derivatives.
"
1704.00674,2017-04-04,"Nonparametric estimation of the conditional distribution at regression
  boundary points","  Nonparametric regression is a standard statistical tool with increased
importance in the Big Data era. Boundary points pose additional difficulties
but local polynomial regression can be used to alleviate them. Local linear
regression, for example, is easy to implement and performs quite well both at
interior as well as boundary points. Estimating the conditional distribution
function and/or the quantile function at a given regressor point is immediate
via standard kernel methods but problems ensue if local linear methods are to
be used. In particular, the distribution function estimator is not guaranteed
to be monotone increasing, and the quantile curves can ""cross"". In the paper at
hand, a simple method of correcting the local linear distribution estimator for
monotonicity is proposed, and its good performance is demonstrated via
simulations and real data examples.
"
1704.01171,2017-04-06,"Rethinking probabilistic prediction in the wake of the 2016 U.S.
  presidential election","  To many statisticians and citizens, the outcome of the most recent U.S.
presidential election represents a failure of data-driven methods on the
grandest scale. This impression has led to much debate and discussion about how
the election predictions went awry -- Were the polls inaccurate? Were the
models wrong? Did we misinterpret the probabilities? -- and how they went right
-- Perhaps the analyses were correct even though the predictions were wrong,
that's just the nature of probabilistic forecasting. With this in mind, we
analyze the election outcome with respect to a core set of effectiveness
principles. Regardless of whether and how the election predictions were right
or wrong, we argue that they were ineffective in conveying the extent to which
the data was informative of the outcome and the level of uncertainty in making
these assessments. Among other things, our analysis sheds light on the
shortcomings of the classical interpretations of probability and its
communication to consumers in the form of predictions. We present here an
alternative approach, based on a notion of validity, which offers two immediate
insights for predictive inference. First, the predictions are more
conservative, arguably more realistic, and come with certain guarantees on the
probability of an erroneous prediction. Second, our approach easily and
naturally reflects the (possibly substantial) uncertainty about the model by
outputting plausibilities instead of probabilities. Had these simple steps been
taken by the popular prediction outlets, the election outcome may not have been
so shocking.
"
1704.01297,2017-04-06,"Automated Diagnosis of Epilepsy Employing Multifractal Detrended
  Fluctuation Analysis Based Features","  This contribution reports an application of MultiFractal Detrended
Fluctuation Analysis, MFDFA based novel feature extraction technique for
automated detection of epilepsy. In fractal geometry, Multifractal Detrended
Fluctuation Analysis MFDFA is a popular technique to examine the
self-similarity of a nonlinear, chaotic and noisy time series. In the present
research work, EEG signals representing healthy, interictal (seizure free) and
ictal activities (seizure) are acquired from an existing available database.
The acquired EEG signals of different states are at first analyzed using MFDFA.
To requisite the time series singularity quantification at local and global
scales, a novel set of fourteen different features. Suitable feature ranking
employing students t-test has been done to select the most statistically
significant features which are henceforth being used as inputs to a support
vector machines (SVM) classifier for the classification of different EEG
signals. Eight different classification problems have been presented in this
paper and it has been observed that the overall classification accuracy using
MFDFA based features are reasonably satisfactory for all classification
problems. The performance of the proposed method are also found to be quite
commensurable and in some cases even better when compared with the results
published in existing literature studied on the similar data set.
"
1704.01732,2017-04-07,"A Mathematically Sensible Explanation of the Concept of Statistical
  Population","  In statistics education, the concept of population is widely felt hard to
grasp, as a result of vague explanations in textbooks. Some textbook authors
therefore chose not to mention it. This paper offers a new explanation by
proposing a new theoretical framework of population and sampling, which aims to
achieve high mathematical sensibleness. In the explanation, the term population
is given clear definition, and the relationship between simple random sampling
and iid random variables are examined mathematically.
"
1704.03812,2020-09-22,"A New Theoretical Interpretation of Measurement Error and Its
  Uncertainty","  The traditional measurement theory interprets the variance as the dispersion
of a measured value, which is actually contrary to a general mathematical
concept that the variance of a constant is 0. This paper will fully demonstrate
that the variance in measurement theory is actually the evaluation of
probability interval of an error instead of the dispersion of a measured value,
point out the key point of mistake in the traditional interpretation, and fully
interpret a series of changes in conceptual logic and processing method brought
about by this new concept.
"
1704.03924,2017-09-13,A Tutorial on Kernel Density Estimation and Recent Advances,"  This tutorial provides a gentle introduction to kernel density estimation
(KDE) and recent advances regarding confidence bands and geometric/topological
features. We begin with a discussion of basic properties of KDE: the
convergence rate under various metrics, density derivative estimation, and
bandwidth selection. Then, we introduce common approaches to the construction
of confidence intervals/bands, and we discuss how to handle bias. Next, we talk
about recent advances in the inference of geometric and topological features of
a density function using KDE. Finally, we illustrate how one can use KDE to
estimate a cumulative distribution function and a receiver operating
characteristic curve. We provide R implementations related to this tutorial at
the end.
"
1704.05630,2018-09-05,"Classical and bayesian componentwise predictors for non-compact
  correlated ARH(1) processes","  A special class of standard Gaussian Autoregressive Hilbertian processes of
order one (Gaussian ARH(1) processes), with bounded linear autocorrelation
operator, which does not satisfy the usual Hilbert-Schmidt assumption, is
considered. To compensate the slow decay of the diagonal coefficients of the
autocorrelation operator, a faster decay velocity of the eigenvalues of the
trace autocovariance operator of the innovation process is assumed. As usual,
the eigenvectors of the autocovariance operator of the ARH(1) process are
considered for projection, since, here, they are assumed to be known. Diagonal
componentwise classical and bayesian estimation of the autocorrelation operator
is studied for prediction. The asymptotic efficiency and equivalence of both
estimators is proved, as well as of their associated componentwise ARH(1)
plugin predictors. A simulation study is undertaken to illustrate the
theoretical results derived.
"
1704.06292,2017-04-24,Remark On Variance Bounds,"  It is shown that the formula for the variance of combined series yields
surprisingly simple proofs of some well known variance bounds.
"
1704.07512,2017-04-26,"Information vs. Uncertainty as the Foundation for a Science of
  Environmental Modeling","  Information accounting provides a better foundation for hypothesis testing
than does uncertainty quantification. A quantitative account of science is
derived under this perspective that alleviates the need for epistemic bridge
principles, solves the problem of ad hoc falsification criteria, and deals with
verisimilitude by facilitating a general approach to process-level diagnostics.
Our argument is that the well-known inconsistencies of both Bayesian and
classical statistical hypothesis tests are due to the fact that probability
theory is an insufficient logic of science. Information theory, as an extension
of probability theory, is required to provide a complete logic on which to base
quantitative theories of empirical learning. The organizing question in this
case becomes not whether our theories or models are more or less true, or about
how much uncertainty is associated with a particular model, but instead whether
there is any information available from experimental data that might allow us
to improve the model. This becomes a formal hypothesis test, provides a theory
of model diagnostics, and suggests a new approach to building dynamical systems
models.
"
1704.08248,2022-06-08,"Modeling and replicating statistical topology, and evidence for CMB
  non-homogeneity","  Under the banner of `Big Data', the detection and classification of structure
in extremely large, high dimensional, data sets, is, one of the central
statistical challenges of our times. Among the most intriguing approaches to
this challenge is `TDA', or `Topological Data Analysis', one of the primary
aims of which is providing non-metric, but topologically informative,
pre-analyses of data sets which make later, more quantitative analyses
feasible. While TDA rests on strong mathematical foundations from Topology, in
applications it has faced challenges due to an inability to handle issues of
statistical reliability and robustness and, most importantly, in an inability
to make scientific claims with verifiable levels of statistical confidence. We
propose a methodology for the parametric representation, estimation, and
replication of persistence diagrams, the main diagnostic tool of TDA. The power
of the methodology lies in the fact that even if only one persistence diagram
is available for analysis -- the typical case for big data applications --
replications can be generated to allow for conventional statistical hypothesis
testing. The methodology is conceptually simple and computationally practical,
and provides a broadly effective statistical procedure for persistence diagram
TDA analysis. We demonstrate the basic ideas on a toy example, and the power of
the approach in a novel and revealing analysis of CMB non-homogeneity.
"
1705.01451,2018-01-17,"Feasibility study on the least square method for fitting non-Gaussian
  noise data","  This study is to investigate the feasibility of least square method in
fitting non-Gaussian noise data. We add different levels of the two typical
non-Gaussian noises, L\'evy and stretched Gaussian noises, to exact value of
the selected functions including linear equations, polynomial and exponential
equations, and the maximum absolute and the mean square errors are calculated
for the different cases. L\'evy and stretched Gaussian distributions have many
applications in fractional and fractal calculus. It is observed that the
non-Gaussian noises are less accurately fitted than the Gaussian noise, but the
stretched Gaussian cases appear to perform better than the L\'evy noise cases.
It is stressed that the least-squares method is inapplicable to the
non-Gaussian noise cases when the noise level is larger than 5%.
"
1705.03560,2017-05-11,"A Dutch Book against Sleeping Beauties Who Are Evidential Decision
  Theorists","  In the context of the Sleeping Beauty problem, it has been argued that
so-called ""halfers"" can avoid Dutch book arguments by adopting evidential
decision theory. I introduce a Dutch book for a variant of the Sleeping Beauty
problem and argue that evidential decision theorists fall prey to it, whether
they are halfers or thirders. The argument crucially requires that an action
can provide evidence for what the agent would do not only at other decision
points where she has exactly the same information, but also at decision points
where she has different but ""symmetric"" information.
"
1705.06332,2017-05-19,Can rational choice guide us to correct {\em de se} beliefs?,"  Significant controversy remains about what constitute correct self-locating
beliefs in scenarios such as the Sleeping Beauty problem, with proponents on
both the ""halfer"" and ""thirder"" sides. To attempt to settle the issue, one
natural approach consists in creating decision variants of the problem,
determining what actions the various candidate beliefs prescribe, and assessing
whether these actions are reasonable when we step back. Dutch book arguments
are a special case of this approach, but other Sleeping Beauty games have also
been constructed to make similar points. Building on a recent article (James
R.~Shaw. {\em De se} belief and rational choice. {\em Synthese},
190(3):491-508, 2013), I show that in general we should be wary of such
arguments, because unintuitive actions may result for reasons that are
unrelated to the beliefs. On the other hand, I show that, when we restrict our
attention to {\em additive} games, then a thirder will necessarily maximize her
{\em ex ante} expected payout, but a halfer in some cases will not (assuming
causal decision theory). I conclude that this does not necessarily settle the
issue and speculate about what might.
"
1705.06828,2017-10-18,"Agent-based simulation of the learning dissemination on a Project-Based
  Learning context considering the human aspects","  This work presents an agent-based simulation (ABS) of the active learning
process in an Electrical Engineering course. In order to generate input data to
the simulation, an active learning methodology developed especially for
part-time degree courses, called Project-Based Learning Agile (PBLA), has been
proposed and implemented at the Regional University of Blumenau (FURB), Brazil.
Through the analysis of survey responses obtained over five consecutive
semesters, using partial least squares path modeling (PLS-PM), it was possible
to generate data parameters to use as an input in a hybrid kind of agent-based
simulation known as PLS agent. The simulation of the scenario suggests that the
learning occur faster when the student has higher levels of humanist's aspects
as self-esteem, self-realization and cooperation.
"
1705.07578,2017-05-23,Semiparametric estimation in the normal variance-mean mixture model,"  In this paper we study the problem of statistical inference on the parameters
of the semiparametric variance-mean mixtures. This class of mixtures has
recently become rather popular in statistical and financial modelling. We
design a semiparametric estimation procedure that first estimates the mean of
the underlying normal distribution and then recovers nonparametrically the
density of the corresponding mixing distribution. We illustrate the performance
of our procedure on simulated and real data.
"
1705.07825,2017-05-23,"Comparing the Finite-Time Performance of Simulation-Optimization
  Algorithms","  We empirically evaluate the finite-time performance of several
simulation-optimization algorithms on a testbed of problems with the goal of
motivating further development of algorithms with strong finite-time
performance. We investigate if the observed performance of the algorithms can
be explained by properties of the problems, e.g., the number of decision
variables, the topology of the objective function, or the magnitude of the
simulation error.
"
1705.07890,2017-05-24,"Explanation and exact formula of Zipfs law evaluated from rank-share
  combinatorics","  This work proves that ranks and shares are statistically dependent on one
another, based on simple combinatorics. It presents a formula for rank-share
distribution and illustrates that Zipfs law, is descended from expected values
of various ranks in the new distribution. All conclusions, formulas and charts
presented here were tested against publicly available statistical data in
different areas. The correlation coefficient between the calculated values and
statistical numbers provided by Bureau of Labor Statistics was 0.99899.
Monte-Carlo simulations were performed as additional evidence.
"
1705.08082,2017-05-24,"An Investigation of the Different Levels of Poverty and the
  Corresponding Variance in Student Academic Prosperity","  Underprivileged students, especially in primary school, have shown to have
less access to educational materials often resulting in general dissatisfaction
in the school system and lower academic performance (Saatcioglu and Rury, 2012,
p.23). The relationship between family socioeconomic status and student
interest in academic endeavors, level of classroom engagement, and
participation in extracurricular programs were analyzed. Socioeconomic status
was categorized as below poverty level, at or above poverty level, 100 to 199
percent of poverty, and 200 percent of poverty or higher (United States Census
Bureau). Student interest, engagement, and persistence were measured as a
scalar quantity of three variables: never, sometimes, and often. The
participation of students in extracurricular activities was also compared based
on the same categories of socioeconomic status. After running the multivariate
analysis of variance, it was found that there was a statistically significant
variance of student academic prosperity and poverty level.
"
1705.08544,2020-07-21,"Data Visualization on Day One: Bringing Big Ideas into Intro Stats Early
  and Often","  In a world awash with data, the ability to think and compute with data has
become an important skill for students in many fields. For that reason,
inclusion of some level of statistical computing in many introductory-level
courses has grown more common in recent years. Existing literature has
documented multiple success stories of teaching statistics with R, bolstered by
the capabilities of R Markdown. In this article, we present an in-class data
visualization activity intended to expose students to R and R Markdown during
the first week of an introductory statistics class. The activity begins with a
brief lecture on exploratory data analysis in R. Students are then placed in
small groups tasked with exploring a new dataset to produce three
visualizations that describe particular insights that are not immediately
obvious from the data. Upon completion, students will have produced a series of
univariate and multivariate visualizations on a real dataset and practiced
describing them.
"
1705.09530,2020-07-21,"Updated guidelines, updated curriculum: The GAISE College Report and
  introductory statistics for the modern student","  Since the 2005 American Statistical Association's (ASA) endorsement of the
Guidelines for Assessment and Instruction in Statistics Education (GAISE)
College Report, changes in the statistics field and statistics education have
had a major impact on the teaching and learning of statistics. We now live in a
world where ""Statistics - the science of learning from data - is the
fastest-growing science, technology, engineering, and math (STEM) undergraduate
degree in the United States,"" according to the ASA, and where many jobs demand
an understanding of how to explore and make sense of data. In light of these
new reports and other changes and demands on the discipline, a group of
volunteers revised the 2005 GAISE College Report. The updated report was
endorsed by the Board of Directors of the American Statistical Association in
July 2016. To help shed additional light on the revision process and subsequent
changes in the report, we review the report and share insights into the
committee's thoughts and assumptions.
"
1706.00599,2018-09-25,On a Class of Objective Priors from Scoring Rules,"  Objective prior distributions represent an important tool that allows one to
have the advantages of using the Bayesian framework even when information about
the parameters of a model is not available. The usual objective approaches work
off the chosen statistical model and in the majority of cases the resulting
prior is improper, which can pose limitations to a practical implementation,
even when the complexity of the model is moderate. In this paper we propose to
take a novel look at the construction of objective prior distributions, where
the connection with a chosen sampling distribution model is removed. We explore
the notion of defining objective prior distributions which allow one to have
some degree of flexibility, in particular in exhibiting some desirable
features, such as being proper, or centered on specific values which would be
of interest in nested model comparisons. The basic tool we use are proper
scoring rules and the main result is a class of objective prior distributions
that can be employed in scenarios where the usual model based priors fail, such
as mixture models and model selection via Bayes factors. In addition, we show
that the proposed class of priors is the result of minimising the information
it contains, providing solid interpretation to the method.
"
1706.01584,2018-05-23,"Multiple Object Tracking in Unknown Backgrounds with Labeled Random
  Finite Sets","  This paper proposes an on-line multiple object tracking algorithm that can
operate in unknown background. In a majority of multiple object tracking
applications, model parameters for background processes such as clutter and
detection are unknown and vary with time, hence the ability of the algorithm to
adaptively learn the these parameters is essential in practice. In this work,
we detail how the Generalized Labeled Multi Bernouli (GLMB) filter a tractable
and provably Bayes optimal multi-object tracker can be tailored to learn
clutter and detection parameters on the fly while tracking. Provided that these
background model parameters do not fluctuate rapidly compared to the data rate,
the proposed algorithm can adapt to the unknown background yielding better
tracking performance.
"
1706.03805,2017-06-14,Fiducial on a string,"  The fiducial argument of Fisher (1973) has been described as his biggest
blunder, but the recent review of Hannig et al. (2016) demonstrates the current
and increasing interest in this brilliant idea. This short note analyses an
example introduced by Seidenfeld (1992) where the fiducial distribution is
restricted to a string.
  Keywords and phrases: Bayesian and fiducial inference, Restrictions on
parameters, Uncertainty quantification, Epistemic probability, Statistics on a
manifold.
"
1706.06354,2018-09-05,"Consistency of the plug-in functional predictor of the
  Ornstein-Uhlenbeck process in Hilbert and Banach spaces","  New results on functional prediction of the Ornstein-Uhlenbeck process in an
autoregressive Hilbert-valued and Banach-valued frameworks are derived.
Specifically, consistency of the maximum likelihood estimator of the
autocorrelation operator, and of the associated plug-in predictor is obtained
in both frameworks.
"
1706.06498,2018-09-05,Asymptotic properties of a componentwise ARH(1) plug-in predictor,"  This paper presents new results on prediction of linear processes in function
spaces. The autoregressive Hilbertian process framework of order one (ARH(1)
process framework) is adopted. A componentwise estimator of the autocorrelation
operator is formulated, from the moment-based estimation of its diagonal
coefficients, with respect to the orthogonal eigenvectors of the
auto-covariance operator, which are assumed to be known. Mean-square
convergence to the theoretical autocorrelation operator, in the space of
Hilbert-Schmidt operators, is proved. Consistency then follows in that space.
For the associated ARH(1) plug-in predictor, mean absolute convergence to the
corresponding conditional expectation, in the considered Hilbert space, is
obtained. Hence, consistency in that space also holds. A simulation study is
undertaken to illustrate the finite-large sample behavior of the formulated
componentwise estimator and predictor. The performance of the presented
approach is compared with alternative approaches in the previous and current
ARH(1) framework literature, including the case of unknown eigenvectors.
"
1706.08137,2017-07-11,A Contemporary Overview of Probabilistic Latent Variable Models,"  In this paper we provide a conceptual overview of latent variable models
within a probabilistic modeling framework, an overview that emphasizes the
compositional nature and the interconnectedness of the seemingly disparate
models commonly encountered in statistical practice.
"
1706.08702,2017-06-28,"A network flow approach to visualising the roles of covariates in random
  forests","  We propose novel applications of parallel coordinates plots and Sankey
diagrams to represent the hierarchies of interacting covariate effects in
random forests. Each visualisation summarises the frequencies of all of the
paths through all of the trees in a random forest. Visualisations of the roles
of covariates in random forests include: ranked bar or dot charts depicting
scalar metrics of the contributions of individual covariates to the predictive
accuracy of the random forest; line graphs depicting various summaries of the
effect of varying a particular covariate on the predictions from the random
forest; heatmaps of metrics of the strengths of interactions between all pairs
of covariates; and parallel coordinates plots for each response class depicting
the distributions of the values of all covariates among the observations most
representative of those predicted to belong that class. Together these
visualisations facilitate substantial insights into the roles of covariates in
a random forest but do not communicate the frequencies of the hierarchies of
covariates effects across the random forest or the orders in which covariates
occur in these hierarchies. Our visualisations address these gaps. We
demonstrate our visualisations using a random forest fitted to publicly
available data and provide a software implementation in the form of an R
package.
"
1707.00944,2017-07-05,A novel entropy recurrence quantification analysis,"  The growing study of time series, especially those related to nonlinear
systems, has challenged the methodologies to characterize and classify
dynamical structures of a signal. Here we conceive a new diagnostic tool for
time series based on the concept of information entropy, in which the
probabilities are associated to microstates defined from the recurrence phase
space. Recurrence properties can properly be studied using recurrence plots, a
methodology based on binary matrices where trajec- tories in phase space of
dynamical systems are evaluated against other embedded trajectory. Our novel
entropy methodology has several advantages compared to the traditional
recurrence entropy defined in the literature, namely, the correct evaluation of
the chaoticity level of the signal, the weak dependence on parameters, correct
evaluation of periodic time series properties and more sensitivity to noise
level of time series. Furthermore, the new entropy quantifier developed in this
manuscript also fixes inconsistent results of the traditional recurrence
entropy concept, reproducing classical results with novel insights.
"
1707.01031,2018-07-04,"Decision-Making and Biases in Cybersecurity Capability Development:
  Evidence from a Simulation Game Experiment","  We developed a simulation game to study the effectiveness of decision-makers
in overcoming two complexities in building cybersecurity capabilities:
potential delays in capability development; and uncertainties in predicting
cyber incidents. Analyzing 1,479 simulation runs, we compared the performances
of a group of experienced professionals with those of an inexperienced control
group. Experienced subjects did not understand the mechanisms of delays any
better than inexperienced subjects; however, experienced subjects were better
able to learn the need for proactive decision-making through an iterative
process. Both groups exhibited similar errors when dealing with the uncertainty
of cyber incidents. Our findings highlight the importance of training for
decision-makers with a focus on systems thinking skills, and lay the groundwork
for future research on uncovering mental biases about the complexities of
cybersecurity.
"
1707.02070,2017-07-10,"Coherent combination of probabilistic outputs for group decision making:
  an algebraic approach","  Current decision support systems address domains that are heterogeneous in
nature and becoming progressively larger. Such systems often require the input
of expert judgement about a variety of different fields and an intensive
computational power to produce the scores necessary to rank the available
policies. Recently, integrating decision support systems have been introduced
to enable a formal Bayesian multi-agent decision analysis to be distributed and
consequently efficient. In such systems, where different panels of experts
oversee disjoint but correlated vectors of variables, each expert group needs
to deliver only certain summaries of the variables under their jurisdiction to
properly derive an overall score for the available policies. Here we present an
algebraic approach that makes this methodology feasible for a wide range of
modelling contexts and that enables us to identify the summaries needed for
such a combination of judgements. We are also able to demonstrate that
coherence, in a sense we formalize here, is still guaranteed when panels only
share a partial specification of their model with other panel members. We
illustrate this algebraic approach by applying it to a specific class of
Bayesian networks and demonstrate how we can use it to derive closed form
formulae for the computations of the joint moments of variables that determine
the score of different policies.
"
1707.02748,2017-07-11,Confidence biases and learning among intuitive Bayesians,"  We design a double-or-quits game to compare the speed of learning one's
specific ability with the speed of rising confidence as the task gets
increasingly difficult. We find that people on average learn to be
overconfident faster than they learn their true ability and we present an
intuitive-Bayesian model of confidence which integrates confidence biases and
learning. Uncertainty about one's true ability to perform a task in isolation
can be responsible for large and stable confidence biases, namely limited
discrimination, the hard--easy effect, the Dunning--Kruger effect, conservative
learning from experience and the overprecision phenomenon (without
underprecision) if subjects act as Bayesian learners who rely only on
sequentially perceived performance cues and contrarian illusory signals induced
by doubt. Moreover, these biases are likely to persist since the Bayesian
aggregation of past information consolidates the accumulation of errors and the
perception of contrarian illusory signals generates conservatism and
under-reaction to events. Taken together, these two features may explain why
intuitive Bayesians make systematically wrong predictions of their own
performance.
"
1707.05769,2020-04-07,Estimation of Inverse Weibull Distribution Under Type-I Hybrid Censoring,"  The hybrid censoring is a mixture of Type I and Type II censoring schemes.
This paper presents the statistical inferences of the Inverse Weibull
distribution when the data are Type-I hybrid censored. First we consider the
maximum likelihood estimators of the unknown parameters. It is observed that
the maximum likelihood estimators can not be obtained in closed form. We
further obtain the Bayes estimators and the corresponding highest posterior
density credible intervals of the unknown parameters under the assumption of
independent gamma priors using the importance sampling procedure. We also
compute the approximate Bayes estimators using Lindley's approximation
technique. We have performed a simulation study and a real data analysis in
order to compare the proposed Bayes estimators with the maximum likelihood
estimators.
"
1707.05804,2017-07-20,"Estimation of P(X > Y ) for Weibull distribution based on hybrid
  censored samples","  A Hybrid censoring scheme is mixture of Type-I and Type-II censoring schemes.
Based on hybrid censored samples, this paper deals with the in- ference on R =
P(X > Y ), when X and Y are two independent Weibull distributions with
different scale parameters, but having the same shape pa- rameter. The maximum
likelihood estimator (MLE), and the approximate MLE (AMLE) of R are obtained.
The asymptotic distribution of the maxi- mum likelihood estimator of R is
obtained. Based on the asymptotic distribu- tion, the confidence interval of R
can be derived. Two bootstrap confidence intervals are also proposed. We
consider the Bayesian estimate of R, and propose the corresponding credible
interval for R. Monte Carlo simulations are performed to compare the different
proposed methods. Analysis of a real data set has also been presented for
illustrative purposes.
"
1707.06537,2017-12-05,"Cognitive Transfer Outcomes for a Simulation-Based Introductory
  Statistics Curriculum","  Cognitive transfer is the ability to apply learned skills and knowledge to
new applications and contexts. This investigation evaluates cognitive transfer
outcomes for a tertiary-level introductory statistics course using the CATALST
curriculum, which exclusively used simulation-based methods to develop
foundations of statistical inference. A common assessment instrument
administered at the end of each course measured learning outcomes for students.
CATALST students showed evidence of both near and far transfer outcomes while
scoring as high, or higher on the assessed learning objectives, when compared
with peers enrolled in similar courses that emphasized parametric inferential
methods (e.g. the t-test).
"
1707.07607,2017-07-25,"We are not alone ! (at least, most of us). Homonymy in large scale
  social groups","  This article brings forward an estimation of the proportion of homonyms in
large scale groups based on the distribution of first names and last names in a
subset of these groups. The estimation is based on the generalization of the
""birthday paradox problem"". The main results is that, in societies such as
France or the United States, identity collisions (based on first + last names)
are frequent. The large majority of the population has at least one homonym.
But in smaller settings, it is much less frequent : even if small groups of a
few thousand people have at least one couple of homonyms, only a few
individuals have an homonym.
"
1707.07625,2018-05-14,Restoring a smooth function from its noisy integrals,"  Numerical (and experimental) data analysis often requires the restoration of
a smooth function from a set of sampled integrals over finite bins. We present
the bin hierarchy method that efficiently computes the maximally smooth
function from the sampled integrals using essentially all the information
contained in the data. We perform extensive tests with different classes of
functions and levels of data quality, including Monte Carlo data suffering from
a severe sign problem and physical data for the Green's function of the
Fr\""ohlich polaron.
"
1707.09076,2017-10-10,Sensitivity Analysis for Unmeasured Confounding in Meta-Analyses,"  Random-effects meta-analyses of observational studies can produce biased
estimates if the synthesized studies are subject to unmeasured confounding. We
propose sensitivity analyses quantifying the extent to which unmeasured
confounding of specified magnitude could reduce to below a certain threshold
the proportion of true effect sizes that are scientifically meaningful. We also
develop converse methods to estimate the strength of confounding capable of
reducing the proportion of scientifically meaningful true effects to below a
chosen threshold. These methods apply when a ""bias factor"" is assumed to be
normally distributed across studies or is assessed across a range of fixed
values. Our estimators are derived using recently proposed sharp bounds on
confounding bias within a single study that do not make assumptions regarding
the unmeasured confounders themselves or the functional form of their
relationships to the exposure and outcome of interest. We provide an R package,
ConfoundedMeta, and a freely available online graphical user interface that
compute point estimates and inference and produce plots for conducting such
sensitivity analyses. These methods facilitate principled use of random-effects
meta-analyses of observational studies to assess the strength of causal
evidence for a hypothesis.
"
1707.09319,2017-07-31,"A Fourier-invariant method for locating point-masses and computing their
  attributes","  Motivated by the interest of observing the growth of cancer cells among
normal living cells and exploring how galaxies and stars are truly formed, the
objective of this paper is to introduce a rigorous and effective method for
counting point-masses, determining their spatial locations, and computing their
attributes. Based on computation of Hermite moments that are Fourier-invariant,
our approach facilitates the processing of both spatial and Fourier data in any
dimension.
"
1708.00060,2017-10-09,"Application of Bayesian Networks for Estimation of Individual
  Psychological Characteristics","  An accurate qualitative and comprehensive assessment of human potential is
one of the most important challenges in any company or collective. We apply
Bayesian networks for developing more accurate overall estimations of
psychological characteristics of an individual, based on psychological test
results, which identify how much an individual possesses a certain trait.
Examples of traits could be a stress resistance, the readiness to take a risk,
the ability to concentrate on certain complicated work. The most common way of
studying psychological characteristics of each individual is testing.
Additionally, the overall estimation is usually based on personal experiences
and the subjective perception of a psychologist or a group of psychologists
about the investigated psychological personality traits.
"
1708.04098,2017-08-15,Statistics Educational Challenge in the 21st Century,"  What do we teach and what should we teach? An honest answer to this question
is painful, very painful--what we teach lags decades behind what we practice.
How can we reduce this `gap' to prepare a data science workforce of trained
next-generation statisticians? This is a challenging open problem that requires
many well-thought-out experiments before finding the secret sauce. My goal in
this article is to lay out some basic principles and guidelines (rather than
creating a pseudo-curriculum based on cherry-picked topics) to expedite this
process for finding an `objective' solution.
"
1708.05069,2017-08-18,"A causation coefficient and taxonomy of correlation/causation
  relationships","  This paper introduces a causation coefficient which is defined in terms of
probabilistic causal models. This coefficient is suggested as the natural
causal analogue of the Pearson correlation coefficient and permits comparing
causation and correlation to each other in a simple, yet rigorous manner.
Together, these coefficients provide a natural way to classify the possible
correlation/causation relationships that can occur in practice and examples of
each relationship are provided. In addition, the typical relationship between
correlation and causation is analyzed to provide insight into why correlation
and causation are often conflated. Finally, example calculations of the
causation coefficient are shown on a real data set.
"
1708.06992,2020-06-26,Econom\'etrie et Machine Learning,"  Econometrics and machine learning seem to have one common goal: to construct
a predictive model, for a variable of interest, using explanatory variables (or
features). However, these two fields developed in parallel, thus creating two
different cultures, to paraphrase Breiman (2001). The first was to build
probabilistic models to describe economic phenomena. The second uses algorithms
that will learn from their mistakes, with the aim, most often to classify
(sounds, images, etc.). Recently, however, learning models have proven to be
more effective than traditional econometric techniques (with a price to pay
less explanatory power), and above all, they manage to manage much larger data.
In this context, it becomes necessary for econometricians to understand what
these two cultures are, what opposes them and especially what brings them
closer together, in order to appropriate tools developed by the statistical
learning community to integrate them into Econometric models.
"
1708.07059,2017-08-24,"Redundancy schemes for engineering coherent systems via a
  signature-based approach","  This paper proposes a signature-based approach for solving redundancy
allocation problems when component lifetimes are not only heterogeneous but
also dependent. The two common schemes for allocations, that is active and
standby redundancies, are considered. If the component lifetimes are
independent, the proposed approach leads to simple manipulations. Various
illustrative examples are also analysed. This method can be implemented for
practical complex engineering systems.
"
1708.09344,2017-08-31,"Stem-ming the Tide: Predicting STEM attrition using student transcript
  data","  Science, technology, engineering, and math (STEM) fields play growing roles
in national and international economies by driving innovation and generating
high salary jobs. Yet, the US is lagging behind other highly industrialized
nations in terms of STEM education and training. Furthermore, many economic
forecasts predict a rising shortage of domestic STEM-trained professions in the
US for years to come. One potential solution to this deficit is to decrease the
rates at which students leave STEM-related fields in higher education, as
currently over half of all students intending to graduate with a STEM degree
eventually attrite. However, little quantitative research at scale has looked
at causes of STEM attrition, let alone the use of machine learning to examine
how well this phenomenon can be predicted. In this paper, we detail our efforts
to model and predict dropout from STEM fields using one of the largest known
datasets used for research on students at a traditional campus setting. Our
results suggest that attrition from STEM fields can be accurately predicted
with data that is routinely collected at universities using only information on
students' first academic year. We also propose a method to model student STEM
intentions for each academic term to better understand the timing of STEM
attrition events. We believe these results show great promise in using machine
learning to improve STEM retention in traditional and non-traditional campus
settings.
"
1709.03154,2017-09-12,Recent progress in log-concave density estimation,"  In recent years, log-concave density estimation via maximum likelihood
estimation has emerged as a fascinating alternative to traditional
nonparametric smoothing techniques, such as kernel density estimation, which
require the choice of one or more bandwidths. The purpose of this article is to
describe some of the properties of the class of log-concave densities on
$\mathbb{R}^d$ which make it so attractive from a statistical perspective, and
to outline the latest methodological, theoretical and computational advances in
the area.
"
1709.03981,2017-09-14,Aggregating incoherent agents who disagree,"  In this paper, we explore how we should aggregate the degrees of belief of of
a group of agents to give a single coherent set of degrees of belief, when at
least some of those agents might be probabilistically incoherent. There are a
number of way of aggregating degrees of belief, and there are a number of ways
of fixing incoherent degrees of belief. When we have picked one of each, should
we aggregate first and then fix, or fix first and then aggregate? Or should we
try to do both at once? And when do these different procedures agree with one
another? In this paper, we focus particularly on the final question.
"
1709.06400,2017-09-20,"Distance Correlation: A New Tool for Detecting Association and Measuring
  Correlation Between Data Sets","  The difficulties of detecting association, measuring correlation, and
establishing cause and effect have fascinated mankind since time immemorial.
Democritus, the Greek philosopher, underscored well the importance and the
difficulty of proving causality when he wrote, ""I would rather discover one
cause than gain the kingdom of Persia."" To address the difficulties of relating
cause and effect, statisticians have developed many inferential techniques.
Perhaps the most well-known method stems from Karl Pearson's coefficient of
correlation, which Pearson introduced in the late 19th century based on ideas
of Francis Galton.
  I will describe in this lecture the recently-devised distance correlation
coefficient and describe its advantages over the Pearson and other classical
measures of correlation. We will examine an application of the distance
correlation coefficient to data drawn from large astrophysical databases, where
it is desired to classify galaxies according to various types. Further, the
lecture will analyze data arising in the ongoing national discussion of the
relationship between state-by-state homicide rates and the stringency of state
laws governing firearm ownership.
  The lecture will also describe a remarkable singular integral which lies at
the core of the theory of the distance correlation coefficient. We will see
that this singular integral admits generalizations to the truncated Maclaurin
expansions of the cosine function and to the theory of spherical functions on
symmetric cones.
"
1710.00190,2017-10-03,"Power analysis for a linear regression model when regressors are matrix
  sampled","  Multiple matrix sampling is a survey methodology technique that randomly
chooses a relatively small subset of items to be presented to survey
respondents for the purpose of reducing respondent burden. The data produced
are missing completely at random (MCAR), and special missing data techniques
should be used in linear regression and other multivariate statistical
analysis. We derive asymptotic variances of regression parameter estimates that
allow us to conduct power analysis for linear regression models fit to the data
obtained via a multiple matrix sampling design. The ideas are demonstrated with
a variation of the Big Five Inventory of psychological traits. An exploration
of the regression parameter space demonstrates instability of the sample size
requirements, and substantial losses of precision with matrix-sampled
regressors. A simulation with non-normal data demonstrates the advantages of a
semi-parametric multiple imputation scheme.
"
1710.01298,2017-10-04,A Coin-Tossing Conundrum,"  It is shown that an equiprobability hypothesis leads to a scenario in which
it is possible to predict the outcome of a single toss of a fair coin with a
success probability greater than 50%. We discuss whether this hypothesis might
be independent of the usual hypotheses governing probability, as well as
whether this hypothesis might be assumed as a result of the Principle of
Indifference. Also discussed are ways to implement or circumvent the
hypothesis.
"
1710.01344,2017-10-05,"Development of models for predicting Torsade de Pointes cardiac
  arrhythmias using perceptron neural networks","  Blockage of some ion channels and in particular, the hERG cardiac potassium
channel delays cardiac repolarization and can induce arrhythmia. In some cases
it leads to a potentially life-threatening arrhythmia known as Torsade de
Pointes (TdP). Therefore recognizing drugs with TdP risk is essential.
Candidate drugs that are determined not to cause cardiac ion channel blockage
are more likely to pass successfully through clinical phases II and III trials
(and preclinical work) and not be withdrawn even later from the marketplace due
to cardiotoxic effects. The objective of the present study is to develop an SAR
model that can be used as an early screen for torsadogenic (causing TdP
arrhythmias) potential in drug candidates. The method is performed using
descriptors comprised of atomic NMR chemical shifts and corresponding
interatomic distances which are combined into a 3D abstract space matrix. The
method is called 3D-SDAR (3 dimensional spectral data-activity relationship)
and can be interrogated to identify molecular features responsible for the
activity, which can in turn yield simplified hERG toxicophores. A dataset of 55
hERG potassium channel inhibitors collected from Kramer et al. consisting of 32
drugs with TdP risk and 23 with no TdP risk was used for training the 3D-SDAR
model.An ANN model with multilayer perceptron was used to define collinearities
among the independent 3D-SDAR features. A composite model from 200 random
iterations with 25% of the molecules in each case yielded the following figures
of merit: training, 99.2 %; internal test sets, 66.7%; external (blind
validation) test set, 68.4%. In the external test set, 70.3% of positive TdP
drugs were correctly predicted. Moreover, toxicophores were generated from TdP
drugs. A 3D-SDAR was successfully used to build a predictive model for
drug-induced torsadogenic and non-torsadogenic drugs.
"
1710.01598,2017-10-27,A geometer's view of the the Cram\'er-Rao bound on estimator variance,"  The classical Cram\'er-Rao inequality gives a lower bound for the variance of
a unbiased estimator of an unknown parameter, in some statistical model of a
random process. In this note we rewrite the statment and proof of the bound
using contemporary geometric language.
"
1710.01946,2022-10-12,Scientific progress despite irreproducibility: A seeming paradox,"  It appears paradoxical that science is producing outstanding new results and
theories at a rapid rate at the same time that researchers are identifying
serious problems in the practice of science that cause many reports to be
irreproducible and invalid. Certainly the practice of science needs to be
improved and scientists are now pursuing this goal. However, in this
perspective we argue that this seeming paradox is not new, has always been part
of the way science works, and likely will remain so. We first introduce the
paradox. We then review a wide range of challenges that appear to make
scientific success difficult. Next, we describe the factors that make science
work-in the past, present, and presumably also in the future. We then suggest
that remedies for the present practice of science need to be applied
selectively so as not to slow progress, and illustrate with a few examples. We
conclude with arguments that communication of science needs to emphasize not
just problems but the enormous successes and benefits that science has brought
and is now bringing to all elements of modern society.
"
1710.02824,2017-11-15,"Beating the bookies with their own numbers - and how the online sports
  betting market is rigged","  The online sports gambling industry employs teams of data analysts to build
forecast models that turn the odds at sports games in their favour. While
several betting strategies have been proposed to beat bookmakers, from expert
prediction models and arbitrage strategies to odds bias exploitation, their
returns have been inconsistent and it remains to be shown that a betting
strategy can outperform the online sports betting market. We designed a
strategy to beat football bookmakers with their own numbers. Instead of
building a forecasting model to compete with bookmakers predictions, we
exploited the probability information implicit in the odds publicly available
in the marketplace to find bets with mispriced odds. Our strategy proved
profitable in a 10-year historical simulation using closing odds, a 6-month
historical simulation using minute to minute odds, and a 5-month period during
which we staked real money with the bookmakers (we made code, data and models
publicly available). Our results demonstrate that the football betting market
is inefficient - bookmakers can be consistently beaten across thousands of
games in both simulated environments and real-life betting. We provide a
detailed description of our betting experience to illustrate how the sports
gambling industry compensates these market inefficiencies with discriminatory
practices against successful clients.
"
1710.05878,2017-10-17,"A response to: ""NIST experts urge caution in use of courtroom evidence
  presentation method""","  A press release from the National Institute of Standards and Technology
(NIST)could potentially impede progress toward improving the analysis of
forensic evidence and the presentation of forensic analysis results in courts
in the United States and around the world. ""NIST experts urge caution in use of
courtroom evidence presentation method"" was released on October 12, 2017, and
was picked up by the phys.org news service. It argues that, except in
exceptional cases, the results of forensic analyses should not be reported as
""likelihood ratios"". The press release, and the journal article by NIST
researchers Steven P. Lund & Harri Iyer on which it is based, identifies some
legitimate points of concern, but makes a strawman argument and reaches an
unjustified conclusion that throws the baby out with the bathwater.
"
1710.07284,2020-02-14,"Replacing P values with frequentist posterior probabilities - as
  possible parameter values must have uniform base-rate prior probabilities by
  definition in a random sampling model","  Possible parameter values in a random sampling model are shown by definition
to have uniform base-rate prior probabilities. This allows a frequentist
posterior probability distribution to be calculated for such possible parameter
values conditional solely on actual study observations. If the likelihood
probability distribution of a random selection is modelled with a symmetrical
continuous function then the frequentist posterior probability of something
equal to or more extreme than the null hypothesis will be equal to the P-value;
otherwise the P value would be an approximation. An idealistic probability of
replication based on an assumption of perfect study methodological
reproducibility can be used as the upper bound of a realistic probability of
replication that may be affected by various confounding factors. Bayesian
distributions can be combined with these frequentist distributions. The
idealistic frequentist posterior probability of replication may be easier than
the P-value for non-statisticians to understand and to interpret.
"
1710.08511,2020-11-17,"An Expectation Maximization Framework for Yule-Simon Preferential
  Attachment Models","  In this paper we develop an Expectation Maximization(EM) algorithm to
estimate the parameter of a Yule-Simon distribution. The Yule-Simon
distribution exhibits the ""rich get richer"" effect whereby an 80-20 type of
rule tends to dominate. These distributions are ubiquitous in industrial
settings. The EM algorithm presented provides both frequentist and Bayesian
estimates of the $\lambda$ parameter. By placing the estimation method within
the EM framework we are able to derive Standard errors of the resulting
estimate. Additionally, we prove convergence of the Yule-Simon EM algorithm and
study the rate of convergence. An explicit, closed form solution for the rate
of convergence of the algorithm is given. Applications including graph node
degree distribution estimation are listed.
"
1710.08728,2020-07-21,Greater data science at baccalaureate institutions,"  Donoho's JCGS (in press) paper is a spirited call to action for
statisticians, who he points out are losing ground in the field of data science
by refusing to accept that data science is its own domain. (Or, at least, a
domain that is becoming distinctly defined.) He calls on writings by John
Tukey, Bill Cleveland, and Leo Breiman, among others, to remind us that
statisticians have been dealing with data science for years, and encourages
acceptance of the direction of the field while also ensuring that statistics is
tightly integrated.
  As faculty at baccalaureate institutions (where the growth of undergraduate
statistics programs has been dramatic), we are keen to ensure statistics has a
place in data science and data science education. In his paper, Donoho is
primarily focused on graduate education. At our undergraduate institutions, we
are considering many of the same questions.
"
1710.08775,2017-10-25,Markov Properties for Graphical Models with Cycles and Latent Variables,"  We investigate probabilistic graphical models that allow for both cycles and
latent variables. For this we introduce directed graphs with hyperedges
(HEDGes), generalizing and combining both marginalized directed acyclic graphs
(mDAGs) that can model latent (dependent) variables, and directed mixed graphs
(DMGs) that can model cycles. We define and analyse several different Markov
properties that relate the graphical structure of a HEDG with a probability
distribution on a corresponding product space over the set of nodes, for
example factorization properties, structural equations properties,
ordered/local/global Markov properties, and marginal versions of these. The
various Markov properties for HEDGes are in general not equivalent to each
other when cycles or hyperedges are present, in contrast with the simpler case
of directed acyclic graphical (DAG) models (also known as Bayesian networks).
We show how the Markov properties for HEDGes - and thus the corresponding
graphical Markov models - are logically related to each other.
"
1710.09713,2017-10-27,"The relationship between the number of editorial board members and the
  scientific output of universities in the chemistry field","  Editorial board members, who are considered the gatekeepers of scientific
journals, play an important role in academia, and may directly or indirectly
affect the scientific output of a university. In this article, we used the
quantile regression method among a sample of 1,387 university in chemistry to
characterize the correlation between the number of editorial board members and
the scientific output of their universities. Furthermore, we used time-series
data and the Granger causality test to explore the causal relationship between
the number of editorial board members and the number of articles of some top
universities. Our results suggest that the number of editorial board members is
positively and significantly related to the scientific output (as measured by
the number of articles, total number of citations, citations per paper, and h
index) of their universities. However, the Granger causality test results
suggest that the causal relationship between the number of editorial board
members and the number of articles of some top universities is not obvious.
Combining these findings with the results of qualitative interviews with
editorial board members, we discuss the causal relationship between the number
of editorial board members and the scientific output of their universities.
"
1711.01598,2017-11-07,Multilayer tensor factorization with applications to recommender systems,"  Recommender systems have been widely adopted by electronic commerce and
entertainment industries for individualized prediction and recommendation,
which benefit consumers and improve business intelligence. In this article, we
propose an innovative method, namely the recommendation engine of multilayers
(REM), for tensor recommender systems. The proposed method utilizes the
structure of a tensor response to integrate information from multiple modes,
and creates an additional layer of nested latent factors to accommodate
between-subjects dependency. One major advantage is that the proposed method is
able to address the ""cold-start"" issue in the absence of information from new
customers, new products or new contexts. Specifically, it provides more
effective recommendations through sub-group information. To achieve scalable
computation, we develop a new algorithm for the proposed method, which
incorporates a maximum block improvement strategy into the cyclic
blockwise-coordinate-descent algorithm. In theory, we investigate both
algorithmic properties for global and local convergence, along with the
asymptotic consistency of estimated parameters. Finally, the proposed method is
applied in simulations and IRI marketing data with 116 million observations of
product sales. Numerical studies demonstrate that the proposed method
outperforms existing competitors in the literature.
"
1711.02580,2017-11-08,"Quantifying the Influence of Component Failure Probability on Cascading
  Blackout Risk","  The risk of cascading blackouts greatly relies on failure probabilities of
individual components in power grids. To quantify how component failure
probabilities (CFP) influences blackout risk (BR), this paper proposes a
sample-induced semi-analytic approach to characterize the relationship between
CFP and BR. To this end, we first give a generic component failure probability
function (CoFPF) to describe CFP with varying parameters or forms. Then the
exact relationship between BR and CoFPFs is built on the abstract
Markov-sequence model of cascading outages. Leveraging a set of samples
generated by blackout simulations, we further establish a sample-induced
semi-analytic mapping between the unbiased estimation of BR and CoFPFs.
Finally, we derive an efficient algorithm that can directly calculate the
unbiased estimation of BR when the CoFPFs change. Since no additional
simulations are required, the algorithm is computationally scalable and
efficient. Numerical experiments well confirm the theory and the algorithm.
"
1711.02639,2017-11-08,On the Virtues of Automated QSAR The New Kid on the Block,"  Quantitative Structure-Activity Relationship (QSAR) has proved an invaluable
tool in medicinal chemistry. Data availability at unprecedented levels through
various databases have collaborated to a resurgence in the interest for QSAR.
In this context, rapid generation of quality predictive models is highly
desirable for hit identification and lead optimization. We showcase the
application of an automated QSAR approach, which randomly selects multiple
training/test sets and utilizes machine-learning algorithms to generate
predictive models. Results demonstrate that AutoQSAR produces models of
improved or similar quality to those generated by practitioners in the field
but in just a fraction of the time. Despite the potential of the concept to the
benefit of the community, the AutoQSAR opportunity has been largely
undervalued.
"
1711.04316,2019-10-23,"Implementation of the Bin Hierarchy Method for restoring a smooth
  function from a sampled histogram","  We present $\texttt{BHM}$, a tool for restoring a smooth function from a
sampled histogram using the bin hierarchy method. The theoretical background of
the method is presented in [arXiv:1707.07625]. The code automatically generates
a smooth polynomial spline with the minimal acceptable number of knots from the
input data. It works universally for any sufficiently regular shaped
distribution and any level of data quality, requiring almost no external
parameter specification. It is particularly useful for large-scale numerical
data analysis. This paper explains the details of the implementation and the
use of the program.
"
1711.07801,2017-11-22,"Why ""Redefining Statistical Significance"" Will Not Improve
  Reproducibility and Could Make the Replication Crisis Worse","  A recent proposal to ""redefine statistical significance"" (Benjamin, et al.
Nature Human Behaviour, 2017) claims that false positive rates ""would
immediately improve"" by factors greater than two and replication rates would
double simply by changing the conventional cutoff for 'statistical
significance' from P<0.05 to P<0.005. I analyze the veracity of these claims,
focusing especially on how Benjamin, et al neglect the effects of P-hacking in
assessing the impact of their proposal. My analysis shows that once P-hacking
is accounted for the perceived benefits of the lower threshold all but
disappear, prompting two main conclusions: (i) The claimed improvements to
false positive rate and replication rate in Benjamin, et al (2017) are
exaggerated and misleading. (ii) There are plausible scenarios under which the
lower cutoff will make the replication crisis worse.
"
1711.09400,2017-11-28,"A Multi Objective Reliable Location-Inventory Capacitated Disruption
  Facility Problem with Penalty Cost Solve with Efficient Meta Historic
  Algorithms","  Logistics network is expected that opened facilities work continuously for a
long time horizon without any failure, but in real world problems, facilities
may face disruptions. This paper studies a reliable joint inventory location
problem to optimize the cost of facility locations, customers assignment, and
inventory management decisions when facilities face failure risks and do not
work. In our model we assume when a facility is out of work, its customers may
be reassigned to other operational facilities otherwise they must endure high
penalty costs associated with losing service. For defining the model closer to
real world problems, the model is proposed based on pmedian problem and the
facilities are considered to have limited capacities. We define a new binary
variable for showing that customers are not assigned to any facilities. Our
problem involves a biobjective model, the first one minimizes the sum of
facility construction costs and expected inventory holding costs, the second
one function that mentions for the first one is minimized maximum expected
customer costs under normal and failure scenarios. For solving this model we
use NSGAII and MOSS algorithms have been applied to find the Pareto archive
solution. Also, Response Surface Methodology (RSM) is applied for optimizing
the NSGAII Algorithm Parameters. We compare the performance of two algorithms
with three metrics and the results show NSGAII is more suitable for our model.
"
1711.10262,2023-04-04,"Julian Ernst Besag, 26 March 1945 -- 6 August 2010, a biographical
  memoir","  Julian Besag was an outstanding statistical scientist, distinguished for his
pioneering work on the statistical theory and analysis of spatial processes,
especially conditional lattice systems. His work has been seminal in
statistical developments over the last several decades ranging from image
analysis to Markov chain Monte Carlo methods. He clarified the role of
auto-logistic and auto-normal models as instances of Markov random fields and
paved the way for their use in diverse applications. Later work included
investigations into the efficacy of nearest neighbour models to accommodate
spatial dependence in the analysis of data from agricultural field trials,
image restoration from noisy data, and texture generation using lattice models.
"
1711.10421,2018-05-31,A Review of Dynamic Network Models with Latent Variables,"  We present a selective review of statistical modeling of dynamic networks. We
focus on models with latent variables, specifically, the latent space models
and the latent class models (or stochastic blockmodels), which investigate both
the observed features and the unobserved structure of networks. We begin with
an overview of the static models, and then we introduce the dynamic extensions.
For each dynamic model, we also discuss its applications that have been studied
in the literature, with the data source listed in Appendix. Based on the
review, we summarize a list of open problems and challenges in dynamic network
modeling with latent variables.
"
1712.00544,2017-12-06,Conducting Highly Principled Data Science: A Statistician's Job and Joy,"  Highly Principled Data Science insists on methodologies that are: (1)
scientifically justified, (2) statistically principled, and (3) computationally
efficient. An astrostatistics collaboration, together with some reminiscences,
illustrates the increased roles statisticians can and should play to ensure
this trio, and to advance the science of data along the way.
"
1712.02410,2019-02-21,"Statistics students' identification of inferential model elements within
  contexts of their own invention","  Statistical thinking partially depends upon an iterative process by which
essential features of a problem setting are identified and mapped onto an
abstract model or archetype, and then translated back into the context of the
original problem setting (Wild and Pfannkuch 1999). Assessment in introductory
statistics often relies on tasks that present students with data in context and
expects them to choose and describe an appropriate model. This study explores
post-secondary student responses to an alternative task that prompts students
to clearly identify a sample, population, statistic, and parameter using a
context of their own invention. The data include free text narrative responses
of a random sample of 500 students from a sample of more than 1600 introductory
statistics students. Results suggest that students' responses often portrayed
sample and population accurately. Portrayals of statistic and parameter were
less reliable and were associated with descriptions of a wide variety of other
concepts. Responses frequently attributed a variable of some kind to the
statistic, or a study design detail to the parameter. Implications for
instruction and research are discussed, including a call for emphasis on a
modeling paradigm in introductory statistics.
"
1712.03168,2018-04-17,On optimal policy in the group testing with incomplete identification,"  Consider a very large (infinite) population of items, where each item
independent from the others is defective with probability p, or good with
probability q=1-p. The goal is to identify N good items as quickly as possible.
The following group testing policy (policy A) is considered: test items
together in the groups, if the test outcome of group i of size n_i is negative,
then accept all items in this group as good, otherwise discard the group. Then,
move to the next group and continue until exact N good items are found. The
goal is to find an optimal testing configuration, i.e., group sizes, under
policy A, such that the expected waiting time to obtain N good items is
minimal. Recently, Gusev (2012) found an optimal group testing configuration
under the assumptions of constant group size and N=\infty. In this note, an
optimal solution under policy A for finite N is provided. Keywords: Dynamic
programming; Optimal design; Partition problem; Shur-convexity
"
1712.04801,2017-12-14,"Open data, open review and open dialogue in making social sciences
  plausible","  Nowadays, protecting trust in social sciences also means engaging in open
community dialogue, which helps to safeguard robustness and improve efficiency
of research methods. The combination of open data, open review and open
dialogue may sound simple but implementation in the real world will not be
straightforward. However, in view of Begley and Ellis's (2012) statement that,
""the scientific process demands the highest standards of quality, ethics and
rigour,"" they are worth implementing. More importantly, they are feasible to
work on and likely will help to restore plausibility to social sciences
research. Therefore, I feel it likely that the triplet of open data, open
review and open dialogue will gradually emerge to become policy requirements
regardless of the research funding source.
"
1712.07349,2017-12-21,Data Science: A Three Ring Circus or a Big Tent?,"  This is part of a collection of discussion pieces on David Donoho's paper 50
Years of Data Science, appearing in Volume 26, Issue 4 of the Journal of
Computational and Graphical Statistics (2017).
"
1801.00371,2018-10-11,Data Science vs. Statistics: Two Cultures?,"  Data science is the business of learning from data, which is traditionally
the business of statistics. Data science, however, is often understood as a
broader, task-driven and computationally-oriented version of statistics. Both
the term data science and the broader idea it conveys have origins in
statistics and are a reaction to a narrower view of data analysis. Expanding
upon the views of a number of statisticians, this paper encourages a big-tent
view of data analysis. We examine how evolving approaches to modern data
analysis relate to the existing discipline of statistics (e.g. exploratory
analysis, machine learning, reproducibility, computation, communication and the
role of theory). Finally, we discuss what these trends mean for the future of
statistics by highlighting promising directions for communication, education
and research.
"
1801.00410,2018-01-03,Enhanced ${q}$-Least Mean Square,"  In this work, a new class of stochastic gradient algorithm is developed based
on $q$-calculus. Unlike the existing $q$-LMS algorithm, the proposed approach
fully utilizes the concept of $q$-calculus by incorporating time-varying $q$
parameter. The proposed enhanced $q$-LMS ($Eq$-LMS) algorithm utilizes a novel,
parameterless concept of error-correlation energy and normalization of signal
to ensure high convergence, stability and low steady-state error. The proposed
algorithm automatically adapts the learning rate with respect to the error. For
the evaluation purpose the system identification problem is considered.
Extensive experiments show better performance of the proposed $Eq$-LMS
algorithm compared to the standard $q$-LMS approach.
"
1801.01528,2018-01-08,"A deep learning approach for detecting traffic accidents from social
  media data","  This paper employs deep learning in detecting the traffic accident from
social media data. First, we thoroughly investigate the 1-year over 3 million
tweet contents in two metropolitan areas: Northern Virginia and New York City.
Our results show that paired tokens can capture the association rules inherent
in the accident-related tweets and further increase the accuracy of the traffic
accident detection. Second, two deep learning methods: Deep Belief Network
(DBN) and Long Short-Term Memory (LSTM) are investigated and implemented on the
extracted token. Results show that DBN can obtain an overall accuracy of 85%
with about 44 individual token features and 17 paired token features. The
classification results from DBN outperform those of Support Vector Machines
(SVMs) and supervised Latent Dirichlet allocation (sLDA). Finally, to validate
this study, we compare the accident-related tweets with both the traffic
accident log on freeways and traffic data on local roads from 15,000 loop
detectors. It is found that nearly 66% of the accident-related tweets can be
located by the accident log and more than 80% of them can be tied to nearby
abnormal traffic data. Several important issues of using Twitter to detect
traffic accidents have been brought up by the comparison including the location
and time bias, as well as the characteristics of influential users and
hashtags.
"
1801.01529,2019-01-24,"A novel calibration framework for survival analysis when a binary
  covariate is measured at sparse time points","  The goals in clinical and cohort studies often include evaluation of the
association of a time-dependent binary treatment or exposure with a survival
outcome. Recently, several impactful studies targeted the association between
aspirin-taking and survival following colorectal cancer diagnosis. Due to
surgery, aspirin-taking value is zero at baseline and may change its value to
one at some time point. Estimating this association is complicated by having
only intermittent measurements on aspirin-taking. Naive, commonly-used, methods
can lead to substantial bias. We present a class of calibration models for the
distribution of the time of status change of the binary covariate. Estimates
obtained from these models are then incorporated into the proportional hazard
partial likelihood in a natural way. We develop nonparametric, semiparametric
and parametric calibration models, and derive asymptotic theory for the methods
that we implement in the aspirin and colorectal cancer study. Our methodology
allows to include additional baseline variables in the calibration models for
the status change time of the binary covariate. We further develop a risk-set
calibration approach that is more useful in settings in which the association
between the binary covariate and survival is strong.
"
1801.02426,2018-01-09,Using Random Variables to Predict Experimental Outcomes,"  We shall show in this paper that there are experiments which are Bernoulli
trials with success probability p > 0.5, and which have the curious feature
that it is possible to correctly predict the outcome with probability > p.
"
1801.05755,2018-01-18,Discussions on non-probabilistic convex modelling for uncertain problems,"  Non-probabilistic convex model utilizes a convex set to quantify the
uncertainty domain of uncertain-but-bounded parameters, which is very effective
for structural uncertainty analysis with limited or poor-quality experimental
data. To overcome the complexity and diversity of the formulations of current
convex models, in this paper, a unified framework for construction of the
non-probabilistic convex models is proposed. By introducing the correlation
analysis technique, the mathematical expression of a convex model can be
conveniently formulated once the correlation matrix of the uncertain parameters
is created. More importantly, from the theoretic analysis level, an evaluation
criterion for convex modelling methods is proposed, which can be regarded as a
test standard for validity verification of subsequent newly proposed convex
modelling methods. And from the practical application level, two model
assessment indexes are proposed, by which the adaptabilities of different
convex models to a specific uncertain problem with given experimental samples
can be estimated. Four numerical examples are investigated to demonstrate the
effectiveness of the present study.
"
1801.06814,2021-09-14,Curriculum Guidelines for Undergraduate Programs in Data Science,"  The Park City Math Institute (PCMI) 2016 Summer Undergraduate Faculty Program
met for the purpose of composing guidelines for undergraduate programs in Data
Science. The group consisted of 25 undergraduate faculty from a variety of
institutions in the U.S., primarily from the disciplines of mathematics,
statistics and computer science. These guidelines are meant to provide some
structure for institutions planning for or revising a major in Data Science.
"
1801.07793,2019-02-19,"A New Correlation Coefficient for Aggregating Non-strict and Incomplete
  Rankings","  We introduce a correlation coefficient that is designed to deal with a
variety of ranking formats including those containing non-strict (i.e.,
with-ties) and incomplete (i.e., unknown) preferences. The correlation
coefficient is designed to enforce a neutral treatment of incompleteness
whereby no assumptions are made about individual preferences involving unranked
objects. The new measure, which can be regarded as a generalization of the
seminal Kendall tau correlation coefficient, is proven to satisfy a set of
metric-like axioms and to be equivalent to a recently developed ranking
distance function associated with Kemeny aggregation. In an effort to further
unify and enhance both robust ranking methodologies, this work proves the
equivalence of an additional distance and correlation-coefficient pairing in
the space of non-strict incomplete rankings. These connections induce new exact
optimization methodologies: a specialized branch and bound algorithm and an
exact integer programming formulation. Moreover, the bridging of these
complementary theories reinforces the singular suitability of the featured
correlation coefficient to solve the general consensus ranking problem. The
latter premise is bolstered by an accompanying set of experiments on random
instances, which are generated via a herein developed sampling technique
connected with the classic Mallows distribution of ranking data. Associated
experiments with the branch and bound algorithm demonstrate that, as data
becomes noisier, the featured correlation coefficient yields relatively fewer
alternative optimal solutions and that the aggregate rankings tend to be closer
to an underlying ground truth shared by a majority.
"
1801.08812,2018-01-29,"Combining Empirical Likelihood and Robust Estimation Methods for Linear
  Regression Models","  Ordinary least square (OLS), maximum likelihood (ML) and robust methods are
the widely used methods to estimate the parameters of a linear regression
model. It is well known that these methods perform well under some
distributional assumptions on error terms. However, these distributional
assumptions on the errors may not be appropriate for some data sets. In these
case, nonparametric methods may be considered to carry on the regression
analysis. Empirical likelihood (EL) method is one of these nonparametric
methods. The EL method maximizes a function, which is multiplication of the
unknown probabilities corresponding to each observation, under some constraints
inherited from the normal equations in OLS estimation method. However, it is
well known that the OLS method has poor performance when there are some
outliers in the data. In this paper, we consider the EL method with robustifyed
constraints. The robustification of the constraints is done by using the robust
M estimation methods for regression. We provide a small simulation study and a
real data example to demonstrate the capability of the robust EL method to
handle unusual observations in the data. The simulation and real data results
reveal that robust constraints are needed when heavy tailedness and/or outliers
are possible in the data.
"
1801.09284,2018-01-30,"Statistical analysis of the effect of the current, potential and
  proposed rules of a game in tennis","  With the aid of mathematical modelling (basic tool is the random walk with
absorbing barriers) we derive subsequent formulas to study the effect of
different versions of possible rules. For different rules the probability of
winning a game, the probability of break point occurrence, the mathematical
expectation of the number of rallies (points) and, the mathematical expectation
of the number of break points in a game are expressed. We check these rules
against ATP statistics for the Top-200 men players. In conclusion, we suggest a
slight but essential modification for the rule of a tennis game, namely ,
second service ( in case of a first service fault) is to be allowed only at the
first three points (rallies). This would partially preserve the traditions
(server has an advantage in the modern game) and at the same time it would
reduce the predictability of the game, significantly increasing in this way the
excitement for the spectators.
"
1802.00967,2018-02-06,"Which Football Player Bears Most Resemblance to Messi? A Statistical
  Analysis","  Many pundits and fans ask themselves the same question: Which football player
bears most resemblance to Lionel Messi? Is it Chelsea's Eden Hazard? Is it
Paulo Dybala, the heir to Messi in the national team of Argentina? Or is the
most alike player to Messi someone completely else? In general, the research on
the evaluation of players' performances originated in the context of baseball
in the USA, but, currently, it is of great importance in almost every team
sport on the planet. Specifically, football clubs' managers can use the data on
player's similarity when looking for replacement of their players by other,
presumably similar ones. Also, the research in the presented direction is
certainly interesting both for football pundits and football fans. Therefore,
the aim of this study is to answer the question from the title with the use of
the statistical analysis based on the data from ongoing league season retrieved
from WhoScored (WS) database. WS provides detailed data (up to 24 parameters
such as goals scored, the number of assists, shots on goal, passes, dribbles or
fouls) for players of TOP 5 European leagues, and ranks them with respect to
their overall performance. For this study, 17 parameters (criteria) most
relevant for an attacking player were used, and a set of 28 players, candidates
to be 'most alike to Messi' from WS TOP 100 list were selected. After data
normalization and application of a proper metric function the most similar
player to Lionel Messi was found.
"
1802.04161,2018-08-03,"Risk Factors Associated with Mortality in Game of Thrones: A
  Longitudinal Cohort Study","  Objective: To assess mortality, and identify the risk factors associated with
mortality in Game of Thrones (GoT). Design and Setting: A longitudinal cohort
study in the fictional kingdom of Westeros and Essos. Participants: All the
characters appearing in the GoT since airing of its first episode with screen
time of greater than or equal to 5 minutes. Main Outcome Measures: All-cause
mortality. Multivariate Cox proportional hazard model was used to assess the
risk factors associated with mortality, represented by hazard ratios, with
episodes as the unit of time. Results: Of the 132 characters, followed up for a
median time of 32 episodes, a total 89 (67.4%) characters died; with external
invasive injury as the most common cause of death, attributing to 42.4% of the
total deaths. Age (in decades) was a significant risk factor for death [HR,
1.24 (95% CI, 1.08-1.43), P=0.0001]. Although statistically non-significant,
allegiance to house Targaryen [HR, 1.10 (95% CI, 0.32-3.77)] was associated
with a higher risk for mortality per episode than house Stark. Characters
residing in South were less likely to die than characters residing in North
[HR, 0.58 (95% CI, 0.29-1.16), P=0.12]. Advisors showed a lower risk of
mortality than the members of houses, with some statistical significance [HR,
0.39 (95% CI, 0.14-1.08), P=0.07]. Conclusions: There is a high mortality rate
among the characters in GoT. Residing in the North and being a member of a
house is very dangerous in GoT. Allegiance to house Stark trended to be safer
than house Targaryen.
"
1802.04878,2018-03-09,Differentiating the pseudo determinant,"  A class of derivatives is defined for the pseudo determinant $Det(A)$ of a
Hermitian matrix $A$. This class is shown to be non-empty and to have a unique,
canonical member $\mathbf{\nabla Det}(A)=Det(A)A^+$, where $A^+$ is the
Moore-Penrose pseudo inverse. The classic identity for the gradient of the
determinant is thus reproduced. Examples are provided, including the maximum
likelihood problem for the rank-deficient covariance matrix of the degenerate
multivariate Gaussian distribution.
"
1802.05292,2018-11-29,"Loss-based approach to two-piece location-scale distributions with
  applications to dependent data","  Two-piece location-scale models are used for modeling data presenting
departures from symmetry. In this paper, we propose an objective Bayesian
methodology for the tail parameter of two particular distributions of the above
family: the skewed exponential power distribution and the skewed generalised
logistic distribution. We apply the proposed objective approach to time series
models and linear regression models where the error terms follow the
distributions object of study. The performance of the proposed approach is
illustrated through simulation experiments and real data analysis. The
methodology yields improvements in density forecasts, as shown by the analysis
we carry out on the electricity prices in Nordpool markets.
"
1802.08179,2018-02-23,Elements of the Kopula (eventological copula) theory,"  New in the probability theory and eventology theory, the concept of Kopula
(eventological copula) is introduced. The theorem on the characterization of
the sets of events by Kopula is proved, which serves as the eventological
pre-image of the well-known Sclar's theorem on copulas (1959). The Kopulas of
doublets and triplets of events are given, as well as of some N-sets of events.
"
1802.08194,2018-02-23,Seeing the forest for the trees? An investigation of network knowledge,"  This paper assesses the empirical content of one of the most prevalent
assumptions in the economics of networks literature, namely the assumption that
decision makers have full knowledge about the networks they interact on. Using
network data from 75 villages, we ask 4,554 individuals to assess whether five
randomly chosen pairs of households in their village are linked through
financial, social, and informational relationships. We find that network
knowledge is low and highly localized, declining steeply with the pair's
network distance to the respondent. 46% of respondents are not even able to
offer a guess about the status of a potential link between a given pair of
individuals. Even when willing to offer a guess, respondents can only correctly
identify the links 37% of the time. We also find that a one-step increase in
the social distance to the pair corresponds to a 10pp increase in the
probability of misidentifying the link. We then investigate the theoretical
implications of this assumption by showing that the predictions of various
models change substantially if agents behave under the more realistic
assumption of incomplete knowledge about the network. Taken together, our
results suggest that the assumption of full network knowledge (i) may serve as
a poor approximation to the real world and (ii) is not innocuous: allowing for
incomplete network knowledge may have first-order implications for a range of
qualitative and quantitative results in various contexts.
"
1802.08761,2018-02-27,"Behavioral-clinical phenotyping with type 2 diabetes self-monitoring
  data","  Objective: To evaluate unsupervised clustering methods for identifying
individual-level behavioral-clinical phenotypes that relate personal biomarkers
and behavioral traits in type 2 diabetes (T2DM) self-monitoring data. Materials
and Methods: We used hierarchical clustering (HC) to identify groups of meals
with similar nutrition and glycemic impact for 6 individuals with T2DM who
collected self-monitoring data. We evaluated clusters on: 1) correspondence to
gold standards generated by certified diabetes educators (CDEs) for 3
participants; 2) face validity, rated by CDEs, and 3) impact on CDEs' ability
to identify patterns for another 3 participants. Results: Gold standard (GS)
included 9 patterns across 3 participants. Of these, all 9 were re-discovered
using HC: 4 GS patterns were consistent with patterns identified by HC (over
50% of meals in a cluster followed the pattern); another 5 were included as
sub-groups in broader clusers. 50% (9/18) of clusters were rated over 3 on
5-point Likert scale for validity, significance, and being actionable. After
reviewing clusters, CDEs identified patterns that were more consistent with
data (70% reduction in contradictions between patterns and participants'
records). Discussion: Hierarchical clustering of blood glucose and
macronutrient consumption appears suitable for discovering behavioral-clinical
phenotypes in T2DM. Most clusters corresponded to gold standard and were rated
positively by CDEs for face validity. Cluster visualizations helped CDEs
identify more robust patterns in nutrition and glycemic impact, creating new
possibilities for visual analytic solutions. Conclusion: Machine learning
methods can use diabetes self-monitoring data to create personalized
behavioral-clinical phenotypes, which may prove useful for delivering
personalized medicine.
"
1802.08858,2021-09-01,A Project Based Approach to Statistics and Data Science,"  In an increasingly data-driven world, facility with statistics is more
important than ever for our students. At institutions without a statistician,
it often falls to the mathematics faculty to teach statistics courses. This
paper presents a model that a mathematician asked to teach statistics can
follow. This model entails connecting with faculty from numerous departments on
campus to develop a list of topics, building a repository of real-world
datasets from these faculty, and creating projects where students interface
with these datasets to write lab reports aimed at consumers of statistics in
other disciplines. The end result is students who are well prepared for
interdisciplinary research, who are accustomed to coping with the
idiosyncrasies of real data, and who have sharpened their technical writing and
speaking skills.
"
1802.09455,2018-02-27,"Assessing the association between pre-course metrics of student
  preparation and student performance in introductory statistics: Results from
  early data on simulation-based inference vs. nonsimulation based inference","  The recent simulation-based inference (SBI) movement in algebra-based
introductory statistics courses (Stat 101) has provided preliminary evidence of
improved student conceptual understanding and retention. However, little is
known about whether these positive effects are preferentially distributed
across types of students entering the course. We consider how two metrics of
Stat 101 student preparation (pre-course performance on concept inventory and
math ACT score) may or may not be associated with end of course student
performance on conceptual inventories. Students across all preparation levels
tended to show improvement in Stat 101, but more improvement was observed
across all student preparation levels in early versions of a SBI course.
Furthermore, students' gains tended to be similar regardless of whether
students entered the course with more preparation or less. Recent data on a
sample of students using a current version of an SBI course showed similar
results, though direct comparison with non-SBI students was not possible.
Overall, our analysis provides additional evidence that SBI curricula are
effective at improving students' conceptual understanding of statistical ideas
post-course regardless student preparation. Further work is needed to better
understand nuances of student improvement based on other student demographics,
prior coursework, as well as instructor and institutional variables.
"
1802.10163,2020-09-14,Markov equivalence of marginalized local independence graphs,"  Symmetric independence relations are often studied using graphical
representations. Ancestral graphs or acyclic directed mixed graphs with
$m$-separation provide classes of symmetric graphical independence models that
are closed under marginalization. Asymmetric independence relations appear
naturally for multivariate stochastic processes, for instance in terms of local
independence. However, no class of graphs representing such asymmetric
independence relations, which is also closed under marginalization, has been
developed. We develop the theory of directed mixed graphs with $\mu$-separation
and show that this provides a graphical independence model class which is
closed under marginalization and which generalizes previously considered
graphical representations of local independence.
  For statistical applications, it is pivotal to characterize graphs that
induce the same independence relations as such a Markov equivalence class of
graphs is the object that is ultimately identifiable from observational data.
Our main result is that for directed mixed graphs with $\mu$-separation each
Markov equivalence class contains a maximal element which can be constructed
from the independence relations alone. Moreover, we introduce the directed
mixed equivalence graph as the maximal graph with edge markings. This graph
encodes all the information about the edges that is identifiable from the
independence relations, and furthermore it can be computed efficiently from the
maximal graph.
"
1802.10420,2018-07-04,Limits on Inferring the Past,"  Here we define and study the properties of retrodictive inference. We derive
equations relating retrodiction entropy and thermodynamic entropy, and as a
special case, show that under equilibrium conditions, the two are identical. We
demonstrate relations involving the KL-divergence and retrodiction probability,
and bound the time rate of change of retrodiction entropy. As a specific case,
we invert various Langevin processes, inferring the initial condition of \(N\)
particles given their final positions at some later time. We evaluate the
retrodiction entropy for Langevin dynamics exactly for special cases, and find
that one's ability to infer the initial state of a system can exhibit two
possible qualitative behaviors depending on the potential energy landscape,
either decreasing indefinitely, or asymptotically approaching a fixed value. We
also study how well we can retrodict points that evolve based on the logistic
map. We find singular changes in the retrodictivity near bifurcations.
Counterintuitively, the transition to chaos is accompanied by maximal
retrodictability.
"
1803.00609,2018-03-05,On Statistical Non-Significance,"  Significance tests are probably the most extended form of inference in
empirical research, and significance is often interpreted as providing greater
informational content than non-significance. In this article we show, however,
that rejection of a point null often carries very little information, while
failure to reject may be highly informative. This is particularly true in
empirical contexts where data sets are large and where there are rarely reasons
to put substantial prior probability on a point null. Our results challenge the
usual practice of conferring point null rejections a higher level of scientific
significance than non-rejections. In consequence, we advocate a visible
reporting and discussion of non-significant results in empirical practice.
"
1803.00613,2018-05-15,A shiny update to an old experiment game,"  Games can be a powerful tool for learning about statistical methodology.
Effective game design involves a fine balance between caricature and realism,
to simultaneously illustrate salient concepts in a controlled setting and serve
as a testament to real-world applicability. Striking that balance is
particularly challenging in response surface and design domains, where
real-world scenarios often play out over long time scales, during which
theories are revised, model and inferential techniques are improved, and
knowledge is updated. Here I present a game, borrowing liberally from one first
played over forty years ago, that attempts to achieve that balance while
reinforcing a cascade of topics in modern nonparametric response surfaces,
sequential design and optimization. The game embeds a blackbox simulation
within a shiny app whose interface is designed to simulate a realistic
information-availability setting, while offering a stimulating, competitive
environment wherein students can try out new methodology, and ultimately
appreciate its power and limitations. Interface, rules, timing with course
material, and evaluation are described, along with a ""case study"" involving a
cohort of students at Virginia Tech.
"
1803.01156,2018-09-28,"A Generalization of the Exponential-Logarithmic Distribution for
  Reliability and Life Data Analysis","  In this paper, we introduce a new two-parameter lifetime distribution, called
the exponential-generalized truncated logarithmic (EGTL) distribution, by
compounding the exponential and generalized truncated logarithmic
distributions. Our procedure generalizes the exponential-logarithmic (EL)
distribution modelling the reliability of systems by the use of first-order
concepts, where the minimum lifetime is considered (Tahmasbi 2008). In our
approach, we assume that a system fails if a given number k of the components
fails and then, we consider the kth-smallest value of lifetime instead of the
minimum lifetime. The reliability and failure rate functions as well as their
properties are presented for some special cases. The estimation of the
parameters is attained by the maximum likelihood, the expectation maximization
algorithm, the method of moments and the Bayesian approach, with a simulation
study performed to illustrate the different methods of estimation. The
application study is illustrated based on two real data sets used in many
applications of reliability.
"
1803.01221,2018-03-06,"Byzantine-Resilient Locally Optimum Detection Using Collaborative
  Autonomous Networks","  In this paper, we propose a locally optimum detection (LOD) scheme for
detecting a weak radioactive source buried in background clutter. We develop a
decentralized algorithm, based on alternating direction method of multipliers
(ADMM), for implementing the proposed scheme in autonomous sensor networks.
Results show that algorithm performance approaches the centralized clairvoyant
detection algorithm in the low SNR regime, and exhibits excellent convergence
rate and scaling behavior (w.r.t. number of nodes). We also devise a
low-overhead, robust ADMM algorithm for Byzantine-resilient detection, and
demonstrate its robustness to data falsification attacks.
"
1803.03667,2018-03-13,"Co-occurrence of the Benford-like and Zipf Laws Arising from the Texts
  Representing Human and Artificial Languages","  We demonstrate that large texts, representing human (English, Russian,
Ukrainian) and artificial (C++, Java) languages, display quantitative patterns
characterized by the Benford-like and Zipf laws. The frequency of a word
following the Zipf law is inversely proportional to its rank, whereas the total
numbers of a certain word appearing in the text generate the uneven
Benford-like distribution of leading numbers. Excluding the most popular words
essentially improves the correlation of actual textual data with the Zipfian
distribution, whereas the Benford distribution of leading numbers (arising from
the overall amount of a certain word) is insensitive to the same elimination
procedure. The calculated values of the moduli of slopes of double
logarithmical plots for artificial languages (C++, Java) are markedly larger
than those for human ones.
"
1803.05555,2018-03-16,"Emanuel Parzen: A Memorial, and a Model With the Two Kernels That He
  Championed","  Manny Parzen passed away in February 2016, and this article is written partly
as a memorial and appreciation. Manny made important contributions to several
areas, but the two that influenced me most were his contributions to kernel
density estimation and to Reproducing Kernel Hilbert Spaces, the two kernels of
the title. Some fond memories of Manny as a PhD advisor begin this memorial,
followed by a discussion of Manny's influence on density estimation and RKHS
methods. A picture gallery of trips comes next, followed by the technical part
of the article. Here our goal is to show how risk models can be built using
RKHS penalized likelihood methods where subjects have personal (sample)
densities which can be used as {\it attributes} in such models.
"
1803.06214,2018-03-19,How sure are we? Two approaches to statistical inference,"  Suppose you are told that taking a statin will reduce your risk of a heart
attack or stroke by 3% in the next ten years, or that women have better
emotional intelligence than men. You may wonder how accurate the 3% is, or how
confident we should be about the assertion about women's emotional
intelligence, bearing in mind that these conclusions are only based on samples
of data? My aim here is to present two statistical approaches to questions like
these. Approach 1 is often called null hypothesis testing but I prefer the
phrase ""baseline hypothesis"": this is the standard approach in many areas of
inquiry but is fraught with problems. Approach 2 can be viewed as a
generalisation of the idea of confidence intervals, or as the application of
Bayes' theorem. Unlike Approach 1, Approach 2 provides a tentative estimate of
the probability of hypotheses of interest. For both approaches, I explain, from
first principles, building only on ""common sense"" statistical concepts like
averages and randomness, both how to derive answers, and the rationale behind
the answers. This is achieved by using computer simulation methods (resampling
and bootstrapping using a spreadsheet available on the web) which avoid the use
of probability distributions (t, normal, etc). Such a minimalist, but
reasonably rigorous, analysis is particularly useful in a discipline like
statistics which is widely used by people who are not specialists. My intended
audience includes both statisticians, and users of statistical methods who are
not statistical experts.
"
1803.07141,2018-11-16,"Quantifying the Contributions of Training Data and Algorithm Logic to
  the Performance of Automated Cause-assignment Algorithms for Verbal Autopsy","  A verbal autopsy (VA) consists of a survey with a relative or close contact
of a person who has recently died. VA surveys are commonly used to infer likely
causes of death for individuals when deaths happen outside of hospitals or
healthcare facilities. Several statistical and algorithmic methods are
available to assign cause of death using VA surveys. Each of these methods
require as inputs some information about the joint distribution of symptoms and
causes. In this note, we examine the generalizability of this symptom-cause
information by comparing different automated coding methods using various
combinations of inputs and evaluation data. VA algorithm performance is
affected by both the specific SCI themselves and the logic of a given
algorithm. Using a variety of performance metrics for all existing VA
algorithms, we demonstrate that in general the adequacy of the information
about the joint distribution between symptoms and cause affects performance at
least as much or more than algorithm logic.
"
1803.07172,2018-09-12,"Beyond Homophily: Incorporating Actor Variables in Actor-oriented
  Network Models","  We consider the specification of effects of numerical actor attributes in
statistical models for directed social networks. A fundamental mechanism is
homophily or assortativity, where actors have a higher likelihood to be tied
with others having similar values of the variable under study. But there are
other mechanisms that may also play a role in how the attribute values of two
actors influence the likelihood of a tie. We discuss three additional
mechanisms: aspiration to send ties to others having high values; conformity in
the sense of sending more ties to others whose values are close to what may be
considered the `social norm'; and sociability, where those having higher values
will tend to send more ties generally. These mechanisms may operate jointly,
and then their effects will be confounded. We present a specification
representing these effects simultaneously by a four-parameter quadratic
function of the values of sender and receiver. Greater flexibility can be
obtained by a five-parameter extension. We argue that empirical researchers
often overlook the possibility that homophily may be confounded with these
other mechanisms, and that for actor attributes that have important effects on
directed networks, these specifications may provide an improvement. An
illustration is given of the dependence of advice ties on academic grades in a
network of MBA students, analyzed by the Stochastic Actor-oriented Model.
"
1803.10118,2019-06-19,"Scientific discovery in a model-centric framework: Reproducibility,
  innovation, and epistemic diversity","  Consistent confirmations obtained independently of each other lend
credibility to a scientific result. We refer to results satisfying this
consistency as reproducible and assume that reproducibility is a desirable
property of scientific discovery. Yet seemingly science also progresses despite
irreproducible results, indicating that the relationship between
reproducibility and other desirable properties of scientific discovery is not
well understood. These properties include early discovery of truth, persistence
on truth once it is discovered, and time spent on truth in a long-term
scientific inquiry. We build a mathematical model of scientific discovery that
presents a viable framework to study its desirable properties including
reproducibility. In this framework, we assume that scientists adopt a
model-centric approach to discover the true model generating data in a
stochastic process of scientific discovery. We analyze the properties of this
process using Markov chain theory, Monte Carlo methods, and agent-based
modeling. We show that the scientific process may not converge to truth even if
scientific results are reproducible and that irreproducible results do not
necessarily imply untrue results. The proportion of different research
strategies represented in the scientific population, scientists' choice of
methodology, the complexity of truth, and the strength of signal contribute to
this counter-intuitive finding. Important insights include that innovative
research speeds up the discovery of scientific truth by facilitating the
exploration of model space and epistemic diversity optimizes across desirable
properties of scientific discovery.
"
1803.10121,2020-01-14,"Quantification of the weight of fingerprint evidence using a ROC-based
  Approximate Bayesian Computation algorithm for model selection","  For more than a century, fingerprints have been used with considerable
success to identify criminals or verify the identity of individuals. The
categorical conclusion scheme used by fingerprint examiners, and more generally
the inference process followed by forensic scientists, have been heavily
criticised in the scientific and legal literature. Instead, scholars have
proposed to characterise the weight of forensic evidence using the Bayes factor
as the key element of the inference process. In forensic science, quantifying
the magnitude of support is equally as important as determining which model is
supported. Unfortunately, the complexity of fingerprint patterns render
likelihood-based inference impossible. In this paper, we use an Approximate
Bayesian Computation model selection algorithm to quantify the weight of
fingerprint evidence. We supplement the ABC algorithm using a Receiver
Operating Characteristic curve to mitigate the effect of the curse of
dimensionality. Our modified algorithm is computationally efficient and makes
it easier to monitor convergence as the number of simulations increase. We use
our method to quantify the weight of fingerprint evidence in forensic science,
but we note that it can be applied to any other forensic pattern evidence.
"
1804.02229,2018-04-09,Theory of Cricket: Target Scores and Predictability,"  We propose a model for recalculating the target score in rain affected
matches based on empirical data. During the development of the current stage of
the Cricket, different methods have been introduced to recalculate the target
scores in interpreted games. Currently, the International Cricket Council (ICC)
uses the Duckworth-Lewis method and have in the past strongly considered
changing to the VJD method. Here, we introduce a simple approach to calculate
target scores in interrupted games by considering the area under a run rate
curve. To calculate the target we have analysed over a decades worth of
empirical data using various statistical methods. As in the case of Duckworth-
Lewis method, we also have two parameters in our model, that is overs and
wickets in combination. We also found that in the one day international cricket
(ODI) wickets play a crucial role whereas in T20 cricket they do not effect the
run rate of the games to the same degree. Using empirical and mathematical
arguments we show that the run scoring distributions are independent of the
innings.
"
1804.02747,2018-04-10,"Fast Conditional Independence Test for Vector Variables with Large
  Sample Sizes","  We present and evaluate the Fast (conditional) Independence Test (FIT) -- a
nonparametric conditional independence test. The test is based on the idea that
when $P(X \mid Y, Z) = P(X \mid Y)$, $Z$ is not useful as a feature to predict
$X$, as long as $Y$ is also a regressor. On the contrary, if $P(X \mid Y, Z)
\neq P(X \mid Y)$, $Z$ might improve prediction results. FIT applies to
thousand-dimensional random variables with a hundred thousand samples in a
fraction of the time required by alternative methods. We provide an extensive
evaluation that compares FIT to six extant nonparametric independence tests.
The evaluation shows that FIT has low probability of making both Type I and
Type II errors compared to other tests, especially as the number of available
samples grows. Our implementation of FIT is publicly available.
"
1804.03732,2019-01-04,"Contest models highlight inherent inefficiencies of scientific funding
  competitions","  Scientific research funding is allocated largely through a system of
soliciting and ranking competitive grant proposals. In these competitions, the
proposals themselves are not the deliverables that the funder seeks, but
instead are used by the funder to screen for the most promising research ideas.
Consequently, some of the funding program's impact on science is squandered
because applying researchers must spend time writing proposals instead of doing
science. To what extent does the community's aggregate investment in proposal
preparation negate the scientific impact of the funding program? Are there
alternative mechanisms for awarding funds that advance science more
efficiently? We use the economic theory of contests to analyze how efficiently
grant proposal competitions advance science, and compare them with recently
proposed, partially randomized alternatives such as lotteries. We find that the
effort researchers waste in writing proposals may be comparable to the total
scientific value of the research that the funding supports, especially when
only a few proposals can be funded. Moreover, when professional pressures
motivate investigators to seek funding for reasons that extend beyond the value
of the proposed science (e.g., promotion, prestige), the entire program can
actually hamper scientific progress when the number of awards is small. We
suggest that lost efficiency may be restored either by partial lotteries for
funding, or by funding researchers based on past scientific success instead of
proposals for future work.
"
1804.04587,2018-04-13,"Power Analysis in a SMART Design: Sample Size Estimation for Determining
  the Best Dynamic Treatment Regime","  Sequential, multiple assignment, randomized trial (SMART) designs have become
increasingly popular in the field of precision medicine by providing a means
for comparing sequences of treatments tailored to the individual patient, i.e.,
dynamic treatment regime (DTR). The construction of evidence-based DTRs
promises a replacement to adhoc one-size-fits-all decisions pervasive in
patient care. However, there are substantial statistical challenges in sizing
SMART designs due to the complex correlation structure between the DTRs
embedded in the design. Since the primary goal of SMARTs is the construction of
an optimal DTR, investigators are interested in sizing SMARTs based on the
ability to screen out DTRs inferior to the optimal DTR by a given amount which
cannot be done using existing methods. In this paper, we fill this gap by
developing a rigorous power analysis framework that leverages multiple
comparisons with the best methodology. Our method employs Monte Carlo
simulation in order to compute the minimum number of individuals to enroll in
an arbitrary SMART. We will evaluate our method through extensive simulation
studies. We will illustrate our method by retrospectively computing the power
in the Extending Treatment Effectiveness of Naltrexone SMART study.
"
1804.04628,2018-04-13,"A Mathematical Approach to Comply with Ethical Constraints in
  Compassionate Use Treatments","  Patients who are seriously ill may ask doctors to treat them with unapproved
medication, about which not much is known, or else with known medication in a
high dosage. Apart from strict legal constraints such cases may involve
difficult ethical questions as e.g. how long a series of treatments of
different patients should be continued. Similar questions also arise in less
serious situations. A physician trusts that a certain combination of freely
available drugs are efficient against a specific disease and tries to help
patients and to follow at the same time the primum-non-nocere principle.
  The objective of this paper is to contribute to the research on such
questions in the form of mathematical models. Arguing in a step-to-step
approach, we will show that certain sequential optimisation problems comply in
a natural way with the true spirit of major ethical principles in medicine. We
then suggest protocols and associate algorithms to find optimal, or
approximately optimal, treatment strategies. Although the contribution may
sometimes be difficult to apply in medical practice, the author thinks that the
rational behind the approach offers a valuable alternative for finding decision
support and should attract attention.
"
1804.04948,2019-03-27,The rationality of irrationality in the Monty Hall problem,"  The rational solution of the Monty Hall problem unsettles many people. Most
people, including the authors, think it feels wrong to switch the initial
choice of one of the three doors, despite having fully accepted the
mathematical proof for its superiority. Many people, if given the choice to
switch, think the chances are fifty-fifty between their options, but still
strongly prefer to stay with their initial choice. Is there some sense behind
these irrational feelings?
  We entertain the possibility that intuition solves the problem of how to
behave in a real game show, not in the abstract textbook version of the Monty
Hall problem. A real showmaster sometimes plays evil, either to make the show
more interesting, to save money, or because he is in a bad mood. A moody
showmaster erases any information advantage the guest could extract by him
opening other doors which drives the chance of the car being behind the chosen
door towards fifty percent. Furthermore, the showmaster could try to read or
manipulate the guest's strategy to the guest's disadvantage. Given this, the
preference to stay with the initial choice turns out to be a very rational
defense strategy of the show's guest against the threat of being manipulated by
its host. Thus, the intuitive feelings most people have about the Monty Hall
problem coincide with what would be a rational strategy for a real-world game
show. Although these investigations are mainly intended to be an entertaining
mathematical commentary on an information-theoretic puzzle, they touch on
interesting psychological questions.
"
1804.06285,2018-05-01,"Bayesian model-data synthesis with an application to global
  Glacio-Isostatic Adjustment","  We introduce a framework for updating large scale geospatial processes using
a model-data synthesis method based on Bayesian hierarchical modelling. Two
major challenges come from updating large-scale Gaussian process and modelling
non-stationarity. To address the first, we adopt the SPDE approach that uses a
sparse Gaussian Markov random fields (GMRF) approximation to reduce the
computational cost and implement the Bayesian inference by using the INLA
method. For non-stationary global processes, we propose two general models that
accommodate commonly-seen geospatial problems. Finally, we show an example of
updating an estimate of global glacial isostatic adjustment (GIA) using GPS
measurements.
"
1804.07192,2018-04-20,Multiple factor analysis of distributional data,"  In the framework of Symbolic Data Analysis (SDA), distribution-variables are
a particular case of multi-valued variables: each unit is represented by a set
of distributions (e.g. histograms, density functions or quantile functions),
one for each variable. Factor analysis (FA) methods are primary exploratory
tools for dimension reduction and visualization. In the present work, we use
Multiple Factor Analysis (MFA) approach for the analysis of data described by
distributional variables. Each distributional variable induces a set new
numeric variable related to the quantiles of each distribution. We call these
new variables as \textit{quantile variables} and the set of quantile variables
related to a distributional one is a block in the MFA approach. Thus, MFA is
performed on juxtaposed tables of quantile variables. \\ We show that the
criterion decomposed in the analysis is an approximation of the variability
based on a suitable metrics between distributions: the squared $L_2$
Wasserstein distance. \\ Applications on simulated and real distributional data
corroborate the method. The interpretation of the results on the factorial
planes is performed by new interpretative tools that are related to the several
characteristics of the distributions (location, scale and shape).
"
1804.07923,2018-04-24,Resolving the Lord's Paradox,"  An explanation to Lord's paradox using ordinary least square regression
models is given. It is not a paradox at all, if the regression parameters are
interpreted as predictive or as causal with stricter conditions and be aware of
laws of averages. We use derivation of a super-model from a given sub-model,
when its residuals can be modelled with other potential predictors as a
solution.
"
1804.07940,2018-04-24,Viewing Simpson's Paradox,"  Well known Simpson's paradox is puzzling and surprising for many, especially
for the empirical researchers and users of statistics. However there is no
surprise as far as mathematical details are concerned. A lot more is written
about the paradox but most of them are beyond the grasp of such users. This
short article is about explaining the phenomenon in an easy way to grasp using
simple algebra and geometry. The mathematical conditions under which the
paradox can occur are made explicit and a simple geometrical illustrations is
used to describe it. We consider the reversal of the association between two
binary variables, say, $X$ and $Y$ by a third binary variable, say, $Z$. We
show that it is always possible to define $Z$ algebraically for non-extreme
dependence between $X$ and $Y$, therefore occurrence of the paradox depends on
identifying it with a practical meaning for it in a given context of interest,
that is up to the subject domain expert. And finally we discuss the paradox in
predictive contexts since in literature it is argued that the paradox is
resolved using causal reasoning.
"
1804.10939,2018-10-09,"BNSP: an R Package for Fitting Bayesian Semiparametric Regression Models
  and Variable Selection","  The R package BNSP provides a unified framework for semiparametric
location-scale regression and stochastic search variable selection. The
statistical methodology that the package is built upon utilizes basis function
expansions to represent semiparametric covariate effects in the mean and
variance functions, and spike-slab priors to perform selection and
regularization of the estimated effects. In addition to the main function that
performs posterior sampling, the package includes functions for assessing
convergence of the sampler, summarizing model fits, visualizing covariate
effects and obtaining predictions for new responses or their means given
feature/covariate vectors.
"
1805.01291,2018-05-04,Benford or not Benford: new results on digits beyond the first,"  In this paper, we will see that the proportion of d as p th digit, where p >
1 and d $\in$ 0, 9, in data (obtained thanks to the hereunder developed model)
is more likely to follow a law whose probability distribution is determined by
a specific upper bound, rather than the generalization of Benford's Law to
digits beyond the first one. These probability distributions fluctuate around
theoretical values determined by Hill in 1995. Knowing beforehand the value of
the upper bound can be a way to find a better adjusted law than Hill's one.
"
1805.01345,2020-02-28,Conjectures on Optimal Nested Generalized Group Testing Algorithm,"  Consider a finite population of $N$ items, where item $i$ has a probability
$p_i$ to be defective. The goal is to identify all items by means of group
testing. This is the generalized group testing problem (hereafter GGTP). In the
case of $\displaystyle p_1=\cdots=p_{N}=p$ \cite{YH1990} proved that the
pairwise testing algorithm is the optimal nested algorithm, with respect to the
expected number of tests, for all $N$ if and only if $\displaystyle p \in
[1-1/\sqrt{2},\,(3-\sqrt{5})/2]$ (R-range hereafter) (an optimal at the
boundary values). In this note, we present a result that helps to define the
generalized pairwise testing algorithm (hereafter GPTA) for the GGTP. We
present two conjectures: (1) when all $p_i, i=1,\ldots,N$ belong to the
R-range, GPTA is the optimal procedure among nested procedures applied to $p_i$
of nondecreasing order; (2) if all $p_i, i=1,\ldots,N$ belong to the R-range,
GPTA the optimal nested procedure, i.e., minimises the expected total number of
tests with respect to all possible testing orders in the class of nested
procedures. Although these conjectures are logically reasonable, we were only
able to empirically verify the first one up to a particular level of $N$. We
also provide a short survey of GGTP.
"
1805.01834,2018-09-24,"On estimands and the analysis of adverse events in the presence of
  varying follow-up times within the benefit assessment of therapies","  The analysis of adverse events (AEs) is a key component in the assessment of
a drug's safety profile. Inappropriate analysis methods may result in
misleading conclusions about a therapy's safety and consequently its
benefit-risk ratio. The statistical analysis of AEs is complicated by the fact
that the follow-up times can vary between the patients included in a clinical
trial. This paper takes as its focus the analysis of AE data in the presence of
varying follow-up times within the benefit assessment of therapeutic
interventions. Instead of approaching this issue directly and solely from an
analysis point of view, we first discuss what should be estimated in the
context of safety data, leading to the concept of estimands. Although the
current discussion on estimands is mainly related to efficacy evaluation, the
concept is applicable to safety endpoints as well. Within the framework of
estimands, we present statistical methods for analysing AEs with the focus
being on the time to the occurrence of the first AE of a specific type. We give
recommendations which estimators should be used for the estimands described.
Furthermore, we state practical implications of the analysis of AEs in clinical
trials and give an overview of examples across different indications. We also
provide a review of current practices of health technology assessment (HTA)
agencies with respect to the evaluation of safety data. Finally, we describe
problems with meta-analyses of AE data and sketch possible solutions.
"
1805.05090,2019-05-28,Hyperspectral Data Analysis in R: the hsdar Package,"  Hyperspectral remote sensing is a promising tool for a variety of
applications including ecology, geology, analytical chemistry and medical
research. This article presents the new \hsdar package for R statistical
software, which performs a variety of analysis steps taken during a typical
hyperspectral remote sensing approach. The package introduces a new class for
efficiently storing large hyperspectral datasets such as hyperspectral cubes
within R. The package includes several important hyperspectral analysis tools
such as continuum removal, normalized ratio indices and integrates two widely
used radiation transfer models. In addition, the package provides methods to
directly use the functionality of the caret package for machine learning tasks.
Two case studies demonstrate the package's range of functionality: First, plant
leaf chlorophyll content is estimated and second, cancer in the human larynx is
detected from hyperspectral data.
"
1805.07580,2018-05-22,"Analytic moment and Laplace transform formulae for the quasi-stationary
  distribution of the Shiryaev diffusion on an interval","  We derive analytic closed-form moment and Laplace transform formulae for the
quasi-stationary distribution of the classical Shiryaev diffusion restricted to
the interval $[0,A]$ with absorption at a given $A>0$.
"
1805.11012,2018-05-29,To Bayes or Not To Bayes? That's no longer the question!,"  This paper seeks to provide a thorough account of the ubiquitous nature of
the Bayesian paradigm in modern statistics, data science and artificial
intelligence. Once maligned, on the one hand by those who philosophically hated
the very idea of subjective probability used in prior specification, and on the
other hand because of the intractability of the computations needed for
Bayesian estimation and inference, the Bayesian school of thought now permeates
and pervades virtually all areas of science, applied science, engineering,
social science and even liberal arts, often in unsuspected ways. Thanks in part
to the availability of powerful computing resources, but also to the literally
unavoidable inherent presence of the quintessential building blocks of the
Bayesian paradigm in all walks of life, the Bayesian way of handling
statistical learning, estimation and inference is not only mainstream but also
becoming the most central approach to learning from the data. This paper
explores some of the most relevant elements to help to the reader appreciate
the pervading power and presence of the Bayesian paradigm in statistics,
artificial intelligence and data science, with an emphasis on how the Gospel
according to Reverend Thomas Bayes has turned out to be the truly good news,
and some cases the amazing saving grace, for all who seek to learn
statistically from the data. To further help the reader gain deeper and
tangible practical insights into the Bayesian machinery, we point to some
computational tools designed for the R Statistical Software Environment to help
explore Bayesian statistical learning.
"
1805.11516,2018-05-30,Absolutely Zero Evidence,"  Statistical analysis is often used to evaluate the evidence for or against
scientific hypotheses, and various statistics (e.g., p-values, likelihood
ratios, Bayes factors) are interpreted as measures of evidence strength. Here I
consider evidence measurement from the point of view of representational
measurement theory, and argue that familiar evidence statistics do not conform
to any legitimate measurement scale type. I then consider the notion of an
absolute scale for evidence measurement, in a sense to be defined, focusing
particularly on the notion of absolute 0 evidence, which turns out to be
something other than what one might have expected.
"
1805.12052,2019-07-17,"Unwinding the model manifold: choosing similarity measures to remove
  local minima in sloppy dynamical systems","  In this paper, we consider the problem of parameter sensitivity in models of
complex dynamical systems through the lens of information geometry. We
calculate the sensitivity of model behavior to variations in parameters. In
most cases, models are sloppy, that is, exhibit an exponential hierarchy of
parameter sensitivities. We propose a parameter classification scheme based on
how the sensitivities scale at long observation times. We show that for
oscillatory models, either with a limit cycle or a strange attractor,
sensitivities can become arbitrarily large, which implies a high
effective-dimensionality on the model manifold. Sloppy models with a single
fixed point have model manifolds with low effective-dimensionality, previously
described as a ""hyper-ribbon"". In contrast, models with high effective
dimensionality translate into multimodal fitting problems. We define a measure
of curvature on the model manifold which we call the \emph{winding frequency}
that estimates the linear density of local minima in the model's parameter
space. We then show how alternative choices of fitting metrics can ""unwind"" the
model manifold and give low winding frequencies. This prescription translates
the model manifold from one of high effective-dimensionality into the
""hyper-ribbon"" structures observed elsewhere. This translation opens the door
for applications of sloppy model analysis and model reduction methods developed
for models with low effective-dimensionality.
"
1806.03971,2018-06-12,Data learning from big data,"  Technology is generating a huge and growing availability of observa tions of
diverse nature. This big data is placing data learning as a central scientific
discipline. It includes collection, storage, preprocessing, visualization and,
essentially, statistical analysis of enormous batches of data. In this paper,
we discuss the role of statistics regarding some of the issues raised by big
data in this new paradigm and also propose the name of data learning to
describe all the activities that allow to obtain relevant knowledge from this
new source of information.
"
1806.06145,2018-10-23,Teaching computational reproducibility for neuroimaging,"  We describe a project-based introduction to reproducible and collaborative
neuroimaging analysis. Traditional teaching on neuroimaging usually consists of
a series of lectures that emphasize the big picture rather than the foundations
on which the techniques are based. The lectures are often paired with practical
workshops in which students run imaging analyses using the graphical interface
of specific neuroimaging software packages. Our experience suggests that this
combination leaves the student with a superficial understanding of the
underlying ideas, and an informal, inefficient, and inaccurate approach to
analysis. To address these problems, we based our course around a substantial
open-ended group project. This allowed us to teach: (a) computational tools to
ensure computationally reproducible work, such as the Unix command line,
structured code, version control, automated testing, and code review and (b) a
clear understanding of the statistical techniques used for a basic analysis of
a single run in an MRI scanner. The emphasis we put on the group project showed
the importance of standard computational tools for accuracy, efficiency, and
collaboration. The projects were broadly successful in engaging students in
working reproducibly on real scientific questions. We propose that a course on
this model should be the foundation for future programs in neuroimaging. We
believe it will also serve as a model for teaching efficient and reproducible
research in other fields of computational science.
"
1806.07440,2019-07-24,Kernel Methods for Nonlinear Connectivity Detection,"  In this paper, we show that the presence of nonlinear coupling between time
series may be detected employing kernel feature space representations alone
dispensing with the need to go back to solve the pre-image problem to gauge
model adequacy. As a consequence, the canonical methodology for model
construction, diagnostics, and Granger connectivity inference applies with no
change other than computation using kernels in lieu of second-order moments.
"
1806.08031,2018-06-22,A Constructive Algebraic Proof of Student's Theorem,"  Student's theorem is an important result in statistics which states that for
normal population, the sample variance is independent from the sample mean and
has a chi-square distribution. The existing proofs of this theorem either
overly rely on advanced tools such as moment generating functions, or fail to
explicitly construct an orthogonal matrix used in the proof. This paper
provides an elegant explicit construction of that matrix, making the algebraic
proof complete. The constructive algebraic proof proposed here is thus very
suitable for being included in textbooks.
"
1806.08683,2018-10-17,"Nonparametric Confidence Regions for Veronese-Whitney Means and
  Antimeans on Planar Kendall Shape Spaces","  In this paper after a brief revision of VW-means, which are extrinsic means
on real and complex projective spaces, relative to the Veronese-Whitney
embeddings, we give two examples of sample VW means computations on planar
Kendall shape spaces. Here we derive large sample and pivotal nonparametric
bootstrap confidence regions for VW-antimeans, using VW-anti-covariance
matrices, and their sample counterparts
"
1806.11427,2018-07-02,"Discussion on Using Stacking to Average Bayesian Predictive
  Distributions by Yao et al","  I begin by summarizing key ideas of the paper under discussion. Then I will
talk about a graphical modeling perspective, posterior contraction rates and
alternative methods of aggregation. Moreover, I will also discuss possible
applications of the stacking method to other problems, in particular,
aggregating (sub)posterior distributions in distributed computing.
"
1807.04269,2019-03-27,"Applying a Dynamical Systems Model and Network Theory to Major
  Depressive Disorder","  Mental disorders like major depressive disorder can be seen as complex
dynamical systems. In this study we investigate the dynamic behaviour of
individuals to see whether or not we can expect a transition to another mood
state. We introduce a mean field model to a binomial process, where we reduce a
dynamic multidimensional system (stochastic cellular automaton) to a
one-dimensional system to analyse the dynamics. Using maximum likelihood
estimation, we can estimate the parameter of interest which, in combination
with a bifurcation diagram, reflects the expectancy that someone has to
transition to another mood state. After validating the proposed method with
simulated data, we apply this method to two empirical examples, where we show
its use in a clinical sample consisting of patients diagnosed with major
depressive disorder, and a general population sample. Results showed that the
majority of the clinical sample was categorized as having an expectancy for a
transition, while the majority of the general population sample did not have
this expectancy. We conclude that the mean field model has great potential in
assessing the expectancy for a transition between mood states. With some
extensions it could, in the future, aid clinical therapists in the treatment of
depressed patients.
"
1807.09681,2018-07-26,"Spatially varying coefficient modeling for large datasets: Eliminating N
  from spatial regressions","  While spatially varying coefficient (SVC) modeling is popular in applied
science, its computational burden is substantial. This is especially true if a
multiscale property of SVC is considered. Given this background, this study
develops a Moran's eigenvector-based spatially varying coefficients (M-SVC)
modeling approach that estimates multiscale SVCs computationally efficiently.
This estimation is accelerated through a (i) rank reduction, (ii)
pre-compression, and (iii) sequential likelihood maximization. Steps (i) and
(ii) eliminate the sample size N from the likelihood function; after these
steps, the likelihood maximization cost is independent of N. Step (iii) further
accelerates the likelihood maximization so that multiscale SVCs can be
estimated even if the number of SVCs, K, is large. The M-SVC approach is
compared with geographically weighted regression (GWR) through Monte Carlo
simulation experiments. These simulation results show that our approach is far
faster than GWR when N is large, despite numerically estimating 2K parameters
while GWR numerically estimates only 1 parameter. Then, the proposed approach
is applied to a land price analysis as an illustration. The developed SVC
estimation approach is implemented in the R package ""spmoran.""
"
1808.01217,2018-08-06,"Sounding Spider: An Efficient Way for Representing Uncertainties in High
  Dimensions","  This article proposes a visualization method for multidimensional data based
on: (i) Animated functional Hypothetical Outcome Plots (f-HOPs); (ii)
3-dimensional Kiviat plot; and (iii) data sonification. In an Uncertainty
Quantification (UQ) framework, such analysis coupled with standard statistical
analysis tools such as Probability Density Functions (PDF) can be used to
augment the understanding of how the uncertainties in the numerical code inputs
translate into uncertainties in the quantity of interest (QoI).
  In contrast with static representation of most advanced techniques such as
functional Highest Density Region (HDR) boxplot or functional boxplot, f-HOPs
is a dynamic visualization that enables the practitioners to infer the dynamics
of the physics and enables to see functional correlations that may exist. While
this technique only allows to represent the QoI, we propose a 3-dimensional
version of the Kiviat plot to encode all input parameters. This new
visualization takes advantage of information from f-HOPs through data
sonification. All in all, this allows to analyse large datasets within a
high-dimensional parameter space and a functional QoI in the same canvas. The
proposed method is assessed and showed its benefits on two related
environmental datasets.
"
1808.02214,2019-11-25,"Allocations of Cold Standbys to Series and Parallel Systems with
  Dependent Components","  In the context of industrial engineering, cold-standby redundancies
allocation strategy is usually adopted to improve the reliability of coherent
systems. This paper investigates optimal allocation strategies of cold standbys
for series and parallel systems comprised of dependent components with
left/right tail weakly stochastic arrangement increasing lifetimes. For the
case of heterogeneous and independent matched cold standbys, it is proved that
better redundancies should be put in the nodes having weaker [better]
components for series [parallel] systems. For the case of homogeneous and
independent cold standbys, it is shown that more redundancies should be put in
standby with weaker [better] components to enhance the reliability of series
[parallel] systems. The results developed here generalize and extend those
corresponding ones in the literature to the case of series and parallel systems
with dependent components. Numerical examples are also presented to provide
guidance for the practical use of our theoretical findings.
"
1808.03201,2020-02-27,An optimal design for hierarchical generalized group testing,"  Choosing an optimal strategy for hierarchical group testing is an important
problem for practitioners who are interested in disease screening with limited
resources. For example, when screening for infectious diseases in large
populations, it is important to use algorithms that minimize the cost of
potentially expensive assays. Black et al. (2015) described this as an
intractable problem unless the number of individuals to screen is small. They
proposed an approximation to an optimal strategy that is difficult to implement
for large population sizes. In this article, we develop an optimal design with
respect to the expected total number of tests that can be obtained using a
novel dynamic programming algorithm. We show that this algorithm is
substantially more efficient than the approach proposed by Black et al. (2015).
In addition, we compare the two designs for imperfect tests. R code is provided
for the practitioner.
"
1808.04964,2018-08-16,A Probabilistic Proof of the Perron-Frobenius Theorem,"  The Perron-Frobenius theorem plays an important role in many areas of
management science and operations research. This paper provides a probabilistic
perspective on the theorem, by discussing a proof that exploits a probabilistic
representation of the Perron-Frobenius eigenvalue and eigenvectors in terms of
the dynamics of a Markov chain. The proof provides conditions in both the
finite-dimensional and infinite-dimensional settings under which the
Perron-Frobenius eigenvalue and eigenvectors exist. Furthermore, the
probabilistic representations that arise can be used to produce a Monte Carlo
algorithm for computing the Perron-Frobenius eigenvalue and eigenvectors that
will be explored elsewhere.
"
1808.05014,2018-08-16,A Conversation with Jon Wellner,"  Jon August Wellner was born in Portland, Oregon, in August 1945. He received
his Bachelor's degree from the University of Idaho in 1968 and his PhD degree
from the University of Washington in 1975. From 1975 until 1983 he was an
Assistant Professor and Associate Professor at the University of Rochester. In
1983 he returned to the University of Washington, and has remained at the UW as
a faculty member since that time. Over the course of a long and distinguished
career, Jon has made seminal contributions to a variety of areas including
empirical processes, semiparametric theory, and shape-constrained inference,
and has co-authored a number of extremely influential books. He has been
honored as the Le Cam lecturer by both the IMS (2015) and the French
Statistical Society (2017). He is a Fellow of the IMS, the ASA, and the AAAS,
and an elected member of the International Statistical Institute. He has served
as co-Editor of Annals of Statistics (2001--2003) and Editor of Statistical
Science (2010--2013), and President of IMS (2016--2017). In 2010 he was made a
Knight of the Order of the Netherlands Lion. In his free time, Jon enjoys
mountain climbing and backcountry skiing in the Cascades and British Columbia.
"
1808.06884,2018-08-22,The Turtleback Diagram for Conditional Probability,"  We elaborate on an alternative representation of conditional probability to
the usual tree diagram. We term the representation `turtleback diagram' for its
resemblance to the pattern on turtle shells. Adopting the set theoretic view of
events and the sample space, the turtleback diagram uses elements from Venn
diagrams---set intersection, complement and partition---for conditioning, with
the additional notion that the area of a set indicates probability whereas the
ratio of areas for conditional probability. Once parts of the diagram are drawn
and properly labeled, the calculation of conditional probability involves only
simple arithmetic on the area of relevant sets. We discuss turtleback diagrams
in relation to other visual representations of conditional probability, and
detail several scenarios in which turtleback diagrams prove useful. By the
equivalence of recursive space partition and the tree, the turtleback diagram
is seen to be equally expressive as the tree diagram for representing abstract
concepts. We also provide empirical data on the use of turtleback diagrams with
undergraduate students in elementary statistics or probability courses.
"
1808.06961,2018-08-22,On the mathematics of the free-choice paradigm,"  Chen and Risen pointed out a logical flaw affecting the conclusions of a
number of past experiments that used the free-choice paradigm to measure
choice-induced attitude change. They went on to design and implement a
free-choice experiment that used a novel type of control group in order to
avoid this logical pitfall. In this paper, we describe a method by which a
free-choice experiment can be correctly conducted even without a control group.
"
1808.07588,2018-09-24,"Stakes are higher, risk is lower: Citation distributions are more equal
  in high quality journals","  Psychology is a discipline standing at the crossroads of hard and social
sciences. Therefore it is especially interesting to study bibliometric
characteristics of psychology journals. We also take two adjacent disciplines,
neurosciences and sociology. One is closer to hard sciences, another is a
social science. We study not the journal citedness itself (impact factor etc.)
but the citation distribution across papers within journals. This is, so to
say, ""indicators of the second order"" which measure the digression from the
journal's average of the citations received by individual papers. As is shown,
such information about journals may also help authors to correct their
publication strategies.
"
1809.00377,2020-01-06,On Some Integral Means,"  Harmonic, Geometric, Arithmetic, Heronian and Contraharmonic means have been
studied by many mathematicians. In 2003, H. Evens studied these means from
geometrical point of view and established some of the inequalities between them
in using a circle and its radius. In 1961, E. Beckenback and R. Bellman
introduced several inequalities corresponding to means. In this paper, we will
introduce the concept of mean functions and integral means and give bounds on
some of these mean functions and integral means.
"
1809.01489,2018-09-06,"A Bayesian GED-Gamma stochastic volatility model for return data: a
  marginal likelihood approach","  Several studies explore inferences based on stochastic volatility (SV)
models, taking into account the stylized facts of return data. The common
problem is that the latent parameters of many volatility models are
high-dimensional and analytically intractable, which means inferences require
approximations using, for example, the Markov Chain Monte Carlo or Laplace
methods. Some SV models are expressed as a linear Gaussian state-space model
that leads to a marginal likelihood, reducing the dimensionality of the
problem. Others are not linearized, and the latent parameters are integrated
out. However, these present a quite restrictive evolution equation. Thus, we
propose a Bayesian GED-Gamma SV model with a direct marginal likelihood that is
a product of the generalized Student's t-distributions in which the latent
states are related across time through a stationary Gaussian evolution
equation. Then, an approximation is made for the prior distribution of
log-precision/volatility, without the need for model linearization. This also
allows for the computation of the marginal likelihood function, where the
high-dimensional latent states are integrated out and easily sampled in blocks
using a smoothing procedure. In addition, extensions of our GED-Gamma model are
easily made to incorporate skew heavy-tailed distributions. We use the Bayesian
estimator for the inference of static parameters, and perform a simulation
study on several properties of the estimator. Our results show that the
proposed model can be reasonably estimated. Furthermore, we provide case
studies of a Brazilian asset and the pound/dollar exchange rate to show the
performance of our approach in terms of fit and prediction.
  Keywords: SV model, New sequential and smoothing procedures, Generalized
Student's t-distribution, Non-Gaussian errors, Heavy tails, Skewness
"
1809.01501,2018-10-03,Non-Gaussian Stochastic Volatility Model with Jumps via Gibbs Sampler,"  In this work, we propose a model for estimating volatility from financial
time series, extending the non-Gaussian family of space-state models with exact
marginal likelihood proposed by Gamerman, Santos and Franco (2013). On the
literature there are models focused on estimating financial assets risk,
however, most of them rely on MCMC methods based on Metropolis algorithms,
since full conditional posterior distributions are not known. We present an
alternative model capable of estimating the volatility, in an automatic way,
since all full conditional posterior distributions are known, and it is
possible to obtain an exact sample of parameters via Gibbs Sampler. The
incorporation of jumps in returns allows the model to capture speculative
movements of the data, so that their influence does not propagate to
volatility. We evaluate the performance of the algorithm using synthetic and
real data time series.
  Keywords: Financial time series, Stochastic volatility, Gibbs Sampler,
Dynamic linear models.
"
1809.02952,2020-07-21,"Data scraping, ingestation, and modeling: bringing data from cars.com
  into the intro stats class","  New tools have made it much easier for students to develop skills to work
with interesting data sets as they begin to extract meaning from data. To fully
appreciate the statistical analysis cycle, students benefit from repeated
experiences collecting, ingesting, wrangling, analyzing data and communicating
results. How can we bring such opportunities into the classroom? We describe a
classroom activity, originally developed by Danny Kaplan (Macalester College),
in which students can expand upon statistical problem solving by hand-scraping
data from cars.com, ingesting these data into R, then carrying out analyses of
the relationships between price, mileage, and model year for a selected type of
car.
"
1809.03561,2019-10-17,"Quantile Regression for Qualifying Match of GEFCom2017 Probabilistic
  Load Forecasting","  We present a simple quantile regression-based forecasting method that was
applied in a probabilistic load forecasting framework of the Global Energy
Forecasting Competition 2017 (GEFCom2017). The hourly load data is log
transformed and split into a long-term trend component and a remainder term.
The key forecasting element is the quantile regression approach for the
remainder term that takes into account weekly and annual seasonalities such as
their interactions. Temperature information is only used to stabilize the
forecast of the long-term trend component. Public holidays information is
ignored. Still, the forecasting method placed second in the open data track and
fourth in the definite data track with our forecasting method, which is
remarkable given simplicity of the model. The method also outperforms the
Vanilla benchmark consistently.
"
1809.04338,2018-09-13,Game time: statistical contests in the classroom,"  We describe a contest in variable selection which was part of a statistics
course for graduate students. In particular, the possibility to create a
contest themselves offered an additional challenge for more advanced students.
Since working with data is becoming more important in teaching statistics, we
greatly encourage other instructors to try the same.
"
1809.04721,2018-09-14,"Perspective from the Literature on the Role of Expert Judgment in
  Scientific and Statistical Research and Practice","  This article, produced as a result of the Symposium on Statistical Inference,
is an introduction to the literature on the function of expertise, judgment,
and choice in the practice of statistics and scientific research. In
particular, expert judgment plays a critical role in conducting Frequentist
hypothesis tests and Bayesian models, especially in selection of appropriate
prior distributions for model parameters. The subtlety of interpreting results
is also discussed. Finally, external recommendations are collected for how to
more effectively encourage proper use of judgment in statistics. The paper
synthesizes the literature for the purpose of creating a single reference and
inciting more productive discussions on how to improve the future of statistics
and science.
"
1809.05731,2018-09-18,"Inter-Rater: Software for analysis of inter-rater reliability by
  permutating pairs of multiple users","  Inter-Rater quantifies the reliability between multiple raters who evaluate a
group of subjects. It calculates the group quantity, Fleiss kappa, and it
improves on existing software by keeping information about each user and
quantifying how each user agreed with the rest of the group. This is
accomplished through permutations of user pairs. The software was written in
Python, can be run in Linux, and the code is deposited in Zenodo and GitHub.
This software can be used for evaluation of inter-rater reliability in
systematic reviews, medical diagnosis algorithms, education applications, and
others.
"
1809.06592,2018-09-19,"The distortion principle for insurance pricing: properties,
  identification and robustness","  Distortion (Denneberg 1990) is a well known premium calculation principle for
insurance contracts. In this paper, we study sensitivity properties of
distortion functionals w.r.t. the assumptions for risk aversion as well as
robustness w.r.t. ambiguity of the loss distribution. Ambiguity is measured by
the Wasserstein distance. We study variances of distances for probability
models and identify some worst case distributions. In addition to the direct
problem we also investigate the inverse problem, that is how to identify the
distortion density on the basis of observations of insurance premia.
"
1809.07401,2019-05-21,"Transmission of Macroeconomic Shocks to Risk Parameters: Their uses in
  Stress Testing","  In this paper, we are interested in evaluating the resilience of financial
portfolios under extreme economic conditions. Therefore, we use empirical
measures to characterize the transmission process of macroeconomic shocks to
risk parameters. We propose the use of an extensive family of models, called
General Transfer Function Models, which condense well the characteristics of
the transmission described by the impact measures. The procedure for estimating
the parameters of these models is described employing the Bayesian approach and
using the prior information provided by the impact measures. In addition, we
illustrate the use of the estimated models from the credit risk data of a
portfolio.
"
1809.08771,2021-08-05,Modeling longitudinal data using matrix completion,"  In clinical practice and biomedical research, measurements are often
collected sparsely and irregularly in time while the data acquisition is
expensive and inconvenient. Examples include measurements of spine bone mineral
density, cancer growth through mammography or biopsy, a progression of
defective vision, or assessment of gait in patients with neurological
disorders. Since the data collection is often costly and inconvenient,
estimation of progression from sparse observations is of great interest for
practitioners.
  From the statistical standpoint, such data is often analyzed in the context
of a mixed-effect model where time is treated as both a fixed-effect
(population progression curve) and a random-effect (individual variability).
Alternatively, researchers analyze Gaussian processes or functional data where
observations are assumed to be drawn from a certain distribution of processes.
These models are flexible but rely on probabilistic assumptions, require very
careful implementation, specific to the given problem, and tend to be slow in
practice.
  In this study, we propose an alternative elementary framework for analyzing
longitudinal data, relying on matrix completion. Our method yields estimates of
progression curves by iterative application of the Singular Value
Decomposition. Our framework covers multivariate longitudinal data, regression,
and can be easily extended to other settings. As it relies on existing tools
for matrix algebra it is efficient and easy to implement.
  We apply our methods to understand trends of progression of motor impairment
in children with Cerebral Palsy. Our model approximates individual progression
curves and explains 30% of the variability. Low-rank representation of
progression trends enables identification of different progression trends in
subtypes of Cerebral Palsy.
"
1809.09527,2019-04-18,"Mostly Harmless Simulations? Using Monte Carlo Studies for Estimator
  Selection","  We consider two recent suggestions for how to perform an empirically
motivated Monte Carlo study to help select a treatment effect estimator under
unconfoundedness. We show theoretically that neither is likely to be
informative except under restrictive conditions that are unlikely to be
satisfied in many contexts. To test empirical relevance, we also apply the
approaches to a real-world setting where estimator performance is known. Both
approaches are worse than random at selecting estimators which minimise
absolute bias. They are better when selecting estimators that minimise mean
squared error. However, using a simple bootstrap is at least as good and often
better. For now researchers would be best advised to use a range of estimators
and compare estimates for robustness.
"
1809.10496,2023-08-01,Benchmarking in cluster analysis: A white paper,"  Note: A revised version of this is now published. Please cite and read (it's
open access): Van Mechelen, I., Boulesteix, A.-L., Dangl, R., Dean, N., Hennig,
C., Leisch, F., Steinley, D., Warrens, M. J. (2023). A white paper on good
research practices in benchmarking: The case of cluster analysis. WIREs Data
Mining and Knowledge Discovery, e1511. https://doi.org/10.1002/widm.1511
  To achieve scientific progress in terms of building a cumulative body of
knowledge, careful attention to benchmarking is of the utmost importance. This
means that proposals of new methods of data pre-processing, new data-analytic
techniques, and new methods of output post-processing, should be extensively
and carefully compared with existing alternatives, and that existing methods
should be subjected to neutral comparison studies. To date, benchmarking and
recommendations for benchmarking have been frequently seen in the context of
supervised learning. Unfortunately, there has been a dearth of guidelines for
benchmarking in an unsupervised setting, with the area of clustering as an
important subdomain. To address this problem, discussion is given to the
theoretical conceptual underpinnings of benchmarking in the field of cluster
analysis by means of simulated as well as empirical data. Subsequently, the
practicalities of how to address benchmarking questions in clustering are dealt
with, and foundational recommendations are made.
"
1810.01729,2018-10-04,Can everyday AI be ethical. Fairness of Machine Learning Algorithms,"  Combining big data and machine learning algorithms, the power of automatic
decision tools induces as much hope as fear. Many recently enacted European
legislation (GDPR) and French laws attempt to regulate the use of these tools.
Leaving aside the well-identified problems of data confidentiality and
impediments to competition, we focus on the risks of discrimination, the
problems of transparency and the quality of algorithmic decisions. The detailed
perspective of the legal texts, faced with the complexity and opacity of the
learning algorithms, reveals the need for important technological disruptions
for the detection or reduction of the discrimination risk, and for addressing
the right to obtain an explanation of the auto- matic decision. Since trust of
the developers and above all of the users (citizens, litigants, customers) is
essential, algorithms exploiting personal data must be deployed in a strict
ethical framework. In conclusion, to answer this need, we list some ways of
controls to be developed: institutional control, ethical charter, external
audit attached to the issue of a label.
"
1810.02294,2019-01-29,Markov Properties of Discrete Determinantal Point Processes,"  Determinantal point processes (DPPs) are probabilistic models for repulsion.
When used to represent the occurrence of random subsets of a finite base set,
DPPs allow to model global negative associations in a mathematically elegant
and direct way. Discrete DPPs have become popular and computationally tractable
models for solving several machine learning tasks that require the selection of
diverse objects, and have been successfully applied in numerous real-life
problems. Despite their popularity, the statistical properties of such models
have not been adequately explored. In this note, we derive the Markov
properties of discrete DPPs and show how they can be expressed using graphical
models.
"
1810.05118,2019-12-13,Canadian Crime Rates in the Penalty Box,"  Over the 1962 to 2016 period, the Canadian violent crime rate has remained
strongly correlated with National Hockey League (NHL) penalties. The Canadian
property crime rate was similarly correlated with stolen base attempts in the
Major League Baseball (MLB). Of course, correlation does not imply causation or
prove association. It is simply presented here as an observation. Curious
readers might be tempted to conduct additional research and ask questions in
order to enhance the conversation, transition away from a state of confusion,
clarify the situation, prevent false attribution, and possibly solve a problem
that economists call identification.
"
1810.06387,2018-10-16,I can see clearly now: reinterpreting statistical significance,"  Null hypothesis significance testing remains popular despite decades of
concern about misuse and misinterpretation. We believe that much of the problem
is due to language: significance testing has little to do with other meanings
of the word ""significance"". Despite the limitations of null-hypothesis tests,
we argue here that they remain useful in many contexts as a guide to whether a
certain effect can be seen clearly in that context (e.g. whether we can clearly
see that a correlation or between-group difference is positive or negative). We
therefore suggest that researchers describe the conclusions of null-hypothesis
tests in terms of statistical ""clarity"" rather than statistical ""significance"".
This simple semantic change could substantially enhance clarity in statistical
communication.
"
1810.10859,2020-03-03,"Complementary Lipschitz continuity results for the distribution of
  intersections or unions of independent random sets in finite discrete spaces","  We prove that intersections and unions of independent random sets in finite
spaces achieve a form of Lipschitz continuity. More precisely, given the
distribution of a random set $\Xi$, the function mapping any random set
distribution to the distribution of its intersection (under independence
assumption) with $\Xi$ is Lipschitz continuous with unit Lipschitz constant if
the space of random set distributions is endowed with a metric defined as the
$L_k$ norm distance between inclusion functionals also known as commonalities.
Moreover, the function mapping any random set distribution to the distribution
of its union (under independence assumption) with $\Xi$ is Lipschitz continuous
with unit Lipschitz constant if the space of random set distributions is
endowed with a metric defined as the $L_k$ norm distance between hitting
functionals also known as plausibilities.
  Using the epistemic random set interpretation of belief functions, we also
discuss the ability of these distances to yield conflict measures. All the
proofs in this paper are derived in the framework of Dempster-Shafer belief
functions. Let alone the discussion on conflict measures, it is straightforward
to transcribe the proofs into the general (non necessarily epistemic) random
set terminology.
"
1810.10946,2018-10-26,"Helix modelling through the Mardia-Holmes model framework and an
  extension of the Mardia-Holmes model","  For noisy two-dimensional data, which are approximately uniformly distributed
near the circumference of an ellipse, Mardia and Holmes (1980) developed a
model to fit the ellipse. In this paper we adapt their methodology to the
analysis of helix data in three dimensions. If the helix axis is known, then
the Mardia-Holmes model for the circular case can be fitted after projecting
the helix data onto the plane normal to the helix axis. If the axis is unknown,
an iterative algorithm has been developed to estimate the axis. The methodology
is illustrated using simulated protein alpha-helices. We also give a
multivariate version of the Mardia-Holmes model which will be applicable for
fitting an ellipsoid and in particular a cylinder.
"
1810.10972,2022-03-25,On a generalised form of subjective probability,"  This paper is motivated by the questions of how to give the concept of
probability an adequate real-world meaning, and how to explain a certain type
of phenomenon that can be found, for instance, in Ellsberg's paradox. It
attempts to answer these questions by constructing an alternative theory to one
that was proposed in earlier papers on the basis of various important
criticisms that were raised against this earlier theory. The conceptual
principles of the corresponding definition of probability are laid out and
explained in detail. In particular, what is required to fully specify a
probability distribution under this definition is not just the distribution
function of the variable concerned, but also an assessment of the internal
and/or the external strength of this function relative to other distribution
functions of interest. This way of defining probability is applied to various
examples and problems including, perhaps most notably, to a long-running
controversy concerning the distinction between Bayesian and fiducial inference.
The characteristics of this definition of probability are carefully evaluated
in terms of the issues that it sets out to address.
"
1810.12430,2020-12-15,"On the agreement between bibliometrics and peer review: evidence from
  the Italian research assessment exercises","  This paper appraises the concordance between bibliometrics and peer review,
by drawing evidence from the data of two experiments realized by the Italian
governmental agency for research evaluation. The experiments were performed for
validating the dual system of evaluation, consisting in the interchangeable use
of bibliometyrics and peer review, adopted by the agency in the research
assessment exercises. The two experiments were based on stratified random
samples of journal articles. Each article was scored by bibliometrics and by
peer review. The degree of concordance between the two evaluations is then
computed. The correct setting of the experiments is defined by developing the
design-based estimation of the Cohen's kappa coefficient and some testing
procedures for assessing the homogeneity of missing proportions between strata.
The results of both experiments show that for each research areas of hard
sciences, engineering and life sciences, the degree of agreement between
bibliometrics and peer review is -- at most -- weak at an individual article
level. Thus, the outcome of the experiments does not validate the use of the
dual system of evaluation in the Italian research assessments. More in general,
the very weak concordance indicates that metrics should not replace peer review
at the level of individual article. Hence, the use of the dual system of
evaluation for reducing costs might introduce unknown biases in a research
assessment exercise.
"
1811.01469,2018-11-06,"Monte Carlo Simulations on robustness of functional location estimator
  based on several functional depth","  Functional data analysis has been a growing field of study in recent decades,
and one fundamental task in functional data analysis is estimating the sample
location. A notion called statistical depth has been extended from multivariate
data to functional data, and it can provide a center-outward order for each
observation within a sample of functional curves. Making use of this intuitive
nature of depth methods, a depth-based trimmed mean where curves with lower
depth values are excluded can be used as a robust location estimator for the
sample. In this project, we first introduced several state-of-the-art depth
approaches for functional data. These depths were half region depth, functional
majority depth, band depth, modified band depth and functional spatial depth.
Then we described a robust location estimator based on functional depth, and
studied performances of these estimators based on different functional depth
approaches via simulation tests. Finally, the test results showed that
estimators based on functional spatial depth and modified band depth exhibited
superior performances.
"
1811.01821,2018-11-09,Statistical reform and the replication crisis,"  The replication crisis has prompted many to call for statistical reform
within the psychological sciences. Here we examine issues within Frequentist
statistics that may have led to the replication crisis, and we examine the
alternative---Bayesian statistics---that many have suggested as a replacement.
The Frequentist approach and the Bayesian approach offer radically different
perspectives on evidence and inference with the Frequentist approach
prioritising error control and the Bayesian approach offering a formal method
for quantifying the relative strength of evidence for hypotheses. We suggest
that rather than mere statistical reform, what is needed is a better
understanding of the different modes of statistical inference and a better
understanding of how statistical inference relates to scientific inference.
"
1811.02021,2018-11-08,Using GitHub Classroom To Teach Statistics,"  Git and GitHub are common tools for keeping track of multiple versions of
data analytic content, which allow for more than one person to simultaneously
work on a project. GitHub Classroom aims to provide a way for students to work
on and submit their assignments via Git and GitHub, giving teachers an
opportunity to teach these version control tools as part of their course. In
the Fall 2017 semester, we implemented GitHub Classroom in two educational
settings--an introductory computational statistics lab and a more advanced
computational statistics course. We found many educational benefits of
implementing GitHub Classroom, such as easily providing coding feedback during
assignments and making students more confident in their ability to collaborate
and use version control tools for future data science work. To encourage and
ease the transition into using GitHub Classroom, we provide free and publicly
available resources--both for students to begin using Git/GitHub and for
teachers to use GitHub Classroom for their own courses.
"
1811.02201,2021-09-21,Optimal spectral shrinkage and PCA with heteroscedastic noise,"  This paper studies the related problems of prediction, covariance estimation,
and principal component analysis for the spiked covariance model with
heteroscedastic noise. We consider an estimator of the principal components
based on whitening the noise, and we derive optimal singular value and
eigenvalue shrinkers for use with these estimated principal components.
Underlying these methods are new asymptotic results for the high-dimensional
spiked model with heteroscedastic noise, and consistent estimators for the
relevant population parameters. We extend previous analysis on out-of-sample
prediction to the setting of predictors with whitening. We demonstrate certain
advantages of noise whitening. Specifically, we show that in a certain
asymptotic regime, optimal singular value shrinkage with whitening converges to
the best linear predictor, whereas without whitening it converges to a
suboptimal linear predictor. We prove that for generic signals, whitening
improves estimation of the principal components, and increases a natural
signal-to-noise ratio of the observations. We also show that for rank one
signals, our estimated principal components achieve the asymptotic minimax
rate.
"
1811.03558,2019-04-30,Iterated Integrals and Population Time Series Analysis,"  One of the core advantages topological methods for data analysis provide is
that the language of (co)chains can be mapped onto the semantics of the data,
providing a natural avenue for human understanding of the results. Here, we
describe such a semantic structure on Chen's classical iterated integral
cochain model for paths in Euclidean space. Specifically, in the context of
population time series data, we observe that iterated integrals provide a
model-free measure of pairwise influence that can be used for causality
inference. Along the way, we survey recent results and applications, review the
current standard methods for causality inference, and briefly provide our
outlook on generalizations to go beyond time series data.
"
1811.03578,2019-09-02,The ASCCR Frame for Learning Essential Collaboration Skills,"  Statistics and data science are especially collaborative disciplines that
typically require practitioners to interact with many different people or
groups. Consequently, interdisciplinary collaboration skills are part of the
personal and professional skills essential for success as an applied
statistician or data scientist. These skills are learnable and teachable, and
learning and improving collaboration skills provides a way to enhance one's
practice of statistics and data science. To help individuals learn these skills
and organizations to teach them, we have developed a framework covering five
essential components of statistical collaboration: Attitude, Structure,
Content, Communication, and Relationship. We call this the ASCCR Frame. This
framework can be incorporated into formal training programs in the classroom or
on the job and can also be used by individuals through self-study. We show how
this framework can be applied specifically to statisticians and data scientists
to improve their collaboration skills and their interdisciplinary impact. We
believe that the ASCCR Frame can help organize and stimulate research and
teaching in interdisciplinary collaboration and call on individuals and
organizations to begin generating evidence regarding its effectiveness.
"
1811.04079,2018-11-13,"Surrogate Modeling of Stochastic Functions - Application to
  computational Electromagnetic Dosimetry","  Metamodeling of complex numerical systems has recently attracted the interest
of the mathematical programming community. Despite the progress in high
performance computing, simulations remain costly, as a matter of fact, the
assessment of the exposure to radio frequency electromagnetic fields is
computationally prohibitive since one simulation can require hours. Moreover,
in many engineering problems, carrying out deterministic numerical operations
without considering uncertainties can lead to unreliable designs. In this paper
we focus on the surrogate modeling of a particular type of computational models
called stochastic simulators. In contrast to deterministic simulators which
yield a unique output for each set of input parameters, stochastic simulators
inherently contain some sources of randomness and the output at a given point
is a probability density function. Characterizing the stochastic simulators is
even more time consuming. This paper represents stochastic simulators as a
stochastic process and describes a metamodeling approach based on the
Karhunen-Lo\`eve spectral decomposition.
"
1811.04525,2019-10-30,Openness and Reproducibility: Insights from a Model-Centric Approach,"  This paper investigates the conceptual relationship between openness and
reproducibility using a model-centric approach, heavily informed by probability
theory and statistics. We first clarify the concepts of reliability,
auditability, replicability, and reproducibility--each of which denotes a
potential scientific objective. Then we advance a conceptual analysis to
delineate the relationship between open scientific practices and these
objectives. Using the notion of an idealized experiment, we identify which
components of an experiment need to be reported and which need to be repeated
to achieve the relevant objective. The model-centric framework we propose aims
to contribute precision and clarity to the discussions surrounding the
so-called reproducibility crisis.
"
1811.06980,2019-04-01,"Batch Self Organizing maps for distributional data using adaptive
  distances","  The paper deals with a Batch Self Organizing Map algorithm (DBSOM) for data
described by distributional-valued variables. This kind of variables is
characterized to take as values one-dimensional probability or frequency
distributions on a numeric support. The objective function optimized in the
algorithm depends on the choice of the distance measure. According to the
nature of the date, the $L_2$ Wasserstein distance is proposed as one of the
most suitable metrics to compare distributions. It is widely used in several
contexts of analysis of distributional data. Conventional batch SOM algorithms
consider that all variables are equally important for the training of the SOM.
However, it is well known that some variables are less relevant than others for
this task. In order to take into account the different contribution of the
variables we propose an adaptive version of the DBSOM algorithm that tackles
this problem with an additional step: a relevance weight is automatically
learned for each distributional-valued variable. Moreover, since the $L_2$
Wasserstein distance allows a decomposition into two components: one related to
the means and one related to the size and shape of the distributions, also
relevance weights are automatically learned for each of the measurement
components to emphasize the importance of the different estimated parameters of
the distributions. Examples of real and synthetic datasets of distributional
data illustrate the usefulness of the proposed DBSOM algorithms.
"
1811.08566,2019-02-11,Castor: Contextual IoT Time Series Data and Model Management at Scale,"  We demonstrate Castor, a cloud-based system for contextual IoT time series
data and model management at scale. Castor is designed to assist Data
Scientists in (a) exploring and retrieving all relevant time series and
contextual information that is required for their predictive modelling tasks;
(b) seamlessly storing and deploying their predictive models in a cloud
production environment; (c) monitoring the performance of all predictive models
in production and (semi-)automatically retraining them in case of performance
deterioration. The main features of Castor are: (1) an efficient pipeline for
ingesting IoT time series data in real time; (2) a scalable, hybrid data
management service for both time series and contextual data; (3) a versatile
semantic model for contextual information which can be easily adopted to
different application domains; (4) an abstract framework for developing and
storing predictive models in R or Python; (5) deployment services which
automatically train and/or score predictive models upon user-defined
conditions. We demonstrate Castor for a real-world Smart Grid use case and
discuss how it can be adopted to other application domains such as Smart
Buildings, Telecommunication, Retail or Manufacturing.
"
1811.10375,2018-11-27,Quantifying Privacy in Nuclear Warhead Authentication Protocols,"  International verification of nuclear warheads is a practical problem in
which the protection of secret warhead information is of paramount importance.
We propose a measure that would enable a weapon owner to evaluate the privacy
of a proposed protocol in a technology-neutral fashion. We show the problem is
reducible to `natural' and `corrective' learning. The natural learning can be
computed without assumptions about the inspector, while the corrective learning
accounts for the inspector's prior knowledge. The natural learning provides the
warhead owner a useful lower bound on the information leaked by the proposed
protocol. Using numerical examples, we demonstrate that the proposed measure
correlates better with the accuracy of a maximum a posteriori probability
estimate than alternative measures.
"
1811.10603,2018-11-28,On some properties of the new Sine-skewed Cardioid Distribution,"  The new Sine Skewed Cardioid (ssc) distribution been just introduced and
characterized by Ahsanullah (2018). Here, we study the asymptotic properties of
its tails by determining its extreme value domain, the characteristic function,
the moments and likelihood estimators of the two parameters, the asymptotic
normality of the moments estimators and the random generation of data from the
\textit{ssc} distribution. Finally, we proceed to a simulation study to show
the performance of the random generation method and the quality of the moments
estimation of the parameters.
"
1811.11301,2019-02-19,"Calculating CVaR and bPOE for Common Probability Distributions With
  Application to Portfolio Optimization and Density Estimation","  Conditional Value-at-Risk (CVaR) and Value-at-Risk (VaR), also called the
superquantile and quantile, are frequently used to characterize the tails of
probability distribution's and are popular measures of risk. Buffered
Probability of Exceedance (bPOE) is a recently introduced characterization of
the tail which is the inverse of CVaR, much like the CDF is the inverse of the
quantile. These quantities can prove very useful as the basis for a variety of
risk-averse parametric engineering approaches. Their use, however, is often
made difficult by the lack of well-known closed-form equations for calculating
these quantities for commonly used probability distribution's. In this paper,
we derive formulas for the superquantile and bPOE for a variety of common
univariate probability distribution's. Besides providing a useful collection
within a single reference, we use these formulas to incorporate the
superquantile and bPOE into parametric procedures. In particular, we consider
two: portfolio optimization and density estimation. First, when portfolio
returns are assumed to follow particular distribution families, we show that
finding the optimal portfolio via minimization of bPOE has advantages over
superquantile minimization. We show that, given a fixed threshold, a single
portfolio is the minimal bPOE portfolio for an entire class of distribution's
simultaneously. Second, we apply our formulas to parametric density estimation
and propose the method of superquantile's (MOS), a simple variation of the
method of moment's (MM) where moment's are replaced by superquantile's at
different confidence levels. With the freedom to select various combinations of
confidence levels, MOS allows the user to focus the fitting procedure on
different portions of the distribution, such as the tail when fitting
heavy-tailed asymmetric data.
"
1812.03940,2018-12-11,"Rapid Prototyping Model for Healthcare Alternative Payment Models:
  Replicating the Federally Qualified Health Center Advanced Primary Care
  Practice Demonstration","  Innovation in healthcare payment and service delivery utilizes high cost,
high risk pilots paired with traditional program evaluations. Decision-makers
are unable to reliably forecast the impacts of pilot interventions in this
complex system, complicating the feasibility assessment of proposed healthcare
models. We developed and validated a Discrete Event Simulation (DES) model of
primary care for patients with Diabetes to allow rapid prototyping and
assessment of models before pilot implementation. We replicated four outcomes
from the Centers for Medicare and Medicaid Services Federally Qualified Health
Center Advanced Primary Care Practice pilot. The DES model simulates a
synthetic population's healthcare experience, including symptom onset,
appointment scheduling, screening, and treatment, as well as the impact of
physician training. A network of detailed event modules was developed from
peer-reviewed literature. Synthetic patients' attributes modify the probability
distributions for event outputs and direct them through an episode of care;
attributes are in turn modified by patients' experiences. Our model replicates
the direction of the effect of physician training on the selected outcomes, and
the strength of the effect increases with the number of trainings. The
simulated effect strength replicates the pilot results for eye exams and
nephropathy screening, but over-estimates results for HbA1c and LDL screening.
Our model will improve decision-makers' abilities to assess the feasibility of
pilot success, with reproducible, literature-based systems models. Our model
identifies intervention and healthcare system components to which outcomes are
sensitive, so these aspects can be monitored and controlled during pilot
implementation. More work is needed to improve replication of HbA1c and LDL
screening, and to elaborate sub-models related to intervention components.
"
1812.06491,2022-08-29,Multiple testing with persistent homology,"  In this paper we propose a computationally efficient multiple hypothesis
testing procedure for persistent homology. The computational efficiency of our
procedure is based on the observation that one can empirically simulate a null
distribution that is universal across many hypothesis testing applications
involving persistence homology. Our observation suggests that one can simulate
the null distribution efficiently based on a small number of summaries of the
collected data and use this null in the same way that p-value tables were used
in classical statistics. To illustrate the efficiency and utility of the null
distribution we provide procedures for rejecting acyclicity with both control
of the Family-Wise Error Rate (FWER) and the False Discovery Rate (FDR). We
will argue that the empirical null we propose is very general conditional on a
few summaries of the data based on simulations and limit theorems for
persistent homology for point processes.
"
1812.07153,2018-12-19,Gaussian Process Mixtures for Estimating Heterogeneous Treatment Effects,"  We develop a Gaussian-process mixture model for heterogeneous treatment
effect estimation that leverages the use of transformed outcomes. The approach
we will present attempts to improve point estimation and uncertainty
quantification relative to past work that has used transformed variable related
methods as well as traditional outcome modeling. Earlier work on modeling
treatment effect heterogeneity using transformed outcomes has relied on tree
based methods such as single regression trees and random forests. Under the
umbrella of non-parametric models, outcome modeling has been performed using
Bayesian additive regression trees and various flavors of weighted single
trees. These approaches work well when large samples are available, but suffer
in smaller samples where results are more sensitive to model misspecification -
our method attempts to garner improvements in inference quality via a correctly
specified model rooted in Bayesian non-parametrics. Furthermore, while we begin
with a model that assumes that the treatment assignment mechanism is known, an
extension where it is learnt from the data is presented for applications to
observational studies. Our approach is applied to simulated and real data to
demonstrate our theorized improvements in inference with respect to two causal
estimands: the conditional average treatment effect and the average treatment
effect. By leveraging our correctly specified model, we are able to more
accurately estimate the treatment effects while reducing their variance.
"
1812.07271,2019-04-10,On a flexible construction of a negative binomial model,"  This work presents a construction of stationary Markov models with
negative-binomial marginal distributions. A simple closed form expression for
the corresponding transition probabilities is given, linking the proposal to
well-known classes of birth and death processes and thus revealing interesting
characterizations. The advantage of having such closed form expressions is
tested on simulated and real data.
"
1812.09998,2019-10-02,Pragmatic hypotheses in the evolution of science,"  This paper introduces pragmatic hypotheses and relates this concept to the
spiral of scientific evolution. Previous works determined a characterization of
logically consistent statistical hypothesis tests and showed that the modal
operators obtained from this test can be represented in the hexagon of
oppositions. However, despite the importance of precise hypothesis in science,
they cannot be accepted by logically consistent tests. Here, we show that this
dilemma can be overcome by the use of pragmatic versions of precise hypotheses.
These pragmatic versions allow a level of imprecision in the hypothesis that is
small relative to other experimental conditions. The introduction of pragmatic
hypotheses allows the evolution of scientific theories based on statistical
hypothesis testing to be interpreted using the narratological structure of
hexagonal spirals, as defined by Pierre Gallais.
"
1812.10551,2019-05-21,Generalized Score Matching for Non-Negative Data,"  A common challenge in estimating parameters of probability density functions
is the intractability of the normalizing constant. While in such cases maximum
likelihood estimation may be implemented using numerical integration, the
approach becomes computationally intensive. The score matching method of
Hyv\""arinen [2005] avoids direct calculation of the normalizing constant and
yields closed-form estimates for exponential families of continuous
distributions over $\mathbb{R}^m$. Hyv\""arinen [2007] extended the approach to
distributions supported on the non-negative orthant, $\mathbb{R}_+^m$. In this
paper, we give a generalized form of score matching for non-negative data that
improves estimation efficiency. As an example, we consider a general class of
pairwise interaction models. Addressing an overlooked inexistence problem, we
generalize the regularized score matching method of Lin et al. [2016] and
improve its theoretical guarantees for non-negative Gaussian graphical models.
"
1812.10800,2018-12-31,"Practical Considerations for Data Collection and Management in Mobile
  Health Micro-randomized Trials","  There is a growing interest in leveraging the prevalence of mobile technology
to improve health by delivering momentary, contextualized interventions to
individuals' smartphones. A just-in-time adaptive intervention (JITAI) adjusts
to an individual's changing state and/or context to provide the right
treatment, at the right time, in the right place. Micro-randomized trials
(MRTs) allow for the collection of data which aid in the construction of an
optimized JITAI by sequentially randomizing participants to different treatment
options at each of many decision points throughout the study. Often, this data
is collected passively using a mobile phone. To assess the causal effect of
treatment on a near-term outcome, care must be taken when designing the data
collection system to ensure it is of appropriately high quality. Here, we make
several recommendations for collecting and managing data from an MRT. We
provide advice on selecting which features to collect and when, choosing
between ""agents"" to implement randomization, identifying sources of missing
data, and overcoming other novel challenges. The recommendations are informed
by our experience with HeartSteps, an MRT designed to test the effects of an
intervention aimed at increasing physical activity in sedentary adults. We also
provide a checklist which can be used in designing a data collection system so
that scientists can focus more on their questions of interest, and less on
cleaning data.
"
1812.11132,2018-12-31,Application of Robust Estimators in Shewhart S-Charts,"  Maintaining the quality of manufactured products at a desired level is known
to increase customer satisfaction and profitability. Shewhart control chart is
the most widely used in statistical process control (SPC) technique to monitor
the quality of products and control process variability. Based on the
assumption of independent and normally distributed data sets, sample mean and
standard deviation statistics are known to be the most efficient conventional
estimators to determine the process location and scale, respectively. On the
other hand, there is not guarantee that the real-world process data would be
normally distributed: outliers may exist, and/or sampled population may be
contaminated. In such cases, efficiency of the conventional estimators is
significantly reduced, and power of the Shewhart charts may be undesirably low,
e.g. occasional outliers in the rational subgroups (Phase I dataset) may
drastically affect the sample mean and standard deviation, resulting a serious
delay in detection of inferior products (Phase II procedure). For more
efficient analyses, it is required to use robust estimators against
contaminations. Consequently, it is determined that robust estimators are more
efficient both against diffuse localized and symmetric-asymmetric
contaminations, and have higher power in detecting disturbances, compared to
conventional methods.
"
1901.01336,2019-01-08,Projective Decomposition and Matrix Equivalence up to Scale,"  A data matrix may be seen simply as a means of organizing observations into
rows ( e.g., by measured object) and into columns ( e.g., by measured variable)
so that the observations can be analyzed with mathematical tools. As a
mathematical object, a matrix defines a linear mapping between points
representing weighted combinations of its rows (the row vector space) and
points representing weighted combinations of its columns (the column vector
space). From this perspective, a data matrix defines a relationship between the
information that labels its rows and the information that labels its columns,
and numerical methods are used to analyze this relationship. A first step is to
normalize the data, transforming each observation from scales convenient for
measurement to a common scale, on which addition and multiplication can
meaningfully combine the different observations. For example, z-transformation
rescales every variable to the same scale, standardized variation from an
expected value, but ignores scale differences between measured objects. Here we
develop the concepts and properties of projective decomposition, which applies
the same normalization strategy to both rows and columns by separating the
matrix into row- and column-scaling factors and a scale-normalized matrix. We
show that different scalings of the same scale-normalized matrix form an
equivalence class, and call the scale-normalized, canonical member of the class
its scale-invariant form that preserves all pairwise relative ratios.
Projective decomposition therefore provides a means of normalizing the broad
class of ratio-scale data, in which relative ratios are of primary interest,
onto a common scale without altering the ratios of interest, and simultaneously
accounting for scale effects for both organizations of the matrix values. Both
of these properties distinguish it from z-transformation.
"
1901.02714,2019-01-10,"Hourly Forecasting of Emergency Department Arrivals : Time Series
  Analysis","  Background: The stochastic behavior of patient arrival at an emergency
department (ED) complicates the management of an ED. More than 50% of hospitals
ED capacity tends to operate beyond its normal capacity and eventually fails to
deliver high-quality care. To address the concern of stochastics ED arrivals,
many types of research has been done using yearly, monthly and weekly time
series forecasting. Aim: Our research team believes that hourly time-series
forecasting of the load can improve ED management by predicting the arrivals of
future patients, and thus, can support strategic decisions in terms of quality
enhancement. Methods: Our research does not involve any human subject, only ED
admission data from January 2014 to August 2017 retrieved from the UnityPoint
Health database. Autoregressive integrated moving average (ARIMA), Holt
Winters, TBATS, and neural network methods were implemented to forecast hourly
ED patient arrival. Findings: ARIMA (3,0,0) (2,1,0) was selected as the best
fit model with minimum Akaike information criterion and Schwartz Bayesian
criterion. The model was stationary and qualified the Box Ljung correlation
test and the Jarque Bera test for normality. The mean error (ME) and root mean
square error (RMSE) were selected as performance measures. An ME of 1.001 and
an RMSE of 1.55 was obtained. Conclusions: ARIMA can be used to provide hourly
forecasts for ED arrivals and can be utilized as a decision support system in
the healthcare industry. Application: This technique can be implemented in
hospitals worldwide to predict ED patient arrival.
"
1901.03696,2019-01-14,Key Factor Not to Drop Out is to Attend the Lecture,"  In addition to the learning check testing results performed at each lectures,
we have extended the factors to find the key dropping out factors. Among them
are, the number of successes in the learning check testing, the number of
attendances to the follow-up program classes, and etc. Then, we have found key
factors strongly related to the students at risk. They are the following. 1)
Badly failed students (score range is 0-39 in the final examination) tend to be
absent for the regular classes and fail in the learning check testing even if
they attended, and they are very reluctant to attend the follow-up program
classes. 2) Successful students (score range is 60-100 in the final
examination) attend classes and get good scores in every learning check
testing. 3) Failed students but not so badly (score range is 40-59 in the final
examination) reveal both sides of features appeared in score range of 0-39 and
score range of 60-100. Therefore, it is crucial to attend the lectures in order
not to drop out. Students who failed in learning check testing more than half
out of all testing times almost absolutely failed in the final examination,
which could cause the drop out. Also, students who were successful to learning
check testing more than two third out of all testing times took better score in
the final examination.
"
1901.03701,2019-01-15,Robust Adaptive Control Charts,"  In statistical process control, procedures are applied that require
relatively strict conditions for their use. If such assumptions are violated,
these methods become inefficient, leading to increased incidence of false
signals. Therefore, a robust version of control charts is sought to be less
sensitive with respect to a breach of normality and independence in
measurements. Robust control charts, however, usually increase the delay in the
detection of assignable causes. This negative effect can, to some extent, be
removed with the aid of an adaptive approach.
"
1901.03876,2019-01-15,Four Fundamental Questions in Probability Theory and Statistics,"  This study has the purpose of addressing four questions that lie at the base
of the probability theory and statistics, and includes two main steps. As
first, we conduct the textual analysis of the most significant works written by
eminent probability theorists. The textual analysis turns out to be a rather
innovative method of study in this domain, and shows how the sampled writers,
no matter he is a frequentist or a subjectivist, share a similar approach. Each
author argues on the multifold aspects of probability then he establishes the
mathematical theory on the basis of his intellectual conclusions. It may be
said that mathematics ranks second. Hilbert foresees an approach far different
from that used by the sampled authors. He proposes to axiomatize the
probability calculus notably to describe the probability concepts using purely
mathematical criteria. In the second stage of the present research we address
the four issues of the probability theory and statistics following the
recommendations of Hilbert. Specifically, we use two theorems that prove how
the frequentist and the subjectivist models are not incompatible as many
believe. Probability has distinct meanings under different hypotheses, and in
turn classical statistics and Bayesian statistics are available for adoption in
different circumstances. Subsequently, these results are commented upon,
followed by our conclusions
"
1901.04689,2019-01-29,Systemic Risk: Conditional Distortion Risk Measures,"  In this paper, we introduce the rich classes of conditional distortion (CoD)
risk measures and distortion risk contribution ($\Delta$CoD) measures as
measures of systemic risk and analyze their properties and representations. The
classes include the well-known conditional Value-at-Risk, conditional Expected
Shortfall, and risk contribution measures in terms of the VaR and ES as special
cases. Sufficient conditions are presented for two random vectors to be ordered
by the proposed CoD-risk measures and distortion risk contribution measures.
These conditions are expressed using the conventional stochastic dominance,
increasing convex/concave, dispersive, and excess wealth orders of the
marginals and canonical positive/negative stochastic dependence notions.
Numerical examples are provided to illustrate our theoretical findings. This
paper is the second in a triplet of papers on systemic risk by the same
authors. In \cite{DLZorder2018a}, we introduce and analyze some new stochastic
orders related to systemic risk. In a third (forthcoming) paper, we attribute
systemic risk to the different participants in a given risky environment.
"
1901.04824,2019-01-16,Approaching Ethical Guidelines for Data Scientists,"  The goal of this article is to inspire data scientists to participate in the
debate on the impact that their professional work has on society, and to become
active in public debates on the digital world as data science professionals.
How do ethical principles (e.g., fairness, justice, beneficence, and
non-maleficence) relate to our professional lives? What lies in our
responsibility as professionals by our expertise in the field? More
specifically this article makes an appeal to statisticians to join that debate,
and to be part of the community that establishes data science as a proper
profession in the sense of Airaksinen, a philosopher working on professional
ethics. As we will argue, data science has one of its roots in statistics and
extends beyond it. To shape the future of statistics, and to take
responsibility for the statistical contributions to data science, statisticians
should actively engage in the discussions. First the term data science is
defined, and the technical changes that have led to a strong influence of data
science on society are outlined. Next the systematic approach from CNIL is
introduced. Prominent examples are given for ethical issues arising from the
work of data scientists. Further we provide reasons why data scientists should
engage in shaping morality around and to formulate codes of conduct and codes
of practice for data science. Next we present established ethical guidelines
for the related fields of statistics and computing machinery. Thereafter
necessary steps in the community to develop professional ethics for data
science are described. Finally we give our starting statement for the debate:
Data science is in the focal point of current societal development. Without
becoming a profession with professional ethics, data science will fail in
building trust in its interaction with and its much needed contributions to
society!
"
1901.06708,2020-10-13,Fitting A Mixture Distribution to Data: Tutorial,"  This paper is a step-by-step tutorial for fitting a mixture distribution to
data. It merely assumes the reader has the background of calculus and linear
algebra. Other required background is briefly reviewed before explaining the
main algorithm. In explaining the main algorithm, first, fitting a mixture of
two distributions is detailed and examples of fitting two Gaussians and
Poissons, respectively for continuous and discrete cases, are introduced.
Thereafter, fitting several distributions in general case is explained and
examples of several Gaussians (Gaussian Mixture Model) and Poissons are again
provided. Model-based clustering, as one of the applications of mixture
distributions, is also introduced. Numerical simulations are also provided for
both Gaussian and Poisson examples for the sake of better clarification.
"
1901.08589,2021-04-09,Organic fiducial inference,"  A substantial generalisation is put forward of the theory of subjective
fiducial inference as it was outlined in earlier papers. In particular, this
theory is extended to deal with cases where the data are discrete or
categorical rather than continuous, and cases where there was important
pre-data knowledge about some or all of the model parameters. The system for
directly expressing and then handling this pre-data knowledge, which is via
what are referred to as global and local pre-data functions for the parameters
concerned, is distinct from that which involves attempting to directly
represent this knowledge in the form of a prior distribution function over
these parameters, and then using Bayes' theorem. In this regard, the individual
attributes of what are identified as three separate types of fiducial argument,
namely the strong, moderate and weak fiducial arguments, form an integral part
of the theory that is developed. Various practical examples of the application
of this theory are presented, including examples involving binomial, Poisson
and multinomial data. The fiducial distribution functions for the parameters of
the models in these examples are interpreted in terms of a generalised
definition of subjective probability that was set out previously.
"
1901.09686,2019-01-29,"Variability in the interpretation of Dutch probability phrases - a risk
  for miscommunication","  Verbal probability phrases are often used to express estimated risk. In this
study, focus was on the numerical interpretation of 29 Dutch probability and
frequency phrases, including several complementary phrases to test (a)symmetry
in their interpretation. Many of these phrases had not been studied before. The
phrases were presented in the context of ordinary situations. The survey was
distributed among both statisticians and non-statisticians with Dutch as their
native language.
  The responses from 881 participants showed a large variability in the
interpretation of Dutch phrases, and the neutral contexts seemed to have no
structural influence. Furthermore, the results demonstrated an asymmetry in the
interpretation of Dutch complementary phrases. The large variability of
interpretations was found among both statisticians and non-statisticians, and
among males and females, however, no structural differences were found between
the groups.
  Concluding, there is a large variability in the interpretation of verbal
probability phrases, even within sub-populations. Therefore, verbal probability
expressions may be a risk for miscommunication.
"
1901.09779,2019-01-29,"Shannon's entropy and its Generalizations towards Statistics,
  Reliability and Information Science during 1948-2018","  Starting from the pioneering works of Shannon and Weiner in 1948, a plethora
of works have been reported on entropy in different directions. Entropy-related
review work in the direction of statistics, reliability and information
science, to the best of our knowledge, has not been reported so far. Here we
have tried to collect all possible works in this direction during the period
1948-2018 so that people interested in entropy, specially the new researchers,
get benefited.
"
1901.10875,2019-10-24,STAR: Statistical Tests with Auditable Results,"  We present STAR: a novel system aimed at solving the complex issue of
""p-hacking"" and false discoveries in scientific studies. STAR provides a
concrete way for ensuring the application of false discovery control procedures
in hypothesis testing, using mathematically provable guarantees, with the goal
of reducing the risk of data dredging. STAR generates an efficiently auditable
certificate which attests to the validity of each statistical test performed on
a dataset. STAR achieves this by using several cryptographic techniques which
are combined specifically for this purpose. Under-the-hood, STAR uses a
decentralized set of authorities (e.g., research institutions), secure
computation techniques, and an append-only ledger which together enable
auditing of scientific claims by 3rd parties and matches real world trust
assumptions. We implement and evaluate a construction of STAR using the
Microsoft SEAL encryption library and SPDZ multi-party computation protocol.
Our experimental evaluation demonstrates the practicality of STAR in multiple
real world scenarios as a system for certifying scientific discoveries in a
tamper-proof way.
"
1902.00002,2019-02-04,"Uncertainty Quantification in Molecular Signals using Polynomial Chaos
  Expansion","  Molecular signals are abundant in engineering and biological contexts, and
undergo stochastic propagation in fluid dynamic channels. The received signal
is sensitive to a variety of input and channel parameter variations. Currently
we do not understand how uncertainty or noise in a variety of parameters affect
the received signal concentration, and nor do we have an analytical framework
to tackle this challenge. In this paper, we utilize Polynomial Chaos Expansion
(PCE) to show to uncertainty in parameters propagates to uncertainty in the
received signal. In demonstrating its applicability, we consider a Turbulent
Diffusion Molecular Communication (TDMC) channel and highlight which parameters
affect the received signals. This can pave the way for future information
theoretic insights, as well as guide experimental design.
"
1902.01918,2019-02-07,"CMS Sematrix: A Tool to Aid the Development of Clinical Quality Measures
  (CQMs)","  As part of the effort to improve quality and to reduce national healthcare
costs, the Centers for Medicare and Medicaid Services (CMS) are responsible for
creating and maintaining an array of clinical quality measures (CQMs) for
assessing healthcare structure, process, outcome, and patient experience across
various conditions, clinical specialties, and settings. The development and
maintenance of CQMs involves substantial and ongoing evaluation of the evidence
on the measure's properties: importance, reliability, validity, feasibility,
and usability. As such, CMS conducts monthly environmental scans of the
published clinical and health service literature. Conducting time consuming,
exhaustive evaluations of the ever-changing healthcare literature presents one
of the largest challenges to an evidence-based approach to healthcare quality
improvement. Thus, it is imperative to leverage automated techniques to aid CMS
in the identification of clinical and health services literature relevant to
CQMs. Additionally, the estimated labor hours and related cost savings of using
CMS Sematrix compared to a traditional literature review are roughly 818 hours
and 122,000 dollars for a single monthly environmental scan.
"
1902.02579,2019-02-08,Characterization of Sine- Skewed von Mises Distribution,"  The von Mises distribution is one of the most important distribution in
statistics to deal with circular data. In this paper we will consider some
basic properties and characterizations of the sine skewed von Mises
distribution.
"
1902.03132,2019-02-11,Learning spatially-correlated temporal dictionaries for calcium imaging,"  Calcium imaging has become a fundamental neural imaging technique, aiming to
recover the individual activity of hundreds of neurons in a cortical region.
Current methods (mostly matrix factorization) are aimed at detecting neurons in
the field-of-view and then inferring the corresponding time-traces. In this
paper, we reverse the modeling and instead aim to minimize the spatial
inference, while focusing on finding the set of temporal traces present in the
data. We reframe the problem in a dictionary learning setting, where the
dictionary contains the time-traces and the sparse coefficient are spatial
maps. We adapt dictionary learning to calcium imaging by introducing
constraints on the norms and correlations of the time-traces, and incorporating
a hierarchical spatial filtering model that correlates the time-trace usage
over the field-of-view. We demonstrate on synthetic and real data that our
solution has advantages regarding initialization, implicitly inferring number
of neurons and simultaneously detecting different neuronal types.
"
1902.04496,2019-02-13,Optimal BIBD-extended designs,"  Balanced incomplete block designs (BIBDs) are a class of designs with v
treatments and b blocks of size k that are optimal with regards to a wide range
of optimality criteria, but it is not clear which designs to choose for
combinations of v, b and k when BIBDs do not exist.
  In 1992, Cheng showed that for sufficiently large b, the designs which are
optimal with respect to commonly used criteria (including the A- and D-
criteria) must be found among (M.S)-optimal designs. In particular, this result
confirmed the conjecture of John and Mitchell in 1977 on the optimality of
regular graph designs (RGDs) in the case of large numbers of blocks.
  We investigate the effect of extending known optimal binary designs by
repeatedly adding the blocks of a BIBD and find boundaries for the number of
block so that these BIBD-extended designs are optimal. In particular, we will
study the designs for k=2 and b=v-1 and b=v: in these cases the A- and
D-optimal designs are not the same but we show that this changes after adding
blocks of a BIBD and the same design becomes A- and D-optimal amongst the
collection of extended designs. Finally, we characterise those RGDs that give
rise to A- and D-optimal extended designs and extend a result on the
D-optimality of the a group-divisible design to A- and D-optimality amongst
BIBD-extended designs.
"
1902.06175,2019-09-05,Optimal Stopping and Utility in a Simple Model of Unemployment Insurance,"  Managing unemployment is one of the key issues in social policies.
Unemployment insurance schemes are designed to cushion the financial and morale
blow of loss of job but also to encourage the unemployed to seek new jobs more
pro-actively due to the continuous reduction of benefit payments. In the
present paper, a simple model of unemployment insurance is proposed with a
focus on optimality of the individual's entry to the scheme. The corresponding
optimal stopping problem is solved, and its similarity and differences with the
perpetual American call option are discussed. Beyond a purely financial point
of view, we argue that in the actuarial context the optimal decisions should
take into account other possible preferences through a suitable utility
function. Some examples in this direction are worked out.
"
1902.07091,2020-10-26,A Combinatorial Solution to Causal Compatibility,"  Within the field of causal inference, it is desirable to learn the structure
of causal relationships holding between a system of variables from the
correlations that these variables exhibit; a sub-problem of which is to certify
whether or not a given causal hypothesis is compatible with the observed
correlations. A particularly challenging setting for assessing causal
compatibility is in the presence of partial information; i.e. when some of the
variables are hidden/latent. This paper introduces the possible worlds
framework as a method for deciding causal compatibility in this difficult
setting. We define a graphical object called a possible worlds diagram, which
compactly depicts the set of all possible observations. From this construction,
we demonstrate explicitly, using several examples, how to prove causal
incompatibility. In fact, we use these constructions to prove causal
incompatibility where no other techniques have been able to. Moreover, we prove
that the possible worlds framework can be adapted to provide a complete
solution to the possibilistic causal compatibility problem. Even more, we also
discuss how to exploit graphical symmetries and cross-world consistency
constraints in order to implement a hierarchy of necessary compatibility tests
that we prove converges to sufficiency.
"
1902.09501,2019-02-26,"Applications of band-limited extrapolation to forecasting of weather and
  financial time series","  This paper describes the practical application of causal extrapolation of
sequences for the purpose of forecasting. The methods and proofs have been
applied to simulations to measure the range which data can be accurately
extrapolated. Real world data from the Australian Stock exchange and the
Australian Bureau of Meteorology have been tested and compared with simple
linear extrapolation of the same data. In a majority of the tested scenarios
casual extrapolation has been proved to be the more effective forecaster.
"
1902.09790,2019-02-27,A note on Fibonacci Sequences of Random Variables,"  The focus of this paper is the random sequences in the form $\{X_{0},X_{1},$
$X_{n}=X_{n-2}+X_{n-1},n=2,3,..\dot{\}},$ referred to as Fibonacci Random
Sequence (FRS). The initial random variables $X_{0}$ and $X_{1}$ are assumed to
be absolutely continuous with joint probability density function (pdf)
$f_{X_{0},X_{1}}.$ The FRS is completely determined by $X_{0}$ and $X_{1}$ and
the members of Fibonacci sequence $\digamma
\equiv\{0,1,1,2,3,5,8,13,21,34,55,89,144,...\}.$ We examine the distributional
and limit properties of the random sequence $X_{n},n=0,1,2,...$ .
"
1903.00049,2019-03-04,Bounds on Bayes Factors for Binomial A/B Testing,"  Bayes factors, in many cases, have been proven to bridge the classic -value
based significance testing and bayesian analysis of posterior odds. This paper
discusses this phenomena within the binomial A/B testing setup (applicable for
example to conversion testing). It is shown that the bayes factor is controlled
by the \emph{Jensen-Shannon divergence} of success ratios in two tested groups,
which can be further bounded by the Welch statistic. As a result, bayesian
sample bounds almost match frequentionist's sample bounds. The link between
Jensen-Shannon divergence and Welch's test as well as the derivation are an
elegant application of tools from information geometry.
"
1903.01706,2019-03-12,Tutorial: Deriving The Efficient Influence Curve for Large Models,"  This paper aims to provide a tutorial for upper level undergraduate and
graduate students in statistics, biostatistics and epidemiology on deriving
influence functions for non-parametric and semi-parametric models. The author
will build on previously known efficiency theory and provide a useful identity
and formulaic technique only relying on the basics of integration which, are
self-contained in this tutorial and can be used in most any setting one might
encounter in practice. The paper provides many examples of such derivations for
well-known influence functions as well as for new parameters of interest. The
influence function remains a central object for constructing efficient
estimators for large models, such as the one-step estimator and the targeted
maximum likelihood estimator. We will not touch upon these estimators at all
but readers familiar with these estimators might find this tutorial of
particular use.
"
1903.01829,2019-03-06,Comparison of plotting system outputs in beginner analysts,"  The R programming language is built on an ecosystem of packages, some that
allow analysts to accomplish the same tasks. For example, there are at least
two clear workflows for creating data visualizations in R: using the base
graphics package (referred to as ""base R"") and the ggplot2 add-on package based
on the grammar of graphics. Here we perform an empirical study of the quality
of scientific graphics produced by beginning R users. In our experiment,
learners taking a data science course on the Coursera platform were randomized
to complete identical plotting exercises in either the base R or the ggplot2
system. Learners were then asked to evaluate their peers in terms of visual
characteristics key to scientific cognition. We observed that graphics created
with the two systems rated similarly on many characteristics. However, ggplot2
graphics were generally judged to be more visually pleasing and, in the case of
faceted scientific plots, easier to understand. Our results suggest that while
both graphic systems are useful in the hands of beginning users, ggplot2's
natural faceting system may be easier to use by beginning users for displaying
more complex relationships.
"
1903.01868,2019-03-06,The Fuzzy ROC,"  The fuzzy ROC extends Receiver Operating Curve (ROC) visualization to the
situation where some data points, falling in an indeterminacy region, are not
classified. It addresses two challenges: definition of sensitivity and
specificity bounds under indeterminacy; and visual summarization of the large
number of possibilities arising from different choices of indeterminacy zones.
"
1903.05514,2019-09-04,Effects of Stochastic Parametrization on Extreme Value Statistics,"  Extreme geophysical events are of crucial relevance to our daily life: they
threaten human lives and cause property damage. To assess the risk and reduce
losses, we need to model and probabilistically predict these events.
Parametrizations are computational tools used in Earth system models, which are
aimed at reproducing the impact of unresolved scales on resolved scales. The
performance of parametrizations has usually been examined on typical events
rather than on extreme events. In this paper we consider a modified version of
the two-level Lorenz'96 model and investigate how two parametrizations of the
fast degrees of freedom perform in terms of the representation of extreme
events. One parametrization is constructed following Wilks (2005) and is
constructed through an empirical fitting procedure; the other parametrization
is constructed through the statistical mechanical approach proposed by Wouters
and Lucarini (2012, 2013). The two strategies show different advantages and
disadvantages. We discover that the agreement between parametrized models and
true model is in general worse when looking at extremes rather than at the bulk
of the statistics. The results suggest that stochastic parametrizations should
be accurately and specifically tested against their performance on extreme
events, as usual optimization procedures might neglect them.
"
1903.06568,2019-09-20,"A response-matrix-centred approach to presenting cross-section
  measurements","  The current canonical approach to publishing cross-section data is to unfold
the reconstructed distributions. Detector effects like efficiency and smearing
are undone mathematically, yielding distributions in true event properties.
This is an ill-posed problem, as even small statistical variations in the
reconstructed data can lead to large changes in the unfolded spectra.
  This work presents an alternative or complementary approach: the
response-matrix-centred forward-folding approach. It offers a convenient way to
forward-fold model expectations in truth space to reconstructed quantities.
These can then be compared to the data directly, similar to what is usually
done with full detector simulations within the experimental collaborations. For
this, the detector response (efficiency and smearing) is parametrised as a
matrix. The effects of the detector on the measurement of a given model is
simulated by simply multiplying the binned truth expectation values by this
response matrix.
  Systematic uncertainties in the detector response are handled by providing a
set of matrices according to the prior distribution of the detector properties
and marginalising over them. Background events can be included in the
likelihood calculation by giving background events their own bins in truth
space.
  To facilitate a straight-forward use of response matrices, a new software
framework has been developed: the Response Matrix Utilities (ReMU). ReMU is a
Python package distributed via the Python Package Index. It only uses widely
available, standard scientific Python libraries and does not depend on any
custom experiment-specific software. It offers all methods needed to build
response matrices from Monte Carlo data sets, use the response matrix to
forward-fold truth-level model predictions, and compare the predictions to real
data using Bayesian or frequentist statistical inference.
"
1903.06625,2019-03-18,Synthesis of High-Resolution Load Profiles with Minimal Data,"  For the estimation of a new energy supply system it is an important to have
high-resolution energy load profile. Such a profile is in general either not
present or very costly to obtain. We will therefore present a method which
synthesizes load profiles from minimal given data, but with maximal resolution.
The general initial data setting includes month integrals and load profiles a
few days. The resulting time series features all important properties to
represent a real energy profile.
"
1903.08880,2019-04-17,"Three issues impeding communication of statistical methodology for
  incomplete data","  We identify three issues permeating the literature on statistical methodology
for incomplete data written for non-specialist statisticians and other
investigators. The first is a mathematical defect in the notation Yobs, Ymis
used to partition the data into observed and missing components. The second are
issues concerning the notation `P(R|Yobs, Ymis)=P(R|Yobs)' used for
communicating the definition of missing at random (MAR). And the third is the
framing of ignorability by emulating complete-data methods exactly, rather than
treating the question of ignorability on its own merits. These issues have been
present in the literature for a long time, and have simple remedies. The
purpose of this paper is to raise awareness of these issues, and to explain how
they can be remedied.
"
1903.10816,2019-04-10,Deterministic bootstrapping for a class of bootstrap methods,"  An algorithm is described that enables efficient deterministic approximate
computation of the bootstrap distribution for any linear bootstrap method
$T_n^*$, alleviating the need for repeated resampling from observations (resp.
input-derived data). In essence, the algorithm computes the distribution
function from a linear mixture of independent random variables each having a
finite discrete distribution. The algorithm is applicable to elementary
bootstrap scenarios (targetting the mean as parameter of interest), for block
bootstrap, as well as for certain residual bootstrap scenarios. Moreover, the
algorithm promises a much broader applicability, in non-bootstrapped hypothesis
testing.
"
1903.11565,2019-03-28,"Modeling the Health Expenditure in Japan, 2011. A Healthy Life Years
  Lost Methodology","  The Healthy Life Years Lost Methodology (HLYL) is introduced to model and
estimate the Health Expenditure in Japan in 2011. The HLYL theory and
estimation methods are presented in our books in the Springer Series on
Demographic Methods and Population Analysis vol. 45 and 46 titled: Exploring
the Health State of a Population by Dynamic Modeling Methods and Demography and
Health Issues: Population Aging, Mortality and Data Analysis. Special
applications appear in Chapters of these books as in The Health-Mortality
Approach in Estimating the Healthy Life Years Lost Compared to the Global
Burden of Disease Studies and Applications in World, USA and Japan and in
Estimation of the Healthy Life Expectancy in Italy Through a Simple Model Based
on Mortality Rate by Skiadas and Arezzo. Here further to present the main part
of the methodology with more details and illustrations, we develop and extend a
life table important to estimate the healthy life years lost along with the
fitting to the health expenditure in the related case. The application results
are quite promising and important to support decision makers and health
agencies with a powerful tool to improve the health expenditure allocation and
the future predictions.
"
1903.12423,2020-01-09,"An innovating Statistical Learning Tool based on Partial Differential
  Equations, intending livestock Data Assimilation","  The realistic modeling intended to quantify precisely some biological
mechanisms is a task requiering a lot of a priori knowledge and generally
leading to heavy mathematical models. On the other hand, the structure of the
classical Machine Learning algorithms, such as Neural Networks, limits their
flexibility and the possibility to take into account the existence of complex
underlying phenomena, such as delay, saturation and accumulation. The aim of
this paper is to reach a compromise between precision, parsimony and
flexibility to design an efficient biomimetic predictive tool extracting
knowledge from livestock data. To achieve this, we build a Mathematical Model
based on Partial Differential Equations (PDE) embarking the mathematical
expression of biological determinants. We made the hypothesis that all the
physico-chemical phenomena occurring in animal body can be summarized by the
evolution of a global information. Therefore the developed PDE system describes
the evolution and the action of an information circulating in an Avatar of the
Real Animal. This Avatar outlines the dynamics of the biological reactions of
animal body in the framework of a specific problem. Each PDE contains
parameters corresponding to biological-like factors which can be learnt from
data by the developed Statistical Learning Tool.
"
1904.01491,2019-04-03,Statistical testing in a Linear Probability Space,"  Imagine that you could calculate of posttest probabilities, i.e. Bayes
theorem with simple addition. This is possible if we stop thinking of
probabilities as ranging from 0 to 1.0. There is a naturally occurring linear
probability space when data are transformed into the logarithm of the odds
ratio (log10 odds). In this space, probabilities are replaced by W (Weight)
where W=log10(probability/(1-probability)). I would like to argue the multiple
benefits of performing statistical testing in a linear probability space: 1)
Statistical testing is accurate in linear probability space but not in other
spaces. 2) Effect size is called Impact (I) and is the difference in means
between two treatments (I=Wmean2-Wmean1). 3) Bayes theorem is simply
Wposttest=Wpretest+Itest. 4) Significance (p value) is replaced by Certainty
(C) which is the W of the p value. Methods to transform data into and out of
linear probability space are described.
"
1904.02107,2022-06-08,Data-driven discovery of coordinates and governing equations,"  The discovery of governing equations from scientific data has the potential
to transform data-rich fields that lack well-characterized quantitative
descriptions. Advances in sparse regression are currently enabling the
tractable identification of both the structure and parameters of a nonlinear
dynamical system from data. The resulting models have the fewest terms
necessary to describe the dynamics, balancing model complexity with descriptive
ability, and thus promoting interpretability and generalizability. This
provides an algorithmic approach to Occam's razor for model discovery. However,
this approach fundamentally relies on an effective coordinate system in which
the dynamics have a simple representation. In this work, we design a custom
autoencoder to discover a coordinate transformation into a reduced space where
the dynamics may be sparsely represented. Thus, we simultaneously learn the
governing equations and the associated coordinate system. We demonstrate this
approach on several example high-dimensional dynamical systems with
low-dimensional behavior. The resulting modeling framework combines the
strengths of deep neural networks for flexible representation and sparse
identification of nonlinear dynamics (SINDy) for parsimonious models. It is the
first method of its kind to place the discovery of coordinates and models on an
equal footing.
"
1904.02961,2019-04-08,"Analytic Evaluation of the Fractional Moments for the Quasi-Stationary
  Distribution of the Shiryaev Martingale on an Interval","  We consider the quasi-stationary distribution of the classical Shiryaev
diffusion restricted to the interval $[0,A]$ with absorption at a fixed $A>0$.
We derive analytically a closed-form formula for the distribution's fractional
moment of an {\em arbitrary} given order $s\in\mathbb{R}$; the formula is
consistent with that previously found by Polunchenko and Pepelyshev (2018) for
the case of $s\in\mathbb{N}$. We also show by virtue of the formula that, if
$s<1$, then the $s$-th fractional moment of the quasi-stationary distribution
becomes that of the exponential distribution (with mean $1/2$) in the limit as
$A\to+\infty$; the limiting exponential distribution is the stationary
distribution of the reciprocal of the Shiryaev diffusion.
"
1904.04330,2019-04-10,"The Contribution Plot: Decomposition and Graphical Display of the RV
  Coefficient, with Application to Genetic and Brain Imaging Biomarkers of
  Alzheimer's Disease","  Alzheimer's disease (AD) is a chronic neurodegenerative disease that causes
memory loss and decline in cognitive abilities. AD is the sixth leading cause
of death in the United States, affecting an estimated 5 million Americans. To
assess the association between multiple genetic variants and multiple
measurements of structural changes in the brain a recent study of AD used a
multivariate measure of linear dependence, the RV coefficient. The authors
decomposed the RV coefficient into contributions from individual variants and
displayed these contributions graphically. We investigate the properties of
such a `contribution plot' in terms of an underlying linear model, and discuss
estimation of the components of the plot when the correlation signal may be
sparse. The contribution plot is applied to simulated data and to genomic and
brain imaging data from the Alzheimer's Disease Neuroimaging Initiative.
"
1904.05329,2019-10-25,GraSPy: Graph Statistics in Python,"  We introduce GraSPy, a Python library devoted to statistical inference,
machine learning, and visualization of random graphs and graph populations.
This package provides flexible and easy-to-use algorithms for analyzing and
understanding graphs with a scikit-learn compliant API. GraSPy can be
downloaded from Python Package Index (PyPi), and is released under the Apache
2.0 open-source license. The documentation and all releases are available at
https://neurodata.io/graspy.
"
1904.05662,2019-07-02,"Statistical witchhunts: Science, justice & the p-value crisis","  We provide accessible insight into the current 'replication crisis' in
'statistical science', by revisiting the old metaphor of 'court trial as
hypothesis test'. Inter alia, we define and diagnose harmful statistical
witch-hunting both in justice and science, which extends to the replication
crisis itself, where a hunt on p-values is currently underway.
"
1904.08730,2019-04-19,"Some ordering properties of highest and lowest order statistics with
  exponentiated Gumble type-II distributed components","  In this paper, we have studied the stochastic comparisons of the highest and
lowest order statistics of exponentiated Gumble type-II distribution with three
parameters. We have compared both the statistics by using three different
stochastic ordering. First, we consider a system with different scale and outer
shape parameters and then we study the usual stochastic ordering of the lowest
and highest order statistics in the sense of multivariate chain majorization.
In addition, we construct two examples to support our results. Second, by using
the vector majorization technique, we study the usual stochastic ordering, the
reversed failure rate ordering and the likelihood ratio ordering with respect
to different outer shape parameters, next, by varying the inner shape
parameter, we discuss the usual stochastic order of the lowest order statistics
and we have shown that the highest order statistics are not comparable in the
usual stochastic ordering by an example.
"
1904.10118,2019-04-26,"Amazon Forest Fires Between 2001 and 2006 and Birth Weight in Porto
  Velho","  Birth weight data (22,012 live-births) from a public hospital in Porto Velho
(Amazon) was used in multiple statistical models to assess the effects of
forest-fire smoke on human reproductive outcome. Mean birth weights for girls
(3,139 g) and boys (3,393 g) were considered statistically different (p-value <
2.2e-16). Among all models analyzed, the means were considered statistically
different only when treated as a function of month and year (p-value = 0.0989,
girls and 0.0079, boys) . The R 2 statistics indicate that the regression
models considered are able to explain 65 % (girls) and 54 % (boys) of the
variation of the mean birth weight.
"
1904.10172,2020-07-10,"ssMousetrack: Analysing computerized tracking data via Bayesian
  state-space models in {R}","  Recent technological advances have provided new settings to enhance
individual-based data collection and computerized-tracking data have became
common in many behavioral and social research. By adopting instantaneous
tracking devices such as computer-mouse, wii, and joysticks, such data provide
new insights for analysing the dynamic unfolding of response process.
ssMousetrack is a R package for modeling and analysing computerized-tracking
data by means of a Bayesian state-space approach. The package provides a set of
functions to prepare data, fit the model, and assess results via simple
diagnostic checks. This paper describes the package and illustrates how it can
be used to model and analyse computerized-tracking data. A case study is also
included to show the use of the package in empirical case studies.
"
1904.10406,2021-01-22,Exponential Random Graph models for Little Networks,"  Statistical models for social networks have enabled researchers to study
complex social phenomena that give rise to observed patterns of relationships
among social actors and to gain a rich understanding of the interdependent
nature of social ties and actors. Much of this research has focused on social
networks within medium to large social groups. To date, these advances in
statistical models for social networks, and in particular, of
Exponential-Family Random Graph Models (ERGMS), have rarely been applied to the
study of small networks, despite small network data in teams, families, and
personal networks being common in many fields. In this paper, we revisit the
estimation of ERGMs for small networks and propose using exhaustive enumeration
when possible. We developed an R package that implements the estimation of
pooled ERGMs for small networks using Maximum Likelihood Estimation (MLE),
called ""ergmito"". Based on the results of an extensive simulation study to
assess the properties of the MLE estimator, we conclude that there are several
benefits of direct MLE estimation compared to approximate methods and that this
creates opportunities for valuable methodological innovations that can be
applied to modeling social networks with ERGMs.
"
1904.11006,2019-04-26,"Introducing Bayesian Analysis with $\text{m&m's}^\circledR$: an
  active-learning exercise for undergraduates","  We present an active-learning strategy for undergraduates that applies
Bayesian analysis to candy-covered chocolate $\text{m&m's}^\circledR$. The
exercise is best suited for small class sizes and tutorial settings, after
students have been introduced to the concepts of Bayesian statistics. The
exercise takes advantage of the non-uniform distribution of
$\text{m&m's}^\circledR~$ colours, and the difference in distributions made at
two different factories. In this paper, we provide the intended learning
outcomes, lesson plan and step-by-step guide for instruction, and open-source
teaching materials. We also suggest an extension to the exercise for the
graduate-level, which incorporates hierarchical Bayesian analysis.
"
1904.11242,2019-04-26,"Governance on Social Media Data: Different Focuses between Government
  and Internet Company","  How governments and Internet companies regulate user data on social media
attracts public attention. This study tried to answer two questions: What kind
of countries send more requests for Facebook user data? What kind of countries
get more requests replies from Facebook? We aim to figure out how a country's
economic, political and social factors affect its government requests for user
data and Facebook's responses rate to those requests. Results show that
countries with higher GDP per capita, a higher level of human freedom and a
lower level of rule of law send more requests for user data; while Facebook
tends to reply to government requests from countries with a higher level of
human freedom and a lower level of political stability. In conclusion,
governments and Facebook show different focuses on governance on social media
data.
"
1904.11752,2019-04-29,"Rushing or Dragging? An Analysis of the ""Universality"" of Correlated
  Fluctuations in Hi-Hat Timing and Dynamics","  A previous analysis of fluctuations in a virtuoso (Jeff Porcaro) drum
performance [R\""as\""anen et al., PLoS ONE 10(6): e0127902 (2015)] demonstrated
that the rhythmic signal comprised both long range correlations and short range
anti-correlations, with a characteristic timescale distinguishing the two
regimes. We have extended R\""as\""anen et al.'s approach to a much larger number
of drum samples (N=132, provided by a total of 58 participants) and to a
different performance (viz., Rush's Tom Sawyer). A key focus of our study was
to test whether the fluctuation dynamics discovered by R\""as\""anen et al. are
""universal"" in the following sense: is the crossover from short-range to
long-range correlated fluctuations a general phenomenon or is it restricted to
particular drum patterns and/or specific drummers? We find no compelling
evidence to suggest that the short-range to long-range correlation crossover
that is characteristic of Porcaro's performance is a common feature of temporal
fluctuations in drum patterns. Moreover, level of experience and/or playing
technique surprisingly do not play a role in influencing a short-range to
long-range correlation cross-over. Our study also highlights that a great deal
of caution needs to be taken when using the detrended fluctuation analysis
technique, particularly with regard to anti-correlated signals.
"
1904.11907,2019-04-29,Evaluating the Success of a Data Analysis,"  A fundamental problem in the practice and teaching of data science is how to
evaluate the quality of a given data analysis, which is different than the
evaluation of the science or question underlying the data analysis. Previously,
we defined a set of principles for describing data analyses that can be used to
create a data analysis and to characterize the variation between data analyses.
Here, we introduce a metric of quality evaluation that we call the success of a
data analysis, which is different than other potential metrics such as
completeness, validity, or honesty. We define a successful data analysis as the
matching of principles between the analyst and the audience on which the
analysis is developed. In this paper, we propose a statistical model and
general framework for evaluating the success of a data analysis. We argue that
this framework can be used as a guide for practicing data scientists and
students in data science courses for how to build a successful data analysis.
"
1904.12190,2019-04-30,"Geological modeling using a recursive convolutional neural networks
  approach","  Resource models are constrained by the extent of geological units that often
depend on the lithology, alteration and mineralization. A three dimensional
model of these geological units must be built from scarce information coming
from drillholes and limited understanding about the geological setting in which
the ore deposit is places. In this work, we present a new technique for
multiple-point geostatistical simulation based on a recursive convolutional
neural network approach (RCNN). The method requires conditioning data and a
training image that depicts the type of geological structures expected to be
found in the deposit. This training image is used to learn the patterns of
categories found and these are imposed in the final simulated model conditioned
by the categories found during logging at the actual drill-hole samples. A
lithological modeling process is carried out in a copper deposit in Chile to
demonstrate the method. Comparison with current techniques and spatial metrics
are used to clarify concepts and RCNN properties. Also, strengths and
weaknesses of the methodology are discussed by briefly reviewing the
theoretical perspective and looking into some of its practical aspects.
"
1904.13207,2019-05-01,"Methods of Estimation for the Three-Parameter Reflected Weibull
  Distribution","  In this paper, we propose methods for the estimation of parameters for the
three-parameter Reflected Weibull distribution. The Moment estimator , Maximum
likelihood estimator and Location and Scale Parameters free maximum likelihood
estimator. The Location and Scale Parameters free maximum likelihood estimator
is based on a data transformation, which avoids the problem of unbounded
likelihood estimator. Through Mont Carlo simulations, we further show that the
Location and Scale Parameters free maximum likelihood estimator performs better
than methods moment and maximum likelihood estimator in terms of bias and root
mean squared error. Finally, two examples based on real data sets are presented
to illustrate methods.
"
1904.13365,2019-05-01,"Fault Diagnosis using Clustering. What Statistical Test to use for
  Hypothesis Testing?","  Predictive maintenance and condition-based monitoring systems have seen
significant prominence in recent years to minimize the impact of machine
downtime on production and its costs. Predictive maintenance involves using
concepts of data mining, statistics, and machine learning to build models that
are capable of performing early fault detection, diagnosing the faults and
predicting the time to failure. Fault diagnosis has been one of the core areas
where the actual failure mode of the machine is identified. In fluctuating
environments such as manufacturing, clustering techniques have proved to be
more reliable compared to supervised learning methods. One of the fundamental
challenges of clustering is developing a test hypothesis and choosing an
appropriate statistical test for hypothesis testing. Most statistical analyses
use some underlying assumptions of the data which most real-world data is
incapable of satisfying those assumptions. This paper is dedicated to
overcoming the following challenge by developing a test hypothesis for fault
diagnosis application using clustering technique and performing PERMANOVA test
for hypothesis testing.
"
1905.00352,2019-05-02,First digit law from Laplace transform,"  The occurrence of digits 1 through 9 as the leftmost nonzero digit of numbers
from real-world sources is distributed unevenly according to an empirical law,
known as Benford's law or the first digit law. It remains obscure why a variety
of data sets generated from quite different dynamics obey this particular law.
We perform a study of Benford's law from the application of the Laplace
transform, and find that the logarithmic Laplace spectrum of the digital
indicator function can be approximately taken as a constant. This particular
constant, being exactly the Benford term, explains the prevalence of Benford's
law. The slight variation from the Benford term leads to deviations from
Benford's law for distributions which oscillate violently in the inverse
Laplace space. We prove that the whole family of completely monotonic
distributions can satisfy Benford's law within a small bound. Our study
suggests that Benford's law originates from the way that we write numbers, thus
should be taken as a basic mathematical knowledge.
"
1905.03121,2020-05-27,A First Course in Data Science,"  Data science is a discipline that provides principles, methodology and
guidelines for the analysis of data for tools, values, or insights. Driven by a
huge workforce demand, many academic institutions have started to offer degrees
in data science, with many at the graduate, and a few at the undergraduate
level. Curricula may differ at different institutions, because of varying
levels of faculty expertise, and different disciplines (such as Math, computer
science, and business etc) in developing the curriculum. The University of
Massachusetts Dartmouth started offering degree programs in data science from
Fall 2015, at both the undergraduate and the graduate level. Quite a few
articles have been published that deal with graduate data science courses, much
less so dealing with undergraduate ones. Our discussion will focus on
undergraduate course structure and function, and specifically, a first course
in data science. Our design of this course centers around a concept called the
data science life cycle. That is, we view tasks or steps in the practice of
data science as forming a process, consisting of states that indicate how it
comes into life, how different tasks in data science depend on or interact with
others until the birth of a data product or the reach of a conclusion.
Naturally, different pieces of the data science life cycle then form individual
parts of the course. Details of each piece are filled up by concepts,
techniques, or skills that are popular in industry. Consequently, the design of
our course is both ""principled"" and practical. A significant feature of our
course philosophy is that, in line with activity theory, the course is based on
the use of tools to transform real data in order to answer strongly motivated
questions related to the data.
"
1905.06318,2019-05-16,Which principal components are most sensitive to distributional changes?,"  PCA is often used in anomaly detection and statistical process control tasks.
For bivariate data, we prove that the minor projection (the least varying
projection) of the PCA-rotated data is the most sensitive to distributional
changes, where sensitivity is defined by the Hellinger distance between
distributions before and after a change. In particular, this is almost always
the case if only one parameter of the bivariate normal distribution changes,
i.e., the change is sparse. Simulations indicate that the minor projections are
the most sensitive for a large range of changes and pre-change settings in
higher dimensions as well. This motivates using the minor projections for
detecting sparse distributional changes in high-dimensional data.
"
1905.08122,2019-05-21,"Non-Parametric Estimation of Spot Covariance Matrix with High-Frequency
  Data","  Estimating spot covariance is an important issue to study, especially with
the increasing availability of high-frequency financial data. We study the
estimation of spot covariance using a kernel method for high-frequency data. In
particular, we consider first the kernel weighted version of realized
covariance estimator for the price process governed by a continuous
multivariate semimartingale. Next, we extend it to the threshold kernel
estimator of the spot covariances when the underlying price process is a
discontinuous multivariate semimartingale with finite activity jumps. We derive
the asymptotic distribution of the estimators for both fixed and shrinking
bandwidth. The estimator in a setting with jumps has the same rate of
convergence as the estimator for diffusion processes without jumps. A
simulation study examines the finite sample properties of the estimators. In
addition, we study an application of the estimator in the context of covariance
forecasting. We discover that the forecasting model with our estimator
outperforms a benchmark model in the literature.
"
1905.08338,2020-08-10,"A response to critiques of ""The reproducibility of research and the
  misinterpretation of p-values""","  I proposed (8, 1, 3) that p values should be supplemented by an estimate of
the false positive risk (FPR). FPR was defined as the probability that, if you
claim that there is a real effect on the basis of p value from a single
unbiased experiment, that you will be mistaken and the result has occurred by
chance. This is a Bayesian quantity and that means that there is an infinitude
of ways to calculate it. My choice of a way to estimate FPR was, therefore,
arbitrary. I maintain that it is a reasonable way, and has the advantage of
being mathematically simpler than other proposals and easier to understand than
other methods. This might make it more easily accepted by users. As always, not
every statistician agrees. This paper is a response to a critique of my 2017
paper (1) by Arandjelovic (2)
"
1905.08381,2019-05-22,Statistical methods research done as science rather than mathematics,"  This paper is about how we study statistical methods. As an example, it uses
the random regressions model, in which the intercept and slope of
cluster-specific regression lines are modeled as a bivariate random effect.
Maximizing this model's restricted likelihood often gives a boundary value for
the random effect correlation or variances. We argue that this is a problem;
that it is a problem because our discipline has little understanding of how
contemporary models and methods map data to inferential summaries; that we lack
such understanding, even for models as simple as this, because of a
near-exclusive reliance on mathematics as a means of understanding; and that
math alone is no longer sufficient. We then argue that as a discipline, we can
and should break open our black-box methods by mimicking the five steps that
molecular biologists commonly use to break open Nature's black boxes: design a
simple model system, formulate hypotheses using that system, test them in
experiments on that system, iterate as needed to reformulate and test
hypotheses, and finally test the results in an ""in vivo"" system. We demonstrate
this by identifying conditions under which the random-regressions restricted
likelihood is likely to be maximized at a boundary value. Resistance to this
approach seems to arise from a view that it lacks the certainty or intellectual
heft of mathematics, perhaps because simulation experiments in our literature
rarely do more than measure a new method's operating characteristics in a small
range of situations. We argue that such work can make useful contributions
including, as in molecular biology, the findings themselves and sometimes the
designs used in the five steps; that these contributions have as much practical
value as mathematical results; and that therefore they merit publication as
much as the mathematical results our discipline esteems so highly.
"
1905.08876,2019-05-31,"Many perspectives on Deborah Mayo's ""Statistical Inference as Severe
  Testing: How to Get Beyond the Statistics Wars""","  The new book by philosopher Deborah Mayo is relevant to data science for
topical reasons, as she takes various controversial positions regarding
hypothesis testing and statistical practice, and also as an entry point to
thinking about the philosophy of statistics. The present article is a slightly
expanded version of a series of informal reviews and comments on Mayo's book.
We hope this discussion will introduce people to Mayo's ideas along with other
perspectives on the topics she addresses.
"
1905.09509,2019-05-24,Leveraging Uncertainty in Deep Learning for Selective Classification,"  The wide and rapid adoption of deep learning by practitioners brought
unintended consequences in many situations such as in the infamous case of
Google Photos' racist image recognition algorithm; thus, necessitated the
utilization of the quantified uncertainty for each prediction. There have been
recent efforts towards quantifying uncertainty in conventional deep learning
methods (e.g., dropout as Bayesian approximation); however, their optimal use
in decision making is often overlooked and understudied. In this study, we
propose a mixed-integer programming framework for classification with reject
option (also known as selective classification), that investigates and combines
model uncertainty and predictive mean to identify optimal classification and
rejection regions. Our results indicate superior performance of our framework
both in non-rejected accuracy and rejection quality on several publicly
available datasets. Moreover, we extend our framework to cost-sensitive
settings and show that our approach outperforms industry standard methods
significantly for online fraud management in real-world settings.
"
1905.09515,2019-05-24,Atlantic Causal Inference Conference (ACIC) Data Analysis Challenge 2017,"  This brief note documents the data generating processes used in the 2017 Data
Analysis Challenge associated with the Atlantic Causal Inference Conference
(ACIC). The focus of the challenge was estimation and inference for conditional
average treatment effects (CATEs) in the presence of targeted selection, which
leads to strong confounding. The associated data files and further plots can be
found on the first author's web page.
"
1905.09715,2019-05-24,"An illustration of the risk of borrowing information via a shared
  likelihood","  A concrete, stylized example illustrates that inferences may be degraded,
rather than improved, by incorporating supplementary data via a joint
likelihood. In the example, the likelihood is assumed to be correctly
specified, as is the prior over the parameter of interest; all that is
necessary for the joint modeling approach to suffer is misspecification of the
prior over a nuisance parameter.
"
1905.09929,2019-05-27,"Discussion of ""Nonparametric generalized fiducial inference for survival
  functions under censoring""","  The following discussion is inspired by the paper Nonparametric generalized
fiducial inference for survival functions under censoring by Cui and Hannig.
The discussion consists of comments on the results, but also indicates it's
importance more generally in the context of fiducial inference. A two page
introduction to fiducial inference is given to provide a context.
"
1905.10209,2019-05-27,A score function for Bayesian cluster analysis,"  We propose a score function for Bayesian clustering. The function is
parameter free and captures the interplay between the within cluster variance
and the between cluster entropy of a clustering. It can be used to choose the
number of clusters in well-established clustering methods such as hierarchical
clustering or $K$-means algorithm.
"
1905.11497,2020-07-10,"Estimating Average Treatment Effects Utilizing Fractional Imputation
  when Confounders are Subject to Missingness","  The problem of missingness in observational data is ubiquitous. When the
confounders are missing at random, multiple imputation is commonly used;
however, the method requires congeniality conditions for valid inferences,
which may not be satisfied when estimating average causal treatment effects.
Alternatively, fractional imputation, proposed by Kim 2011, has been
implemented to handling missing values in regression context. In this article,
we develop fractional imputation methods for estimating the average treatment
effects with confounders missing at random. We show that the fractional
imputation estimator of the average treatment effect is asymptotically normal,
which permits a consistent variance estimate. Via simulation study, we compare
fractional imputation's accuracy and precision with that of multiple
imputation.
"
1905.12081,2020-06-25,"Semi-Supervised Learning, Causality and the Conditional Cluster
  Assumption","  While the success of semi-supervised learning (SSL) is still not fully
understood, Sch\""olkopf et al. (2012) have established a link to the principle
of independent causal mechanisms. They conclude that SSL should be impossible
when predicting a target variable from its causes, but possible when predicting
it from its effects. Since both these cases are somewhat restrictive, we extend
their work by considering classification using cause and effect features at the
same time, such as predicting disease from both risk factors and symptoms.
While standard SSL exploits information contained in the marginal distribution
of all inputs (to improve the estimate of the conditional distribution of the
target given inputs), we argue that in our more general setting we should use
information in the conditional distribution of effect features given causal
features. We explore how this insight generalises the previous understanding,
and how it relates to and can be exploited algorithmically for SSL.
"
1906.03762,2019-06-11,Incorporating Open Data into Introductory Courses in Statistics,"  The 2016 Guidelines for Assessment and Instruction in Statistics Education
(GAISE) College Report emphasized six recommendations to teach introductory
courses in statistics. Among them: use of real data with context and purpose.
Many educators have created databases consisting of multiple data sets for use
in class; sometimes making hundreds of data sets available. Yet `the context
and purpose' component of the data may remain elusive if just a generic
database is made available.
  We describe the use of open data in introductory courses. Countries and
cities continue to share data through open data portals. Hence, educators can
find regional data that engages their students more effectively. We present
excerpts from case studies that show the application of statistical methods to
data on: crime, housing, rainfall, tourist travel, and others. Data wrangling
and discussion of results are recognized as important case study components.
Thus the open data based case studies attend most GAISE College Report
recommendations. Reproducible \textsf{R} code is made available for each case
study. Example uses of open data in more advanced courses in statistics are
also described.
"
1906.08360,2019-06-21,Frequentist Inference without Repeated Sampling,"  Frequentist inference typically is described in terms of hypothetical
repeated sampling but there are advantages to an interpretation that uses a
single random sample. Contemporary examples are given that indicate
probabilities for random phenomena are interpreted as classical probabilities,
and this interpretation is applied to statistical inference using urn models.
Both classical and limiting relative frequency interpretations can be used to
communicate statistical inference, and the effectiveness of each is discussed.
Recent descriptions of p-values, confidence intervals, and power are viewed
through the lens of classical probability based on a single random sample from
the population.
"
1906.08605,2020-05-18,"Quantifying impacts of the drought 2018 on European ecosystems in
  comparison to 2003","  In recent decades, an increasing persistence of atmospheric circulation
patterns has been observed. In the course of the associated long-lasting
anticyclonic summer circulations, heat waves and drought spells often coincide,
leading to so-called hotter droughts. Previous hotter droughts caused a
decrease in agricultural yields and increase in tree mortality, and thus, had a
remarkable effect on carbon budgets and negative economic impacts.
Consequently, a quantification of ecosystem responses to hotter droughts and a
better understanding of the underlying mechanisms is crucial. In this context,
the European hotter drought of the year 2018 may be considered as a key event.
As a first step towards the quantification of its causes and consequences, we
here assess anomalies of atmospheric circulation patterns, temperature loads,
and climatic water balance as potential drivers of ecosystem responses as
quantified by remote sensing using the MODIS vegetation indices NDVI and EVI.
To place the drought of 2018 within a climatological context, we compare its
climatic features and ecosystem response with the extreme hot drought of 2003.
Our results indicated 2018 to be characterized by a climatic dipole, featuring
extremely hot and dry weather conditions north of the Alps but comparably cool
and moist conditions across large parts of the Mediterranean. Analyzing
ecosystem response of five dominant land-cover classes, we found significant
positive effects of April-July climatic water balance on ecosystem
productivity. Negative drought impacts appeared to affect a larger area in 2018
compared to 2003. We found a significantly higher sensitivity of pastures and
arable land to climatic water balance compared to forests in both years. This
study quantifies the drought of 2018 as a yet unprecedented event and provides
valuable insights into the heterogeneous drought responses of European
ecosystems.
"
1906.08726,2019-06-21,On the probability of a causal inference is robust for internal validity,"  The internal validity of observational study is often subject to debate. In
this study, we define the counterfactuals as the unobserved sample and intend
to quantify its relationship with the null hypothesis statistical testing
(NHST). We propose the probability of a causal inference is robust for internal
validity, i.e., the PIV, as a robustness index of causal inference. Formally,
the PIV is the probability of rejecting the null hypothesis again based on both
the observed sample and the counterfactuals, provided the same null hypothesis
has already been rejected based on the observed sample. Under either
frequentist or Bayesian framework, one can bound the PIV of an inference based
on his bounded belief about the counterfactuals, which is often needed when the
unconfoundedness assumption is dubious. The PIV is equivalent to statistical
power when the NHST is thought to be based on both the observed sample and the
counterfactuals. We summarize the process of evaluating internal validity with
the PIV into an eight-step procedure and illustrate it with an empirical
example (i.e., Hong and Raudenbush (2005)).
"
1906.10564,2019-09-24,A Role for Symmetry in the Bayesian Solution of Differential Equations,"  The interpretation of numerical methods, such as finite difference methods
for differential equations, as point estimators suggests that formal
uncertainty quantification can also be performed in this context. Competing
statistical paradigms can be considered and Bayesian probabilistic numerical
methods (PNMs) are obtained when Bayesian statistical principles are deployed.
Bayesian PNM have the appealing property of being closed under composition,
such that uncertainty due to different sources of discretisation in a numerical
method can be jointly modelled and rigorously propagated. Despite recent
attention, no exact Bayesian PNM for the numerical solution of ordinary
differential equations (ODEs) has been proposed. This raises the fundamental
question of whether exact Bayesian methods for (in general nonlinear) ODEs even
exist. The purpose of this paper is to provide a positive answer for a limited
class of ODE. To this end, we work at a foundational level, where a novel
Bayesian PNM is proposed as a proof-of-concept. Our proposal is a synthesis of
classical Lie group methods, to exploit underlying symmetries in the gradient
field, and non-parametric regression in a transformed solution space for the
ODE. The procedure is presented in detail for first and second order ODEs and
relies on a certain strong technical condition -- existence of a solvable Lie
algebra -- being satisfied. Numerical illustrations are provided.
"
1906.11102,2019-06-27,"Hybrid Resource Scheduling for Aggregation in Massive Machine-type
  Communication Networks","  Data aggregation is a promising approach to enable massive machine-type
communication (mMTC). Here, we first characterize the aggregation phase where a
massive number of machine-type devices transmits to their respective
aggregator. By using non-orthogonal multiple access (NOMA), we present a hybrid
access scheme where several machine-type devices (MTDs) share the same
orthogonal channel. Then, we assess the relaying phase where the aggregatted
data is forwarded to the base station. The system performance is investigated
in terms of average number of MTDs that are simultaneously served under
imperfect successive interference cancellation (SIC) at the aggregator for two
scheduling schemes, namely random resource scheduling (RRS) and
channel-dependent resource scheduling (CRS), which is then used to assess the
performance of data forwarding phase.
"
1906.11720,2019-06-28,"Detecting and classifying moments in basketball matches using sensor
  tracked data","  Data analytics in sports is crucial to evaluate the performance of single
players and the whole team. The literature proposes a number of tools for both
offence and defence scenarios. Data coming from tracking location of players,
in this respect, may be used to enrich the amount of useful information. In
basketball, however, actions are interleaved with inactive periods. This paper
describes a methodological approach to automatically identify active periods
during a game and to classify them as offensive or defensive. The method is
based on the application of thresholds to players kinematic parameters, whose
values undergo a tuning strategy similar to Receiver Operating Characteristic
curves, using a ground truth extracted from the video of the games.
"
1906.11976,2019-09-16,"Multivariate Big Data Analysis for Intrusion Detection: 5 steps from the
  haystack to the needle","  The research literature on cybersecurity incident detection & response is
very rich in automatic detection methodologies, in particular those based on
the anomaly detection paradigm. However, very little attention has been devoted
to the diagnosis ability of the methods, aimed to provide useful information on
the causes of a given detected anomaly. This information is of utmost
importance for the security team to reduce the time from detection to response.
In this paper, we present Multivariate Big Data Analysis (MBDA), a complete
intrusion detection approach based on 5 steps to effectively handle massive
amounts of disparate data sources. The approach has been designed to deal with
the main characteristics of Big Data, that is, the high volume, velocity and
variety. The core of the approach is the Multivariate Statistical Network
Monitoring (MSNM) technique proposed in a recent paper. Unlike in state of the
art machine learning methodologies applied to the intrusion detection problem,
when an anomaly is identified in MBDA the output of the system includes the
detail of the logs of raw information associated to this anomaly, so that the
security team can use this information to elucidate its root causes. MBDA is
based in two open software packages available in Github: the MEDA Toolbox and
the FCParser. We illustrate our approach with two case studies. The first one
demonstrates the application of MBDA to semistructured sources of information,
using the data from the VAST 2012 mini challenge 2. This complete case study is
supplied in a virtual machine available for download. In the second case study
we show the Big Data capabilities of the approach in data collected from a real
network with labeled attacks.
"
1907.00486,2020-03-11,"Asymptotic behavior of the length of the longest increasing subsequences
  of random walks","  We numerically estimate the leading asymptotic behavior of the length $L_{n}$
of the longest increasing subsequence of random walks with step increments
following Student's $t$-distribution with parameter in the range $1/2 \leq \nu
\leq 5$. We find that the expected value $\mathbb{E}(L_{n}) \sim
n^{\theta}\ln{n}$ with $\theta$ decreasing from $\theta(\nu=1/2) \approx 0.70$
to $\theta(\nu \geq 5/2) \approx 0.50$. For random walks with distribution of
step increments of finite variance ($\nu > 2$), this confirms previous
observation of $\mathbb{E}(L_{n}) \sim \sqrt{n}\ln{n}$ to leading order. We
note that this asymptotic behavior (including the subleading term) resembles
that of the largest part of random integer partitions under the uniform measure
and that, curiously, both random variables seem to follow Gumbel statistics. We
also provide more refined estimates for the asymptotic behavior of
$\mathbb{E}(L_{n})$ for random walks with step increments of finite variance.
"
1907.02676,2019-07-16,"On the Convergence Rate of the Quasi- to Stationary Distribution for the
  Shiryaev-Roberts Diffusion","  For the classical Shiryaev--Roberts martingale diffusion considered on the
interval $[0,A]$, where $A>0$ is a given absorbing boundary, it is shown that
the rate of convergence of the diffusion's quasi-stationary cumulative
distribution function (cdf), $Q_{A}(x)$, to its stationary cdf, $H(x)$, as
$A\to+\infty$, is no worse than $O(\log(A)/A)$, uniformly in $x\ge0$. The
result is established explicitly, by constructing new tight lower- and
upper-bounds for $Q_{A}(x)$ using certain latest monotonicity properties of the
modified Bessel $K$ function involved in the exact closed-form formula for
$Q_{A}(x)$ recently obtained by Polunchenko (2017).
"
1907.02912,2020-06-15,On Finite Exchangeability and Conditional Independence,"  We study the independence structure of finitely exchangeable distributions
over random vectors and random networks. In particular, we provide necessary
and sufficient conditions for an exchangeable vector so that its elements are
completely independent or completely dependent. We also provide a sufficient
condition for an exchangeable vector so that its elements are marginally
independent. We then generalize these results and conditions for exchangeable
random networks. In this case, it is demonstrated that the situation is more
complex. We show that the independence structure of exchangeable random
networks lies in one of six regimes that are two-fold dual to one another,
represented by undirected and bidirected independence graphs in graphical model
sense with graphs that are complement of each other. In addition, under certain
additional assumptions, we provide necessary and sufficient conditions for the
exchangeable network distributions to be faithful to each of these graphs.
"
1907.04242,2019-10-02,Topological Information Data Analysis,"  This paper presents methods that quantify the structure of statistical
interactions within a given data set, and was first used in \cite{Tapia2018}.
It establishes new results on the k-multivariate mutual-informations (I_k)
inspired by the topological formulation of Information introduced in. In
particular we show that the vanishing of all I_k for 2\leq k \leq n of n random
variables is equivalent to their statistical independence. Pursuing the work of
Hu Kuo Ting and Te Sun Han, we show that information functions provide
co-ordinates for binary variables, and that they are analytically independent
on the probability simplex for any set of finite variables. The maximal
positive I_k identifies the variables that co-vary the most in the population,
whereas the minimal negative I_k identifies synergistic clusters and the
variables that differentiate-segregate the most the population. Finite data
size effects and estimation biases severely constrain the effective computation
of the information topology on data, and we provide simple statistical tests
for the undersampling bias and the k-dependences following. We give an example
of application of these methods to genetic expression and unsupervised
cell-type classification. The methods unravel biologically relevant subtypes,
with a sample size of 41 genes and with few errors. It establishes generic
basic methods to quantify the epigenetic information storage and a unified
epigenetic unsupervised learning formalism. We propose that higher-order
statistical interactions and non identically distributed variables are
constitutive characteristics of biological systems that should be estimated in
order to unravel their significant statistical structure and diversity. The
topological information data analysis presented here allows to precisely
estimate this higher-order structure characteristic of biological systems.
"
1907.05453,2019-11-11,"Model based Level Shift Detection in Autocorrelated Data Streams using a
  moving window","  Standard Control Chart techniques to detect level shift in data streams
assume independence between observations. As data today is collected with high
frequency, this assumption is seldom valid. To overcome this, we propose to
adapt the off-line test procedure for detection of outliers based on one-step
prediction errors proposed by Tsay (1988) into an on-line framework by
considering a moving window. Further, we present two algorithms, that in
combination, estimate an appropriate test value for our control chart. We test
our method on AR(1) processes exposed to level shifts of different sizes and
compare it to CUSUM applied to one-step prediction errors. We find that, even
though both methods perform comparable when tuned correctly, our method has
higher probability of identifying the correct change point of the process.
Furthermore, for more complicated processes our method is easier to tune, as
the range of window size to be tested is independent of the process.
"
1907.05947,2019-07-19,"Truth, Proof, and Reproducibility: There's no counter-attack for the
  codeless","  Current concerns about reproducibility in many research communities can be
traced back to a high value placed on empirical reproducibility of the physical
details of scientific experiments and observations. For example, the detailed
descriptions by 17th century scientist Robert Boyle of his vacuum pump
experiments are often held to be the ideal of reproducibility as a cornerstone
of scientific practice. Victoria Stodden has claimed that the computer is an
analog for Boyle's pump -- another kind of scientific instrument that needs
detailed descriptions of how it generates results. In the place of Boyle's
hand-written notes, we now expect code in open source programming languages to
be available to enable others to reproduce and extend computational
experiments. In this paper we show that there is another genealogy for
reproducibility, starting at least from Euclid, in the production of proofs in
mathematics. Proofs have a distinctive quality of being necessarily
reproducible, and are the cornerstone of mathematical science. However, the
task of the modern mathematical scientist has drifted from that of blackboard
rhetorician, where the craft of proof reigned, to a scientific workflow that
now more closely resembles that of an experimental scientist. So, what is proof
in modern mathematics? And, if proof is unattainable in other fields, what is
due scientific diligence in a computational experimental environment? How do we
measure truth in the context of uncertainty? Adopting a manner of Lakatosian
conversant conjecture between two mathematicians, we examine how proof informs
our practice of computational statistical inquiry. We propose that a
reorientation of mathematical science is necessary so that its reproducibility
can be readily assessed.
"
1907.06145,2020-11-12,"Leveraging Auxiliary Information on Marginal Distributions in
  Nonignorable Models for Item and Unit Nonresponse","  Often, government agencies and survey organizations know the population
counts or percentages for some of the variables in a survey. These may be
available from auxiliary sources, for example, administrative databases or
other high quality surveys. We present and illustrate a model-based framework
for leveraging such auxiliary marginal information when handling unit and item
nonresponse. We show how one can use the margins to specify different
missingness mechanisms for each type of nonresponse. We use the framework to
impute missing values in voter turnout in a subset of data from the U.S.\
Current Population Survey (CPS). In doing so, we examine the sensitivity of
results to different assumptions about the unit and item nonresponse.
"
1907.08703,2022-07-14,"Rediscovering a little known fact about the t-test and the F-test:
  Algebraic, Geometric, Distributional and Graphical Considerations","  We discuss the role that the null hypothesis should play in the construction
of a test statistic used to make a decision about that hypothesis. To construct
the test statistic for a point null hypothesis about a binomial proportion, a
common recommendation is to act as if the null hypothesis is true. We argue
that, on the surface, the one-sample t-test of a point null hypothesis about a
Gaussian population mean does not appear to follow the recommendation. We show
how simple algebraic manipulations of the usual t-statistic lead to an
equivalent test procedure consistent with the recommendation. We provide
geometric intuition regarding this equivalence and we consider extensions to
testing nested hypotheses in Gaussian linear models. We discuss an application
to graphical residual diagnostics where the form of the test statistic makes a
practical difference. By examining the formulation of the test statistic from
multiple perspectives in this familiar example, we provide simple, concrete
illustrations of some important issues that can guide the formulation of
effective solutions to more complex statistical problems.
"
1907.09333,2019-07-23,Continuously Updated Data Analysis Systems,"  When doing data science, it's important to know what you're building. This
paper describes an idealized final product of a data science project, called a
Continuously Updated Data-Analysis System (CUDAS). The CUDAS concept
synthesizes ideas from a range of successful data science projects, such as
Nate Silver's FiveThirtyEight. A CUDAS can be built for any context, such as
the state of the economy, the state of the climate, and so on. To demonstrate,
we build two CUDAS systems. The first provides continuously-updated ratings for
soccer players, based on the newly developed Augmented Adjusted Plus-Minus
statistic. The second creates a large dataset of synthetic ecosystems, which is
used for agent-based modeling of infectious diseases.
"
1907.13612,2021-12-07,"MSNM-Sensor: An Applied Network Monitoring Tool for Anomaly Detection in
  Complex Networks and Systems","  Technology evolves quickly. Low-cost and ready-to-connect devices are
designed to provide new services and applications. Smart grids or smart
healthcare systems are some examples of these applications, all of which are in
the context of smart cities. In this total-connectivity scenario, some security
issues arise since the larger the number of connected devices is, the greater
the surface attack dimension. In this way, new solutions for monitoring and
detecting security events are needed to address new challenges brought about by
this scenario, among others, the large number of devices to monitor, the large
amount of data to manage and the real-time requirement to provide quick
security event detection and, consequently, quick response to attacks. In this
work, a practical and ready-to-use tool for monitoring and detecting security
events in these environments is developed and introduced. The tool is based on
the Multivariate Statistical Network Monitoring (MSNM) methodology for
monitoring and anomaly detection and we call it MSNM-Sensor. Although it is in
its early development stages, experimental results based on the detection of
well-known attacks in hierarchical network systems prove the suitability of
this tool for more complex scenarios, such as those found in smart cities or
IoT ecosystems.
"
1908.04670,2019-08-14,A Proof of First Digit Law from Laplace Transform,"  The first digit law, also known as Benford's law or the significant digit
law, is an empirical phenomenon that the leading digit of numbers from real
world sources favors small ones in a form $\log(1+{1}/{d})$, where $d=1, 2,
..., 9$. Such a law keeps elusive for over one hundred years because it was
obscure whether this law is due to the logical consequence of the number system
or some mysterious mechanism of the nature. We provide a simple and elegant
proof of this law from the application of the Laplace transform, which is an
important tool of mathematical methods in physics. We reveal that the first
digit law is originated from the basic property of the number system, thus it
should be attributed as a basic mathematical knowledge for wide applications.
"
1908.06346,2019-08-20,"Karl Pearson and the Logic of Science: Renouncing Causal Understanding
  (the Bride) and Inverted Spinozism","  Karl Pearson is the leading figure of XX century statistics. He and his
co-workers crafted the core of the theory, methods and language of frequentist
or classical statistics -- the prevalent inductive logic of contemporary
science. However, before working in statistics, K.Pearson had other interests
in life, namely, in this order, philosophy, physics, and biological heredity.
Key concepts of his philosophical and epistemological system of anti-Spinozism
(a form of transcendental idealism) are carried over to his subsequent works on
the logic of scientific discovery. This article's main goal is to analyze
K.Pearson early philosophical and theological ideas and to investigate how the
same ideas came to influence contemporary science, either directly or
indirectly -- by the use of variant theories, methods and dialects of
statistics, corresponding to variant statistical inference procedures and their
specific belief calculi.
"
1908.06934,2024-09-10,"Cumulants of multiinformation density in the case of a multivariate
  normal distribution","  We consider a generalization of information density to a partitioning into $N
\geq 2$ subvectors. We calculate its cumulant-generating function and its
cumulants, showing that these quantities are only a function of all the
regression coefficients associated with the partitioning.
"
1908.07372,2019-08-21,Stochastic differential theory of cricket,"  A new formalism for analyzing the progression of cricket game using
Stochastic differential equation (SDE) is introduced. This theory enables a
quantitative way of representing every team using three key variables which
have physical meaning associated with them. This is in contrast with the
traditional system of rating/ranking teams based on combination of different
statical cumulants. Further more, using this formalism, a new method to
calculate the winning probability as a progression of number of balls is given.
"
1908.07521,2023-02-07,"Distributed Hypothesis Testing over a Noisy Channel: Error-exponents
  Trade-off","  A two-terminal distributed binary hypothesis testing problem over a noisy
channel is studied. The two terminals, called the observer and the decision
maker, each has access to $n$ independent and identically distributed samples,
denoted by $\mathbf{U}$ and $\mathbf{V}$, respectively. The observer
communicates to the decision maker over a discrete memoryless channel, and the
decision maker performs a binary hypothesis test on the joint probability
distribution of $(\mathbf{U},\mathbf{V})$ based on $\mathbf{V}$ and the noisy
information received from the observer. The trade-off between the exponents of
the type I and type II error probabilities is investigated. Two inner bounds
are obtained, one using a separation-based scheme that involves type-based
compression and unequal error-protection channel coding, and the other using a
joint scheme that incorporates type-based hybrid coding. The separation-based
scheme is shown to recover the inner bound obtained by Han and Kobayashi for
the special case of a rate-limited noiseless channel, and also the one obtained
by the authors previously for a corner point of the trade-off. Finally, we show
via an example that the joint scheme achieves a strictly tighter bound than the
separation-based scheme for some points of the error-exponents trade-off.
"
1908.08991,2023-01-05,"Football is becoming more predictable; Network analysis of 88 thousands
  matches in 11 major leagues","  In recent years excessive monetization of football and professionalism among
the players has been argued to have affected the quality of the match in
different ways. On the one hand, playing football has become a high-income
profession and the players are highly motivated; on the other hand, stronger
teams have higher incomes and therefore afford better players leading to an
even stronger appearance in tournaments that can make the game more imbalanced
and hence predictable. To quantify and document this observation, in this work
we take a minimalist network science approach to measure the predictability of
football over 26 years in major European leagues. We show that over time, the
games in major leagues have indeed become more predictable. We provide further
support for this observation by showing that inequality between teams has
increased and the home-field advantage has been vanishing ubiquitously. We do
not include any direct analysis on the effects of monetization on football's
predictability or therefore, lack of excitement, however, we propose several
hypotheses which could be tested in future analyses.
"
1908.09431,2019-08-27,"Multichannel signal detection in interference and noise when signal
  mismatch happens","  In this paper, we consider the problem of detecting a multichannel signal in
interference and noise when signal mismatch happens. We first propose two
selective detectors, since their strong selectivity is preferred in some
situations. However, these two detectors would not be suitable candidates if a
robust detector is needed. To overcome this shortcoming, we then devise a
tunable detector, which is parametrized by a non-negative scaling factor,
referred to as the tunable parameter. By adjusting the tunable parameter, the
proposed detector can smoothly change its capability in rejecting or robustly
detecting a mismatch signal. Moreover, one selective detector and the tunable
detector with an appropriate tunable parameter can provide nearly the same
detection performance as existing detectors in the absence of signal mismatch.
We obtain analytical expressions for the probabilities of detection (PDs) and
probabilities of false alarm (PFAs) of the three proposed detectors, which are
verified by Monte Carlo simulations.
"
1908.09830,2019-08-28,"A statistical framework for measuring the temporal stability of human
  mobility patterns","  Despite the growing popularity of human mobility studies that collect GPS
location data, the problem of determining the minimum required length of GPS
monitoring has not been addressed in the current statistical literature. In
this paper we tackle this problem by laying out a theoretical framework for
assessing the temporal stability of human mobility based on GPS location data.
We define several measures of the temporal dynamics of human spatiotemporal
trajectories based on the average velocity process, and on activity
distributions in a spatial observation window. We demonstrate the use of our
methods with data that comprise the GPS locations of 185 individuals over the
course of 18 months. Our empirical results suggest that GPS monitoring should
be performed over periods of time that are significantly longer than what has
been previously suggested. Furthermore, we argue that GPS study designs should
take into account demographic groups.
  KEYWORDS: Density estimation; global positioning systems (GPS); human
mobility; spatiotemporal trajectories; temporal dynamics
"
1908.10024,2019-08-28,The Poisson binomial distribution -- Old & New,"  This is an expository article on the Poisson binomial distribution. We review
lesser known results and recent progress on this topic, including geometry of
polynomials and distribution learning. We also provide examples to illustrate
the use of the Poisson binomial machinery. Some open questions of approximating
rational fractions of the Poisson binomial are presented.
"
1908.10971,2019-11-06,"Robust Registration of Astronomy Catalogs with Applications to the
  Hubble Space Telescope","  Astrometric calibration of images with a small field of view is often
inferior to the internal accuracy of the source detections due to the small
number of accessible guide stars. One important experiment with such challenges
is the Hubble Space Telescope (HST). A possible solution is to cross-calibrate
overlapping fields instead of just relying on standard stars. Following the
approach of \citet{2012ApJ...761..188B}, we use infinitesimal 3D rotations for
fine-tuning the calibration but devise a better objective that is robust to a
large number of false candidates in the initial set of associations. Using
Bayesian statistics, we accommodate bad data by explicitly modeling the
quality, which yields a formalism essentially identical to an $M$-estimation in
robust statistics. Our results on simulated and real catalogs show great
potentials for improving the HST calibration, and those with similar
challenges.
"
1908.11364,2019-08-30,Introduction to Geodetic Time Series Analysis,"  This contribution is the chapter 2 of the book ""geodetic time series
analysis"" (10.1007/978-3-030-21718-1). The book is dedicated to the art of
fitting a trajectory model to those geodetic time series in order to extract
accurate geophysical information with realistic error bars in geodymanics and
environmental geodesy related studies. In the vast amount of the literature
published on this topic in the past 25 years, we are specifically interested in
parametric algorithms which are estimating both functional and stochastic
models using various Bayesian statistical tools (maximum likelihood, Monte
Carlo Markov chain, Kalman filter, least squares variance component estimation,
information criteria). This chapter will focus on how the parameters of the
trajectory model can be estimated. It is meant to give researchers new to this
topic an easy introduction to the theory with references to key books and
articles where more details can be found. In addition, we hope that it
refreshes some of the details for the more experienced readers. We pay special
attention to the modelling of the noise which has received much attention in
the literature in the last years and highlight some of the numerical aspects.
"
1909.00061,2021-03-02,"Investigating Sprawl using AIC and Recursive Partitioning Trees: A
  Machine Learning Approach to Assessing the Association between Poverty and
  Commute Time","  Sprawl, according to Glaeser and Kahn, is the 21st century phenomenon that
some people are not dependent on city-living due to automobiles and therefore
can live outside public transportation spheres and cities. This is usually seen
as pleasant and accompanied by improved qualities of life, but as they
addressed, the problem remains that sprawl causes loss of jobs for those who
cannot afford luxurious alternatives but only inferior substitutes (Glaeser and
Kahn 2004). Therefore, through our question, we hope to suggest that sprawl has
occurred in the U.S. and poverty is one of the consequences.
"
1909.00225,2019-09-04,Statistical Robust Chinese Remainder Theorem for Multiple Numbers,"  Generalized Chinese Remainder Theorem (CRT) is a well-known approach to solve
ambiguity resolution related problems. In this paper, we study the robust CRT
reconstruction for multiple numbers from a view of statistics. To the best of
our knowledge, it is the first rigorous analysis on the underlying statistical
model of CRT-based multiple parameter estimation. To address the problem, two
novel approaches are established. One is to directly calculate a conditional
maximum a posteriori probability (MAP) estimation of the residue clustering,
and the other is based on a generalized wrapped Gaussian mixture model to
iteratively search for MAP of both estimands and clustering. Residue error
correcting codes are introduced to improve the robustness further. Experimental
results show that the statistical schemes achieve much stronger robustness
compared to state-of-the-art deterministic schemes, especially in heavy-noise
scenarios.
"
1909.01219,2019-09-04,The maximum likelihood degree of a chemical reaction at the equilibrium,"  The complexity of a maximum likelihood estimation is measured by its maximum
likelihood degree ($ML$ degree). In this paper we study the maximum likelihood
problem associated to chemical networks composed by one single chemical
reaction under the equilibrium assumption.
"
1909.02282,2019-09-06,"Reduced-bias estimation of spatial econometric models with incompletely
  geocoded data","  The application of state-of-the-art spatial econometric models requires that
the information about the spatial coordinates of statistical units is
completely accurate, which is usually the case in the context of areal data.
With micro-geographic point-level data, however, such information is inevitably
affected by locational errors, that can be generated intentionally by the data
producer for privacy protection or can be due to inaccuracy of the geocoding
procedures. This unfortunate circumstance can potentially limit the use of the
spatial econometric modelling framework for the analysis of micro data. Indeed,
some recent contributions (see e.g. Arbia, Espa and Giuliani 2016) have shown
that the presence of locational errors may have a non-negligible impact on the
results. In particular, wrong spatial coordinates can lead to downward bias and
increased variance in the estimation of model parameters. This contribution
aims at developing a strategy to reduce the bias and produce more reliable
inference for spatial econometrics models with location errors. The validity of
the proposed approach is assessed by means of a Monte Carlo simulation study
under different real-case scenarios. The study results show that the method is
promising and can make the spatial econometric modelling of micro-geographic
data possible.
"
1909.02989,2020-12-22,A P\'olya-Gamma Sampler for a Generalized Logistic Regression,"  In this paper we introduce a novel Bayesian data augmentation approach for
estimating the parameters of the generalised logistic regression model. We
propose a P\'olya-Gamma sampler algorithm that allows us to sample from the
exact posterior distribution, rather than relying on approximations. A
simulation study illustrates the flexibility and accuracy of the proposed
approach to capture heavy and light tails in binary response data of different
dimensions. The methodology is applied to two different real datasets, where we
demonstrate that the P\'olya-Gamma sampler provides more precise estimates than
the empirical likelihood method, outperforming approximate approaches.
"
1909.03433,2019-09-10,"Distributionally Robust Optimization with Correlated Data from Vector
  Autoregressive Processes","  We present a distributionally robust formulation of a stochastic optimization
problem for non-i.i.d vector autoregressive data. We use the Wasserstein
distance to define robustness in the space of distributions and we show, using
duality theory, that the problem is equivalent to a finite convex-concave
saddle point problem. The performance of the method is demonstrated on both
synthetic and real data.
"
1909.03540,2021-03-02,"Inference In High-dimensional Single-Index Models Under Symmetric
  Designs","  The problem of statistical inference for regression coefficients in a
high-dimensional single-index model is considered. Under elliptical symmetry,
the single index model can be reformulated as a proxy linear model whose
regression parameter is identifiable. We construct estimates of the regression
coefficients of interest that are similar to the debiased lasso estimates in
the standard linear model and exhibit similar properties: root-n-consistency
and asymptotic normality. The procedure completely bypasses the estimation of
the unknown link function, which can be extremely challenging depending on the
underlying structure of the problem. Furthermore, under Gaussianity, we propose
more efficient estimates of the coefficients by expanding the link function in
the Hermite polynomial basis. Finally, we illustrate our approach via carefully
designed simulation experiments.
"
1909.03784,2021-12-02,A new approach of chain sampling inspection plan,"  To develop decision rules regarding acceptance or rejection of production
lots based on sample data is the purpose of acceptance sampling inspection
plan. Dependent sampling procedures cumulate results from several preceding
production lots when testing is expensive or destructive. This chaining of past
lots reduce the sizes of the required samples, essential for acceptance or
rejection of production lots. In this article, a new approach for chaining the
past lot(s) results proposed, named as modified chain group acceptance sampling
inspection plan, requires a smaller sample size than the commonly used sampling
inspection plan, such as group acceptance sampling inspection plan and single
acceptance sampling inspection plan. A comparison study has been done between
the proposed and group acceptance sampling inspection plan as well as single
acceptance sampling inspection plan. A example has been given to illustrate the
proposed plan in a good manner.
"
1909.03813,2020-05-05,INTEREST: INteractive Tool for Exploring REsults from Simulation sTudies,"  Simulation studies allow us to explore the properties of statistical methods.
They provide a powerful tool with a multiplicity of aims; among others:
evaluating and comparing new or existing statistical methods, assessing
violations of modelling assumptions, helping with the understanding of
statistical concepts, and supporting the design of clinical trials. The
increased availability of powerful computational tools and usable software has
contributed to the rise of simulation studies in the current literature.
However, simulation studies involve increasingly complex designs, making it
difficult to provide all relevant results clearly. Dissemination of results
plays a focal role in simulation studies: it can drive applied analysts to use
methods that have been shown to perform well in their settings, guide
researchers to develop new methods in a promising direction, and provide
insights into less established methods. It is crucial that we can digest
relevant results of simulation studies. Therefore, we developed INTEREST: an
INteractive Tool for Exploring REsults from Simulation sTudies. The tool has
been developed using the Shiny framework in R and is available as a web app or
as a standalone package. It requires uploading a tidy format dataset with the
results of a simulation study in R, Stata, SAS, SPSS, or comma-separated
format. A variety of performance measures are estimated automatically along
with Monte Carlo standard errors; results and performance summaries are
displayed both in tabular and graphical fashion, with a wide variety of
available plots. Consequently, the reader can focus on simulation parameters
and estimands of most interest. In conclusion, INTEREST can facilitate the
investigation of results from simulation studies and supplement the reporting
of results, allowing researchers to share detailed results from their
simulations and readers to explore them freely.
"
1909.04323,2019-09-11,"Investigating the completeness and omission roads of OpenStreetMap data
  in Hubei, China by comparing with Street Map and Street View","  OpenStreetMap (OSM) is a free map of the world which can be edited by global
volunteers. Existing studies have showed that completeness of OSM road data in
some developing countries (e.g. China) is much lower, resulting in concern in
utilizing the data in various applications. But very few have focused on
investigating what types of road are still poorly mapped. This study aims not
only to investigate the completeness of OSM road datasets in China but also to
investigate what types of road (called omission roads) have not been mapped,
which is achieved by referring to both Street Map and Street View. 16
prefecture-level divisions in the urban areas of Hubei (China) were used as
study areas. Results showed that: (1) the completeness for most
prefecture-level divisions was at a low-to-medium level; most roads (in the
Street Map), however, with traffic conditions had already been mapped well. (2)
Most of the omission OSM roads were either private roads, or public roads not
having yet been named and with only one single lane, indicating their lack of
importance in the urban road network. We argue that although the OSM road
datasets in China are incomplete, they may still be used for several
applications.
"
1909.04486,2019-09-11,Data Science in Biomedicine,"  We highlight the role of Data Science in Biomedicine. Our manuscript goes
from the general to the particular, presenting a global definition of Data
Science and showing the trend for this discipline together with the terms of
cloud computing and big data. In addition, since Data Science is mostly related
to areas like economy or business, we describe its importance in biomedicine.
Biomedical Data Science (BDS) presents the challenge of dealing with data
coming from a range of biological and medical research, focusing on
methodologies to advance the biomedical science discoveries, in an
interdisciplinary context.
"
1909.06523,2019-09-17,Justifying the Norms of Inductive Inference,"  Bayesian inference is limited in scope because it cannot be applied in
idealized contexts where none of the hypotheses under consideration is true and
because it is committed to always using the likelihood as a measure of
evidential favoring, even when that is inappropriate. The purpose of this paper
is to study inductive inference in a very general setting where finding the
truth is not necessarily the goal and where the measure of evidential favoring
is not necessarily the likelihood. I use an accuracy argument to argue for
probabilism and I develop a new kind of argument to argue for two general
updating rules, both of which are reasonable in different contexts. One of the
updating rules has standard Bayesian updating, Bissiri et al's (2016) general
Bayesian updating, Douven's (2016) IBE-based updating, and Vassend's (2019a)
quasi-Bayesian updating as special cases. The other updating rule is novel.
"
1909.06990,2020-03-11,"Testing claims of the GW170817 binary neutron star inspiral affecting
  $\beta$-decay rates","  On August 17, 2017, the first gravitational wave signal from a binary neutron
star inspiral (GW170817) was detected by Advanced LIGO and Advanced VIRGO. Here
we present radioactive $\beta$-decay rates of three independent sources
$^{44}$Ti, $^{60}$Co and $^{137}$Cs, monitored during the same period by a
precision experiment designed to investigate the decay of long-lived
radioactive sources. We do not find any significant correlations between decay
rates in a 5\,h time interval following the GW170817 observation. This
contradicts a previous claim published in this journal of an observed
2.5$\sigma$ Pearson Correlation between fluctuations in the number of observed
decays from two $\beta$-decaying isotopes ($^{32}$Si and $^{36}$Cl) in the same
time interval. By correcting for the choice of an arbitrary time interval, we
find no evidence of a correlation above 1.5$\sigma$ confidence. In addition, we
argue that such analyses on correlations in arbitrary time intervals should
always correct for the so-called Look-Elsewhere effect by quoting the global
significance.
"
1909.07026,2020-03-25,"On the Hurwitz zeta function with an application to the beta-exponential
  distribution","  We prove a monotonicity property of the Hurwitz zeta function which, in turn,
translates into a chain of inequalities for polygamma functions of different
orders. We provide a probabilistic interpretation of our result by exploiting a
connection between Hurwitz zeta function and the cumulants of the
beta-exponential distribution.
"
1909.09138,2019-09-23,Uncovering Sociological Effect Heterogeneity using Machine Learning,"  Individuals do not respond uniformly to treatments, events, or interventions.
Sociologists routinely partition samples into subgroups to explore how the
effects of treatments vary by covariates like race, gender, and socioeconomic
status. In so doing, analysts determine the key subpopulations based on
theoretical priors. Data-driven discoveries are also routine, yet the analyses
by which sociologists typically go about them are problematic and seldom move
us beyond our expectations, and biases, to explore new meaningful subgroups.
Emerging machine learning methods allow researchers to explore sources of
variation that they may not have previously considered, or envisaged. In this
paper, we use causal trees to recursively partition the sample and uncover
sources of treatment effect heterogeneity. We use honest estimation, splitting
the sample into a training sample to grow the tree and an estimation sample to
estimate leaf-specific effects. Assessing a central topic in the social
inequality literature, college effects on wages, we compare what we learn from
conventional approaches for exploring variation in effects to causal trees.
Given our use of observational data, we use leaf-specific matching and
sensitivity analyses to address confounding and offer interpretations of
effects based on observed and unobserved heterogeneity. We encourage
researchers to follow similar practices in their work on variation in
sociological effects.
"
1909.09155,2020-08-26,An introduction to Bent Jorgensen's ideas,"  We briefly expose some key aspects of the theory and use of dispersion
models, for which Bent Jorgensen played a crucial role as a driving force and
an inspiration source. Starting with the general notion of dispersion models,
built using minimalistic mathematical assumptions, we specialize in two classes
of families of distributions with different statistical flavors: exponential
dispersion and proper dispersion models. The construction of dispersion models
involves the solution of integral equations that are, in general, untractable.
These difficulties disappear when a more mathematical structure is assumed: it
reduces to the calculation of a moment generating function or of a
Riemann-Stieltjes integral for the exponential dispersion and the proper
dispersion models, respectively. A new technique for constructing dispersion
models based on characteristic functions is introduced turning the integral
equations above into a tractable convolution equation and yielding examples of
dispersion models that are neither proper dispersion nor exponential dispersion
models. A corollary is that the cardinality of regular and non-regular
dispersion models are both large.
  Some selected applications are discussed including exponential families
non-linear models (for which generalized linear models are particular cases)
and several models for clustered and dependent data based on a latent Levy
process.
"
1909.12313,2020-03-10,A Conceptual Introduction to Markov Chain Monte Carlo Methods,"  Markov Chain Monte Carlo (MCMC) methods have become a cornerstone of many
modern scientific analyses by providing a straightforward approach to
numerically estimate uncertainties in the parameters of a model using a
sequence of random samples. This article provides a basic introduction to MCMC
methods by establishing a strong conceptual understanding of what problems MCMC
methods are trying to solve, why we want to use them, and how they work in
theory and in practice. To develop these concepts, I outline the foundations of
Bayesian inference, discuss how posterior distributions are used in practice,
explore basic approaches to estimate posterior-based quantities, and derive
their link to Monte Carlo sampling and MCMC. Using a simple toy problem, I then
demonstrate how these concepts can be used to understand the benefits and
drawbacks of various MCMC approaches. Exercises designed to highlight various
concepts are also included throughout the article.
"
1909.13186,2020-09-14,Causal screening for dynamical systems,"  Many classical algorithms output graphical representations of causal
structures by testing conditional independence among a set of random variables.
In dynamical systems, local independence can be used analogously as a testable
implication of the underlying data-generating process. We suggest some
inexpensive methods for causal screening which provide output with a sound
causal interpretation under the assumption of ancestral faithfulness. The
popular model class of linear Hawkes processes is used to provide an example of
a dynamical causal model. We argue that for sparse causal graphs the output
will often be close to complete. We give examples of this framework and apply
it to a challenging biological system.
"
1910.00006,2019-10-02,Spatial methods and their applications to environmental and climate data,"  Environmental and climate processes are often distributed over large
space-time domains. Their complexity and the amount of available data make
modelling and analysis a challenging task. Statistical modelling of environment
and climate data can have several different motivations including
interpretation or characterisation of the data. Results from statistical
analysis are often used as a integral part of larger environmental studies.
Spatial statistics is an active and modern statistical field, concerned with
the quantitative analysis of spatial data; their dependencies and
uncertainties. Spatio-temporal statistics extends spatial statistics through
the addition of time to the, two or three, spatial dimensions. The focus of
this introductory paper is to provide an overview of spatial methods and their
application to environmental and climate data. This paper also gives an
overview of several important topics including large data sets and
non-stationary covariance structures. Further, it is discussed how Bayesian
hierarchical models can provide a flexible way of constructing models.
Hierarchical models may seem to be a good solution, but they have challenges of
their own such as, parameter estimation. Finally, the application of
spatio-temporal models to the LANDCLIM data (LAND cover - CLIMate interactions
in NW Europe during the Holocene) will be discussed.
"
1910.00256,2021-03-11,"A review of problem- and team-based methods for teaching statistics in
  Higher Education","  The teaching of statistics in higher education in the UK is still largely
lecture-based. This is despite recommendations such as those given by the
American Statistical Association's GAISE report that more emphasis should be
placed on active learning strategies where students take more responsibility
for their own learning. One possible model is that of collaborative learning,
where students learn in groups through carefully crafted `problems', which has
long been suggested as a strategy for teaching statistics.
  In this article, we review two specific approaches that fall under the
collaborative learning model: problem- and team-based learning. We consider the
evidence for changing to this model of teaching in statistics, as well as give
practical suggestions on how this could be implemented in typical statistics
classes in Higher Education.
"
1910.02042,2019-10-07,"A reckless guide to P-values: local evidence, global errors","  This chapter demystifies P-values, hypothesis tests and significance tests,
and introduces the concepts of local evidence and global error rates. The local
evidence is embodied in \textit{this} data and concerns the hypotheses of
interest for \textit{this} experiment, whereas the global error rate is a
property of the statistical analysis and sampling procedure. It is shown using
simple examples that local evidence and global error rates can be, and should
be, considered together when making inferences. Power analysis for experimental
design for hypothesis testing are explained, along with the more locally
focussed expected P-values. Issues relating to multiple testing, HARKing, and
P-hacking are explained, and it is shown that, in many situation, their effects
on local evidence and global error rates are in conflict, a conflict that can
always be overcome by a fresh dataset from replication of key experiments.
Statistics is complicated, and so is science. There is no singular right way to
do either, and universally acceptable compromises may not exist. Statistics
offers a wide array of tools for assisting with scientific inference by
calibrating uncertainty, but statistical inference is not a substitute for
scientific inference. P-values are useful indices of evidence and deserve their
place in the statistical toolbox of basic pharmacologists.
"
1910.02381,2019-10-08,Scalings for Tokamak Energy Confinement,"  On the basis of an analysis of the ITER L-mode energy confinement database,
two new scaling expressions for tokamak L-mode energy confinement are proposed,
namely a power law scaling and an offset-linear scaling. The analysis indicates
that the present multiplicity of scaling expressions for the energy confinement
time TE in tokamaks (Goldston, Kaye, Odajima-Shimomura, Rebut-Lallia, etc.) is
due both to the lack of variation of a key parameter combination in the
database, fs = 0.32 R a^.75 k^ 5 ~ A a^.25 k^.5, and to variations in the
dependence of rE on the physical parameters among the different tokamaks in the
database. By combining multiples of fs and another factor, fq = 1.56 a^2 kB/R
Ip = qeng/3.2, which partially reflects the tokamak to tokamak variation of the
dependence of TE on q and therefore implicitly the dependence of TE on Ip and
n,., the two proposed confinement scaling expressions can be transformed to
forms very close to most of the common scaling expressions. To reduce the
multiplicity of the scalings for energy confinement, the database must be
improved by adding new data with significant variations in fs, and the physical
reasons for the tokamak to tokamak variation of some of the dependences of the
energy confinement time on tokamak parameters must be clarified
"
1910.03368,2019-10-10,"Computing the Expected Value of Sample Information Efficiently:
  Expertise and Skills Required for Four Model-Based Methods","  Objectives: Value of information (VOI) analyses can help policy-makers make
informed decisions about whether to conduct and how to design future studies.
Historically, a computationally expensive method to compute the Expected Value
of Sample Information (EVSI) restricted the use of VOI to simple decision
models and study designs. Recently, four EVSI approximation methods have made
such analyses more feasible and accessible. We provide practical
recommendations for analysts computing EVSI by evaluating these novel methods.
Methods: Members of the Collaborative Network for Value of Information (ConVOI)
compared the inputs, analyst's expertise and skills, and software required for
four recently developed approximation methods. Information was also collected
on the strengths and limitations of each approximation method. Results: All
four EVSI methods require a decision-analytic model's probabilistic sensitivity
analysis (PSA) output. One of the methods also requires the model to be re-run
to obtain new PSA outputs for each EVSI estimation. To compute EVSI, analysts
must be familiar with at least one of the following skills: advanced regression
modeling, likelihood specification, and Bayesian modeling. All methods have
different strengths and limitations, e.g., some methods handle evaluation of
study designs with more outcomes more efficiently while others quantify
uncertainty in EVSI estimates. All methods are programmed in the statistical
language R and two of the methods provide online applications. Conclusion: Our
paper helps to inform the choice between four efficient EVSI estimation
methods, enabling analysts to assess the methods' strengths and limitations and
select the most appropriate EVSI method given their situation and skills.
"
1910.03558,2019-10-09,A Step by Step Mathematical Derivation and Tutorial on Kalman Filters,"  We present a step by step mathematical derivation of the Kalman filter using
two different approaches. First, we consider the orthogonal projection method
by means of vector-space optimization. Second, we derive the Kalman filter
using Bayesian optimal filtering. We provide detailed proofs for both methods
and each equation is expanded in detail.
"
1910.04118,2019-10-10,"Experimental investigation of vertical turbulent transport of a passive
  scalar in a boundary layer: statistics and visibility graph analysis","  The dynamics of a passive scalar plume in a turbulent boundary layer is
experimentally investigated via vertical turbulent transport time-series. Data
are acquired in a rough-wall turbulent boundary layer that develops in a
recirculating wind tunnel set-up. Two source sizes in an elevated position are
considered in order to investigate the influence of the emission conditions on
the plume dynamics. The analysis is focused on the effects of the meandering
motion and the relative dispersion. First, classical statistics are
investigated. We found that (in accordance with previous studies) the
meandering motion is the main responsible for differences in the variance and
intermittency, as well as the kurtosis and power spectral density, between the
two source sizes. On the contrary, the mean and the skewness are slightly
affected by the emission conditions. To characterize the temporal structure of
the turbulent transport series, the visibility algorithm is exploited to carry
out a complex network-based analysis. Two network metrics -- the average peak
occurrence and the assortativity coefficient -- are analysed, as they can
capture the temporal occurrence of extreme events and their relative intensity
in the series. The effects of the meandering motion and the relative dispersion
of the plume are discussed in the view of the network metrics, revealing that a
stronger meandering motion is associated with higher values of both the average
peak occurrence and the assortativity coefficient. The network-based analysis
advances the level of information of classical statistics, by characterizing
the impact of the emission conditions on the temporal structure of the signals
in terms of extreme events and their relative intensity. In this way, complex
networks provide -- through the evaluation of network metrics -- an effective
tool for time-series analysis of experimental data.
"
1910.05271,2019-10-14,A Test for Shared Patterns in Cross-modal Brain Activation Analysis,"  Determining the extent to which different cognitive modalities (understood
here as the set of cognitive processes underlying the elaboration of a stimulus
by the brain) rely on overlapping neural representations is a fundamental issue
in cognitive neuroscience. In the last decade, the identification of shared
activity patterns has been mostly framed as a supervised learning problem. For
instance, a classifier is trained to discriminate categories (e.g. faces vs.
houses) in modality I (e.g. perception) and tested on the same categories in
modality II (e.g. imagery). This type of analysis is often referred to as
cross-modal decoding. In this paper we take a different approach and instead
formulate the problem of assessing shared patterns across modalities within the
framework of statistical hypothesis testing. We propose both an appropriate
test statistic and a scheme based on permutation testing to compute the
significance of this test while making only minimal distributional assumption.
We denote this test cross-modal permutation test (CMPT). We also provide
empirical evidence on synthetic datasets that our approach has greater
statistical power than the cross-modal decoding method while maintaining low
Type I errors (rejecting a true null hypothesis). We compare both approaches on
an fMRI dataset with three different cognitive modalities (perception, imagery,
visual search). Finally, we show how CMPT can be combined with Searchlight
analysis to explore spatial distribution of shared activity patterns.
"
1910.05818,2020-08-31,"A Bayesian Statistics Course for Undergraduates: Bayesian Thinking,
  Computing, and Research","  We propose a semester-long Bayesian statistics course for undergraduate
students with calculus and probability background. We cultivate students'
Bayesian thinking with Bayesian methods applied to real data problems. We
leverage modern Bayesian computing techniques not only for implementing
Bayesian methods, but also to deepen students' understanding of the methods.
Collaborative case studies further enrich students' learning and provide
experience to solve open-ended applied problems. The course has an emphasis on
undergraduate research, where accessible academic journal articles are read,
discussed, and critiqued in class. With increased confidence and familiarity,
students take the challenge of reading, implementing, and sometimes extending
methods in journal articles for their course projects.
"
1910.06964,2019-10-17,\texttt{code::proof}: Prepare for \emph{most} weather conditions,"  Computational tools for data analysis are being released daily on
repositories such as the Comprehensive R Archive Network. How we integrate
these tools to solve a problem in research is increasingly complex and
requiring frequent updates. To mitigate these \emph{Kafkaesque} computational
challenges in research, this manuscript proposes \emph{toolchain walkthrough},
an opinionated documentation of a scientific workflow. As a practical
complement to our proof-based argument~(Gray and Marwick, arXiv, 2019) for
reproducible data analysis, here we focus on the practicality of setting up a
reproducible research compendia, with unit tests, as a measure of
\texttt{code::proof}, confidence in computational algorithms.
"
1910.07325,2019-10-17,"Multivariate Forecasting Evaluation: On Sensitive and Strictly Proper
  Scoring Rules","  In recent years, probabilistic forecasting is an emerging topic, which is why
there is a growing need of suitable methods for the evaluation of multivariate
predictions. We analyze the sensitivity of the most common scoring rules,
especially regarding quality of the forecasted dependency structures.
Additionally, we propose scoring rules based on the copula, which uniquely
describes the dependency structure for every probability distribution with
continuous marginal distributions. Efficient estimation of the considered
scoring rules and evaluation methods such as the Diebold-Mariano test are
discussed. In detailed simulation studies, we compare the performance of the
renowned scoring rules and the ones we propose. Besides extended synthetic
studies based on recently published results we also consider a real data
example. We find that the energy score, which is probably the most widely used
multivariate scoring rule, performs comparably well in detecting forecast
errors, also regarding dependencies. This contradicts other studies. The
results also show that a proposed copula score provides very strong distinction
between models with correct and incorrect dependency structure. We close with a
comprehensive discussion on the proposed methodology.
"
1910.08880,2021-09-23,"Improved error rates for sparse (group) learning with Lipschitz loss
  functions","  We study a family of sparse estimators defined as minimizers of some
empirical Lipschitz loss function -- which include the hinge loss, the logistic
loss and the quantile regression loss -- with a convex, sparse or group-sparse
regularization. In particular, we consider the L1 norm on the coefficients, its
sorted Slope version, and the Group L1-L2 extension. We propose a new
theoretical framework that uses common assumptions in the literature to
simultaneously derive new high-dimensional L2 estimation upper bounds for all
three regularization schemes. %, and to improve over existing results. For L1
and Slope regularizations, our bounds scale as $(k^*/n) \log(p/k^*)$ --
$n\times p$ is the size of the design matrix and $k^*$ the dimension of the
theoretical loss minimizer $\B{\beta}^*$ -- and match the optimal minimax rate
achieved for the least-squares case. For Group L1-L2 regularization, our bounds
scale as $(s^*/n) \log\left( G / s^* \right) + m^* / n$ -- $G$ is the total
number of groups and $m^*$ the number of coefficients in the $s^*$ groups which
contain $\B{\beta}^*$ -- and improve over the least-squares case. We show that,
when the signal is strongly group-sparse, Group L1-L2 is superior to L1 and
Slope. In addition, we adapt our approach to the sub-Gaussian linear regression
framework and reach the optimal minimax rate for Lasso, and an improved rate
for Group-Lasso. Finally, we release an accelerated proximal algorithm that
computes the nine main convex estimators of interest when the number of
variables is of the order of $100,000s$.
"
1910.09407,2019-10-22,Incomplete Reparameterizations and Equivalent Metrics,"  Reparameterizing a probabilisitic system is common advice for improving the
performance of a statistical algorithm like Markov chain Monte Carlo, even
though in theory such reparameterizations should leave the system, and the
performance of any algorithm, invariant. In this paper I show how the
reparameterizations common in practice are only incomplete reparameterizations
which result in different interactions between a target probabilistic system
and a given algorithm. I then consider how these changing interactions manifest
in the context of Markov chain Monte Carlo algorithms defined on Riemannian
manifolds. In particular I show how any incomplete reparameterization is
equivalent to modifying the metric geometry directly.
"
1910.11616,2022-05-09,"baymedr: An R Package and Web Application for the Calculation of Bayes
  Factors for Superiority, Equivalence, and Non-Inferiority Designs","  Clinical trials often seek to determine the superiority, equivalence, or
non-inferiority of an experimental condition (e.g., a new drug) compared to a
control condition (e.g., a placebo or an already existing drug). The use of
frequentist statistical methods to analyze data for these types of designs is
ubiquitous even though they have several limitations. Bayesian inference
remedies many of these shortcomings and allows for intuitive interpretations.
In this article, we outline the frequentist conceptualization of superiority,
equivalence, and non-inferiority designs and discuss its disadvantages.
Subsequently, we explain how Bayes factors can be used to compare the relative
plausibility of competing hypotheses. We present baymedr, an R package and web
application, that provides user-friendly tools for the computation of Bayes
factors for superiority, equivalence, and non-inferiority designs. Instructions
on how to use baymedr are provided and an example illustrates how already
existing results can be reanalyzed with baymedr.
"
1910.13347,2019-10-30,"Jump balls, rating falls, and elite status: A sensitivity analysis of
  three quarterback rating statistics","  Quarterback performance can be difficult to rank, and much effort has been
spent in creating new rating systems. However, the input statistics for such
ratings are subject to randomness and factors outside the quarterback's
control. To investigate this variance, we perform a sensitivity analysis of
three quarterback rating statistics: the Traditional 1971 rating by Smith, the
Burke, and the Wages of Wins ratings. The comparisons are made at the team
level for the 32 NFL teams from 2002-2015, thus giving each case an even 16
games. We compute quarterback ratings for each offense with 1-5 additional
touchdowns, 1-5 fewer interceptions, 1-5 additional sacks, and a 1-5 percent
increase in the passing completion rate. Our sensitivity analysis provides
insight into whether an elite passing team could seem mediocre or vice versa
based on random outcomes. The results indicate that the Traditional rating is
the most sensitive statistic with respect to touchdowns, interceptions, and
completions, whereas the Burke rating is most sensitive to sacks. The analysis
suggests that team passing offense rankings are highly sensitive to aspects of
football that are out of the quarterback's hands (e.g., deflected passes that
lead to interceptions). Thus, on the margins, we show arguments about whether a
specific quarterback has entered the elite or remains mediocre are irrelevant.
"
1910.14018,2020-01-01,"Analytical representation of Gaussian processes in the
  $\mathcal{A}-\mathcal{T}$ plane","  Closed-form expressions, parametrized by the Hurst exponent $H$ and the
length $n$ of a time series, are derived for paths of fractional Brownian
motion (fBm) and fractional Gaussian noise (fGn) in the
$\mathcal{A}-\mathcal{T}$ plane, composed of the fraction of turning points
$\mathcal{T}$ and the Abbe value $\mathcal{A}$. The exact formula for
$\mathcal{A}_{\rm fBm}$ is expressed via Riemann $\zeta$ and Hurwitz $\zeta$
functions. A very accurate approximation, yielding a simple exponential form,
is obtained. Finite-size effects, introduced by the deviation of fGn's variance
from unity, and asymptotic cases are discussed. Expressions for $\mathcal{T}$
for fBm, fGn, and differentiated fGn are also presented. The same methodology,
valid for any Gaussian process, is applied to autoregressive moving average
processes, for which regions of availability of the $\mathcal{A}-\mathcal{T}$
plane are derived and given in analytic form. Locations in the
$\mathcal{A}-\mathcal{T}$ plane of some real-world examples as well as
generated data are discussed for illustration.
"
1911.00535,2022-09-01,"Think-aloud interviews: A tool for exploring student statistical
  reasoning","  Think-aloud interviews have been a valuable but underused tool in statistics
education research. Think-alouds, in which students narrate their reasoning in
real time while solving problems, differ in important ways from other types of
cognitive interviews and related education research methods. Beyond the uses
already found in the statistics literature -- mostly validating the wording of
statistical concept inventory questions and studying student misconceptions --
we suggest other possible use cases for think-alouds and summarize
best-practice guidelines for designing think-aloud interview studies. Using
examples from our own experiences studying the local student body for our
introductory statistics courses, we illustrate how research goals should inform
study-design decisions and what kinds of insights think-alouds can provide. We
hope that our overview of think-alouds encourages more statistics educators and
researchers to begin using this method.
"
1911.01607,2019-11-06,"Multivariate Time-Between-Events Monitoring -- An overview and some
  (overlooked) underlying complexities","  We review methods for monitoring multivariate time-between-events (TBE) data.
We present some underlying complexities that have been overlooked in the
literature. It is helpful to classify multivariate TBE monitoring applications
into two fundamentally different scenarios. One scenario involves monitoring
individual vectors of TBE data. The other involves the monitoring of several,
possibly correlated, temporal point processes in which events could occur at
different rates. We discuss performance measures and advise the use of
time-between-signal based metrics for the design and comparison of methods. We
re-evaluate an existing multivariate TBE monitoring method, offer some advice
and some directions for future research.
"
1911.03336,2021-10-07,"Hierarchical Clustering for Smart Meter Electricity Loads based on
  Quantile Autocovariances","  In order to improve the efficiency and sustainability of electricity systems,
most countries worldwide are deploying advanced metering infrastructures, and
in particular household smart meters, in the residential sector. This
technology is able to record electricity load time series at a very high
frequency rates, information that can be exploited to develop new clustering
models to group individual households by similar consumptions patterns. To this
end, in this work we propose three hierarchical clustering methodologies that
allow capturing different characteristics of the time series. These are based
on a set of ""dissimilarity"" measures computed over different features: quantile
auto-covariances, and simple and partial autocorrelations. The main advantage
is that they allow summarizing each time series in a few representative
features so that they are computationally efficient, robust against outliers,
easy to automatize, and scalable to hundreds of thousands of smart meters
series. We evaluate the performance of each clustering model in a real-world
smart meter dataset with thousands of half-hourly time series. The results show
how the obtained clusters identify relevant consumption behaviors of households
and capture part of their geo-demographic segmentation. Moreover, we apply a
supervised classification procedure to explore which features are more relevant
to define each cluster.
"
1911.04235,2020-02-26,Mittag-Leffler functions in superstatistics,"  Nowadays, there is a series of complexities in biophysics that require a
suitable approach to determine the measurable quantity. In this way, the
superstatistics has been an important tool to investigate dynamic aspects of
particles, organisms and substances immersed in systems with non-homogeneous
temperatures (or diffusivity). The superstatistics admits a general Boltzmann
factor that depends on the distribution of intensive parameters $\beta$
(inverse-diffusivity). Each value of intensive parameter is associated with a
local equilibrium in the system. In this work, we investigate the consequences
of Mittag-Leffler function on the definition of f-distribution of a complex
system. Thus, using the techniques belonging to the fractional calculus with
non-singular kernels, we constructed a distribution to intensive parameters
using the Mittag-Leffler function. This function implies distributions with
power-law behaviour to high energy values in the context of Cohen-Beck
superstatistics. This work aims to present the generalised probabilities
distribution in statistical mechanics under a new perspective of the
Mittag-Leffler function inspired in Atangana-Baleanu and Prabhakar forms.
"
1911.04317,2019-11-12,Machine Learning for high speed channel optimization,"  Design of printed circuit board (PCB) stack-up requires the consideration of
characteristic impedance, insertion loss and crosstalk. As there are many
parameters in a PCB stack-up design, the optimization of these parameters needs
to be efficient and accurate. A less optimal stack-up would lead to expensive
PCB material choices in high speed designs. In this paper, an efficient global
optimization method using parallel and intelligent Bayesian optimization is
proposed for the stripline design.
"
1911.04616,2019-11-13,Item Response Theory based Ensemble in Machine Learning,"  In this article, we propose a novel probabilistic framework to improve the
accuracy of a weighted majority voting algorithm. In order to assign higher
weights to the classifiers which can correctly classify hard-to-classify
instances, we introduce the Item Response Theory (IRT) framework to evaluate
the samples' difficulty and classifiers' ability simultaneously. Three models
are created with different assumptions suitable for different cases. When
making an inference, we keep a balance between the accuracy and complexity. In
our experiment, all the base models are constructed by single trees via
bootstrap. To explain the models, we illustrate how the IRT ensemble model
constructs the classifying boundary. We also compare their performance with
other widely used methods and show that our model performs well on 19 datasets.
"
1911.05610,2021-02-09,Online detection of cascading change-points,"  We propose an online detection procedure for cascading failures in the
network from sequential data, which can be modeled as multiple correlated
change-points happening during a short period. We consider a temporal diffusion
network model to capture the temporal dynamic structure of multiple
change-points and develop a sequential Shewhart procedure based on the
generalized likelihood ratio statistics based on the diffusion network model
assuming unknown post-change distribution parameters. We also tackle the
computational complexity posed by the unknown propagation. Numerical
experiments demonstrate the good performance for detecting cascade failures.
"
1911.08628,2020-01-23,Symbolic Formulae for Linear Mixed Models,"  A statistical model is a mathematical representation of an often simplified
or idealised data-generating process. In this paper, we focus on a particular
type of statistical model, called linear mixed models (LMMs), that is widely
used in many disciplines e.g.~agriculture, ecology, econometrics, psychology.
Mixed models, also commonly known as multi-level, nested, hierarchical or panel
data models, incorporate a combination of fixed and random effects, with LMMs
being a special case. The inclusion of random effects in particular gives LMMs
considerable flexibility in accounting for many types of complex correlated
structures often found in data. This flexibility, however, has given rise to a
number of ways by which an end-user can specify the precise form of the LMM
that they wish to fit in statistical software. In this paper, we review the
software design for specification of the LMM (and its special case, the linear
model), focusing in particular on the use of high-level symbolic model formulae
and two popular but contrasting R-packages in lme4 and asreml.
"
1911.09049,2021-01-26,Sharp hypotheses and bispatial inference,"  A fundamental class of inferential problems are those characterised by there
having been a substantial degree of pre-data (or prior) belief that the value
of a model parameter was equal or lay close to a specified value, which may,
for example, be the value that indicates the absence of an effect. Standard
ways of tackling problems of this type, including the Bayesian method, are
often highly inadequate in practice. To address this issue, an inferential
framework called bispatial inference is put forward, which can be viewed as
both a generalisation and radical reinterpretation of existing approaches to
inference that are based on P values. It is shown that to obtain an appropriate
post-data density function for a given parameter, it is often convenient to
combine a special type of bispatial inference, which is constructed around
one-sided P values, with a previously outlined form of fiducial inference.
Finally, by using what are called post-data opinion curves, this
bispatial-fiducial theory is naturally extended to deal with the general
scenario in which any number of parameters may be unknown. The application of
the theory is illustrated in various examples, which are especially relevant to
the analysis of clinical trial data.
"
1911.13170,2019-12-02,The Scenario Culture,"  Scenario Analysis is a risk assessment tool that aims to evaluate the impact
of a small number of distinct plausible future scenarios. In this paper, we
provide an overview of important aspects of Scenario Analysis including when it
is appropriate, the design of scenarios, uncertainty and encouraging
creativity. Each of these issues is discussed in the context of climate, energy
and legal scenarios.
"
1912.00568,2019-12-03,Inference for Synthetic Control Methods with Multiple Treated Units,"  Although the Synthetic Control Method (SCM) is now widely applied, its most
commonly-used inference method, placebo test, is often problematic, especially
when the treatment is not uniquely assigned. This paper discuss the problems
with the placebo test under multivariate treatment case. And, to improve the
power of inferences, I further propose an Andrews-type procedure as it
potentially solve some drawbacks of placebo test. Simulations are conducted to
show the Andrews' test is often valid and powerful, compared with the placebo
test.
"
1912.02819,2019-12-09,"The limits of the sample spiked eigenvalues for a high-dimensional
  generalized Fisher matrix and its applications","  A generalized spiked Fisher matrix is considered in this paper. We establish
a criterion for the description of the support of the limiting spectral
distribution of high-dimensional generalized Fisher matrix and study the almost
sure limits of the sample spiked eigenvalues where the population covariance
matrices are arbitrary which successively removed an unrealistic condition
posed in the previous works, that is, the covariance matrices are assumed to be
diagonal or diagonal block-wise structure. In addition, we also give a
consistent estimator of the population spiked eigenvalues. A series of
simulations are conducted that support the theoretical results and illustrate
the accuracy of our estimators.
"
1912.04432,2019-12-11,Variable selection for transportability,"  Transportability provides a principled framework to address the problem of
applying study results to new populations. Here, we consider the problem of
selecting variables to include in transport estimators. We provide a brief
overview of the transportability framework and illustrate that while selection
diagrams are a vital first step in variable selection, these graphs alone
identify a sufficient but not strictly necessary set of variables for
generating an unbiased transport estimate. Next, we conduct a simulation
experiment assessing the impact of including unnecessary variables on the
performance of the parametric g-computation transport estimator. Our results
highlight that the types of variables included can affect the bias, variance,
and mean squared error of the estimates. We find that addition of variables
that are not causes of the outcome but whose distributions differ between the
source and target populations can increase the variance and mean squared error
of the transported estimates. On the other hand, inclusion of variables that
are causes of the outcome (regardless of whether they modify the causal
contrast of interest or differ in distribution between the populations) reduces
the variance of the estimates without increasing the bias. Finally, exclusion
of variables that cause the outcome but do not modify the causal contrast of
interest does not increase bias. These findings suggest that variable selection
approaches for transport should prioritize identifying and including all causes
of the outcome in the study population rather than focusing on variables whose
distribution may differ between the study sample and target population.
"
1912.05588,2020-06-22,Parametric mode regression for bounded responses,"  We propose new parametric frameworks of regression analysis with the
conditional mode of a bounded response as the focal point of interest.
Covariate effects estimation and prediction based on the maximum likelihood
method under two new classes of regression models are demonstrated. We also
develop graphical and numerical diagnostic tools to detect various sources of
model misspecification. Predictions based on different central tendency
measures inferred using various regression models are compared using synthetic
data in simulations. Finally, we conduct regression analysis for data from the
Alzheimer's Disease Neuroimaging Initiative to demonstrate practical
implementation of the proposed methods. Supplementary materials that contain
technical details, and additional simulation and data analysis results are
available online.
"
1912.07442,2019-12-17,Time-based analysis of the NBA hot hand fallacy,"  The debate surrounding the hot hand in the NBA has been ongoing for many
years. However, many of the previous works on this theme has focused on only
the very next sequential shot attempt, often on very select players. This work
looks in more detail the effect of a made or missed shot on the next series of
shots over a two-year span, with time between shots shown to be a critical
factor in the analysis. Also, multi-year streakiness is analyzed, and all
indications are that players cannot really sustain their good (or bad) fortune
from year to year.
"
1912.08200,2019-12-19,Visualisation of Brain Statistics with R-packages ggseg and ggseg3d,"  There is an increased emphasis on visualizing neuroimaging results in more
intuitive ways. Common statistical tools for dissemination, such as bar charts,
lack the spatial dimension that is inherent in neuroimaging data. Here we
present two packages for the statistical software R, ggseg and ggseg3d, that
integrate this spatial component. The ggseg and ggseg3d packages visualize
pre-defined brain segmentations as both 2D polygons and 3D meshes,
respectively. Both packages are integrated with other well-established
R-packages, allowing great flexibility. In this tutorial, we present the main
data and functions in the ggseg and ggseg3d packages for brain atlas
visualization. The main highlighted functions are able to display brain
segmentation plots in R. Further, the accompanying ggsegExtra-package includes
a wider collection of atlases, and is intended for community-based efforts to
develop more compatible atlases to ggseg and ggseg3d. Overall, the
ggseg-packages facilitate parcellation-based visualizations in R, improve and
ease the dissemination of the results, and increase the efficiency of the
workflows.
"
1912.08378,2020-04-22,Spherically Restricted Random Hyperbolic Diffusion,"  This paper investigates solutions of hyperbolic diffusion equations in
$\mathbb{R}^3$ with random initial conditions. The solutions are given as
spatial-temporal random fields. Their restrictions to the unit sphere $S^2$ are
studied. All assumptions are formulated in terms of the angular power spectrum
or the spectral measure of the random initial conditions. Approximations to the
exact solutions are given. Upper bounds for the mean-square convergence rates
of the approximation fields are obtained. The smoothness properties of the
exact solution and its approximation are also investigated. It is demonstrated
that the H\""{o}lder-type continuity of the solution depends on the decay of the
angular power spectrum. Conditions on the spectral measure of initial
conditions that guarantee short or long-range dependence of the solutions are
given. Numerical studies are presented to verify the theoretical findings.
"
1912.10866,2021-09-14,Quantile Diffusions for Risk Analysis,"  We develop a novel approach for the construction of quantile processes
governing the stochastic dynamics of quantiles in continuous time. Two classes
of quantile diffusions are identified: the first, which we largely focus on,
features a dynamic random quantile level and allows for direct interpretation
of the resulting quantile process characteristics such as location, scale,
skewness and kurtosis, in terms of the model parameters. The second type are
function-valued quantile diffusions and are driven by stochastic parameter
processes, which determine the entire quantile function at each point in time.
By the proposed innovative and simple -- yet powerful -- construction method,
quantile processes are obtained by transforming the marginals of a diffusion
process under a composite map consisting of a distribution and a quantile
function. Such maps, analogous to rank transmutation maps, produce the
marginals of the resulting quantile process. We discuss the relationship and
differences between our approach and existing methods and characterisations of
quantile processes in discrete and continuous time. As an example of an
application of quantile diffusions, we show how probability measure
distortions, a form of dynamic tilting, can be induced. Though particularly
useful in financial mathematics and actuarial science, examples of which are
given in this work, measure distortions feature prominently across multiple
research areas. For instance, dynamic distributional approximations
(statistics), non-parametric and asymptotic analysis (mathematical statistics),
dynamic risk measures (econometrics), behavioural economics, decision making
(operations research), signal processing (information theory), and not least in
general risk theory including applications thereof, for example in the context
of climate change.
"
1912.11369,2019-12-25,"AVaN Pack: An Analytical/Numerical Solution for Variance-Based
  Sensitivity Analysis","  Sensitivity analysis is an important concept to analyze the influences of
parameters in a system, an equation or a collection of data. The methods used
for sensitivity analysis are divided into deterministic and statistical
techniques. Generally, deterministic techniques analyze fixed points of a model
whilst stochastic techniques analyze a range of values. Deterministic methods
fail in analyze the entire range of input values and stochastic methods
generate outcomes with random errors. In this manuscript, we are interested in
stochastic methods, mainly in variance-based techniques such as Variance and
Sobol indices, since this class of techniques is largely used on literature.
The objective of this manuscript is to present an analytical solution for
variance based sensitive analysis. As a result of this research, two small
programs were developed in Javascript named as AVaN Pack (Analysis of Variance
through Numerical solution). These programs allow users to find the
contribution of each individual parameter in any function by means of a
mathematical solution, instead of sampling-based ones.
"
1912.12527,2020-01-01,Bayesian estimation of large dimensional time varying VARs using copulas,"  This paper provides a simple, yet reliable, alternative to the (Bayesian)
estimation of large multivariate VARs with time variation in the conditional
mean equations and/or in the covariance structure. With our new methodology,
the original multivariate, n dimensional model is treated as a set of n
univariate estimation problems, and cross-dependence is handled through the use
of a copula. Thus, only univariate distribution functions are needed when
estimating the individual equations, which are often available in closed form,
and easy to handle with MCMC (or other techniques). Estimation is carried out
in parallel for the individual equations. Thereafter, the individual posteriors
are combined with the copula, so obtaining a joint posterior which can be
easily resampled. We illustrate our approach by applying it to a large
time-varying parameter VAR with 25 macroeconomic variables.
"
1912.13076,2022-09-01,"Expanding the scope of statistical computing: Training statisticians to
  be software engineers","  Traditionally, statistical computing courses have taught the syntax of a
particular programming language or specific statistical computation methods.
Since the publication of Nolan and Temple Lang (2010), we have seen a greater
emphasis on data wrangling, reproducible research, and visualization. This
shift better prepares students for careers working with complex datasets and
producing analyses for multiple audiences. But, we argue, statisticians are now
often called upon to develop statistical software, not just analyses, such as R
packages implementing new analysis methods or machine learning systems
integrated into commercial products. This demands different skills.
  We describe a graduate course that we developed to meet this need by focusing
on four themes: programming practices; software design; important algorithms
and data structures; and essential tools and methods. Through code review and
revision, and a semester-long software project, students practice all the
skills of software engineering. The course allows students to expand their
understanding of computing as applied to statistical problems while building
expertise in the kind of software development that is increasingly the province
of the working statistician. We see this as a model for the future evolution of
the computing curriculum in statistics and data science.
"
2001.00007,2020-01-03,"A generalization of the symmetrical and optimal
  probability-to-possibility transformations","  Possibility and probability theories are alternative and complementary ways
to deal with uncertainty, which has motivated over the last years an interest
for the study of ways to transform probability distributions into possibility
distributions and conversely. This paper studies the advantages and
shortcomings of two well-known discrete probability to possibility
transformations: the optimal transformation and the symmetrical transformation,
and presents a novel parametric family of probability to possibility
transformations which generalizes them and alleviate their shortcomings,
showing a big potential for practical application. The paper also introduces a
novel fuzzy measure of specificity for probability distributions based on the
concept of fuzzy subsethood and presents a empirical validation of the
generalized transformation usefulness applying it to the text authorship
attribution problem.
"
2001.01988,2020-11-10,"Implementing version control with Git and GitHub as a learning objective
  in statistics and data science courses","  A version control system records changes to a file or set of files over time
so that changes can be tracked and specific versions of a file can be recalled
later. As such, it is an essential element of a reproducible workflow that
deserves due consideration among the learning objectives of statistics and data
science courses. This paper describes experiences and implementation decisions
of four contributing faculty who are teaching different courses at a variety of
institutions. Each of these faculty have set version control as a learning
objective and successfully integrated one such system (Git) into one or more
statistics courses. The various approaches described in the paper span
different implementation strategies to suit student background, course type,
software choices, and assessment practices. By presenting a wide range of
approaches to teaching Git, the paper aims to serve as a resource for
statistics and data science instructors teaching courses at any level within an
undergraduate or graduate curriculum.
"
2001.02036,2020-01-08,"Selection Induced Contrast Estimate (SICE) Effect: An Attempt to
  Quantify the Impact of Some Patient Selection Criteria in Randomized Clinical
  Trials","  Defining the Inclusion/Exclusion (I/E) criteria of a trial is one of the most
important steps during a trial design. Increasingly complex I/E criteria
potentially create information imbalance and transparency issues between the
people who design and run the trials and those who consume the information
produced by the trials. In order to better understand and quantify the impact
of a category of I/E criteria on observed treatment effects, a concept, named
the Selection Induced Contrast Estimate (SICE) effect, is introduced and
formulated in this paper. The SICE effect can exist in controlled clinical
trials when treatment affects the correlation between a marker used for
selection and the response of interest. This effect is demonstrated with both
simulations and real clinical trial data. Although the statistical elements
behind the SICE effect have been well studied, explicitly formulating and
studying this effect can benefit several areas, including better transparency
in I/E criteria, meta-analysis of multiple clinical trials, treatment effect
interpretation in real-world medical practice, etc.
"
2001.02168,2022-06-22,"Exact Bayesian inference for discretely observed Markov Jump Processes
  using finite rate matrices","  We present new methodologies for Bayesian inference on the rate parameters of
a discretely observed continuous-time Markov jump processes with a countably
infinite state space. The usual method of choice for inference, particle Markov
chain Monte Carlo (particle MCMC), struggles when the observation noise is
small. We consider the most challenging regime of exact observations and
provide two new methodologies for inference in this case: the minimal extended
state space algorithm (MESA) and the nearly minimal extended state space
algorithm (nMESA). By extending the Markov chain Monte Carlo state space, both
MESA and nMESA use the exponentiation of finite rate matrices to perform exact
Bayesian inference on the Markov jump process even though its state space is
countably infinite. Numerical experiments show improvements over particle MCMC
of between a factor of three and several orders of magnitude.
"
2001.04110,2025-03-03,"Resolving the induction problem: Can we state with complete confidence
  via induction that the sun rises forever?","  Induction is a form of reasoning that starts with a particular example and
generalizes to a rule, namely, a hypothesis. However, establishing the truth of
a hypothesis is problematic due to the potential occurrence of conflicting
events, also known as the induction problem. The sunrise problem, first
introduced by Laplace (1814), is a quintessential example of the
probability-based induction. In his solution, a zero probability is always
assigned to the hypothesis that the sun rises forever, regardless of the number
of observations made. This is a symptom of fundamental deficiency of
probability-based induction: A hypothesis can never be accepted via the
Bayes-Laplace approach. Alternative priors have been proposed to address this
issue, but they have failed to fully overcome the deficiency. We investigate
why this occurs and demonstrate that the confidence does not exhibit such a
deficiency, as it is not a probability and therefore does not adhere to Bayes'
rule. The confidence is neither a likelihood to allow not only a reconciliation
between epistemic and aleatory interpretations of probability but also a
resolution in agreement with the evidence by enabling us to accept a hypothesis
with complete confidence as a rational decision.
"
2001.04237,2020-01-15,Exponential moving average versus moving exponential average,"  In this note we discuss the mathematical tools to define trend indicators
which are used to describe market trends. We explain the relation between
averages and moving averages on the one hand and the so called exponential
moving average (EMA) on the other hand. We present a lot of examples and give
the definition of the most frequently used trend indicator, the MACD, and
discuss its properties.
"
2001.04977,2020-12-17,"Error control in the numerical posterior distribution in the Bayesian UQ
  analysis of a semilinear evolution PDE","  We elaborate on results obtained in \cite{christen2018} for controlling the
numerical posterior error for Bayesian UQ problems, now considering forward
maps arising from the solution of a semilinear evolution partial differential
equation. Results in \cite{christen2018} demand an estimate for the absolute
global error (AGE) of the numeric forward map. Our contribution is a numerical
method for computing the AGE for semilinear evolution PDEs and shows the
potential applicability of \cite{christen2018} in this important wide range
family of PDEs. Numerical examples are given to illustrate the efficiency of
the proposed method, obtaining numerical posterior distributions for unknown
parameters that are nearly identical to the corresponding theoretical
posterior, by keeping their Bayes factor close to 1.
"
2001.05294,2020-01-16,Distributions of differences of Riemann zeta zeros,"  We study distributions of differences of unscaled Riemann zeta zeros,
$\gamma-\gamma'$, at large. We show, that independently of the location of the
zeros, their differences have similar statistical properties. The distributions
of differences are skewed towards the nearest zeta zero, have local maximum of
variance and local minimum of kurtosis at or near each zeta zero. Furthermore,
we show that distributions can be fitted with Johnson probability density
function, despite the value of skewness or kurtosis of the distribution.
"
2001.07625,2020-01-22,"MonteCarloMeasurements.jl: Nonlinear Propagation of Arbitrary
  Multivariate Distributions by means of Method Overloading","  This manuscript outlines a software package that facilitates working with
probability distributions by means of Monte-Carlo methods, in a way that allows
for propagation of multivariate probability distributions through arbitrary
functions. We provide a \emph{type} that represents probability distributions
by an internal vector of unweighted samples, \texttt{Particles}, which is a
subtype of a \texttt{Real} number and behaves just like a regular real number
in calculations by means of method overloading. This makes the software easy to
work with and presents minimal friction for the user. We highlight how this
design facilitates optimal usage of SIMD instructions and showcase the package
for uncertainty propagation through an off-the-shelf ODE solver, as well as for
robust probabilistic optimization with automatic differentiation.
"
2001.07648,2023-02-16,"When black box algorithms are (not) appropriate: a principled
  prediction-problem ontology","  In the 1980s a new, extraordinarily productive way of reasoning about
algorithms emerged. In this paper, we introduce the term ""outcome reasoning"" to
refer to this form of reasoning. Though outcome reasoning has come to dominate
areas of data science, it has been under-discussed and its impact
under-appreciated. For example, outcome reasoning is the primary way we reason
about whether ``black box'' algorithms are performing well. In this paper we
analyze outcome reasoning's most common form (i.e., as ""the common task
framework"") and its limitations. We discuss why a large class of
prediction-problems are inappropriate for outcome reasoning. As an example, we
find the common task framework does not provide a foundation for the deployment
of an algorithm in a real world situation. Building off of its core features,
we identify a class of problems where this new form of reasoning can be used in
deployment. We purposefully develop a novel framework so both technical and
non-technical people can discuss and identify key features of their prediction
problem and whether or not it is suitable for outcome reasoning.
"
2001.07649,2022-02-01,"Integrating data science ethics into an undergraduate major: A case
  study","  We present a programmatic approach to incorporating ethics into an
undergraduate major in statistical and data sciences. We discuss
departmental-level initiatives designed to meet the National Academy of
Sciences recommendation for integrating ethics into the curriculum from
top-to-bottom as our majors progress from our introductory courses to our
senior capstone course, as well as from side-to-side through co-curricular
programming. We also provide six examples of data science ethics modules used
in five different courses at our liberal arts college, each focusing on a
different ethical consideration. The modules are designed to be portable such
that they can be flexibly incorporated into existing courses at different
levels of instruction with minimal disruption to syllabi. We connect our
efforts to a growing body of literature on the teaching of data science ethics,
present assessments of our effectiveness, and conclude with next steps and
final thoughts.
"
2001.10440,2020-06-12,"Analyzing Factors Associated with Fatal Road Crashes: A Machine Learning
  Approach","  Road traffic injury accounts for a substantial human and economic burden
globally. Understanding risk factors contributing to fatal injuries is of
paramount importance. In this study, we proposed a model that adopts a hybrid
ensemble machine learning classifier structured from sequential minimal
optimization and decision trees to identify risk factors contributing to fatal
road injuries. The model was constructed, trained, tested, and validated using
the Lebanese Road Accidents Platform (LRAP) database of 8482 road crash
incidents, with fatality occurrence as the outcome variable. A sensitivity
analysis was conducted to examine the influence of multiple factors on fatality
occurrence. Seven out of the nine selected independent variables were
significantly associated with fatality occurrence, namely, crash type, injury
severity, spatial cluster-ID, and crash time (hour). Evidence gained from the
model data analysis will be adopted by policymakers and key stakeholders to
gain insights into major contributing factors associated with fatal road
crashes and to translate knowledge into safety programs and enhanced road
policies.
"
2001.10488,2022-11-15,"Statistical Consequences of Fat Tails: Real World Preasymptotics,
  Epistemology, and Applications","  The monograph investigates the misapplication of conventional statistical
techniques to fat tailed distributions and looks for remedies, when possible.
  Switching from thin tailed to fat tailed distributions requires more than
""changing the color of the dress"". Traditional asymptotics deal mainly with
either n=1 or $n=\infty$, and the real world is in between, under of the ""laws
of the medium numbers"" --which vary widely across specific distributions. Both
the law of large numbers and the generalized central limit mechanisms operate
in highly idiosyncratic ways outside the standard Gaussian or Levy-Stable
basins of convergence.
  A few examples:
  + The sample mean is rarely in line with the population mean, with effect on
""naive empiricism"", but can be sometimes be estimated via parametric methods.
  + The ""empirical distribution"" is rarely empirical.
  + Parameter uncertainty has compounding effects on statistical metrics.
  + Dimension reduction (principal components) fails.
  + Inequality estimators (GINI or quantile contributions) are not additive and
produce wrong results.
  + Many ""biases"" found in psychology become entirely rational under more
sophisticated probability distributions
  + Most of the failures of financial economics, econometrics, and behavioral
economics can be attributed to using the wrong distributions.
  This book, the first volume of the Technical Incerto, weaves a narrative
around published journal articles.
"
2001.10489,2020-01-29,"An efficient surrogate-aided importance sampling framework for
  reliability analysis","  Surrogates in lieu of expensive-to-evaluate performance functions can
accelerate the reliability analysis greatly. This paper proposes a new
two-stage framework for surrogate-aided reliability analysis named Surrogates
for Importance Sampling (S4IS). In the first stage, a coarse surrogate is built
to gain the information about failure regions; the second stage zooms into the
important regions and improves the accuracy of the failure probability
estimator by adaptively selecting support points therein. The learning
functions are proposed to guide the selection of support points such that the
exploration and exploitation can be dynamically balanced. As a generic
framework, S4IS has the potential to incorporate different types of surrogates
(Gaussian Processes, Support Vector Machines, Neural Network, etc.). The
effectiveness and efficiency of S4IS is validated by five illustrative
examples, which involve system reliability, highly nonlinear limit-state
function, small failure probability and moderately high dimensionality. The
implementation of S4IS is made available to download at
https://github.com/RobinSeaside/S4IS.
"
2002.00152,2020-03-17,"Predicting IoT Service Adoption towards Smart Mobility in Malaysia:
  SEM-Neural Hybrid Pilot Study","  Smart city is synchronized with digital environment and its transportation
system is vitalized with RFID sensors, Internet of Things (IoT) and Artificial
Intelligence. However, without user's behavioral assessment of technology, the
ultimate usefulness of smart mobility cannot be achieved. This paper aims to
formulate the research framework for prediction of antecedents of smart
mobility by using SEM-Neural hybrid approach towards preliminary data analysis.
This research undertook smart mobility services adoption in Malaysia as study
perspective and applied the Technology Acceptance Model (TAM) as theoretical
basis. An extended TAM model was hypothesized with five external factors
(digital dexterity, IoT service quality, intrusiveness concerns, social
electronic word of mouth and subjective norm). The data was collected through a
pilot survey in Klang Valley, Malaysia. Then responses were analyzed for
reliability, validity and accuracy of model. Finally, the causal relationship
was explained by Structural Equation Modeling (SEM) and Artificial Neural
Networking (ANN). The paper will share better understanding of road technology
acceptance to all stakeholders to refine, revise and update their policies. The
proposed framework will suggest a broader approach to individual level
technology acceptance.
"
2002.02832,2020-06-03,"Bayesian inference of quasi-linear radial diffusion parameters using Van
  Allen Probes","  The Van Allen radiation belts in the magnetosphere have been extensively
studied using models based on radial diffusion theory, which is based on a
quasi-linear approach with prescribed inner and outer boundary conditions. The
1-d diffusion model requires the knowledge of a diffusion coefficient and an
electron loss timescale, which are typically parameterized in terms of various
quantities such as the spatial ($L$) coordinate or a geomagnetic index (for
example, $Kp$). These terms are empirically derived, not directly measurable,
and hence are not known precisely, due to the inherent non-linearity of the
process and the variable boundary conditions. In this work, we demonstrate a
probabilistic approach by inferring the values of the diffusion and loss term
parameters, along with their uncertainty, in a Bayesian framework, where
identification is obtained using the Van Allen Probe measurements. Our results
show that the probabilistic approach statistically improves the performance of
the model, compared to the parameterization employed in the literature.
"
2002.04384,2020-02-12,"A Case Study of Promoting Informal Inferential Reasoning in Learning
  Sampling Distribution for High School Students","  Drawing inference from data is an important skill for students to understand
their everyday life, so that the sampling distribution as a central topic in
statistical inference is necessary to be learned by the students. However,
little is known about how to teach the topic for high school students,
especially in Indonesian context. Therefore, the present study provides a
teaching experiment to support the students' informal inferential reasoning in
understanding the sampling distribution, as well as the students' perceptions
toward the teaching experiment. The subjects in the present study were three
11th-grader of one private school in Yogyakarta majoring in mathematics and
natural science. The method of data collection was direct observation of
sampling distribution learning process, interviews, and documentation. The
present study found that that informal inferential reasoning with problem-based
learning using contextual problems and real data could support the students to
understand the sampling distribution, and they also gave positive responses
about their learning experience.
"
2002.04663,2020-02-13,"TDEFSI: Theory Guided Deep Learning Based Epidemic Forecasting with
  Synthetic Information","  Influenza-like illness (ILI) places a heavy social and economic burden on our
society. Traditionally, ILI surveillance data is updated weekly and provided at
a spatially coarse resolution. Producing timely and reliable high-resolution
spatiotemporal forecasts for ILI is crucial for local preparedness and optimal
interventions. We present TDEFSI (Theory Guided Deep Learning Based Epidemic
Forecasting with Synthetic Information), an epidemic forecasting framework that
integrates the strengths of deep neural networks and high-resolution
simulations of epidemic processes over networks. TDEFSI yields accurate
high-resolution spatiotemporal forecasts using low-resolution time series data.
During the training phase, TDEFSI uses high-resolution simulations of epidemics
that explicitly model spatial and social heterogeneity inherent in urban
regions as one component of training data. We train a two-branch recurrent
neural network model to take both within-season and between-season
low-resolution observations as features, and output high-resolution detailed
forecasts. The resulting forecasts are not just driven by observed data but
also capture the intricate social, demographic and geographic attributes of
specific urban regions and mathematical theories of disease propagation over
networks. We focus on forecasting the incidence of ILI and evaluate TDEFSI's
performance using synthetic and real-world testing datasets at the state and
county levels in the USA. The results show that, at the state level, our method
achieves comparable/better performance than several state-of-the-art methods.
At the county level, TDEFSI outperforms the other methods. The proposed method
can be applied to other infectious diseases as well.
"
2002.04916,2020-02-13,"Technology-enhanced pre-instructional peer assessment: Exploring
  students' perceptions in a Statistical Methods course","  There has been strong interest among higher education institution in
implementing technology-enhanced peer assessment as a tool for enhancing
students' learning. However, little is known on how to use the peer assessment
system in pre-instructional activities. This study aims to explore how
technology-enhanced peer assessment can be embedded into pre-instructional
activities to enhance students' learning. Therefore, the present study was an
explorative descriptive study that used the qualitative approach to attain the
research aim. This study used a questionnaire, students' reflections, and
interview in collecting student's perceptions toward the interventions. The
results suggest that the technology-enhanced pre-instructional peer assessment
helps students to prepare the new content acquisition and become a source of
students' motivation in improving their learning performance for the following
main body of the lesson. A set of practical suggestions is also proposed for
designing and implementing technology-enhanced pre-instructional peer
assessment.
"
2002.05222,2020-02-14,Inverse Ising techniques to infer underlying mechanisms from data,"  As a problem in data science the inverse Ising (or Potts) problem is to infer
the parameters of a Gibbs-Boltzmann distributions of an Ising (or Potts) model
from samples drawn from that distribution. The algorithmic and computational
interest stems from the fact that this inference task cannot be done
efficiently by the maximum likelihood criterion, since the normalizing constant
of the distribution (the partition function) can not be calculated exactly and
efficiently. The practical interest on the other hand flows from several
outstanding applications, of which the most well known has been predicting
spatial contacts in protein structures from tables of homologous protein
sequences. Most applications to date have been to data that has been produced
by a dynamical process which, as far as it is known, cannot be expected to
satisfy detailed balance. There is therefore no a priori reason to expect the
distribution to be of the Gibbs-Boltzmann type, and no a priori reason to
expect that inverse Ising (or Potts) techniques should yield useful
information. In this review we discuss two types of problems where progress
nevertheless can be made. We find that depending on model parameters there are
phases where, in fact, the distribution is close to Gibbs-Boltzmann
distribution, a non-equilibrium nature of the under-lying dynamics
notwithstanding. We also discuss the relation between inferred Ising model
parameters and parameters of the underlying dynamics.
"
2002.07671,2021-05-31,"Can visualization alleviate dichotomous thinking? Effects of visual
  representations on the cliff effect","  Common reporting styles for statistical results in scientific articles, such
as p-values and confidence intervals (CI), have been reported to be prone to
dichotomous interpretations, especially with respect to the null hypothesis
significance testing framework. For example when the p-value is small enough or
the CIs of the mean effects of a studied drug and a placebo are not
overlapping, scientists tend to claim significant differences while often
disregarding the magnitudes and absolute differences in the effect sizes. This
type of reasoning has been shown to be potentially harmful to science.
Techniques relying on the visual estimation of the strength of evidence have
been recommended to reduce such dichotomous interpretations but their
effectiveness has also been challenged. We ran two experiments on researchers
with expertise in statistical analysis to compare several alternative
representations of confidence intervals and used Bayesian multilevel models to
estimate the effects of the representation styles on differences in
researchers' subjective confidence in the results. We also asked the
respondents' opinions and preferences in representation styles. Our results
suggest that adding visual information to classic CI representation can
decrease the tendency towards dichotomous interpretations - measured as the
`cliff effect': the sudden drop in confidence around p-value 0.05 - compared
with classic CI visualization and textual representation of the CI with
p-values. All data and analyses are publicly available at
https://github.com/helske/statvis.
"
2002.07966,2021-04-16,"Integrated organic inference (IOI): A reconciliation of statistical
  paradigms","  It is recognised that the Bayesian approach to inference can not adequately
cope with all the types of pre-data beliefs about population quantities of
interest that are commonly held in practice. In particular, it generally
encounters difficulty when there is a lack of such beliefs over some or all the
parameters of a model, or within certain partitions of the parameter space
concerned. To address this issue, a fairly comprehensive theory of inference is
put forward called integrated organic inference that is based on a fusion of
Fisherian and Bayesian reasoning. Depending on the pre-data knowledge that is
held about any given model parameter, inferences are made about the parameter
conditional on all other parameters using one of three methods of inference,
namely organic fiducial inference, bispatial inference and Bayesian inference.
The full conditional post-data densities that result from doing this are then
combined using a framework that allows a joint post-data density for all the
parameters to be sensibly formed without requiring these full conditional
densities to be compatible. Various examples of the application of this theory
are presented. Finally, the theory is defended against possible criticisms
partially in terms of what was previously defined as generalised subjective
probability.
"
2002.08465,2020-02-21,"Descriptive and Predictive Analysis of Euroleague Basketball Games and
  the Wisdom of Basketball Crowds","  In this study we focus on the prediction of basketball games in the
Euroleague competition using machine learning modelling. The prediction is a
binary classification problem, predicting whether a match finishes 1 (home win)
or 2 (away win). Data is collected from the Euroleague's official website for
the seasons 2016-2017, 2017-2018 and 2018-2019, i.e. in the new format era.
Features are extracted from matches' data and off-the-shelf supervised machine
learning techniques are applied. We calibrate and validate our models. We find
that simple machine learning models give accuracy not greater than 67% on the
test set, worse than some sophisticated benchmark models. Additionally, the
importance of this study lies in the ""wisdom of the basketball crowd"" and we
demonstrate how the predicting power of a collective group of basketball
enthusiasts can outperform machine learning models discussed in this study. We
argue why the accuracy level of this group of ""experts"" should be set as the
benchmark for future studies in the prediction of (European) basketball games
using machine learning.
"
2002.09700,2020-02-25,Online Statistics Teaching and Learning,"  For statistics courses at all levels, teaching and learning online poses
challenges in different aspects. Particular online challenges include how to
effectively and interactively conduct exploratory data analyses, how to
incorporate statistical programming, how to include individual or team
projects, and how to present mathematical derivations efficiently and
effectively.
  This article draws from the authors' experience with seven different online
statistics courses to address some of the aforementioned challenges. One course
is an online exploratory data analysis course taught at Bowling Green State
University. A second course is an upper level Bayesian statistics course taught
at Vassar College and shared among 10 liberal arts colleges through a hybrid
model. We alo describes a five-course MOOC specialization on Coursera, offered
by Duke University.
"
2002.09713,2021-01-15,"Connections between statistical practice in elementary particle physics
  and the severity concept as discussed in Mayo's Statistical Inference as
  Severe Testing","  For many years, philosopher-of-statistics Deborah Mayo has been advocating
the concept of severe testing as a key part of hypothesis testing. Her recent
book, Statistical Inference as Severe Testing, is a comprehensive exposition of
her arguments in the context of a historical study of many threads of
statistical inference, both frequentist and Bayesian. Her foundational point of
view is called error statistics, emphasizing frequentist evaluation of the
errors called Type I and Type II in the Neyman-Pearson theory of frequentist
hypothesis testing. Since the field of elementary particle physics (also known
as high energy physics) has strong traditions in frequentist inference, one
might expect that something like the severity concept was independently
developed in the field. Indeed, I find that, at least operationally
(numerically), we high-energy physicists have long interpreted data in ways
that map directly onto severity. Whether or not we subscribe to Mayo's
philosophical interpretations of severity is a more complicated story that I do
not address here.
"
2002.11610,2020-02-27,Liquid Scorecards,"  Traditional credit scorecards are generalized additive models (GAMs) with
step functions as the component functions. The shapes of the step functions may
be constrained in order to satisfy the PILE (Palatability, Interpretability,
Legal, Explain-ability) constraints. Before 2003, FICO used Linear Programming
to find the traditional scorecard that approximately maximizes divergence
subject to the PILE constraints. In this paper, I introduce the Liquid
Scorecard, that allows the component functions to be, at least partially,
smooth curves. I use Quadratic Programming and B-Spline theory to find the
Liquid Scorecard that exactly maximizes divergence subject to the PILE
constraints. FICO uses aspects of this technology to develop the famous FICO
Credit Score.
"
2002.11767,2020-06-22,"""Playing the whole game"": A data collection and analysis exercise with
  Google Calendar","  We provide a computational exercise suitable for early introduction in an
undergraduate statistics or data science course that allows students to 'play
the whole game' of data science: performing both data collection and data
analysis. While many teaching resources exist for data analysis, such resources
are not as abundant for data collection given the inherent difficulty of the
task. Our proposed exercise centers around student use of Google Calendar to
collect data with the goal of answering the question 'How do I spend my time?'
On the one hand, the exercise involves answering a question with near universal
appeal, but on the other hand, the data collection mechanism is not beyond the
reach of a typical undergraduate student. A further benefit of the exercise is
that it provides an opportunity for discussions on ethical questions and
considerations that data providers and data analysts face in today's age of
large-scale internet-based data collection.
"
2002.12690,2021-06-09,Criteria for the numerical constant recognition,"  The need for recognition/approximation of functions in terms of elementary
functions/operations emerges in many areas of experimental mathematics,
numerical analysis, computer algebra systems, model building, machine learning,
approximation and data compression. One of the most underestimated methods is
the symbolic regression. In the article, reductionist approach is applied,
reducing full problem to constant functions, i.e, pure numbers (decimal,
floating-point). However, existing solutions are plagued by lack of solid
criteria distinguishing between random formula, matching approximately or
literally decimal expansion and probable ''exact'' (the best) expression match
in the sense of Occam's razor. In particular, convincing STOP criteria for
search were never developed. In the article, such a criteria, working in
statistical sense, are provided. Recognition process can be viewed as (1)
enumeration of all formulas in order of increasing Kolmogorov complexity K (2)
random process with appropriate statistical distribution (3) compression of a
decimal string. All three approaches are remarkably consistent, and provide
essentially the same limit for practical depth of search. Tested unique
formulas count must not exceed 1/sigma, where sigma is relative numerical error
of the target constant. Beyond that, further search is pointless, because, in
the view of approach (1), number of equivalent expressions within error bounds
grows exponentially; in view of (2), probability of random match approaches 1;
in view of (3) compression ratio much smaller than 1.
"
2003.01973,2020-03-05,"What Does the ""Mean"" Really Mean?","  The arithmetic average of a collection of observed values of a homogeneous
collection of quantities is often taken to be the most representative
observation. There are several arguments supporting this choice the moment of
inertia being the most familiar. But what does this mean?
  In this note, we bring forth the Kolmogorov-Nagumo point of view that the
arithmetic average is a special case of a sequence of functions of a special
kind, the quadratic and the geometric means being some of the other cases. The
median fails to belong to this class of functions. The Kolmogorov-Nagumo
interpretation is the most defensible and the most definitive one for the
arithmetic average, but its essence boils down to the fact that this average is
merely an abstraction which has meaning only within its mathematical set-up.
"
2003.02791,2020-03-06,"Exploiting disagreement between high-dimensional variable selectors for
  uncertainty visualization","  We propose Combined Selection and Uncertainty Visualizer (CSUV), which
estimates the set of true covariates in high-dimensional linear regression and
visualizes selection uncertainties by exploiting the (dis)agreement among
different base selectors. Our proposed method selects covariates that get
selected the most frequently by the different variable selection methods on
subsampled data. The method is generic and can be used with different existing
variable selection methods. We demonstrate its variable selection performance
using real and simulated data. The variable selection method and its
uncertainty illustration tool are publicly available as R package CSUV
(https://github.com/christineyuen/CSUV). The graphical tool is also available
online via https://csuv.shinyapps.io/csuv
"
2003.02941,2021-09-03,"Asymptotic relatively more efficient test with auxiliary information:
  the case of the $Z$-test and the chi-square test","  The main goal of this article is to study how an auxiliary information can be
used to improve the efficiency of two famous statistical tests: the $ Z$-test
and the chi-square test. Many definitions of auxiliary information can be found
in the statistical literature. In this article, the notion of auxiliary
information is discussed from a very general point of view and depends on the
relevant test. These two statistical tests are modified so that this
information is taken into account. It is shown in particular that the
efficiency of these new tests is improved in the sense of Pitman's ARE. Some
statistical examples illustrate the use of this method.
"
2003.03098,2020-03-09,"Bernoulli Trials With Skewed Propensities for Certification and
  Validation","  The impetus for writing this paper are the well publicized media reports that
software failure was the cause of the two recent mishaps of the Boeing 737 Max
aircraft. The problem considered here though, is a specific one, in the sense
that it endeavors to address the general matter of conditions under which an
item such as a drug, a material specimen, or a complex, system can be certified
for use based on a large number of Bernoulli trials, all successful. More
broadly, the paper is an attempt to answer the old and honorable philosophical
question, namely,"" when can empirical testing on its own validate a law of
nature?"" Our message is that the answer depends on what one starts with,
namely, what is one's prior distribution, what unknown does this prior
distribution endow, and what has been observed as data.
  The paper is expository in that it begins with a historical overview, and
ends with some new ideas and proposals for addressing the question posed. In
the sequel, it also articulates on Popper's notion of ""propensity"" and its role
in providing a proper framework for Bayesian inference under Bernoulli trials,
as well as the need to engage with posterior distributions that are
subjectively specified; that is, without a recourse to the usual Bayesian prior
to posterior iteration.
"
2003.03152,2020-03-09,How and Why Did Probability Theory Come About?,"  This paper is a top down historical perspective on the several phases in the
development of probability from its prehistoric origins to its modern day
evolution, as one of the key methodologies in artificial intelligence, data
science, and machine learning. It is written in honor of Barry Arnold's
birthday for his many contributions to statistical theory and methodology.
Despite the fact that much of Barry's work is technical, a descriptive document
to mark his achievements should not be viewed as being out of line. Barry's
dissertation adviser at Stanford (he received a Ph.D. in Statistics there) was
a philosopher of Science who dug deep in the foundations and roots of
probability, and it is this breadth of perspective is what Barry has inherent.
The paper is based on lecture materials compiled by the first author from
various published sources, and over a long period of time. The material below
gives a limited list of references, because the cast of characters is many, and
their contributions are a part of the historical heritage of those of us who
are interested in probability, statistics, and the many topics they have
spawned.
"
2003.03686,2020-03-10,"Flight restrictions from China during the COVID-2019 Coronavirus
  outbreak","  This short note provides estimates of the number of passengers that travel
from China to all world airports in the period October 2019 - March 2020 on the
basis of historical data. From this baseline we subtract the expected reduction
in the number of passengers taking into account the temporary ban of some
routes which was put in place since 23 January 2020 following the COVID-2019
Coronavirus outbreak. The results indicate a reduction of the number of
passengers in the period January - March 2020 of -2.5%. This calculation
considers only the complete closure of routes (not just direct flights) and not
the reduction in the number of passengers on still active direct and indirect
connections. At the moment of writing, with such partial information it is
premature to quantify economic losses on the civil air transport and tourism
industry. This note is meant to provide a baseline that be extended to all
countries of origin and updated as more recent data will become available.
"
2003.03970,2020-03-10,Bayes' Theorem under Conditional Independence,"  In this article we provide a substantial discussion on the statistical
concept of conditional independence, which is not routinely mentioned in most
elementary statistics and mathematical statistics textbooks. Under the
assumption of conditional independence, an extended version of Bayes' Theorem
is then proposed with illustrations from both hypothetical and real-world
examples of disease diagnosis.
"
2003.04008,2022-05-30,Anna Karenina and The Two Envelopes Problem,"  The Anna Karenina principle is named after the opening sentence in the
eponymous novel: Happy families are all alike; every unhappy family is unhappy
in its own way. The Two Envelopes Problem (TEP) is a much-studied paradox in
probability theory, mathematical economics, logic, and philosophy. Time and
again a new analysis is published in which an author claims finally to explain
what actually goes wrong in this paradox. Each author (the present author
included) emphasizes what is new in their approach and concludes that earlier
approaches did not get to the root of the matter. We observe that though a
logical argument is only correct if every step is correct, an apparently
logical argument which goes astray can be thought of as going astray at
different places. This leads to a comparison between the literature on TEP and
a successful movie franchise: it generates a succession of sequels, and even
prequels, each with a different director who approaches the same basic premise
in a personal way. We survey resolutions in the literature with a view to
synthesis, correct common errors, and give a new theorem on order properties of
an exchangeable pair of random variables, at the heart of most TEP variants and
interpretations. A theorem on asymptotic independence between the amount in
your envelope and the question whether it is smaller or larger shows that the
pathological situation of improper priors or infinite expectation values has
consequences as we merely approach such a situation.
"
2003.05510,2020-05-20,Optimal dose calibration in radiotherapy,"  In this paper, the tools provided by the theory of Optimal Experimental
Design are applied to a nonlinear calibration model. This is motivated by the
need of estimating radiation doses using radiochromic films for radiotherapy
purposes. The calibration model is in this case nonlinear and the explanatory
variable cannot be worked out explicitly from the model. In this case an
experimental design has to be found on the dependent variable. For that, the
inverse function theorem will be used to obtain an information matrix to be
optimized. Optimal designs on the response variable are computed from two
different perspectives, first for fitting the model and estimating each of the
parameters and then for predicting the proper dose to be applied to the
patient. While the first is a common point of view in a general context of the
Optimal Experimental Design, the latter is actually the main objective of the
calibration problem for the practitioners and algorithms for computing these
optimal designs are also provided.
"
2003.05814,2021-03-30,Level set and density estimation on manifolds,"  We tackle the problem of the estimation of the level sets L_f({\lambda}) of
the density f of a random vector X supported on a smooth manifold M\subsetR^d ,
from an iid sample of X. To do that we introduce a kernel-based estimator f^n,h
, which is a slightly modified version of the one proposed in [45], and proves
its a.s. uniform convergence to f . Then, we propose two estimators of L f
({\lambda}), the first one is a plug-in: L f^n,h ({\lambda}), which is proven
to be a.s. consistent in Hausdorff distance and distance in measure, if L
f({\lambda}) does not meet the boundary of M . While the second one assumes
that L f({\lambda}) is r-convex, and is estimated by means of the r-convex hull
of L f^n,h({\lambda}). The performance of our proposal is illustrated through
some simulated examples. In a real data example we analyze the intensity and
direction of strong and moderate winds.
"
2003.06500,2020-03-17,An R Autograder for PrairieLearn,"  We describe how we both use and extend the PrarieLearn framework by taking
advantage of its built-in support for external auto-graders. By using a custom
Docker container, we can match our course requirements perfectly. Moreover, by
relying on the flexibility of the interface we can customize our Docker
container. A specific extension for unit testing is described which creates
context-dependent difference between student answers and reference solution
providing a more comprehensive response at test time.
"
2003.06797,2020-03-17,On new data sources for the production of official statistics,"  In the past years we have witnessed the rise of new data sources for the
potential production of official statistics, which, by and large, can be
classified as survey, administrative, and digital data. Apart from the
differences in their generation and collection, we claim that their lack of
statistical metadata, their economic value, and their lack of ownership by data
holders pose several entangled challenges lurking the incorporation of new data
into the routinely production of official statistics. We argue that every
challenge must be duly overcome in the international community to bring new
statistical products based on these sources. These challenges can be naturally
classified into different entangled issues regarding access to data,
statistical methodology, quality, information technologies, and management. We
identify the most relevant to be necessarily tackled before new data sources
can be definitively considered fully incorporated into the production of
official statistics.
"
2003.09507,2020-03-24,Space Filling Split Plot Design using Fast Flexible Filling,"  In this article, an adaption of an algorithm for the creation of experimental
designs by Lekivetz and Jones (2015) is suggested, dealing with constraints
around randomization. Split-plot design of experiments is used, when the levels
of some factors cannot be modified as easily as others. While most split-plot
designs deal in the context of I-optimal or D-optimal designs for continuous
response outputs, a space filling design strategy is suggested in here. The
proposed designs are evaluated based on different design criteria, as well as
an analytical example.
"
2003.09650,2020-09-11,"Compound Poisson approximations in $\ell_p$-norm for sums of weakly
  dependent vectors","  The distribution of the sum of 1-dependent lattice vectors with supports on
coordinate axes is approximated by a multivariate compound Poisson distribution
and by signed compound Poisson measure. The local and $\ell_\alpha$-norms are
used to obtain the error bounds. The Heinrich method is used for the proofs.
"
2003.10234,2020-03-24,"Determining feature importance for actionable climate change mitigation
  policies","  Given the importance of public support for policy change and implementation,
public policymakers and researchers have attempted to understand the factors
associated with this support for climate change mitigation policy. In this
article, we compare the feasibility of using different supervised learning
methods for regression using a novel socio-economic data set which measures
public support for potential climate change mitigation policies. Following this
model selection, we utilize gradient boosting regression, a well-known
technique in the machine learning community, but relatively uncommon in public
policy and public opinion research, and seek to understand what factors among
the several examined in previous studies are most central to shaping public
support for mitigation policies in climate change studies. The use of this
method provides novel insights into the most important factors for public
support for climate change mitigation policies. Using national survey data, we
find that the perceived risks associated with climate change are more decisive
for shaping public support for policy options promoting renewable energy and
regulating pollutants. However, we observe a very different behavior related to
public support for increasing the use of nuclear energy where climate change
risk perception is no longer the sole decisive feature. Our findings indicate
that public support for renewable energy is inherently different from that for
nuclear energy reliance with the risk perception of climate change, dominant
for the former, playing a subdued role for the latter.
"
2003.10878,2020-12-10,The Gauss' Bayes Factor,"  In 'Theoria motus corporum coelestium in sectionibus conicis solem ambientum'
Gauss presents, as a theorem and with emphasis, the rule to update the ratio of
probabilities of complementary hypotheses, in the light of an observed event
which could be due to either of them. Although he focused on a priori equally
probable hypotheses, in order to solve the problem on which he was interested
in, the theorem can be easily extended to the general case. But, curiously, I
have not been able to find references to his result in the literature.
"
2003.11021,2020-11-12,"Exploring the Effects of COVID-19 Containment Policies on Crime: An
  Empirical Analysis of the Short-term Aftermath in Los Angeles","  This work investigates whether and how COVID-19 containment policies had an
immediate impact on crime trends in Los Angeles. The analysis is conducted
using Bayesian structural time-series and focuses on nine crime categories and
on the overall crime count, daily monitored from January 1st 2017 to March 28th
2020. We concentrate on two post-intervention time windows - from March 4th to
March 16th and from March 4th to March 28th 2020 - to dynamically assess the
short-term effects of mild and strict policies. In Los Angeles, overall crime
has significantly decreased, as well as robbery, shoplifting, theft, and
battery. No significant effect has been detected for vehicle theft, burglary,
assault with a deadly weapon, intimate partner assault, and homicide. Results
suggest that, in the first weeks after the interventions are put in place,
social distancing impacts more directly on instrumental and less serious
crimes. Policy implications are also discussed.
"
2003.11635,2020-03-27,Review of The Book of Why: The New Science of Cause and Effect,"  Book review published as: Aronow, Peter M. and Fredrik S\""avje (2020), ""The
Book of Why: The New Science of Cause and Effect."" Journal of the American
Statistical Association, 115: 482-485.
"
2003.12530,2020-03-30,"Identification of Choquet capacity in multicriteria sorting problems
  through stochastic inverse analysis","  In multicriteria decision aiding (MCDA), the Choquet integral has been used
as an aggregation operator to deal with the case of interacting decision
criteria. While the application of the Choquet integral for ranking problems
have been receiving most of the attention, this paper rather focuses on
multicriteria sorting problems (MCSP). In the Choquet integral context, a
practical problem that arises is related to the elicitation of parameters known
as the Choquet capacities. We address the problem of Choquet capacity
identification for MCSP by applying the Stochastic Acceptability Multicriteri
Analysis (SMAA), proposing the SMAA-S-Choquet method. The proposed method is
also able to model uncertain data that may be present in both decision matrix
and limiting profiles, the latter a parameter associated with the sorting
problematic. We also introduce two new descriptive measures in order to conduct
reverse analysis regarding the capacities: the Scenario Acceptability Index and
the Scenario Central Capacity vector.
"
2003.13518,2020-07-15,Ramsey's contributions to probability and legal theory,"  Review of Cheryl Misak, Frank Ramsey: A Sheer Excess of Powers (Oxford
University Press, 2020).
"
2004.00527,2020-10-06,"Globally intensity-reweighted estimators for $K$- and pair correlation
  functions","  We introduce new estimators of the inhomogeneous $K$-function and the pair
correlation function of a spatial point process as well as the cross
$K$-function and the cross pair correlation function of a bivariate spatial
point process under the assumption of second-order intensity-reweighted
stationarity. These estimators rely on a 'global' normalization factor which
depends on an aggregation of the intensity function, whilst the existing
estimators depend 'locally' on the intensity function at the individual
observed points. The advantages of our new global estimators over the existing
local estimators are demonstrated by theoretical considerations and a
simulation study.
"
2004.00973,2020-04-03,A Monte Carlo comparison of categorical tests of independence,"  The $X^2$ and $G^2$ tests are the most frequently applied tests for testing
the independence of two categorical variables. However, no one, to the best of
our knowledge has compared them, extensively, and ultimately answer the
question of which to use and when. Further, their applicability in cases with
zero frequencies has been debated and (non parametric) permutation tests are
suggested. In this work we perform extensive Monte Carlo simulation studies
attempting to answer both aforementioned points. As expected, in large sample
sized cases ($>1,000$) the $X^2$ and $G^2$ are indistinguishable. In the small
sample sized cases ($\leq 1,000$) though, we provide strong evidence supporting
the use of the $X^2$ test regardless of zero frequencies for the case of
unconditional independence. Also, we suggest the use of the permutation based
$G^2$ test for testing conditional independence, at the cost of being
computationally more expensive. The $G^2$ test exhibited inferior performance
and its use should be limited.
"
2004.01534,2020-10-16,"Layer entanglement in multiplex, temporal multiplex, and coupled
  multilayer networks","  Complex networks, such as transportation networks, social networks, or
biological networks, capture the complex system they model often by
representing only one type of interactions. In real world systems, there may be
many different aspects that connect entities together. These can be captured
using multilayer networks, which combine different modalities of interactions
in a single model. Coupling in multilayer networks may exhibit different
properties which can be related to the very nature of the data they model (or
to events in time-dependant data). We hypothesise that such properties may be
reflected in the way layers are intertwined. In this paper, we investigated
these through the prism of layer entanglement in coupled multilayer networks.
We test over 30 real-life networks in 6 different disciplines (social, genetic,
transport, co-authorship, trade, and neuronal networks). We further propose a
random generator, displaying comparable patterns of elementary layer
entanglement and transition coupling entanglement across 1,329,696 synthetic
coupled multilayer networks. Our experiments demonstrate difference of layer
entanglement across disciplines, and even suggest a link between entanglement
intensity and homophily. We additionally study entanglement in 3 real world
temporal datasets displaying a potential rise in entanglement activity prior to
other network activity.
"
2004.02708,2020-04-07,"Sequential adaptive strategy for population-based sampling of a rare and
  clustered disease","  An innovative sampling strategy is proposed, which applies to large-scale
population-based surveys targeting a rare trait that is unevenly spread over a
geographical area of interest. Our proposal is characterised by the ability to
tailor the data collection to specific features and challenges of the survey at
hand. It is based on integrating an adaptive component into a sequential
selection, which aims to both intensify detection of positive cases, upon
exploiting the spatial clusterisation, and provide a flexible framework for
managing logistical and budget constraints. To account for the selection bias,
a ready-to-implement weighting system is provided to release unbiased and
accurate estimates. Empirical evidence is illustrated from tuberculosis
prevalence surveys, which are recommended in many countries and supported by
the WHO as an emblematic example of the need for an improved sampling design.
Simulation results are also given to illustrate strengths and weaknesses of the
proposed sampling strategy with respect to traditional cross-sectional
sampling.
"
2004.04019,2020-04-09,"A machine learning methodology for real-time forecasting of the
  2019-2020 COVID-19 outbreak using Internet searches, news alerts, and
  estimates from mechanistic models","  We present a timely and novel methodology that combines disease estimates
from mechanistic models with digital traces, via interpretable machine-learning
methodologies, to reliably forecast COVID-19 activity in Chinese provinces in
real-time. Specifically, our method is able to produce stable and accurate
forecasts 2 days ahead of current time, and uses as inputs (a) official health
reports from Chinese Center Disease for Control and Prevention (China CDC), (b)
COVID-19-related internet search activity from Baidu, (c) news media activity
reported by Media Cloud, and (d) daily forecasts of COVID-19 activity from
GLEAM, an agent-based mechanistic model. Our machine-learning methodology uses
a clustering technique that enables the exploitation of geo-spatial
synchronicities of COVID-19 activity across Chinese provinces, and a data
augmentation technique to deal with the small number of historical disease
activity observations, characteristic of emerging outbreaks. Our model's
predictive power outperforms a collection of baseline models in 27 out of the
32 Chinese provinces, and could be easily extended to other geographies
currently affected by the COVID-19 outbreak to help decision makers.
"
2004.04265,2020-04-10,"Extremist ideology as a complex contagion: the spread of far-right
  radicalization in the United States between 2005-2017","  Increasing levels of far-right extremist violence have generated public
concern about the spread of radicalization in the United States. Previous
research suggests that radicalized individuals are destabilized by various
environmental (or endemic) factors, exposed to extremist ideology, and
subsequently reinforced by members of their community. As such, the spread of
radicalization may proceed through a social contagion process, in which
extremist ideologies behave like complex contagions that require multiple
exposures for adoption. In this study, I applied an epidemiological method
called two-component spatio-temporal intensity modeling to data from 416
far-right extremists exposed in the United States between 2005 and 2017. The
results indicate that patterns of far-right radicalization in the United States
are consistent with a complex contagion process, in which reinforcement is
required for transmission. Both social media usage and group membership enhance
the spread of extremist ideology, suggesting that online and physical
organizing remain primary recruitment tools of the far-right movement.
Additionally, I identified several endemic factors, such as poverty, that
increase the probability of radicalization in particular regions. Future
research should investigate how specific interventions, such as online
counter-narratives to battle propaganda, may be effectively implemented to
mitigate the spread of far-right extremism in the United States.
"
2004.04267,2020-04-10,On Weighted Generalized Entropy for Double Truncated Distribution,"  The notion of weighted Renyi's entropy for truncated random variables has
recently been proposed in the information-theoretic literature. In this paper,
we introduce a generalized measure of it for double truncated distribution,
namely weighted generalized interval entropy (WGIE), and study it in the
context of reliability analysis. Several properties, including monotonicity,
bounds and uniqueness of WGIE are investigated. Moreover, a simulation study is
carried out to demonstrate the performance of the estimates of the proposed
measure using simulated and real data sets. The role of WGIE in reliability
modeling has also been investigated for a real-life problem.
"
2004.04373,2020-08-26,"A Review of Vibration-Based Damage Detection in Civil Structures: From
  Traditional Methods to Machine Learning and Deep Learning Applications","  Monitoring structural damage is extremely important for sustaining and
preserving the service life of civil structures. While successful monitoring
provides resolute and staunch information on the health, serviceability,
integrity and safety of structures; maintaining continuous performance of a
structure depends highly on monitoring the occurrence, formation and
propagation of damage. Damage may accumulate on structures due to different
environmental and human-induced factors. Numerous monitoring and detection
approaches have been developed to provide practical means for early warning
against structural damage or any type of anomaly. Considerable effort has been
put into vibration-based methods, which utilize the vibration response of the
monitored structure to assess its condition and identify structural damage.
Meanwhile, with emerging computing power and sensing technology in the last
decade, Machine Learning (ML) and especially Deep Learning (DL) algorithms have
become more feasible and extensively used in vibration-based structural damage
detection with elegant performance and often with rigorous accuracy. While
there have been multiple review studies published on vibration-based structural
damage detection, there has not been a study where the transition from
traditional methods to ML and DL methods are described and discussed. This
paper aims to fulfill this gap by presenting the highlights of the traditional
methods and provide a comprehensive review of the most recent applications of
ML and DL algorithms utilized for vibration-based structural damage detection
in civil structures.
"
2004.04734,2020-05-26,"Learning as We Go: An Examination of the Statistical Accuracy of COVID19
  Daily Death Count Predictions","  This paper provides a formal evaluation of the predictive performance of a
model (and its various updates) developed by the Institute for Health Metrics
and Evaluation (IHME) for predicting daily deaths attributed to COVID19 for
each state in the United States. The IHME models have received extensive
attention in social and mass media, and have influenced policy makers at the
highest levels of the United States government. For effective policy making the
accurate assessment of uncertainty, as well as accurate point predictions, are
necessary because the risks inherent in a decision must be taken into account,
especially in the present setting of a novel disease affecting millions of
lives. To assess the accuracy of the IHME models, we examine both forecast
accuracy as well as the predictive performance of the 95% prediction intervals
provided by the IHME models. We find that the initial IHME model underestimates
the uncertainty surrounding the number of daily deaths substantially.
Specifically, the true number of next day deaths fell outside the IHME
prediction intervals as much as 70% of the time, in comparison to the expected
value of 5%. In addition, we note that the performance of the initial model
does not improve with shorter forecast horizons. Regarding the updated models,
our analyses indicate that the later models do not show any improvement in the
accuracy of the point estimate predictions. In fact, there is some evidence
that this accuracy has actually decreased over the initial models. Moreover,
when considering the updated models, while we observe a larger percentage of
states having actual values lying inside the 95% prediction intervals (PI), our
analysis suggests that this observation may be attributed to the widening of
the PIs. The width of these intervals calls into question the usefulness of the
predictions to drive policy making and resource allocation.
"
2004.05217,2020-04-14,"Multiple repairable systems under dependent competing risks with
  nonparametric Frailty","  The aim of this article is to analyze data from multiple repairable systems
under the presence of dependent competing risks. In order to model this
dependence structure, we adopted the well-known shared frailty model. This
model provides a suitable theoretical basis for generating dependence between
the components failure times in the dependent competing risks model. It is
known that the dependence effect in this scenario influences the estimates of
the model parameters. Hence, under the assumption that the cause-specific
intensities follow a PLP, we propose a frailty-induced dependence approach to
incorporate the dependence among the cause-specific recurrent processes.
Moreover, the misspecification of the frailty distribution may lead to errors
when estimating the parameters of interest. Because of this, we considered a
Bayesian nonparametric approach to model the frailty density in order to offer
more flexibility and to provide consistent estimates for the PLP model, as well
as insights about heterogeneity among the systems. Both simulation studies and
real case studies are provided to illustrate the proposed approaches and
demonstrate their validity.
"
2004.05796,2020-10-09,"Explicit Estimation of Derivatives from Data and Differential Equations
  by Gaussian Process Regression","  In this work, we employ the Bayesian inference framework to solve the problem
of estimating the solution and particularly, its derivatives, which satisfy a
known differential equation, from the given noisy and scarce observations of
the solution data only. To address the key issue of accuracy and robustness of
derivative estimation, we use the Gaussian processes to jointly model the
solution, the derivatives, and the differential equation. By regarding the
linear differential equation as a linear constraint, a Gaussian process
regression with constraint method (GPRC) is developed to improve the accuracy
of prediction of derivatives. For nonlinear differential equations, we propose
a Picard-iteration-like approximation of linearization around the Gaussian
process obtained only from data so that our GPRC can be still iteratively
applicable. Besides, a product of experts method is applied to ensure the
initial or boundary condition is considered to further enhance the prediction
accuracy of the derivatives. We present several numerical results to illustrate
the advantages of our new method in comparison to the standard data-driven
Gaussian process regression.
"
2004.06054,2020-04-14,"Decomposition of Total Effect with the Notion of Natural Counterfactual
  Interaction Effect","  Mediation analysis serves as a crucial tool to obtain causal inference based
on directed acyclic graphs, which has been widely employed in the areas of
biomedical science, social science, epidemiology and psychology. Decomposition
of total effect provides a deep insight to fully understand the casual
contribution from each path and interaction term. Since the four-way
decomposition method was proposed to identify the mediated interaction effect
in counterfactual framework, the idea had been extended to a more sophisticated
scenario with non-sequential multiple mediators. However, the method exhibits
limitations as the causal structure contains direct causal edges between
mediators, such as inappropriate modeling of dependence and
non-identifiability. We develop the notion of natural counterfactual
interaction effect and find that the decomposition of total effect can be
consistently realized with our proposed notion. Furthermore, natural
counterfactual interaction effect overcomes the drawbacks and possesses a clear
and significant interpretation, which may largely improve the capacity of
researchers to analyze highly complex causal structures.
"
2004.06178,2020-04-27,Estimating the COVID-19 Infection Rate: Anatomy of an Inference Problem,"  As a consequence of missing data on tests for infection and imperfect
accuracy of tests, reported rates of population infection by the SARS CoV-2
virus are lower than actual rates of infection. Hence, reported rates of severe
illness conditional on infection are higher than actual rates. Understanding
the time path of the COVID-19 pandemic has been hampered by the absence of
bounds on infection rates that are credible and informative. This paper
explains the logical problem of bounding these rates and reports illustrative
findings, using data from Illinois, New York, and Italy. We combine the data
with assumptions on the infection rate in the untested population and on the
accuracy of the tests that appear credible in the current context. We find that
the infection rate might be substantially higher than reported. We also find
that the infection fatality rate in Italy is substantially lower than reported.
"
2004.06464,2020-08-14,Winning by hiding behind others: An analysis of speed skating data,"  In some athletic races, such as cycling and types of speed skating races,
athletes have to complete a relatively long distance at a high speed in the
presence of direct opponents. To win such a race, athletes are motivated to
hide behind others to suppress energy consumption before a final moment of the
race. This situation seems to produce a social dilemma: players want to hide
behind others, whereas if a group of players attempts to do so, they may all
lose to other players that overtake them. To support that speed skaters are
involved in such a social dilemma, we analyzed video footage data for 14 mass
start skating races to find that skaters that hid behind others to avoid air
resistance for a long time before the final lap tended to win. Furthermore, the
finish rank of the skaters in mass start races was independent of the record of
the same skaters in time-trial races measured in the absence of direct
opponents. The results suggest that how to strategically cope with a skater's
dilemma may be a key determinant for winning long-distance and high-speed races
with direct opponents.
"
2004.06721,2020-04-16,"Daily growth rate of scientific production on Covid-19. Analysis in
  databases and open access repositories","  The scientific community is facing one of its greatest challenges in solving
a global health problem: COVID-19 pandemic. This situation has generated an
unprecedented volume of publications. What is the volume, in terms of
publications, of research on COVID-19? The general objective of this research
work is to obtain a global vision of the daily growth of scientific production
on COVID-19 in different databases (Dimensions, Web of Science Core Collection,
Scopus-Elsevier, Pubmed and eight repositories). In relation to the results
obtained, Dimensions indexes a total of 9435 publications (69% with peer review
and 2677 preprints) well above Scopus (1568) and WoS (718). This is a classic
biliometric phenomenon of exponential growth (R2 = 0.92). The global growth
rate is 500 publications and the production doubles every 15 days. In the case
of Pubmed the weekly growth is around 1000 publications. Of the eight
repositories analysed, Pubmed Central, Medrxiv and SSRN are the leaders.
Despite their enormous contribution, the journals continue to be the core of
scientific communication. Finally, it has been established that three out of
every four publications on the COVID-19 are available in open access. The
information explosion demands a serious and coordinated response from
information professionals, which places us at the centre of the information
pandemic.
"
2004.07052,2020-05-29,"Social network-based distancing strategies to flatten the COVID 19 curve
  in a post-lockdown world","  Social distancing and isolation have been introduced widely to counter the
COVID-19 pandemic. However, more moderate contact reduction policies become
desirable owing to adverse social, psychological, and economic consequences of
a complete or near-complete lockdown. Adopting a social network approach, we
evaluate the effectiveness of three targeted distancing strategies designed to
'keep the curve flat' and aid compliance in a post-lockdown world. These are
limiting interaction to a few repeated contacts, seeking similarity across
contacts, and strengthening communities via triadic strategies. We simulate
stochastic infection curves that incorporate core elements from infection
models, ideal-type social network models, and statistical relational event
models. We demonstrate that strategic reduction of contact can strongly
increase the efficiency of social distancing measures, introducing the
possibility of allowing some social contact while keeping risks low. This
approach provides nuanced insights to policy makers for effective social
distancing that can mitigate negative consequences of social isolation.
"
2004.07859,2022-09-16,"Coronavirus (COVID-19): ARIMA based time-series analysis to forecast
  near future","  COVID-19, a novel coronavirus, is currently a major worldwide threat. It has
infected more than a million people globally leading to hundred-thousands of
deaths. In such grave circumstances, it is very important to predict the future
infected cases to support prevention of the disease and aid in the healthcare
service preparation. Following that notion, we have developed a model and then
employed it for forecasting future COVID-19 cases in India. The study indicates
an ascending trend for the cases in the coming days. A time series analysis
also presents an exponential increase in the number of cases. It is supposed
that the present prediction models will assist the government and medical
personnel to be prepared for the upcoming conditions and have more readiness in
healthcare systems.
"
2004.09010,2020-04-21,"Leveraging Big Data Analytics in Healthcare Enhancement: Trends,
  Challenges and Opportunities","  Clinicians decisions are becoming more and more evidence-based meaning in no
other field the big data analytics so promising as in healthcare. Due to the
sheer size and availability of healthcare data, big data analytics has
revolutionized this industry and promises us a world of opportunities. It
promises us the power of early detection, prediction, prevention and helps us
to improve the quality of life. Researchers and clinicians are working to
inhibit big data from having a positive impact on health in the future.
Different tools and techniques are being used to analyze, process, accumulate,
assimilate and manage large amount of healthcare data either in structured or
unstructured form. In this paper, we would like to address the need of big data
analytics in healthcare: why and how can it help to improve life?. We present
the emerging landscape of big data and analytical techniques in the five
sub-disciplines of healthcare i.e.medical image analysis and imaging
informatics, bioinformatics, clinical informatics, public health informatics
and medical signal analytics. We presents different architectures, advantages
and repositories of each discipline that draws an integrated depiction of how
distinct healthcare activities are accomplished in the pipeline to facilitate
individual patients from multiple perspectives. Finally the paper ends with the
notable applications and challenges in adoption of big data analytics in
healthcare.
"
2004.10326,2020-04-23,"Comparison of Clinical Episode Outcomes between Bundled Payments for
  Care Improvement (BPCI) Initiative Participants and Non-Participants","  Objective: To evaluate differences in major outcomes between Bundled Payments
for Care Improvement (BPCI) participating providers and non-participating
providers for both Major Joint Replacement of the Lower Extremity (MJRLE) and
Acute Myocardial Infarction (AMI) episodes. Methods: A
difference-in-differences approach estimated the differential change in
outcomes for Medicare beneficiaries who had an MJRLE or AMI at a BPCI
participating hospital between the baseline (January 2011 through September
2013) and intervention (October 2013 through December 2016) periods and
beneficiaries with the same episode (MJRLE or AMI) at a matched comparison
hospital. Main Outcomes and Measures: Medicare payments, LOS, and readmissions
during the episode, which includes the anchor hospitalization and the 90-day
post discharge period. Results: Mean total Medicare payments for an MJRLE
episode and the 90-day post discharge period declined $444 more (p < 0.0001)
for Medicare beneficiaries with episodes initiated in a BPCI-participating
provider than for the beneficiaries in a comparison provider. This reduction
was mainly due to reduced institutional post-acute care (PAC) payments. Slight
reductions in carrier payments and LOS were estimated. Readmission rates were
not statistically different between the BPCI and the comparison populations.
These findings suggest that PAC use can be reduced without adverse effects on
recovery from MJRLE. The lack of statistically significant differences in
effects for AMI could be explained by a smaller sample size or more
heterogenous recovery paths in AMI. Conclusions: Our findings suggest that, as
currently designed, bundled payments can be effective in reducing payments for
MJRLE episodes of care, but not necessarily for AMI. Most savings came from the
declines in PAC. These findings are consistent with the results reported in the
BPCI model evaluation for CMS.
"
2004.11267,2021-05-06,Hierarchical Bayesian propulsion power models for marine vessels,"  Assessing the magnitude of fuel consumption of marine traffic is a
challenging task. The consumption can be reduced by the ways the vessels are
operated, to achieve both improved cost efficiency and reduced CO2 emissions.
Mathematical models for predicting ships' consumption are in a central role in
both of these tasks. Nowadays, many ships are equipped with data collection
systems, which enable data-based calibration of the consumption models.
Typically this calibration procedure is carried out independently for each
particular ship, using only data collected from the ship in question. In this
paper, we demonstrate a hierarchical Bayesian modeling approach, where we fit a
single model over many vessels, with the assumption that the parameters of
vessels of same type and similar characteristics (e.g. vessel size) are likely
close to each other. The benefits of such an approach are two-fold; 1) we can
borrow information about parameters that are not well informed by the
vessel-specific data using data from similar ships, and 2) we can use the final
hierarchical model to predict the behavior of a vessel from which we don't have
any data, based only on its characteristics. In this paper, we discuss the
basic concept and present a first simple version of the model. We apply the
Stan statistical modeling tool for the model fitting and use real data from 64
cruise ships collected via the widely used commercial Eniram platform. By using
Bayesian statistical methods we obtain uncertainties for the model predictions,
too. The prediction accuracy of the model is compared to an existing data-free
modeling approach.
"
2004.12716,2021-10-27,The Local Partial Autocorrelation Function and Some Applications,"  The classical regular and partial autocorrelation functions are powerful
tools for stationary time series modelling and analysis. However, it is
increasingly recognized that many time series are not stationary and the use of
classical global autocorrelations can give misleading answers. This article
introduces two estimators of the local partial autocorrelation function and
establishes their asymptotic properties. The article then illustrates the use
of these new estimators on both simulated and real time series. The examples
clearly demonstrate the strong practical benefits of local estimators for time
series that exhibit nonstationarities.
"
2005.08415,2020-05-19,Selective Confidence Intervals for Martingale Regression Model,"  In this paper we consider the problem of constructing confidence intervals
for coefficients of martingale regression models (in particular, time series
models) after variable selection. Although constructing confidence intervals
are common practice in statistical analysis, it is challenging in our framework
due to the data-dependence of the selected model and the correlation among the
variables being selected and not selected. We first introduce estimators for
the selected coefficients and show that it is consistent under martingale
regression model, in which the observations can be dependent and the errors can
be heteroskedastic. Then we use the estimators together with a resampling
approach to construct confidence intervals. Our simulation results show that
our approach outperforms other existing approaches in various data structures.
"
2005.13974,2020-05-29,"On the Bound of Cumulative Return in Trading Series and the Verification
  Using Technical Trading Rules","  Although there is a wide use of technical trading rules in stock markets, the
profitability of them still remains controversial. This paper first presents
and proves the upper bound of cumulative return, and then introduces many of
conventional technical trading rules. Furthermore, with the help of bootstrap
methodology, we investigate the profitability of technical trading rules on
different international stock markets, including developed markets and emerging
markets. At last, the results show that the technical trading rules are hard to
beat the market, and even less profitable than the random trading strategy.
"
2005.14292,2020-06-01,Linear Regression under Special Relativity,"  This study investigated the problem posed by using ordinary least squares
(OLS) to estimate parameters of simple linear regression under a specific
context of special relativity, where an independent variable is restricted to
an open interval, (-c, c). It is found that the OLS estimate for the slope
coefficient is not invariant under Lorentz velocity transformation.
Accordingly, an alternative estimator for the parameters of linear regression
under special relativity is proposed. This estimator can be considered a
generalization of the OLS estimator under special relativity; when c approaches
to infinity, the proposed estimator and its variance converges to the OLS
estimator and its variance, respectively. The variance of the proposed
estimator is larger than that of the OLS estimator, which implies that
hypothesis testing using the OLS estimator and its variance may result in a
liberal test under special relativity.
"
2006.00741,2020-06-02,"Correcting misclassification errors in crowdsourced ecological data: A
  Bayesian perspective","  Many research domains use data elicited from ""citizen scientists"" when a
direct measure of a process is expensive or infeasible. However, participants
may report incorrect estimates or classifications due to their lack of skill.
We demonstrate how Bayesian hierarchical models can be used to learn about
latent variables of interest, while accounting for the participants' abilities.
The model is described in the context of an ecological application that
involves crowdsourced classifications of georeferenced coral-reef images from
the Great Barrier Reef, Australia. The latent variable of interest is the
proportion of coral cover, which is a common indicator of coral reef health.
The participants' abilities are expressed in terms of sensitivity and
specificity of a correctly classified set of points on the images. The model
also incorporates a spatial component, which allows prediction of the latent
variable in locations that have not been surveyed. We show that the model
outperforms traditional weighted-regression approaches used to account for
uncertainty in citizen science data. Our approach produces more accurate
regression coefficients and provides a better characterization of the latent
process of interest. This new method is implemented in the probabilistic
programming language Stan and can be applied to a wide number of problems that
rely on uncertain citizen science data.
"
2006.02956,2020-06-05,"A Fair, Traceable, Auditable and Participatory Randomization Tool for
  Legal Systems","  Many real-world scenarios require the random selection of one or more
individuals from a pool of eligible candidates. One example of especial social
relevance refers to the legal system, in which the jurors and judges are
commonly picked according to some probability distribution aiming to avoid
biased decisions. In this scenario, ensuring auditability of the random drawing
procedure is imperative to promote confidence in its fairness. With this goal
in mind, this article describes a protocol for random drawings specially
designed for use in legal systems. The proposed design combines the following
properties: security by design, ensuring the fairness of the random draw as
long as at least one participant behaves honestly; auditability by any
interested party, even those having no technical background, using only public
information; and statistical robustness, supporting drawings where candidates
may have distinct probability distributions. Moreover, it is capable of
inviting and engaging as participating stakeholders the main interested parties
of a legal process, in a way that promotes process transparency, public trust
and institutional resilience. An open-source implementation is also provided as
supplementary material.
"
2006.03351,2020-06-08,Extracting Spatiotemporal Demand for Public Transit from Mobility Data,"  With people constantly migrating to different urban areas, our mobility needs
for work, services and leisure are transforming rapidly. The changing urban
demographics pose several challenges for the efficient management of transit
services. To forecast transit demand, planners often resort to sociological
investigations or modelling that are either difficult to obtain, inaccurate or
outdated. How can we then estimate the variegated demand for mobility? We
propose a simple method to identify the spatiotemporal demand for public
transit in a city. Using a Gaussian mixture model, we decompose empirical
ridership data into a set of temporal demand profiles representative of
ridership over any given day. A case of approximately 4.6 million daily transit
traces from the Greater London region reveals distinct demand profiles. We find
that a weighted mixture of these profiles can generate any station traffic
remarkably well, uncovering spatially concentric clusters of mobility needs.
Our method of analysing the spatiotemporal geography of a city can be extended
to other urban regions with different modes of public transit.
"
2006.03722,2020-06-09,"MMSE Bounds Under Kullback-Leibler Divergence Constraints on the Joint
  Input-Output Distribution","  This paper proposes a new family of lower and upper bounds on the minimum
mean squared error (MMSE). The key idea is to minimize/maximize the MMSE
subject to the constraint that the joint distribution of the input-output
statistics lies in a Kullback-Leibler divergence ball centered at some Gaussian
reference distribution. Both bounds are tight and are attained by Gaussian
distributions whose mean is identical to that of the reference distribution and
whose covariance matrix is determined by a scalar parameter that can be
obtained by finding the root of a monotonic function. The upper bound
corresponds to a minimax optimal estimator and provides performance guarantees
under distributional uncertainty. The lower bound provides an alternative to
well-known inequalities in estimation theory, such as the Cram\'er-Rao bound,
that is potentially tighter and defined for a larger class of distributions.
Examples of applications in signal processing and information theory illustrate
the usefulness of the proposed bounds in practice.
"
2006.04565,2020-06-09,A Survey of Bayesian Statistical Approaches for Big Data,"  The modern era is characterised as an era of information or Big Data. This
has motivated a huge literature on new methods for extracting information and
insights from these data. A natural question is how these approaches differ
from those that were available prior to the advent of Big Data. We present a
review of published studies that present Bayesian statistical approaches
specifically for Big Data and discuss the reported and perceived benefits of
these approaches. We conclude by addressing the question of whether focusing
only on improving computational algorithms and infrastructure will be enough to
face the challenges of Big Data.
"
2006.05748,2020-06-11,A different approach for choosing a threshold in peaks over threshold,"  Abstract In Extreme Value methodology the choice of threshold plays an
important role in efficient modelling of observations exceeding the threshold.
The threshold must be chosen high enough to ensure an unbiased extreme value
index but choosing the threshold too high results in uncontrolled variances.
This paper investigates a generalized model that can assist in the choice of
optimal threshold values in the \gamma positive domain. A Bayesian approach is
considered by deriving a posterior distribution for the unknown generalized
parameter. Using the properties of the posterior distribution allows for a
method to choose an optimal threshold without visual inspection.
"
2006.07136,2020-12-03,Fourier Analysis and Benford Random Variables,"  This paper has several major purposes. The central purpose is to describe the
""Benford analysis"" of a positive random variable and to summarize some results
from investigations into base dependence of Benford random variables. The
principal tools used to derive these results are Fourier series and Fourier
transforms, and a second major purpose of this paper is to present an
introductory exposition about these tools. My motivation for writing this paper
is twofold. First, I think the theory of Benford random variables and the
Benford analysis of a positive random variable are interesting and deserve to
be better known. Second, I think that Benford analysis provides a really
excellent illustration of the utility of Fourier series and transforms, and
reveals certain interconnections between series and transforms that are not
obvious from the usual way these subjects are introduced.
"
2006.09059,2020-12-29,"Explicit formulas for the joint third and fourth central moments of the
  multinomial distribution","  We give the first explicit formulas for the joint third and fourth central
moments of the multinomial distribution, by differentiating the moment
generating function. A general formula for the joint factorial moments was
previously given in Mosimann (1962).
"
2006.11345,2020-06-23,Bringing Visual Inference to the Classroom,"  In the classroom, we traditionally visualize inferential concepts using
static graphics or interactive apps. For example, there is a long history of
using apps to visualize sampling distributions. Recent developments in
statistical graphics have created an opportunity to bring additional
visualizations into the classroom to hone student understanding. Specifically,
the lineup protocol for visual inference provides a framework for students see
the difference between signal and noise by embedding a plot of observed data in
a field of null (noise) plots. Lineups have proven valuable in visualizing
randomization/permutation tests, diagnosing models, and even conducting valid
inference when distributional assumptions break down. This paper provides an
overview of how the lineup protocol for visual inference can be used to hone
understanding of key statistical topics throughout the statistics curricula.
"
2006.11570,2020-06-23,Continuous limits of Heterogeneous Continuous Time Random Walk model,"  Continuous time random Walk model has been versatile analytical formalism for
studying and modeling diffusion processes in heterogeneous structures, such as
disordered or porous media.
  We are studying the continuous limits of Heterogeneous Continuous Time Random
Walk model, when a random walk is making jumps on a graph within different
time-length.
  We apply the concept of a generalized master equation to study heterogeneous
continuous-time random walks on networks.
  Depending on the interpretations of the waiting time distributions the
generalized master equation gives different forms of continuous equations.
"
2006.14004,2020-06-26,Predicting First Passage Percolation Shapes Using Neural Networks,"  Many random growth models have the property that the set of discovered sites,
scaled properly, converges to some deterministic set as time grows. Such
results are known as shape theorems. Typically, not much is known about the
shapes. For first passage percolation on $\mathbb{Z}^d$ we only know that the
shape is convex, compact, and inherits all the symmetries of $\mathbb{Z}^d$.
Using simulated data we construct and fit a neural network able to adequately
predict the shape of the set of discovered sites from the mean, standard
deviation, and percentiles of the distribution of the passage times. The
purpose of the note is two-fold. The main purpose is to give researchers a new
tool for \textit{quickly} getting an impression of the shape from the
distribution of the passage times -- instead of having to wait some time for
the simulations to run, as is the only available way today. The second purpose
of the note is simply to introduce modern machine learning methods into this
area of discrete probability, and a hope that it stimulates further research.
"
2006.14737,2020-06-29,"Monitoring of process and risk-adjusted medical outcomes using a
  multi-stage MEWMA chart","  Most statistical process control programmes in healthcare focus on
surveillance of outcomes at the final stage of a procedure, such as mortality
or failure rates. Such an approach ignores the multi-stage nature of these
procedures, in which a patient progresses through several stages prior to the
final stage. In this paper, we develop a multi-stage control chart based on a
multivariate exponentially weighted moving average (EWMA) test statistic
derived from score equations. This allows simultaneous monitoring of all
intermediate and final stage outcomes of a healthcare process, with adjustment
for underlying patient risk factors and dependence between outcome variables.
Use of the EWMA test statistics allows quick detection of small gradual changes
in any part of the process. Three advantages of the approach are: better
understanding of how outcomes at different stages relate to each other,
explicit monitoring of upstream stage outcomes may help curtail trends that
lead to poorer end-stage outcomes and understanding the impact of each stage
can help determine the most effective allocation of quality improvement
resources. Simulations are performed to test the control charts under various
types of hypothesised shifts, and the results are summarised using
out-of-control average run lengths.
"
2006.16051,2021-05-21,"Modeling random and non-random decision uncertainty in ratings data: A
  fuzzy beta model","  Modeling human ratings data subject to raters' decision uncertainty is an
attractive problem in applied statistics. In view of the complex interplay
between emotion and decision making in rating processes, final raters' choices
seldom reflect the true underlying raters' responses. Rather, they are
imprecisely observed in the sense that they are subject to a non-random
component of uncertainty, namely the decision uncertainty. The purpose of this
article is to illustrate a statistical approach to analyse ratings data which
integrates both random and non-random components of the rating process. In
particular, beta fuzzy numbers are used to model raters' non-random decision
uncertainty and a variable dispersion beta linear model is instead adopted to
model the random counterpart of rating responses. The main idea is to quantify
characteristics of latent and non-fuzzy rating responses by means of random
observations subject to fuzziness. To do so, a fuzzy version of the
Expectation-Maximization algorithm is adopted to both estimate model's
parameters and compute their standard errors. Finally, the characteristics of
the proposed fuzzy beta model are investigated by means of a simulation study
as well as two case studies from behavioral and social contexts.
"
2006.16333,2021-03-10,Inference in Bayesian Additive Vector Autoregressive Tree Models,"  Vector autoregressive (VAR) models assume linearity between the endogenous
variables and their lags. This assumption might be overly restrictive and could
have a deleterious impact on forecasting accuracy. As a solution, we propose
combining VAR with Bayesian additive regression tree (BART) models. The
resulting Bayesian additive vector autoregressive tree (BAVART) model is
capable of capturing arbitrary non-linear relations between the endogenous
variables and the covariates without much input from the researcher. Since
controlling for heteroscedasticity is key for producing precise density
forecasts, our model allows for stochastic volatility in the errors. We apply
our model to two datasets. The first application shows that the BAVART model
yields highly competitive forecasts of the US term structure of interest rates.
In a second application, we estimate our model using a moderately sized
Eurozone dataset to investigate the dynamic effects of uncertainty on the
economy.
"
2007.00238,2021-08-04,"Popper's falsification and corroboration from the statistical
  perspectives","  The role of probability appears unchallenged as the key measure of
uncertainty, used among other things for practical induction in the empirical
sciences. Yet, Popper was emphatic in his rejection of inductive probability
and of the logical probability of hypotheses; furthermore, for him, the degree
of corroboration cannot be a probability. Instead he proposed a deductive
method of testing. In many ways this dialectic tension has many parallels in
statistics, with the Bayesians on logico-inductive side vs the non-Bayesians or
the frequentists on the other side. Simplistically Popper seems to be on the
frequentist side, but recent synthesis on the non-Bayesian side might direct
the Popperian views to a more nuanced destination. Logical probability seems
perfectly suited to measure partial evidence or support, so what can we use if
we are to reject it? For the past 100 years, statisticians have also developed
a related concept called likelihood, which has played a central role in
statistical modelling and inference. Remarkably, this Fisherian concept of
uncertainty is largely unknown or at least severely under-appreciated in
non-statistical literature. As a measure of corroboration, the likelihood
satisfies the Popperian requirement that it is not a probability. Our aim is to
introduce the likelihood and its recent extension via a discussion of two
well-known logical fallacies in order to highlight that its lack of recognition
may have led to unnecessary confusion in our discourse about falsification and
corroboration of hypotheses. We highlight the 100 years of development of
likelihood concepts. The year 2021 will mark the 100-year anniversary of the
likelihood, so with this paper we wish it a long life and increased
appreciation in non-statistical literature.
"
2007.00765,2020-07-03,A Statistical Overview on Data Privacy,"  The eruption of big data with the increasing collection and processing of
vast volumes and variety of data have led to breakthrough discoveries and
innovation in science, engineering, medicine, commerce, criminal justice, and
national security that would not have been possible in the past. While there
are many benefits to the collection and usage of big data, there are also
growing concerns among the general public on what personal information is
collected and how it is used. In addition to legal policies and regulations,
technological tools and statistical strategies also exist to promote and
safeguard individual privacy, while releasing and sharing useful
population-level information. In this overview, I introduce some of these
approaches, as well as the existing challenges and opportunities in statistical
data privacy research and applications to better meet the practical needs of
privacy protection and information sharing.
"
2007.01360,2020-07-06,A New ECDF Two-Sample Test Statistic,"  Empirical cumulative distribution functions (ECDFs) have been used to test
the hypothesis that two samples come from the same distribution since the
seminal contribution by Kolmogorov and Smirnov. This paper describes a
statistic which is usable under the same conditions as Kolmogorov-Smirnov, but
provides more power than other extant tests in that vein. I demonstrate a valid
(conservative) procedure for producing finite-sample p-values. I outline the
close relationship between this statistic and its two main predecessors. I also
provide a public R package (CRAN: twosamples [2018]) implementing the testing
procedure in $O(N\log(N))$ time with $O(N)$ memory. Using the package's
functions, I perform several simulation studies showing the power improvements.
"
2007.01539,2020-07-06,"A method to find an efficient and robust sampling strategy under model
  uncertainty","  We consider the problem of deciding on sampling strategy, in particular
sampling design. We propose a risk measure, whose minimizing value guides the
choice. The method makes use of a superpopulation model and takes into account
uncertainty about its parameters. The method is illustrated with a real
dataset, yielding satisfactory results. As a baseline, we use the strategy that
couples probability proportional-to-size sampling with the difference
estimator, as it is known to be optimal when the superpopulation model is fully
known. We show that, even under moderate misspecifications of the model, this
strategy is not robust and can be outperformed by some alternatives
"
2007.03611,2020-07-08,P-Values in a Post-Truth World,"  The role of statisticians in society is to provide tools, techniques, and
guidance with regards to how much to trust data. This role is increasingly more
important with more data and more misinformation than ever before. The American
Statistical Association recently released two statements on p-values, and
provided four guiding principles. We evaluate their claims using these
principles and find that they failed to adhere to them. In this age of
distrust, we have an opportunity to be role models of trustworthiness, and
responsibility to take it.
"
2007.04180,2020-07-09,A Bayesian Redesign of the First Probability/Statistics Course,"  The traditional calculus-based introduction to statistical inference consists
of a semester of probability followed by a semester of frequentist inference.
Cobb (2015) challenges the statistical education community to rethink the
undergraduate statistics curriculum. In particular, he suggests that we should
focus on two goals: making fundamental concepts accessible and minimizing
prerequisites to research. Using five underlying principles of Cobb, we
describe a new calculus-based introduction to statistics based on
simulation-based Bayesian computation.
"
2007.04758,2020-07-10,A Bivariate Compound Dynamic Contagion Process for Cyber Insurance,"  As corporates and governments become more digital, they become vulnerable to
various forms of cyber attack. Cyber insurance products have been used as risk
management tools, yet their pricing does not reflect actual risk, including
that of multiple, catastrophic and contagious losses. For the modelling of
aggregate losses from cyber events, in this paper we introduce a bivariate
compound dynamic contagion process, where the bivariate dynamic contagion
process is a point process that includes both externally excited joint jumps,
which are distributed according to a shot noise Cox process and two separate
self-excited jumps, which are distributed according to the branching structure
of a Hawkes process with an exponential fertility rate, respectively. We
analyse the theoretical distributional properties for these processes
systematically, based on the piecewise deterministic Markov process developed
by Davis (1984) and the univariate dynamic contagion process theory developed
by Dassios and Zhao (2011). The analytic expression of the Laplace transform of
the compound process and its moments are presented, which have the potential to
be applicable to a variety of problems in credit, insurance, market and other
operational risks. As an application of this process, we provide insurance
premium calculations based on its moments. Numerical examples show that this
compound process can be used for the modelling of aggregate losses from cyber
events. We also provide the simulation algorithm for statistical analysis,
further business applications and research.
"
2007.05236,2021-08-12,"Adaptive reconstruction of imperfectly-observed monotone functions, with
  applications to uncertainty quantification","  Motivated by the desire to numerically calculate rigorous upper and lower
bounds on deviation probabilities over large classes of probability
distributions, we present an adaptive algorithm for the reconstruction of
increasing real-valued functions. While this problem is similar to the
classical statistical problem of isotonic regression, the optimisation setting
alters several characteristics of the problem and opens natural algorithmic
possibilities. We present our algorithm, establish sufficient conditions for
convergence of the reconstruction to the ground truth, and apply the method to
synthetic test cases and a real-world example of uncertainty quantification for
aerodynamic design.
"
2007.05748,2023-11-21,"Probability Models in Statistical Data Analysis: Uses, Interpretations,
  Frequentism-As-Model","  Note: Published now as a chapter in ""Handbook of the History and Philosophy
of Mathematical Practice"" (Springer Nature, editor B. Sriraman,
https://doi.org/10.1007/978-3-030-19071-2_105-1).
  The application of mathematical probability theory in statistics is quite
controversial. Controversies regard both the interpretation of probability, and
approaches to statistical inference. After having given an overview of the main
approaches, I will propose a re-interpretation of frequentist probability. Most
statisticians are aware that probability models interpreted in a frequentist
manner are not really true in objective reality, but only idealisations. I
argue that this is often ignored when actually applying frequentist methods and
interpreting the results, and that keeping up the awareness for the essential
difference between reality and models can lead to a more appropriate use and
interpretation of frequentist models and methods, called
""frequentism-as-model"". This is elaborated showing connections to existing
work, appreciating the special role of independently and identically
distributed observations and subject matter knowledge, giving an account of how
and under what conditions models that are not true can be useful, giving
detailed interpretations of tests and confidence intervals, confronting their
implicit compatibility logic with the inverse probability logic of Bayesian
inference, re-interpreting the role of model assumptions, appreciating
robustness, and the role of ""interpretative equivalence"" of models. Epistemic
probability shares the issue that its models are only idealisations, and an
analogous ""epistemic-probability-as-model"" can also be developed.
"
2007.05857,2020-07-14,"Reliability of decisions based on tests: Fourier analysis of Boolean
  decision functions","  Items in a test are often used as a basis for making decisions and such tests
are therefore required to have good psychometric properties, like
unidimensionality. In many cases the sum score is used in combination with a
threshold to decide between pass or fail, for instance. Here we consider
whether such a decision function is appropriate, without a latent variable
model, and which properties of a decision function are desirable. We consider
reliability (stability) of the decision function, i.e., does the decision
change upon perturbations, or changes in a fraction of the outcomes of the
items (measurement error). We are concerned with questions of whether the sum
score is the best way to aggregate the items, and if so why. We use ideas from
test theory, social choice theory, graphical models, computer science and
probability theory to answer these questions. We conclude that a weighted sum
score has desirable properties that (i) fit with test theory and is observable
(similar to a condition like conditional association), (ii) has the property
that a decision is stable (reliable), and (iii) satisfies Rousseau's criterion
that the input should match the decision. We use Fourier analysis of Boolean
functions to investigate whether a decision function is stable and to figure
out which (set of) items has proportionally too large an influence on the
decision. To apply these techniques we invoke ideas from graphical models and
use a pseudo-likelihood factorisation of the probability distribution.
"
2007.06065,2021-09-06,"The Effects of Vacant Lot Greening and the Impact of Land Use and
  Business Presence on Crime","  We examine the effect of the Philadelphia LandCare (PLC) vacant lot greening
initiative on crime and the extent to which surrounding land uses and business
types moderate this intervention. We rely on a propensity score matching
analysis to account for substantial differences in demographic, economic, land
use, and business characteristics between greened and ungreened vacant lots. We
estimate larger and more significant crime reductions around vacant lots that
are greened in our matched pairs analysis compared to unmatched analyses. The
effects of vacant lot greening on crime are larger in areas with high
residential and low commercial land use and are moderated by the presence of
different types of nearby businesses.
"
2007.06543,2020-07-14,Dynamics of ternary statistical experiments with equilibrium state,"  We study the scenarios of the dynamics of ternary statistical experiments,
modeled employing difference equations. The important features are a balance
condition and the existence of a steady-state (equilibrium). We give a
classification of scenarios of the model evolution which are significantly
different between them, depending on the domain of the values of the model
basic parameters.
"
2007.06701,2020-07-15,Incertitudes et mesures,"  Educational guide focused on the statistical treatment of measurement
uncertainties. The conditions of application of current practices are detailed
and precised: mean values, central limit theorem, linear regression. The last
two chapters are devoted to an introduction to the Bayesian inference and a
series of application cases: machine failure date, elimination of a background
noise, linear adjustment with elimination of outliers.
"
2007.08160,2020-08-03,"The rule of conditional probability is valid in quantum theory [Comment
  on Gelman & Yao's ""Holes in Bayesian Statistics""]","  In a recent manuscript, Gelman & Yao (2020) claim that ""the usual rules of
conditional probability fail in the quantum realm"" and that ""probability theory
isn't true (quantum physics)"" and purport to support these statements with the
example of a quantum double-slit experiment. The present comment recalls some
relevant literature in quantum theory and shows that (i) Gelman & Yao's
statements are false; in fact, the quantum example confirms the rules of
probability theory; (ii) the particular inequality found in the quantum example
can be shown to appear also in very non-quantum examples, such as drawing from
an urn; thus there is nothing peculiar to quantum theory in this matter. A
couple of wrong or imprecise statements about quantum theory in the cited
manuscript are also corrected.
"
2007.09996,2022-02-24,Social Learning in Non-Stationary Environments,"  Potential buyers of a product or service, before making their decisions, tend
to read reviews written by previous consumers. We consider Bayesian consumers
with heterogeneous preferences, who sequentially decide whether to buy an item
of unknown quality, based on previous buyers' reviews. The quality is
multi-dimensional and may occasionally vary over time; the reviews are also
multi-dimensional. In the simple uni-dimensional and static setting, beliefs
about the quality are known to converge to its true value. Our paper extends
this result in several ways. First, a multi-dimensional quality is considered,
second, rates of convergence are provided, third, a dynamical Markovian model
with varying quality is studied. In this dynamical setting the cost of learning
is shown to be small.
"
2007.10121,2020-07-21,"Strategic Evaluation in Optimizing the Internal Supply Chain Using
  TOPSIS: Evidence In A Coil Winding Machine Manufacturer","  Most of the manufacturing firm aims to optimize their Supply Chain in terms
of improved profitability of its products through value Addition. This study
takes a critical look into the factors that affect the Performance of internal
supply chain with respect to specific criteria. Accordingly, ranking these
factors to get the critical dimensions of supply chain performance in the
manufacturing industry. A semi-structured interview with the pre-defined set of
questions used to collect the responses from decision makers of the firm. Multi
criteria decision-making tool called TOPSIS is used to evaluate the responses
and rank the factors. The results of this indicate that supplier relationship
and inventory planning were most principal factors positively influencing
on-time delivery of the product, production flexibility, cost savings,
additional costs. This study helps to identify and optimize the process
parameters using objective and subjective evaluation approach. The combined
influence of the thought process of the manager to optimize the internal supply
chain is extracted in this work.
"
2007.12210,2020-07-27,Reproducible Research: A Retrospective,"  Rapid advances in computing technology over the past few decades have spurred
two extraordinary phenomena in science: large-scale and high-throughput data
collection coupled with the creation and implementation of complex statistical
algorithms for data analysis. Together, these two phenomena have brought about
tremendous advances in scientific discovery but have also raised two serious
concerns, one relatively new and one quite familiar. The complexity of modern
data analyses raises questions about the reproducibility of the analyses,
meaning the ability of independent analysts to re-create the results claimed by
the original authors using the original data and analysis techniques. While
seemingly a straightforward concept, reproducibility of analyses is typically
thwarted by the lack of availability of the data and computer code that were
used in the analyses. A much more general concern is the replicability of
scientific findings, which concerns the frequency with which scientific claims
are confirmed by completely independent investigations. While the concepts of
reproduciblity and replicability are related, it is worth noting that they are
focused on quite different goals and address different aspects of scientific
progress. In this review, we will discuss the origins of reproducible research,
characterize the current status of reproduciblity in public health research,
and connect reproduciblity to current concerns about replicability of
scientific findings. Finally, we describe a path forward for improving both the
reproducibility and replicability of public health research in the future.
"
2007.12631,2020-07-27,On the Programmatic Generation of Reproducible Documents,"  Reproducible document standards, like R Markdown, facilitate the programmatic
creation of documents whose content is itself programmatically generated. While
these documents are generally not complete in the sense that they will not
include prose content, generated by an author to provide context, a narrative,
etc., programmatic generation can provide substantial efficiencies for
structuring and constructing documents. This paper explores the programmatic
generation of reproducible by distinguishing components than can be created by
computational means from those requiring human-generated prose, providing
guidelines for the generation of these documents, and identifying a use case in
clinical trial reporting. These concepts and use case are illustrated through
the listdown package for the R programming environment, which is is currently
available on the Comprehensive R Archive Network (CRAN).
"
2007.13646,2020-08-03,"Asymmetry approach to study for chemotherapy treatment and devices
  failure times data using modified Power function distribution with some
  modified estimators","  In order to improve the already existing models that are used extensively in
bio sciences and applied sciences research, a new class of Weighted Power
function distribution (WPFD) has been proposed with its various properties and
different modifications to be more applicable in real life. We have provided
the mathematical derivations for the new distribution including moments,
incomplete moments, conditional moments, inverse moments, mean residual
function, vitality function, order statistics, mills ratio, information
function, Shannon entropy, Bonferroni and Lorenz curves and quantile function.
We have also characterized the WPFD, based on doubly truncated mean. The aim of
the study is to increase the application of the Power function distribution.
The main feature of the proposed distribution is that there is no induction of
parameters as compare to the other generalization of the distributions, which
are complexed having many parameters. We have used R programming to estimate
the parameters of the new class of WPFD using Maximum Likelihood Method (MLM),
Percentile Estimators (P.E) and their modified estimators. After analyzing the
data, we conclude that the proposed model WPFD performs better in the data sets
while compared to different competitor models.
"
2007.15583,2023-04-13,"Performance Analysis of Metaheuristic Optimization Algorithms in
  Estimating the Interfacial Heat Transfer Coefficient on Directional
  Solidification","  In this paper is proposed an evaluation of ten metaheuristic optimization
algorithms applied on the inverse optimization of the Interfacial Heat Transfer
Coefficient (IHTC) coupled on the solidification phenomenon. It was considered
an upward directional solidification system for Al-7wt.% Si alloy and, for IHTC
model, a exponential time function. All thermophysical properties of the alloy
were considered constant. Scheil Rule was used as segregation model ahead
phase-transformation interface. Optimization results from Markov Chain Monte
Carlo method (MCMC) were considered as reference. Based on average, quantiles
95% and 5%, kurtosis, average iterations and absolute errors of the
metaheuristic methods, in relation to MCMC results, the Flower Pollination
Algorithm (FPA) and Moth-Flame Optimization (MFO) presented the most
appropriate results, outperforming the other methods in this particular
phenomenon, based on these metrics. The regions with the most probable values
for parameters in IHTC time function were also determined.
"
2007.15634,2023-05-30,On the Nature and Types of Anomalies: A Review of Deviations in Data,"  Anomalies are occurrences in a dataset that are in some way unusual and do
not fit the general patterns. The concept of the anomaly is typically
ill-defined and perceived as vague and domain-dependent. Moreover, despite some
250 years of publications on the topic, no comprehensive and concrete overviews
of the different types of anomalies have hitherto been published. By means of
an extensive literature review this study therefore offers the first
theoretically principled and domain-independent typology of data anomalies and
presents a full overview of anomaly types and subtypes. To concretely define
the concept of the anomaly and its different manifestations, the typology
employs five dimensions: data type, cardinality of relationship, anomaly level,
data structure, and data distribution. These fundamental and data-centric
dimensions naturally yield 3 broad groups, 9 basic types, and 63 subtypes of
anomalies. The typology facilitates the evaluation of the functional
capabilities of anomaly detection algorithms, contributes to explainable data
science, and provides insights into relevant topics such as local versus global
anomalies.
"
2008.00315,2020-08-04,A fresh look at introductory data science,"  The proliferation of vast quantities of available datasets that are large and
complex in nature has challenged universities to keep up with the demand for
graduates trained in both the statistical and the computational set of skills
required to effectively plan, acquire, manage, analyze, and communicate the
findings of such data. To keep up with this demand, attracting students early
on to data science as well as providing them a solid foray into the field
becomes increasingly important. We present a case study of an introductory
undergraduate course in data science that is designed to address these needs.
Offered at Duke University, this course has no pre-requisites and serves a wide
audience of aspiring statistics and data science majors as well as humanities,
social sciences, and natural sciences students. We discuss the unique set of
challenges posed by offering such a course and in light of these challenges, we
present a detailed discussion into the pedagogical design elements, content,
structure, computational infrastructure, and the assessment methodology of the
course. We also offer a repository containing all teaching materials that are
open-source, along with supplemental materials and the R code for reproducing
the figures found in the paper.
"
2008.02048,2020-08-06,On the sum of ordered spacings,"  We provide the analytic forms of the distributions for the sum of ordered
spacings. We do this both for the case where the boundaries are included in the
calculation of the spacings and the case where they are excluded. Both the
probability densities as well as their cumulatives are provided. These results
will have useful applications in the physical sciences and possibly elsewhere.
"
2008.03122,2020-08-10,"Sulla decifratura di Enigma -- Come un reverendo del XVIII secolo
  contribu\`i alla sconfitta degli U-boot tedeschi durante la Seconda Guerra
  Mondiale","  This article, written in Italian language, explores the contribution given by
Bayes' rule and by subjective probability in the work at Bletchley Park towards
cracking Enigma cyphered messages during WWII.
  --
  In questo articolo, scritto in Italiano, esploriamo il contributo dato dal
teorema di Bayes e dalle idee della probabilit\`a soggettiva nel lavoro
compiuto a Bletchley Park che ha portato a decifrare i messaggi cifrati con
macchine Enigma durante la Seconda Guerra Mondiale.
"
2008.03256,2020-08-10,"Opening practice: supporting Reproducibility and Critical spatial data
  science","  This paper reflects on a number of trends towards a more open and
reproducible approach to geographic and spatial data science over recent years.
In particular it considers trends towards Big Data, and the impacts this is
having on spatial data analysis and modelling. It identifies a turn in academia
towards coding as a core analytic tool, and away from proprietary software
tools offering 'black boxes' where the internal workings of the analysis are
not revealed. It is argued that this closed form software is problematic, and
considers a number of ways in which issues identified in spatial data analysis
(such as the MAUP) could be overlooked when working with closed tools, leading
to problems of interpretation and possibly inappropriate actions and policies
based on these. In addition, this paper and considers the role that
reproducible and open spatial science may play in such an approach, taking into
account the issues raised. It highlights the dangers of failing to account for
the geographical properties of data, now that all data are spatial (they are
collected somewhere), the problems of a desire for n=all observations in data
science and it identifies the need for a critical approach. This is one in
which openness, transparency, sharing and reproducibility provide a mantra for
defensible and robust spatial data science.
"
2008.04475,2021-07-20,Stick-breaking processes with exchangeable length variables,"  Our object of study is the general class of stick-breaking processes with
exchangeable length variables. These generalize well-known Bayesian
non-parametric priors in an unexplored direction. We give conditions to assure
the respective species sampling process is proper and the corresponding prior
has full support. For a rich sub-class we explain how, by tuning a single
$[0,1]$-valued parameter, the stochastic ordering of the weights can be
modulated, and Dirichlet and Geometric priors can be recovered. A general
formula for the distribution of the latent allocation variables is derived and
an MCMC algorithm is proposed for density estimation purposes.
"
2008.05109,2020-08-13,A Bayesian Approach to Spherical Factor Analysis for Binary Data,"  Factor models are widely used across diverse areas of application for
purposes that include dimensionality reduction, covariance estimation, and
feature engineering. Traditional factor models can be seen as an instance of
linear embedding methods that project multivariate observations onto a lower
dimensional Euclidean latent space. This paper discusses a new class of
geometric embedding models for multivariate binary data in which the embedding
space correspond to a spherical manifold, with potentially unknown dimension.
The resulting models include traditional factor models as a special case, but
provide additional flexibility. Furthermore, unlike other techniques for
geometric embedding, the models are easy to interpret, and the uncertainty
associated with the latent features can be properly quantified. These
advantages are illustrated using both simulation studies and real data on
voting records from the U.S. Senate.
"
2008.06709,2020-12-22,Randomization and Fair Judgment in Law and Science,"  Randomization procedures are used in legal and statistical applications,
aiming to shield important decisions from spurious influences. This article
gives an intuitive introduction to randomization and examines some intended
consequences of its use related to truthful statistical inference and fair
legal judgment. This article also presents an open-code Java implementation for
a cryptographically secure, statistically reliable, transparent, traceable, and
fully auditable randomization tool.
"
2008.06755,2020-08-18,"Tackling COVID-19 through Responsible AI Innovation: Five Steps in the
  Right Direction","  Innovations in data science and AI/ML have a central role to play in
supporting global efforts to combat COVID-19. The versatility of AI/ML
technologies enables scientists and technologists to address an impressively
broad range of biomedical, epidemiological, and socioeconomic challenges. This
wide-reaching scientific capacity, however, also raises a diverse array of
ethical challenges. The need for researchers to act quickly and globally in
tackling SARS-CoV-2 demands unprecedented practices of open research and
responsible data sharing at a time when innovation ecosystems are hobbled by
proprietary protectionism, inequality, and a lack of public trust. Moreover,
societally impactful interventions like digital contact tracing are raising
fears of surveillance creep and are challenging widely held commitments to
privacy, autonomy, and civil liberties. Prepandemic concerns that data-driven
innovations may function to reinforce entrenched dynamics of societal inequity
have likewise intensified given the disparate impact of the virus on vulnerable
social groups and the life-and-death consequences of biased and discriminatory
public health outcomes. To address these concerns, I offer five steps that need
to be taken to encourage responsible research and innovation. These provide a
practice-based path to responsible AI/ML design and discovery centered on open,
accountable, equitable, and democratically governed processes and products.
When taken from the start, these steps will not only enhance the capacity of
innovators to tackle COVID-19 responsibly, they will, more broadly, help to
better equip the data science and AI/ML community to cope with future pandemics
and to support a more humane, rational, and just society.
"
2008.06869,2020-08-18,SECODA: Segmentation- and Combination-Based Detection of Anomalies,"  This study introduces SECODA, a novel general-purpose unsupervised
non-parametric anomaly detection algorithm for datasets containing continuous
and categorical attributes. The method is guaranteed to identify cases with
unique or sparse combinations of attribute values. Continuous attributes are
discretized repeatedly in order to correctly determine the frequency of such
value combinations. The concept of constellations, exponentially increasing
weights and discretization cut points, as well as a pruning heuristic are used
to detect anomalies with an optimal number of iterations. Moreover, the
algorithm has a low memory imprint and its runtime performance scales linearly
with the size of the dataset. An evaluation with simulated and real-life
datasets shows that this algorithm is able to identify many different types of
anomalies, including complex multidimensional instances. An evaluation in terms
of a data quality use case with a real dataset demonstrates that SECODA can
bring relevant and practical value to real-world settings.
"
2008.07478,2022-11-15,"Presenting the Probabilities of Different Effect Sizes: Towards a Better
  Understanding and Communication of Statistical Uncertainty","  How should social scientists understand and communicate the uncertainty of
statistically estimated causal effects? I propose we utilize the posterior
distribution of a causal effect and present the probability of the effect being
greater (in absolute terms) than different minimum effect sizes. Probability is
an intuitive measure of uncertainty for understanding and communication. In
addition, the proposed approach needs no decision threshold for an uncertainty
measure or an effect size, unlike the conventional approaches. I apply the
proposed approach to a previous social scientific study, showing it enables
richer inference than the significance-vs.-insignificance approach taken by the
original study. The accompanying R package makes my approach easy to implement.
"
2008.07840,2021-03-30,"Glucodensities: a new representation of glucose profiles using
  distributional data analysis","  Biosensor data has the potential ability to improve disease control and
detection. However, the analysis of these data under free-living conditions is
not feasible with current statistical techniques. To address this challenge, we
introduce a new functional representation of biosensor data, termed the
glucodensity, together with a data analysis framework based on distances
between them. The new data analysis procedure is illustrated through an
application in diabetes with continuous-time glucose monitoring (CGM) data. In
this domain, we show marked improvement with respect to state of the art
analysis methods. In particular, our findings demonstrate that i) the
glucodensity possesses an extraordinary clinical sensitivity to capture the
typical biomarkers used in the standard clinical practice in diabetes, ii)
previous biomarkers cannot accurately predict glucodensity, so that the latter
is a richer source of information, and iii) the glucodensity is a natural
generalization of the time in range metric, this being the gold standard in the
handling of CGM data. Furthermore, the new method overcomes many of the
drawbacks of time in range metrics, and provides deeper insight into assessing
glucose metabolism.
"
2008.11813,2020-08-28,The use of multiple models within an organisation,"  Organisations, whether in government, industry or commerce, are required to
make decisions in a complex and uncertain environment. The way models are used
is intimately connected to the way organisations make decisions and the context
in which they make them. Typically, in a complex organisation, multiple related
models will often be used in support of a decision. For example, engineering
models might be combined with financial models and macro-economic models in
order to decide whether to invest in new production capability. Different parts
of a complex organisation might operate their own related models which might
then be presented to a central decision maker. Yet in practice, there is little
awareness of the practical challenges of using models in a robust way to
support decision making. There is significant scope to improve decision making
though an enhanced understanding of the role and limitations of modelling and
through the application of cutting edge methodologies and organisational best
practice. This report is in the form of a 'white paper', whose purpose is to
identify key issues for consideration whist postulating tentative approaches to
these issues that might be worthy of further exploration, focussing on both
technical and organisational aspects. It begins with a framework for
consideration of how model-based decisions are made in organisations. It then
looks more closely at the questions of uncertainty and multiple models. It then
postulates some technical statistical and organisational approaches for
managing some of these issues. Finally, it considers the way forward, and the
possible focus for further work.
"
2008.13637,2021-04-06,"Should policy makers trust composite indices? A commentary on the
  pitfalls of inappropriate indices for policy formation","  This paper critically discusses the use and merits of global indices, in
particular, the Global Health Security Index or GHSI (Cameron et 2019) in times
of an imminent crisis, like the current pandemic. The index ranked 195
countries according to their expected preparedness in case of a pandemic or
other biological threat. The Covid-19 pandemic provides the background to
compare each country's predicted performance from the GHSI with the actual
performance. In general, there is an inverted relation between predicted versus
actual performance, i.e. the predicted top performers are among those that are
the worst hit. Obviously, this reflects poorly on the potential policy uses of
the index in imminent crisis management. The paper also uses two different data
sets, one from the Worldmeter on the spread of the Covid-19 pandemics, and the
other one from the INGSA policy tracker, to make comparisons between the actual
introduction of pandemic response policies and the corresponding death rate in
29 selected countries.
"
2009.00646,2023-06-16,Non-asymptotic robustness analysis of regression depth median,"  The maximum depth estimator (aka depth median) ($\bs{\beta}^*_{RD}$) induced
from regression depth (RD) of Rousseeuw and Hubert (1999) (RH99) is one of the
most prevailing estimators in regression. It possesses outstanding robustness
similar to the univariate location counterpart. Indeed, $\bs{\beta}^*_{RD}$
can, asymptotically, resist up to $33\%$ contamination without breakdown, in
contrast to the $0\%$ for the traditional (least squares and least absolute
deviations) estimators (see Van Aelst and Rousseeuw, 2000) (VAR00)). The
results from VAR00 are pioneering, yet they are limited to regression-symmetric
populations (with a strictly positive density) and the $\epsilon$-contamination
and maximum-bias model. With a fixed finite-sample size practice, the most
prevailing measure of robustness for estimators is the finite-sample breakdown
point (FSBP) (Donoho and Huber (1983)). Despite many attempts made in the
literature, only sporadic partial results on FSBP for $\bs{\beta}^*_{RD}$ were
obtained whereas an exact FSBP for $\bs{\beta}^*_{RD}$ remained open in the
last twenty-plus years. Furthermore, is the asymptotic breakdown value $1/3$
(the limit of an increasing sequence of finite-sample breakdown values)
relevant in the finite-sample practice? (Or what is the difference between the
finite-sample and the limit breakdown values?). Such discussions are yet to be
given in the literature. This article addresses the above issues, revealing an
intrinsic connection between the regression depth of $\bs{\beta}^*_{RD}$ and
the newly obtained exact FSBP. It justifies the employment of
$\bs{\beta}^*_{RD}$ as a robust alternative to the traditional estimators and
demonstrates the necessity and the merit of using the FSBP in finite-sample
real practice.
"
2009.00912,2021-06-16,"Extreme precipitation events in the Mediterranean: Spatiotemporal
  characteristics and connection to large-scale atmospheric flow patterns","  The Mediterranean is strongly affected by Extreme Precipitation Events
(EPEs), sometimes leading to negative impacts on society, economy, and the
environment. Understanding such natural hazards and their drivers is essential
to mitigate related risks. Here, EPEs over the Mediterranean between 1979 and
2019 are analyzed, using ERA5 dataset from ECMWF. EPEs are determined based on
the 99th percentile of the daily distribution (P99). The different EPE
characteristics are assessed, based on seasonality and spatiotemporal
dependencies. To better understand the connection to large-scale atmospheric
flow patterns, Empirical Orthogonal Function (EOF) analysis and subsequent
K-means clustering are used to quantify the importance of weather regimes to
EPE frequency. The analysis is performed for three different variables,
depicting atmospheric variability in the lower and middle troposphere: Sea
level pressure (SLP), temperature at 850 hPa (T850), and geopotential height at
500 hPa (Z500). Results show a clear spatial division in EPEs occurrence, with
winter (autumn) being the season of highest EPEs frequency for the eastern
(western) Mediterranean. There is a high degree of temporal dependencies with
20% of the EPEs (median value of all studied grid-cells), occurring up to 1
week after a preceding P99 event at the same location. Local orography is a key
modulator of the spatiotemporal connections and substantially enhances the
probability of co-occurrence of EPEs even for distant locations. The clustering
clearly demonstrates the prevalence of distinct synoptic-scale atmospheric
conditions during the occurrence of EPEs for different locations within the
region. Results indicate that clustering based on a combination of SLP and Z500
can increase the conditional probability of EPEs by more than three (3) times
(median value for all grid cells) from the nominal probability of 1% for the
P99 EPEs.
"
2009.02099,2022-01-11,Defending the P-value,"  Attacks on the P-value are nothing new, but the recent attacks are
increasingly more serious. They come from more mainstream sources, with
widening targets such as a call to retire the significance testing altogether.
While well meaning, I believe these attacks are nevertheless misdirected:
Blaming the P-value for the naturally tentative trial-and-error process of
scientific discoveries, and presuming that banning the P-value would make the
process cleaner and less error-prone. However tentative, the skeptical
scientists still have to form unambiguous opinions, proximately to move forward
in their investigations and ultimately to present results to the wider
community. With obvious reasons, they constantly need to balance between the
false-positive and false-negative errors. How would banning the P-value or
significance tests help in this balancing act? It seems trite to say that this
balance will always depend on the relative costs or the trade-off between the
errors. These costs are highly context specific, varying by area of
applications or by stage of investigation. A calibrated but tunable knob, such
as that given by the P-value, is needed for controlling this balance. This
paper presents detailed arguments in support of the P-value.
"
2009.03650,2021-11-17,"Can we trust the standardized mortality ratio? A formal analysis and
  evaluation based on axiomatic requirements","  Background: The standardized mortality ratio (SMR) is often used to assess
and compare hospital performance. While it has been recognized that hospitals
may differ in their SMRs due to differences in patient composition, there is a
lack of rigorous analysis of this and other - largely unrecognized - properties
of the SMR. Methods: This paper proposes five axiomatic requirements for
adequate standardized mortality measures: strict monotonicity, case-mix
insensitivity, scale insensitivity, equivalence principle, and dominance
principle. Given these axiomatic requirements, effects of variations in patient
composition, hospital size, and actual and expected mortality rates on the SMR
were examined using basic algebra and calculus. In this regard, we
distinguished between standardization using expected mortality rates derived
from a different dataset (external standardization) and standardization based
on a dataset including the considered hospitals (internal standardization).
Results: Under external standardization, the SMR fulfills the axiomatic
requirements of strict monotonicity and scale insensitivity but violates the
requirement of case-mix insensitivity, the equivalence principle, and the
dominance principle. All axiomatic requirements not fulfilled under external
standardization are also not fulfilled under internal standardization. In
addition, the SMR under internal standardization is scale sensitive and
violates the axiomatic requirement of strict monotonicity. Conclusions: The SMR
fulfills only two (none) out of the five proposed axiomatic requirements under
external (internal) standardization. Generally, the SMRs of hospitals are
differently affected by variations in case mix and actual and expected
mortality rates unless the hospitals are identical in these characteristics.
These properties hamper valid assessment and comparison of hospital performance
based on the SMR.
"
2009.04747,2021-11-22,"Testing the first-order separability hypothesis for spatio-temporal
  point patterns","  First-order separability of a spatio-temporal point process plays a
fundamental role in the analysis of spatio-temporal point pattern data. While
it is often a convenient assumption that simplifies the analysis greatly,
existing non-separable structures should be accounted for in the model
construction. We propose three different tests to investigate this hypothesis
as a step of preliminary data analysis. The first two tests are exact or
asymptotically exact for Poisson processes. The first test based on
permutations and global envelopes allows us to detect at which spatial and
temporal locations or lags the data deviate from the null hypothesis. The
second test is a simple and computationally cheap $\chi^2$-test. The third test
is based on statistical reconstruction method and can be generally applied for
non-Poisson processes. The performance of the first two tests is studied in a
simulation study for Poisson and non-Poisson models. The third test is applied
to the real data of the UK 2001 epidemic foot and mouth disease.
"
2009.04834,2020-09-11,Variance decompositions for extensive-form games,"  Quantitative measures of randomness in games are useful for game design and
have implications for gambling law. We treat the outcome of a game as a random
variable and derive a closed-form expression and estimator for the variance in
the outcome attributable to a player of the game. We analyze poker hands to
show that randomness in the cards dealt has little influence on the outcomes of
each hand. A simple example is given to demonstrate how variance decompositions
can be used to measure other interesting properties of games.
"
2009.05247,2022-05-10,"A simulation study of semiparametric estimation in copula models based
  on minimum Alpha-Divergence","  The purpose of this paper is to introduce two semiparametric methods for the
estimation of copula parameter. These methods are based on minimum
Alpha-Divergence between a non-parametric estimation of copula density using
local likelihood probit transformation method and a true copula density
function. A Monte Carlo study is performed to measure the performance of these
methods based on Hellinger distance and Neyman divergence as special cases of
Alpha-Divergence. Simulation results are compared to the Maximum
Pseudo-Likelihood (MPL) estimation as a conventional estimation method in
well-known bivariate copula models. These results show that the proposed method
based on Minimum Pseudo Hellinger Distance estimation has a good performance in
small sample size and weak dependency situations. The parameter estimation
methods are applied to a real data set in Hydrology.
"
2009.05319,2020-09-16,"A Study on the Possible Effects of the Implementation of the Nordic
  Model in India on Crime Rates and Sexually Transmitted Diseases","  Prostitution is one of the root causes of sex trafficking and the
transmission of sexual diseases. The rules and regulations followed by the
Indian government to regulate the same, fall under the umbrella of the
abolitionism model. Neo-abolitionism (also known as the Nordic model) is a new
legislative model that has been introduced by the Nordic countries to regulate
prostitution. The purpose of this research paper is to examine the possible
effects of the application of the Nordic model on the crime rates and the
spread of sexually transmitted diseases in India. Further, we also aim to study
the effects of the implementation of Neo-abolitionism in Sweden.
"
2009.06615,2020-09-15,"Multilevel regression with poststratification for the national level
  Viber/Street poll on the 2020 presidential election in Belarus","  Independent sociological polls are forbidden in Belarus. Online polls
performed without sound scientific rigour do not yield representative results.
Yet, both inside and outside Belarus it is of great importance to obtain
precise estimates of the ratings of all candidates. These ratings could
function as reliable proxies for the election's outcomes. We conduct an
independent poll based on the combination of the data collected via Viber and
on the streets of Belarus. The Viber and the street data samples consist of
almost 45000 and 1150 unique observations respectively. Bayesian regressions
with poststratification were build to estimate ratings of the candidates and
rates of early voting turnout for the population as a whole and within various
focus subgroups. We show that both the officially announced results of the
election and early voting rates are highly improbable. With a probability of at
least 95%, Sviatlana Tikhanouskaya's rating lies between 75% and 80%, whereas
Aliaksandr Lukashenka's rating lies between 13% and 18% and early voting rate
predicted by the method ranges from 9% to 13% of those who took part in the
election. These results contradict the officially announced outcomes, which are
10.12%, 80.11%, and 49.54% respectively and lie far outside even the 99.9%
credible intervals predicted by our model. The only marginal groups of people
where the upper bounds of the 99.9% credible intervals of the rating of
Lukashenka are above 50% are people older than 60 and uneducated people. For
all other marginal subgroups, including rural residents, even the upper bounds
of 99.9% credible intervals for Lukashenka are far below 50%. The same is true
for the population as a whole. Thus, with a probability of at least 99.9%
Lukashenka could not have had enough electoral support to win the 2020
presidential election in Belarus.
"
2009.06810,2021-11-23,"Using Known Words to Learn More Words: A Distributional Analysis of
  Child Vocabulary Development","  Why do children learn some words before others? Understanding individual
variability across children and also variability across words, may be
informative of the learning processes that underlie language learning. We
investigated item-based variability in vocabulary development using lexical
properties of distributional statistics derived from a large corpus of
child-directed speech. Unlike previous analyses, we predicted word trajectories
cross-sectionally, shedding light on trends in vocabulary development that may
not have been evident at a single time point. We also show that whether one
looks at a single age group or across ages as a whole, the best distributional
predictor of whether a child knows a word is the number of other known words
with which that word tends to co-occur. Keywords: age of acquisition;
vocabulary development; lexical diversity; child-directed speech;
"
2009.07765,2021-02-23,"A note on the closed-form solution for the longest head run problem of
  Abraham de Moivre","  The problem of the longest head run was introduced and solved by Abraham de
Moivre in the second edition of his book Doctrine of Chances (de Moivre, 1738).
The closed-form solution as a finite sum involving binomial coefficients was
provided in Uspensky (1937). Since then, the problem and its variations and
extensions have found broad interest and diverse applications. Surprisingly, a
very simple closed form can be obtained, which we present in this note.
"
2009.11646,2020-09-25,"Risk upper bounds for RKHS ridge group sparse estimator in the
  regression model with non-Gaussian and non-bounded error","  We consider the problem of estimating a meta-model of an unknown regression
model with non-Gaussian and non-bounded error. The meta-model belongs to a
reproducing kernel Hilbert space constructed as a direct sum of Hilbert spaces
leading to an additive decomposition including the variables and interactions
between them. The estimator of this meta-model is calculated by minimizing an
empirical least-squares criterion penalized by the sum of the Hilbert norm and
the empirical $L^2$-norm. In this context, the upper bounds of the empirical
$L^2$ risk and the $L^2$ risk of the estimator are established.
"
2010.02205,2020-10-07,Dealing with multiple testing: To adjust or not to adjust,"  Multiple testing problems arise naturally in scientific studies because of
the need to capture or convey more information with more variables. The
literature is enormous, but the emphasis is primarily methodological, providing
numerous methods with their mathematical justification and practical
implementation. Our aim is to highlight the logical issues involved in the
application of multiple testing adjustment.
"
2010.02211,2020-10-07,"Likelihood-based solution to the Monty Hall puzzle and a related
  3-prisoner paradox","  The Monty Hall puzzle has been solved and dissected in many ways, but always
using probabilistic arguments, so it is considered a probability puzzle. In
this paper the puzzle is set up as an orthodox statistical problem involving an
unknown parameter, a probability model and an observation. This means we can
compute a likelihood function, and the decision to switch corresponds to
choosing the maximum likelihood solution. One advantage of the likelihood-based
solution is that the reasoning applies to a single game, unaffected by the
future plan of the host. I also describe an earlier version of the puzzle in
terms of three prisoners: two to be executed and one released. Unlike the goats
and the car, these prisoners have consciousness, so they can think about
exchanging punishments. When two of them do that, however, we have a paradox,
where it is advantageous for both to exchange their punishment with each other.
Overall, the puzzle and the paradox are useful examples of statistical
thinking, so they are excellent teaching topics.
"
2010.03827,2020-10-09,A spatial functional count model for heterogeneity analysis in time,"  A spatial curve dynamical model framework is adopted for functional
prediction of counts in a spatiotemporal log-Gaussian Cox process model. Our
spatial functional estimation approach handles both wavelet-based heterogeneity
analysis in time, and spectral analysis in space. Specifically, model fitting
is achieved by minimising the information divergence or relative entropy
between the multiscale model underlying the data and the corresponding
candidates in the spatial spectral domain. A simulation study is carried out
within the family of log-Gaussian Spatial Autoregressive l2-valued processes
(SARl2 processes) to illustrate the asymptotic properties of the proposed
spatial functional estimators. We apply our modelling strategy to
spatiotemporal prediction of respiratory disease mortality.
"
2010.06698,2022-01-28,Product risk assessment: a Bayesian network approach,"  Product risk assessment is the overall process of determining whether a
product, which could be anything from a type of washing machine to a type of
teddy bear, is judged safe for consumers to use. There are several methods used
for product risk assessment, including RAPEX, which is the primary method used
by regulators in the UK and EU. However, despite its widespread use, we
identify several limitations of RAPEX including a limited approach to handling
uncertainty and the inability to incorporate causal explanations for using and
interpreting test data. In contrast, Bayesian Networks (BNs) are a rigorous,
normative method for modelling uncertainty and causality which are already used
for risk assessment in domains such as medicine and finance, as well as
critical systems generally. This article proposes a BN model that provides an
improved systematic method for product risk assessment that resolves the
identified limitations with RAPEX. We use our proposed method to demonstrate
risk assessments for a teddy bear and a new uncertified kettle for which there
is no testing data and the number of product instances is unknown. We show
that, while we can replicate the results of the RAPEX method, the BN approach
is more powerful and flexible.
"
2010.07017,2020-10-15,Computational Skills by Stealth in Secondary School Data Science,"  The unprecedented growth in the availability of data of all types and
qualities and the emergence of the field of data science has provided an
impetus to finally realizing the implementation of the full breadth of the
Nolan and Temple Lang proposed integration of computing concepts into
statistics curricula at all levels in statistics and new data science programs
and courses. Moreover, data science, implemented carefully, opens accessible
pathways to stem for students for whom neither mathematics nor computer science
are natural affinities, and who would traditionally be excluded. We discuss a
proposal for the stealth development of computational skills in students' first
exposure to data science through careful, scaffolded exposure to computation
and its power. The intent of this approach is to support students, regardless
of interest and self-efficacy in coding, in becoming data-driven learners, who
are capable of asking complex questions about the world around them, and then
answering those questions through the use of data-driven inquiry. This
discussion is presented in the context of the International Data Science in
Schools Project which recently published computer science and statistics
consensus curriculum frameworks for a two-year secondary school data science
program, designed to make data science accessible to all.
"
2010.08104,2020-10-19,"On statistical deficiency: Why the test statistic of the matching method
  is hopelessly underpowered and uniquely informative","  The random variate m is, in combinatorics, a basis for comparing
permutations, as well as the solution to a centuries-old riddle involving the
mishandling of hats. In statistics, m is the test statistic for a disused null
hypothesis statistical test (NHST) of association, the matching method. In this
paper, I show that the matching method has an absolute and relatively low limit
on its statistical power. I do so first by reinterpreting Rae's theorem, which
describes the joint distributions of m with several rank correlation statistics
under a true null. I then derive this property solely from m's unconditional
sampling distribution, on which basis I develop the concept of a deficient
statistic: a statistic that is insufficient and inconsistent and inefficient
with respect to its parameter. Finally, I demonstrate an application for m that
makes use of its deficiency to qualify the sampling error in a jointly
estimated sample correlation.
"
2010.08317,2020-10-19,"Methods to Deal with Unknown Populational Minima during Parameter
  Inference","  There is a myriad of phenomena that are better modelled with semi-infinite
distribution families, many of which are studied in survival analysis. When
performing inference, lack of knowledge of the populational minimum becomes a
problem, which can be dealt with by making a good guess thereof, or by
handcrafting a grid of initial parameters that will be useful for that
particular problem. These solutions are fine when analyzing a single set of
samples, but it becomes unfeasible when there are multiple datasets and a
case-by-case analysis would be too time consuming. In this paper we propose
methods to deal with the populational minimum in algorithmic, efficient and/or
simple ways. Six methods are presented and analyzed, two of which have full
theoretical support, but lack simplicity. The other four are simple and have
some theoretical grounds in non-parametric results such as the law of iterated
logarithm, and they exhibited very good results when it comes to maximizing
likelihood and being able to recycle the grid of initial parameters among the
datasets. With our results, we hope to ease the inference process for
practitioners, and expect that these methods will eventually be included in
software packages themselves.
"
2010.09563,2021-05-18,"A tutorial comparing different covariate balancing methods with an
  application evaluating the causal effects of substance use treatment programs
  for adolescents","  Randomized controlled trials are the gold standard for measuring causal
effects. However, they are often not always feasible, and causal treatment
effects must be estimated from observational data. Observational studies do not
allow robust conclusions about causal relationships unless statistical
techniques account for the imbalance of pretreatment confounders across groups
while key assumptions hold. Propensity score and balance weighting (PSBW) are
useful techniques that aim to reduce the imbalances between treatment groups by
weighting the groups to look alike on the observed confounders. There are many
methods available to estimate PSBW. However, it is unclear a priori which will
achieve the best trade-off between covariate balance and effective sample size.
Moreover, it is critical to assess the validity of key assumptions required for
robust estimation of the needed treatment effects, including the overlap and no
unmeasured confounding assumptions. We present a step-by-step guide to
covariate balancing strategies, including how to evaluate overlap, obtain
estimates of PSBW, check for covariate balance, and assess sensitivity to
unobserved confounding. We compare the performance of several estimation
methods using a case study examining the relative effectiveness of substance
use treatment programs and provide a user-friendly web application that can
implement the proposed steps.
"
2010.10513,2020-10-22,Does preregistration improve the credibility of research findings?,"  Preregistration entails researchers registering their planned research
hypotheses, methods, and analyses in a time-stamped document before they
undertake their data collection and analyses. This document is then made
available with the published research report to allow readers to identify
discrepancies between what the researchers originally planned to do and what
they actually ended up doing. This historical transparency is supposed to
facilitate judgments about the credibility of the research findings. The
present article provides a critical review of 17 of the reasons behind this
argument. The article covers issues such as HARKing, multiple testing,
p-hacking, forking paths, optional stopping, researchers' biases, selective
reporting, test severity, publication bias, and replication rates. It is
concluded that preregistration's historical transparency does not facilitate
judgments about the credibility of research findings when researchers provide
contemporary transparency in the form of (a) clear rationales for current
hypotheses and analytical approaches, (b) public access to research data,
materials, and code, and (c) demonstrations of the robustness of research
conclusions to alternative interpretations and analytical approaches.
"
2010.12095,2020-10-26,"Causes of Misleading Statistics and Research Results Irreproducibility:
  A Concise Review","  Bad statistics make research papers unreproducible and misleading. For the
most part, the reasons for such misusage of numerical data have been found and
addressed years ago by experts and proper practical solutions have been
presented instead. Yet, we still see numerous instances of statistical
fallacies in modern researches which without a doubt play a significant role in
the research reproducibility crisis. In this paper, we review different bad
practices that impact the research process from its beginning to its very end.
Additionally, we briefly propose open science as a universal methodology that
can facilitate the entire research life cycle.
"
2010.14555,2021-05-03,"Covariate-adjusted Fisher randomization tests for the average treatment
  effect","  Fisher's randomization test (FRT) delivers exact $p$-values under the strong
null hypothesis of no treatment effect on any units whatsoever and allows for
flexible covariate adjustment to improve the power. Of interest is whether the
procedure could also be valid for testing the weak null hypothesis of zero
average treatment effect. Towards this end, we evaluate two general strategies
for FRT with covariate-adjusted test statistics: that based on the residuals
from an outcome model with only the covariates, and that based on the output
from an outcome model with both the treatment and the covariates. Based on
theory and simulation, we recommend using the ordinary least squares (OLS) fit
of the observed outcome on the treatment, centered covariates, and their
interactions for covariate adjustment, and conducting FRT with the robust
$t$-value of the treatment as the test statistic. The resulting FRT is
finite-sample exact for the strong null hypothesis, asymptotically valid for
the weak null hypothesis, and more powerful than the unadjusted analog under
alternatives, all irrespective of whether the linear model is correctly
specified or not. We develop the theory for complete randomization, cluster
randomization, stratified randomization, and rerandomization, respectively, and
give a recommendation for the test procedure and test statistic under each
design. We first focus on the finite-population perspective and then extend the
result to the super-population perspective, highlighting the difference in
standard errors. Motivated by the similarity in procedure, we also evaluate the
design-based properties of five existing permutation tests originally for
linear models and show the superiority of the proposed FRT for testing the
treatment effects.
"
2010.16025,2020-11-02,"Evaluation of approaches for accommodating interactions and non-linear
  terms in multiple imputation of incomplete three-level data","  Three-level data structures arising from repeated measures on individuals
clustered within larger units are common in health research studies. Missing
data are prominent in such studies and are often handled via multiple
imputation (MI). Although several MI approaches can be used to account for the
three-level structure, including adaptations to single- and two-level
approaches, when the substantive analysis model includes interactions or
quadratic effects these too need to be accommodated in the imputation model. In
such analyses, substantive model compatible (SMC) MI has shown great promise in
the context of single-level data. While there have been recent developments in
multilevel SMC MI, to date only one approach that explicitly handles incomplete
three-level data is available. Alternatively, researchers can use pragmatic
adaptations to single- and two-level MI approaches, or two-level SMC-MI
approaches. We describe the available approaches and evaluate them via
simulation in the context of a three three-level random effects analysis models
involving an interaction between the incomplete time-varying exposure and time,
an interaction between the time-varying exposure and an incomplete time-fixed
confounder, or a quadratic effect of the exposure. Results showed that all
approaches considered performed well in terms of bias and precision when the
target analysis involved an interaction with time, but the three-level SMC MI
approach performed best when the target analysis involved an interaction
between the time-varying exposure and an incomplete time-fixed confounder, or a
quadratic effect of the exposure. We illustrate the methods using data from the
Childhood to Adolescence Transition Study.
"
2011.00992,2020-11-03,"The P-T Probability Framework for Semantic Communication, Falsification,
  Confirmation, and Bayesian Reasoning","  Many researchers want to unify probability and logic by defining logical
probability or probabilistic logic reasonably. This paper tries to unify
statistics and logic so that we can use both statistical probability and
logical probability at the same time. For this purpose, this paper proposes the
P-T probability framework, which is assembled with Shannon's statistical
probability framework for communication, Kolmogorov's probability axioms for
logical probability, and Zadeh's membership functions used as truth functions.
Two kinds of probabilities are connected by an extended Bayes' theorem, with
which we can convert a likelihood function and a truth function from one to
another. Hence, we can train truth functions (in logic) by sampling
distributions (in statistics). This probability framework was developed in the
author's long-term studies on semantic information, statistical learning, and
color vision. This paper first proposes the P-T probability framework and
explains different probabilities in it by its applications to semantic
information theory. Then, this framework and the semantic information methods
are applied to statistical learning, statistical mechanics, hypothesis
evaluation (including falsification), confirmation, and Bayesian reasoning.
Theoretical applications illustrate the reasonability and practicability of
this framework. This framework is helpful for interpretable AI. To interpret
neural networks, we need further study.
"
2011.01041,2020-11-03,"New definitions (measures) of skewness, mean and dispersion of fuzzy
  numbers -- by way of a new representation as parameterized curves","  We give a geometrically motivated measure of skewness, define a mean value
triangle number, and dispersion (in that order) of a fuzzy number without
reference or seeking analogy to the namesake but parallel concepts in
probability theory. These measures come about by way of a new representation of
fuzzy numbers as parameterized curves respectively their associated tangent
bundle. Importantly skewness and dispersion are given as functions of $\alpha$
(the degree of membership) and such may be given separately and pointwise at
each $\alpha$-level, as well as overall. This allows for e.g., when a
mathematical model is formulated in fuzzy numbers, to run optimization programs
level-wise thereby encapsuling with deliberate accuracy the involved membership
functions' characteristics while increasing the computational complexity by
only a multiplicative factor compared to the same program formulated in real
variables and parameters. As an example the work offers a contribution to the
recently very popular fuzzy mean-variance-skewness portfolio optimization.
"
2011.02677,2022-06-02,The causal foundations of applied probability and statistics,"  Statistical science (as opposed to mathematical statistics) involves far more
than probability theory, for it requires realistic causal models of data
generators - even for purely descriptive goals. Statistical decision theory
requires more causality: Rational decisions are actions taken to minimize costs
while maximizing benefits, and thus require explication of causes of loss and
gain. Competent statistical practice thus integrates logic, context, and
probability into scientific inference and decision using narratives filled with
causality. This reality was seen and accounted for intuitively by the founders
of modern statistics, but was not well recognized in the ensuing statistical
theory (which focused instead on the causally inert properties of probability
measures). Nonetheless, both statistical foundations and basic statistics can
and should be taught using formal causal models. The causal view of statistical
science fits within a broader information-processing framework which
illuminates and unifies frequentist, Bayesian, and related probability-based
foundations of statistics. Causality theory can thus be seen as a key component
connecting computation to contextual information, not extra-statistical but
instead essential for sound statistical training and applications.
"
2011.06689,2020-11-16,"An exploratory assessment of a multidimensional healthcare and economic
  data on COVID-19 in Nigeria","  The coronavirus disease of 2019 (COVID-19) is a pandemic that is ravaging
Nigeria and the world at large. This data article provides a dataset of daily
updates of COVID-19 as reported online by the Nigeria Centre for Disease
Control (NCDC) from February 27, 2020 to September 29, 2020. The data were
obtained through web scraping from different sources and it includes some
economic variables such as the Nigeria budget for each state in 2020,
population estimate, healthcare facilities, and the COVID-19 laboratories in
Nigeria. The dataset has been processed using the standard of the FAIR data
principle which encourages its findability, accessibility, interoperability,
and reusability and will be relevant to researchers in different fields such as
Data Science, Epidemiology, Earth Modelling, and Health Informatics.
"
2011.07559,2020-11-17,"Semiparametric inference for the scale-mixture of normal partial linear
  regression model with censored data","  In the framework of censored data modeling, the classical linear regression
model that assumes normally distributed random errors has received increasing
attention in recent years, mainly for mathematical and computational
convenience. However, practical studies have often criticized this linear
regression model due to its sensitivity to departure from the normality and
from the partial nonlinearity. This paper proposes to solve these potential
issues simultaneously in the context of the partial linear regression model by
assuming that the random errors follow a scale-mixture of normal (SMN) family
of distributions. The proposed method allows us to model data with great
flexibility, accommodating heavy tails, and outliers. By implementing the
B-spline function and using the convenient hierarchical representation of the
SMN distributions, a computationally analytical EM-type algorithm is developed
to perform maximum likelihood inference of the model parameters. Various
simulation studies are conducted to investigate the finite sample properties as
well as the robustness of the model in dealing with the heavy-tails distributed
datasets. Real-word data examples are finally analyzed for illustrating the
usefulness of the proposed methodology.
"
2011.09851,2020-11-20,Digital trace data collection through data donation,"  A potentially powerful method of social-scientific data collection and
investigation has been created by an unexpected institution: the law. Article
15 of the EU's 2018 General Data Protection Regulation (GDPR) mandates that
individuals have electronic access to a copy of their personal data, and all
major digital platforms now comply with this law by providing users with ""data
download packages"" (DDPs). Through voluntary donation of DDPs, all data
collected by public and private entities during the course of citizens' digital
life can be obtained and analyzed to answer social-scientific questions - with
consent. Thus, consented DDPs open the way for vast new research opportunities.
However, while this entirely new method of data collection will undoubtedly
gain popularity in the coming years, it also comes with its own questions of
representativeness and measurement quality, which are often evaluated
systematically by means of an error framework. Therefore, in this paper we
provide a blueprint for digital trace data collection using DDPs, and devise a
""total error framework"" for such projects. Our error framework for digital
trace data collection through data donation is intended to facilitate high
quality social-scientific investigations using DDPs while critically reflecting
its unique methodological challenges and sources of error. In addition, we
provide a quality control checklist to guide researchers in leveraging the vast
opportunities afforded by this new mode of investigation.
"
2011.10589,2021-01-08,"CompModels: A suite of computer model test functions for Bayesian
  optimization","  The CompModels package for R provides a suite of computer model test
functions that can be used for computer model prediction/emulation, uncertainty
quantification, and calibration, but in particular, the sequential optimization
of computer models. The package is a mix of real-world physics problems, known
mathematical functions, and black-box functions that have been converted into
computer models with the goal of Bayesian (i.e., sequential) optimization in
mind. Likewise, the package contains computer models that represent either the
constrained or unconstrained optimization case, each with varying levels of
difficulty. In this paper, we illustrate the use of the package with both
real-world examples and black-box functions by solving constrained optimization
problems via Bayesian optimization. Ultimately, the package is shown to provide
users with a source of computer model test functions that are reproducible,
shareable, and that can be used for benchmarking of novel optimization methods.
"
2011.11177,2020-11-24,"Gonogo: An R Implementation of Test Methods to Perform, Analyze and
  Simulate Sensitivity Experiments","  This work provides documentation for a suite of R functions contained in
gonogo.R. The functions provide sensitivity testing practitioners and
researchers with an ability to conduct, analyze and simulate various
sensitivity experiments involving binary responses and a single stimulus level
(e.g., drug dosage, drop height, velocity, etc.). Included are the modern Neyer
and 3pod adaptive procedures, as well as the Bruceton and Langlie. The latter
two benchmark procedures are capable of being performed according to
generalized up-down transformed-response rules. Each procedure is designated
phase-one of a three-phase experiment. The goal of phase-one is to achieve
overlapping data. The two additional (and optional) refinement phases utilize
the D-optimal criteria and the Robbins-Monro-Joseph procedure. The goals of the
two refinement phases are to situate testing in the vicinity of the median and
tails of the latent response distribution, respectively.
"
2011.13758,2020-11-30,"Comparisons of proportions in k dose groups against a negative control
  assuming order restriction: Williams-type test vs. closed test procedures","  The comparison of proportions is considered in the asymptotic generalized
linear model with the odds ratio as effect size. When several doses are
compared with a control assuming an order restriction, a Williams-type trend
test can be used. As an alternative, two variants of the closed testing
approach are considered, one using global Williams-tests in the partition
hypotheses, one with pairwise contrasts. Their advantages in terms of power and
simplicity are demonstrated. Related R-code is provided.
"
2012.03854,2022-02-09,Forecasting: theory and practice,"  Forecasting has always been at the forefront of decision making and planning.
The uncertainty that surrounds the future is both exciting and challenging,
with individuals and organisations seeking to minimise risks and maximise
utilities. The large number of forecasting applications calls for a diverse set
of forecasting methods to tackle real-life challenges. This article provides a
non-systematic review of the theory and the practice of forecasting. We provide
an overview of a wide range of theoretical, state-of-the-art models, methods,
principles, and approaches to prepare, produce, organise, and evaluate
forecasts. We then demonstrate how such theoretical concepts are applied in a
variety of real-life contexts.
  We do not claim that this review is an exhaustive list of methods and
applications. However, we wish that our encyclopedic presentation will offer a
point of reference for the rich work that has been undertaken over the last
decades, with some key insights for the future of forecasting theory and
practice. Given its encyclopedic nature, the intended mode of reading is
non-linear. We offer cross-references to allow the readers to navigate through
the various topics. We complement the theoretical concepts and applications
covered by large lists of free or open-source software implementations and
publicly-available databases.
"
2012.04062,2020-12-09,"Including climate system feedbacks in calculations of the social cost of
  methane","  Integrated assessment models (IAMs) are valuable tools that consider the
interactions between socioeconomic systems and the climate system.
Decision-makers and policy analysts employ IAMs to calculate the marginalized
monetary cost of climate damages resulting from an incremental emission of a
greenhouse gas. Used within the context of regulating anthropogenic methane
emissions, this metric is called the social cost of methane (SC-CH$_4$).
Because several key IAMs used for social cost estimation contain a simplified
model structure that prevents the endogenous modeling of non-CO$_2$ greenhouse
gases, very few estimates of the SC-CH$_4$ exist. For this reason, IAMs should
be updated to better represent methane cycle dynamics that are consistent with
comprehensive Earth System Models. We include feedbacks of climate change on
the methane cycle to estimate the SC-CH$_4$. Our expected value for the
SC-CH$_4$ is \$1163/t-CH$_4$ under a constant 3.0% discount rate. This
represents a 44% increase relative to a mean estimate without feedbacks on the
methane cycle.
"
2012.05405,2021-12-21,"PoolTestR: An R package for estimating prevalence and regression
  modelling with pooled samples","  Pooled testing (also known as group testing), where diagnostic tests are
performed on pooled samples, has broad applications in the surveillance of
diseases in animals and humans. An increasingly common use case is molecular
xenomonitoring (MX), where surveillance of vector-borne diseases is conducted
by capturing and testing large numbers of vectors (e.g. mosquitoes). The R
package PoolTestR was developed to meet the needs of increasingly large and
complex molecular xenomonitoring surveys but can be applied to analyse any data
involving pooled testing. PoolTestR includes simple and flexible tools to
estimate prevalence and fit fixed- and mixed-effect generalised linear models
for pooled data in frequentist and Bayesian frameworks. Mixed-effect models
allow users to account for the hierarchical sampling designs that are often
employed in surveys, including MX. We demonstrate the utility of PoolTestR by
applying it to a large synthetic dataset that emulates a MX survey with a
hierarchical sampling design.
"
2012.06077,2020-12-14,"Casting Multiple Shadows: High-Dimensional Interactive Data
  Visualisation with Tours and Embeddings","  Non-linear dimensionality reduction (NLDR) methods such as t-distributed
stochastic neighbour embedding (t-SNE) are ubiquitous in the natural sciences,
however, the appropriate use of these methods is difficult because of their
complex parameterisations; analysts must make trade-offs in order to identify
structure in the visualisation of an NLDR technique. We present visual
diagnostics for the pragmatic usage of NLDR methods by combining them with a
technique called the tour. A tour is a sequence of interpolated linear
projections of multivariate data onto a lower dimensional space. The sequence
is displayed as a dynamic visualisation, allowing a user to see the shadows the
high-dimensional data casts in a lower dimensional view. By linking the tour to
an NLDR view, we can preserve global structure and through user interactions
like linked brushing observe where the NLDR view may be misleading. We display
several case studies from both simulations and single cell transcriptomics,
that shows our approach is useful for cluster orientation tasks.
"
2012.06301,2020-12-14,"Spatial variation in the basic reproduction number of COVID-19: A
  systematic review","  OBJECTIVES: Estimates of the basic reproduction number (R0) of COVID-19 vary
across countries. This paper aims to characterise the spatial variability in R0
across the first six months of the global COVID-19 outbreak, and to explore
social factors that impact R0 estimates at national and regional level.
  METHODS: We searched PubMed, LitCOVID and the WHO COVID-19 database from
January to June 2020. Peer-reviewed English-language papers were included that
provided R0 estimates. For each study, the value of the estimate, country under
study and publication month were extracted. The median R0 value was calculated
per country, and the median and variance were calculated per region. For each
country with an R0 estimate, the Human Development Index (HDI), Sustainable
Mobility Index (SMI), median age, population density and development status
were obtained from external sources.
  RESULTS: A total of 81 studies were included in the analysis. These studies
provided at least one estimate of R0, along with sufficient methodology to
explain how the value was calculated. Values of R0 ranged between 0.48 and
14.8, and between 0.48 and 6.7 when excluding outliers.
  CONCLUSIONS: This systematic review provides a comprehensive overview of the
estimates of the basic reproduction number of COVID-19 globally and highlights
the spatial heterogeneity in R0. Higher values were recorded in more developed
countries, and countries with an older population or more sustainable mobility.
Countries with higher population density had lower R0 estimates. For most
regions, variability in R0 spiked initially before reducing and stabilising as
more estimates became available.
"
2012.08257,2020-12-16,"Ordering results of extreme order statistics from multiple-outlier scale
  models with dependence","  In this paper, we focus on stochastic comparisons of extreme order statistics
stemming from multiple-outlier scale models with dependence. Archimedean copula
is used to model dependence structure among nonnegative random variables.
Sufficient conditions are obtained for comparison of the largest order
statistics in the sense of the usual stochastic, reversed hazard rate, star and
Lorenz orders. The smallest order statistics are also compared with respect to
the usual stochastic, hazard rate, star and Lorenz orders. To illustrate the
theoretical establishments, some examples are provided.
"
2012.10472,2020-12-22,"A Survey on the Visual Perceptions of Gaussian Noise Filtering on
  Photography","  Statisticians, as well as machine learning and computer vision experts, have
been studying image reconstitution through denoising different domains of
photography, such as textual documentation, tomographic, astronomical, and
low-light photography. In this paper, we apply common inferential kernel
filters in the R and python languages, as well as Adobe Lightroom's denoise
filter, and compare their effectiveness in removing noise from JPEG images. We
ran standard benchmark tests to evaluate each method's effectiveness for
removing noise. In doing so, we also surveyed students at Elon University about
their opinion of a single filtered photo from a collection of photos processed
by the various filter methods. Many scientists believe that noise filters cause
blurring and image quality loss so we analyzed whether or not people felt as
though denoising causes any quality loss as compared to their noiseless images.
Individuals assigned scores indicating the image quality of a denoised photo
compared to its noiseless counterpart on a 1 to 10 scale. Survey scores are
compared across filters to evaluate whether there were significant differences
in image quality scores received. Benchmark scores were compared to the visual
perception scores. Then, an analysis of covariance test was run to identify
whether or not survey training scores explained any unplanned variation in
visual scores assigned by students across the filter methods.
"
2012.12144,2022-10-11,"Integrating computing in the statistics and data science curriculum:
  Creative structures, novel skills and habits, and ways to teach computational
  thinking","  Nolan and Temple Lang (2010) argued for the fundamental role of computing in
the statistics curriculum. In the intervening decade the statistics education
community has acknowledged that computational skills are as important to
statistics and data science practice as mathematics. There remains a notable
gap, however, between our intentions and our actions. In this special issue of
the *Journal of Statistics and Data Science Education* we have assembled a
collection of papers that (1) suggest creative structures to integrate
computing, (2) describe novel data science skills and habits, and (3) propose
ways to teach computational thinking. We believe that it is critical for the
community to redouble our efforts to embrace sophisticated computing in the
statistics and data science curriculum. We hope that these papers provide
useful guidance for the community to move these efforts forward.
"
2012.12499,2020-12-24,Beyond Strictly Proper Scoring Rules: The Importance of Being Local,"  The evaluation of probabilistic forecasts plays a central role both in the
interpretation and in the use of forecast systems and their development.
Probabilistic scores (scoring rules) provide statistical measures to assess the
quality of probabilistic forecasts. Often, many probabilistic forecast systems
are available while evaluations of their performance are not standardized, with
different scoring rules being used to measure different aspects of forecast
performance. Even when the discussion is restricted to strictly proper scoring
rules, there remains considerable variability between them; indeed strictly
proper scoring rules need not rank competing forecast systems in the same order
when none of these systems are perfect. The locality property is explored to
further distinguish scoring rules. The nonlocal strictly proper scoring rules
considered are shown to have a property that can produce ""unfortunate""
evaluations. Particularly the fact that Continuous Rank Probability Score
prefers the outcome close to the median of the forecast distribution regardless
the probability mass assigned to the value at/near the median raises concern to
its use. The only local strictly proper scoring rules, the logarithmic score,
has direct interpretations in terms of probabilities and bits of information.
The nonlocal strictly proper scoring rules, on the other hand, lack meaningful
direct interpretation for decision support. The logarithmic score is also shown
to be invariant under smooth transformation of the forecast variable, while the
nonlocal strictly proper scoring rules considered may, however, change their
preferences due to the transformation. It is therefore suggested that the
logarithmic score always be included in the evaluation of probabilistic
forecasts.
"
2012.12891,2020-12-24,New plans orthogonal through the block factor,"  In the present paper we construct plans orthogonal through the block factor
(POTBs). We describe procedures for adding blocks as well as factors to an
initial plan and thus generate a bigger plan. Using these procedures we
construct POTBs for symmetrical experiments with factors having three or more
levels. We also construct a series of plans inter-class orthogonal through the
block factor for two-level factors.
"
2012.13045,2020-12-25,"Regret Bound Balancing and Elimination for Model Selection in Bandits
  and RL","  We propose a simple model selection approach for algorithms in stochastic
bandit and reinforcement learning problems. As opposed to prior work that
(implicitly) assumes knowledge of the optimal regret, we only require that each
base algorithm comes with a candidate regret bound that may or may not hold
during all rounds. In each round, our approach plays a base algorithm to keep
the candidate regret bounds of all remaining base algorithms balanced, and
eliminates algorithms that violate their candidate bound. We prove that the
total regret of this approach is bounded by the best valid candidate regret
bound times a multiplicative factor. This factor is reasonably small in several
applications, including linear bandits and MDPs with nested function classes,
linear bandits with unknown misspecification, and LinUCB applied to linear
bandits with different confidence parameters. We further show that, under a
suitable gap-assumption, this factor only scales with the number of base
algorithms and not their complexity when the number of rounds is large enough.
Finally, unlike recent efforts in model selection for linear stochastic
bandits, our approach is versatile enough to also cover cases where the context
information is generated by an adversarial environment, rather than a
stochastic one.
"
2012.13427,2020-12-29,Reproducible Workflow,"  Reproducibility has been consistently identified as an important component of
scientific research. Although there is a general consensus on the importance of
reproducibility along with the other commonly used 'R' terminology (i.e.,
Replicability, Repeatability etc.), there is some disagreement on the usage of
these terms, including conflicting definitions used by different parts of the
research community. In this encyclopedia article, we explore the different
definitions used in scientific literature (specifically pertaining to
computational research), whether there is a need for a single standardized
definition and provide an alternative based on non-functional requirements. We
also describe the role of reproducibility (and other R's) in scientific
workflows.
"
2012.14390,2020-12-29,Kill The Math and Let the Introductory Course Be Born,"  Our introductory classes in statistics and data science use too much
mathematics. The key causal effect which our students want our classes to have
is to improve their future performance and opportunities. The more professional
their computing skills (in the context of data analysis), the greater their
likely success. Introductory courses should feature almost no
mathematical/statistical formulas beyond simple algebra.
"
2012.14503,2021-01-01,"Growth, development, and structural change at the firm-level: The
  example of the PR China","  Understanding the microeconomic details of technological catch-up processes
offers great potential for informing both innovation economics and development
policy. We study the economic transition of the PR China from an agrarian
country to a high-tech economy as one example for such a case. It is clear from
past literature that rapidly rising productivity levels played a crucial role.
However, the distribution of labor productivity in Chinese firms has not been
comprehensively investigated and it remains an open question if this can be
used to guide economic development. We analyze labor productivity and the
dynamic change of labor productivity in firm-level data for the years 1998-2013
from the Chinese Industrial Enterprise Database. We demonstrate that both
variables are conveniently modeled as L\'evy alpha-stable distributions,
provide parameter estimates and analyze dynamic changes to this distribution.
We find that the productivity gains were not due to super-star firms, but due
to a systematic shift of the entire distribution with otherwise mostly
unchanged characteristics. We also found an emerging right-skew in the
distribution of labor productivity change. While there are significant
differences between the 31 provinces and autonomous regions of the P.R. China,
we also show that there are systematic relations between micro-level and
province-level variables. We conclude with some implications of these findings
for development policy.
"
2012.14941,2021-01-01,"The wealth of nations and the health of populations: A
  quasi-experimental design of the impact of sovereign debt crises on child
  mortality","  The wealth of nations and the health of populations are intimately strongly
associated, yet the extent to which economic prosperity (GDP per capita) causes
improved health remains disputed. The purpose of this article is to analyze the
impact of sovereign debt crises (SDC) on child mortality, using a sample of 57
low- and middle-income countries surveyed by the Demographic and Health Survey
between the years 1990 and 2015. These surveys supply 229 household data and
containing about 3 million childbirth history records. This focus on SDC
instead of GDP provides a quasi-experimental moment in which the influence of
unobserved confounding is less than a moment analyzing the normal fluctuations
of GDP. This study measures child mortality at six thresholds: neonatal,
under-one (infant), under-two, under-three, under-four, and under-five
mortality. Using a machine-learning (ML) model for causal inference, this study
finds that while an SDC causes an adverse yet statistically insignificant
effect on neonatal mortality, all other child mortality group samples are
adversely affected between a probability of 0.12 to 0.14 (all statistically
significant at the 95-percent threshold). Through this ML, this study also
finds that the most important treatment heterogeneity moderator, in the entire
adjustment set, is whether a child is born in a low-income country.
"
2101.00548,2021-01-05,"Better understanding of the multivariate hypergeometric distribution
  with implications in design-based survey sampling","  Multivariate hypergeometric distribution arises frequently in elementary
statistics and probability courses, for simultaneously studying the occurence
law of specified events, when sampling without replacement from a finite
population with fixed number of classification. Covariance matrix of this
distribution is well known to be identical to its multinomial counterpart
multiplied by 1-(n-1)/(N-1), with N and n being population and sample sizes,
respectively. It appears to however, have been less discussed in the literature
about the meaning of this relationship, especially regarding the specific form
of the multiplier. Based on an augmenting argument together with probabilistic
symmetry, we present a more transparent understanding for the covariance
structure of the multivariate hypergeometric distribution. We discuss
implications of these combined techniques and provide a unified description
about the relative efficiency for estimating population mean based on simple
random sampling, probability proportional-to-size sampling and adaptive cluster
sampling, with versus without replacement. We also provide insight into the
classic random group method for variance estimation.
"
2101.03491,2022-05-10,"gwpcorMapper: an interactive mapping tool for exploring geographically
  weighted correlation and partial correlation in high-dimensional geospatial
  datasets","  Exploratory spatial data analysis (ESDA) plays a key role in research that
includes geographic data. In ESDA, analysts often want to be able to visualize
observations and local relationships on a map. However, software dedicated to
visualizing local spatial relations be-tween multiple variables in high
dimensional datasets remains undeveloped. This paper introduces gwpcorMapper, a
newly developed software application for mapping geographically weighted
correlation and partial correlation in large multivariate datasets.
gwpcorMap-per facilitates ESDA by giving researchers the ability to interact
with map components that describe local correlative relationships. We built
gwpcorMapper using the R Shiny framework. The software inherits its core
algorithm from GWpcor, an R library for calculating the geographically weighted
correlation and partial correlation statistics. We demonstrate the application
of gwpcorMapper by using it to explore census data in order to find meaningful
relationships that describe the work-life environment in the 23 special wards
of Tokyo, Japan. We show that gwpcorMapper is useful in both variable selection
and parameter tuning for geographically weighted statistics. gwpcorMapper
highlights that there are strong statistically clear local variations in the
relationship between the number of commuters and the total number of hours
worked when considering the total population in each district across the 23
special wards of Tokyo. Our application demonstrates that the ESDA process with
high-dimensional geospatial data using gwpcorMapper has applications across
multiple fields.
"
2101.04611,2021-01-19,"Directed Hybrid Random Networks Mixing Preferential Attachment with
  Uniform Attachment Mechanisms","  Motivated by the complexity of network data, we propose a directed hybrid
random network that mixes preferential attachment (PA) rules with uniform
attachment (UA) rules. When a new edge is created, with probability $p\in
[0,1]$, it follows the PA rule. Otherwise, this new edge is added between two
uniformly chosen nodes. Such mixture makes the in- and out-degrees of a fixed
node grow at a slower rate, compared to the pure PA case, thus leading to
lighter distributional tails. Useful inference methods for the proposed hybrid
model are then provided and applied to both synthetic and real datasets. We see
that with extra flexibility given by the parameter $p$, the hybrid random
network provides a better fit to real-world scenarios, where lighter tails from
in- and out-degrees are observed.
"
2101.05677,2021-01-15,"Improving non-deterministic uncertainty modelling in Industry 4.0
  scheduling","  The latest Industrial revolution has helped industries in achieving very high
rates of productivity and efficiency. It has introduced data aggregation and
cyber-physical systems to optimize planning and scheduling. Although,
uncertainty in the environment and the imprecise nature of human operators are
not accurately considered for into the decision making process. This leads to
delays in consignments and imprecise budget estimations. This widespread
practice in the industrial models is flawed and requires rectification. Various
other articles have approached to solve this problem through stochastic or
fuzzy set model methods. This paper presents a comprehensive method to
logically and realistically quantify the non-deterministic uncertainty through
probabilistic uncertainty modelling. This method is applicable on virtually all
Industrial data sets, as the model is self adjusting and uses
epsilon-contamination to cater to limited or incomplete data sets. The results
are numerically validated through an Industrial data set in Flanders, Belgium.
The data driven results achieved through this robust scheduling method
illustrate the improvement in performance.
"
2101.05744,2023-03-28,A comparative study of scoring systems by simulations,"  Scoring rules aggregate individual rankings by assigning some points to each
position in each ranking such that the total sum of points provides the overall
ranking of the alternatives. They are widely used in sports competitions
consisting of multiple contests. We study the tradeoff between two risks in
this setting: (1) the threat of early clinch when the title has been clinched
before the last contest(s) of the competition take place; (2) the danger of
winning the competition without finishing first in any contest. In particular,
four historical points scoring systems of the Formula One World Championship
are compared with the family of geometric scoring rules, recently proposed by
an axiomatic approach. The schemes used in practice are found to be competitive
with respect to these goals, and the current rule seems to be a reasonable
compromise close to the Pareto frontier. Our results shed more light on the
evolution of the Formula One points scoring systems and contribute to the issue
of choosing the set of point values.
"
2101.06854,2021-01-19,Statistical Analysis of Quantum Annealing,"  Quantum computers use quantum resources to carry out computational tasks and
may outperform classical computers in solving certain computational problems.
Special-purpose quantum computers such as quantum annealers employ quantum
adiabatic theorem to solve combinatorial optimization problems. In this paper,
we compare classical annealings such as simulated annealing and quantum
annealings that are done by the D-Wave machines both theoretically and
numerically. We show that if the classical and quantum annealing are
characterized by equivalent Ising models, then solving an optimization problem,
i.e., finding the minimal energy of each Ising model, by the two annealing
procedures, are mathematically identical. For quantum annealing, we also derive
the probability lower-bound on successfully solving an optimization problem by
measuring the system at the end of the annealing procedure. Moreover, we
present the Markov chain Monte Carlo (MCMC) method to realize quantum annealing
by classical computers and investigate its statistical properties. In the
numerical section, we discuss the discrepancies between the MCMC based
annealing approaches and the quantum annealing approach in solving optimization
problems.
"
2101.07097,2022-06-14,"The Sources of Statistical Bias Series: Simulated Demonstrations to
  Illustrate the Causes and Effects of Biases in Statistical Estimates","  When teaching and discussing statistical assumptions, our focus is oftentimes
placed on how to test and address potential violations rather than the effects
of violating assumptions on the estimates produced by our statistical models.
The latter represents a potential avenue to help us better understand the
impact of researcher degrees of freedom on the statistical estimates we
produce. The Violating Assumptions Series is an endeavor I have undertaken to
demonstrate the effects of violating assumptions on the estimates produced
across various statistical models. The series will review assumptions
associated with estimating causal associations, as well as more complicated
statistical models including, but not limited to, multilevel models, path
models, structural equation models, and Bayesian models. In addition to the
primary goal, the series of posts is designed to illustrate how simulations can
be used to develop a comprehensive understanding of applied statistics.
"
2101.07710,2021-01-20,"The effect of Hybrid Principal Components Analysis on the Signal
  Compression Functional Regression: With EEG-fMRI Application","  Objective: In some situations that exist both scalar and functional data,
called mixed and hybrid data, the hybrid PCA (HPCA) was introduced. Among the
regression models for the hybrid data, we can count covariate-adjusted HPCA,
the Semi-functional partial linear regression, function-on-function (FOF)
regression with signal compression, and functional additive regression, models.
In this article, we study the effects of HPCA decomposition of hybrid data on
the prediction accuracy of the FOF regression with signal compressions. Method:
We stated a two-step procedure for incorporating the HPCA in the functional
regressions. The first step is reconstructing the data based on the HPCAs and
the second step is merging data on the other dimensions and calculate the
point-wise average of the desired functional dimension. We also choose the
number of HPCA based on Mean Squared Perdition Error (MSPE). Result: In the two
simulations, we show that the regression models with the first HPCA have the
best accuracy prediction and model fit summaries among no HPCA and all HPCAs
with a training/testing approach. Finally, we applied our methodology to the
EEG-fMRI dataset. Conclusions: We conclude that our methodology improves the
prediction of the experiments with the EEG datasets. And we recommend that
instead of using the functional PCA on the desired dimension, reconstruct the
data with HPCA and average it on the other two dimensions for functional
regression models.
"
2101.08162,2021-01-22,Lessons from the German Tank Problem,"  During World War II the German army used tanks to devastating advantage. The
Allies needed accurate estimates of their tank production and deployment. They
used two approaches to find these values: spies, and statistics. This note
describes the statistical approach. Assuming the tanks are labeled
consecutively starting at 1, if we observe $k$ serial numbers from an unknown
number $N$ of tanks, with the maximum observed value $m$, then the best
estimate for $N$ is $m(1 + 1/k) - 1$. This is now known as the German Tank
Problem, and is a terrific example of the applicability of mathematics and
statistics in the real world. The first part of the paper reproduces known
results, specifically deriving this estimate and comparing its effectiveness to
that of the spies. The second part presents a result we have not found in print
elsewhere, the generalization to the case where the smallest value is not
necessarily 1. We emphasize in detail why we are able to obtain such clean,
closed-form expressions for the estimates, and conclude with an appendix
highlighting how to use this problem to teach regression and how statistics can
help us find functional relationships.
"
2101.11470,2021-07-20,Listwise Deletion in High Dimensions,"  We consider the properties of listwise deletion when both $n$ and the number
of variables grow large. We show that when (i) all data has some idiosyncratic
missingness and (ii) the number of variables grows superlogarithmically in $n$,
then, for large $n$, listwise deletion will drop all rows with probability 1.
Using two canonical datasets from the study of comparative politics and
international relations, we provide numerical illustration that these problems
may emerge in real world settings. These results suggest, in practice, using
listwise deletion may mean using few of the variables available to the
researcher.
"
2101.11857,2021-08-09,Best Practices in Statistical Computing,"  The world is becoming increasingly complex, both in terms of the rich sources
of data we have access to as well as in terms of the statistical and
computational methods we can use on those data. These factors create an
ever-increasing risk for errors in our code and sensitivity in our findings to
data preparation and execution of complex statistical and computing methods.
The consequences of coding and data mistakes can be substantial. Openness
(e.g., providing others with data code) and transparency (e.g., requiring that
data processing and code follow standards) are two key solutions to help
alleviate concerns about replicability and errors. In this paper, we describe
the key steps for implementing a code quality assurance (QA) process for
researchers to follow to improve their coding practices throughout a project to
assure the quality of the final data, code, analyses and ultimately the
results. These steps include: (i) adherence to principles for code writing and
style that follow best practices, (ii) clear written documentation that
describes code, workflow and key analytic decisions; (iii) careful version
control, (iv) good data management; and (iv) regular testing and review.
Following all these steps will greatly improve the ability of a study to assure
results are accurate and reproducible. The responsibility for code QA falls not
only on individual researchers but institutions, journals, and funding agencies
as well.
"
2101.12150,2021-03-31,The Agnostic Structure of Data Science Methods,"  In this paper we argue that data science is a coherent and novel approach to
empirical problems that, in its most general form, does not build understanding
about phenomena. Within the new type of mathematization at work in data
science, mathematical methods are not selected because of any relevance for a
problem at hand; mathematical methods are applied to a specific problem only by
'forcing', i.e. on the basis of their ability to reorganize the data for
further analysis and the intrinsic richness of their mathematical structure. In
particular, we argue that deep learning neural networks are best understood
within the context of forcing optimization methods. We finally explore the
broader question of the appropriateness of data science methods in solving
problems. We argue that this question should not be interpreted as a search for
a correspondence between phenomena and specific solutions found by data science
methods; rather, it is the internal structure of data science methods that is
open to precise forms of understanding.
"
2102.00757,2021-02-02,Demographic perspectives in research on global environmental change,"  Human population is at the centre of research on global environmental change.
On the one hand, population dynamics influence the environment and the global
climate system through consumption-based carbon emissions. On the other hand,
health and wellbeing of the population is already being affected by climate
change. The knowledge on population dynamics and population heterogeneity thus
is fundamental in improving our understanding of how population size,
composition and distribution influence global environmental change and how
these changes affect subgroups of population differentially by demographic
characteristics and spatial distribution. Existing theoretical concepts and
methodological tools in demography can be readily applied to the study of
population and global environmental change. In the past couple of decades,
demographic research has enriched climate change research both in the analysis
of the impact of population dynamics on the global climate system as well as
the impact of climate change on human population. What is missing in the
literature is the study that investigates how global environmental change
affect current and future demographic processes and consequently population
trends. If global environmental change does influence fertility, mortality and
migration, the three key demographic components underlying population change,
population estimates and forecast need to adjust from the climate feedback in
population projections. Indisputably, this is the new area of research that
directly requires expertise in population science and contribution from
demographers.
"
2102.01647,2021-02-03,"A Novel Use of Discrete Wavelet Transform Features in the Prediction of
  Epileptic Seizures from EEG Data","  This paper demonstrates the predictive superiority of discrete wavelet
transform (DWT) over previously used methods of feature extraction in the
diagnosis of epileptic seizures from EEG data. Classification accuracy,
specificity, and sensitivity are used as evaluation metrics. We specifically
show the immense potential of 2 combinations (DWT-db4 combined with SVM and
DWT-db2 combined with RF) as compared to others when it comes to diagnosing
epileptic seizures either in the balanced or the imbalanced dataset. The
results also highlight that MFCC performs less than all the DWT used in this
study and that, The mean-differences are statistically significant respectively
in the imbalanced and balanced dataset. Finally, either in the balanced or the
imbalanced dataset, the feature extraction techniques, the models, and the
interaction between them have a statistically significant effect on the
classification accuracy.
"
2102.01892,2021-02-04,A few statistical principles for data science,"  In any other circumstance, it might make sense to define the extent of the
terrain (Data Science) first, and then locate and describe the landmarks
(Principles). But this data revolution we are experiencing defies a cadastral
survey. Areas are continually being annexed into Data Science. For example,
biometrics was traditionally statistics for agriculture in all its forms but
now, in Data Science, it means the study of characteristics that can be used to
identify an individual. Examples of non-intrusive measurements include height,
weight, fingerprints, retina scan, voice, photograph/video (facial landmarks
and facial expressions), and gait. A multivariate analysis of such data would
be a complex project for a statistician, but a software engineer might appear
to have no trouble with it at all. In any applied-statistics project, the
statistician worries about uncertainty and quantifies it by modelling data as
realisations generated from a probability space. Another approach to
uncertainty quantification is to find similar data sets, and then use the
variability of results between these data sets to capture the uncertainty. Both
approaches allow 'error bars' to be put on estimates obtained from the original
data set, although the interpretations are different. A third approach, that
concentrates on giving a single answer and gives up on uncertainty
quantification, could be considered as Data Engineering, although it has staked
a claim in the Data Science terrain. This article presents a few (actually
nine) statistical principles for data scientists that have helped me, and
continue to help me, when I work on complex interdisciplinary projects.
"
2102.03106,2021-02-08,"ROBustness In Network (robin): an R package for Comparison and
  Validation of communities","  In network analysis, many community detection algorithms have been developed,
however, their implementation leaves unaddressed the question of the
statistical validation of the results. Here we present robin(ROBustness In
Network), an R package to assess the robustness of the community structure of a
network found by one or more methods to give indications about their
reliability. The procedure initially detects if the community structure found
by a set of algorithms is statistically significant and then compares two
selected detection algorithms on the same graph to choose the one that better
fits the network of interest. We demonstrate the use of our package on the
American College Football benchmark dataset.
"
2102.03240,2021-02-08,De-carbonization of global energy use during the COVID-19 pandemic,"  The COVID-19 pandemic has disrupted human activities, leading to
unprecedented decreases in both global energy demand and GHG emissions. Yet a
little known that there is also a low carbon shift of the global energy system
in 2020. Here, using the near-real-time data on energy-related GHG emissions
from 30 countries (about 70% of global power generation), we show that the
pandemic caused an unprecedented de-carbonization of global power system,
representing by a dramatic decrease in the carbon intensity of power sector
that reached a historical low of 414.9 tCO2eq/GWh in 2020. Moreover, the share
of energy derived from renewable and low-carbon sources (nuclear, hydro-energy,
wind, solar, geothermal, and biomass) exceeded that from coal and oil for the
first time in history in May of 2020. The decrease in global net energy demand
(-1.3% in the first half of 2020 relative to the average of the period in
2016-2019) masks a large down-regulation of fossil-fuel-burning power plants
supply (-6.1%) coincident with a surge of low-carbon sources (+6.2%).
Concomitant changes in the diurnal cycle of electricity demand also favored
low-carbon generators, including a flattening of the morning ramp, a lower
midday peak, and delays in both the morning and midday load peaks in most
countries. However, emission intensities in the power sector have since
rebounded in many countries, and a key question for climate mitigation is thus
to what extent countries can achieve and maintain lower, pandemic-level carbon
intensities of electricity as part of a green recovery.
"
2102.03667,2022-06-09,"COVIDHunter: An Accurate, Flexible, and Environment-Aware Open-Source
  COVID-19 Outbreak Simulation Model","  Background: Early detection and isolation of COVID-19 patients are essential
for successful implementation of mitigation strategies and eventually curbing
the disease spread. With a limited number of daily COVID-19 tests performed in
every country, simulating the COVID-19 spread along with the potential effect
of each mitigation strategy currently remains one of the most effective ways in
managing the healthcare system and guiding policy-makers. Methods: We introduce
COVIDHunter, a flexible and accurate COVID-19 outbreak simulation model that
evaluates the current mitigation measures that are applied to a region and
provides suggestions on what strength the upcoming mitigation measure should
be. The key idea of COVIDHunter is to quantify the spread of COVID-19 in a
geographical region by simulating the average number of new infections caused
by an infected person considering the effect of external factors, such as
environmental conditions (e.g., climate, temperature, humidity) and mitigation
measures. Results: Using Switzerland as a case study, COVIDHunter estimates
that if the policy-makers relax the mitigation measures by 50% for 30 days then
both the daily capacity need for hospital beds and daily number of deaths
increase exponentially by an average of 5.1x, who may occupy ICU beds and
ventilators for a period of time. Unlike existing models, the COVIDHunter model
accurately monitors and predicts the daily number of cases, hospitalizations,
and deaths due to COVID-19. Our model is flexible to configure and simple to
modify for modeling different scenarios under different environmental
conditions and mitigation measures. Availability: We release the source code of
the COVIDHunter implementation at https://github.com/CMU- SAFARI/COVIDHunter
and show how to flexibly configure our model for any scenario and easily extend
it for different measures and conditions than we account for.
"
2102.04421,2021-02-09,"A Text Mining Discovery of Similarities and Dissimilarities Among Sacred
  Scriptures","  The careful examination of sacred texts gives valuable insights into human
psychology, different ideas regarding the organization of societies as well as
into terms like truth and God. To improve and deepen our understanding of
sacred texts, their comparison, and their separation is crucial. For this
purpose, we use our data set has nine sacred scriptures. This work deals with
the separation of the Quran, the Asian scriptures Tao-Te-Ching, the Buddhism,
the Yogasutras, and the Upanishads as well as the four books from the Bible,
namely the Book of Proverbs, the Book of Ecclesiastes, the Book of
Ecclesiasticus, and the Book of Wisdom. These scriptures are analyzed based on
the natural language processing NLP creating the mathematical representation of
the corpus in terms of frequencies called document term matrix (DTM). After
this analysis, machine learning methods like supervised and unsupervised
learning are applied to perform classification. Here we use the Multinomial
Naive Bayes (MNB), the Super Vector Machine (SVM), the Random Forest (RF), and
the K-nearest Neighbors (KNN). We obtain that among these methods MNB is able
to predict the class of a sacred text with an accuracy of about 85.84 %.
"
2102.08087,2021-11-05,"Making the most of your day: online learning for optimal allocation of
  time","  We study online learning for optimal allocation when the resource to be
allocated is time. %Examples of possible applications include job scheduling
for a computing server, a driver filling a day with rides, a landlord renting
an estate, etc. An agent receives task proposals sequentially according to a
Poisson process and can either accept or reject a proposed task. If she accepts
the proposal, she is busy for the duration of the task and obtains a reward
that depends on the task duration. If she rejects it, she remains on hold until
a new task proposal arrives. We study the regret incurred by the agent, first
when she knows her reward function but does not know the distribution of the
task duration, and then when she does not know her reward function, either.
This natural setting bears similarities with contextual (one-armed) bandits,
but with the crucial difference that the normalized reward associated to a
context depends on the whole distribution of contexts.
"
2102.08847,2023-04-04,Differential Privacy for Government Agencies -- Are We There Yet?,"  Government agencies typically need to take potential risks of disclosure into
account whenever they publish statistics based on their data or give external
researchers access to collected data. In this context, the promise of formal
privacy guarantees offered by concepts such as differential privacy seems to be
the panacea enabling the agencies to quantify and control the privacy loss
incurred by any data release exactly. Nevertheless, despite the excitement in
academia and industry, most agencies -- with the prominent exception of the
U.S. Census Bureau -- have been reluctant to even consider the concept for
their data release strategy. This paper discusses potential reasons for this.
We argue that the requirements for implementing differential privacy approaches
at government agencies are often fundamentally different from the requirements
in industry. This raises many challenges and questions that still need to be
addressed before the concept can be used as an overarching principle when
sharing data with the public. The paper does not offer any solutions to these
challenges. Instead, we hope to stimulate some collaborative research efforts,
as we believe that many of the problems can only be addressed by
interdisciplinary collaborations.
"
2102.08994,2021-02-19,"Big Data meets Causal Survey Research: Understanding Nonresponse in the
  Recruitment of a Mixed-mode Online Panel","  Survey scientists increasingly face the problem of high-dimensionality in
their research as digitization makes it much easier to construct
high-dimensional (or ""big"") data sets through tools such as online surveys and
mobile applications. Machine learning methods are able to handle such data, and
they have been successfully applied to solve \emph{predictive} problems.
However, in many situations, survey statisticians want to learn about
\emph{causal} relationships to draw conclusions and be able to transfer the
findings of one survey to another. Standard machine learning methods provide
biased estimates of such relationships. We introduce into survey statistics the
double machine learning approach, which gives approximately unbiased estimators
of causal parameters, and show how it can be used to analyze survey nonresponse
in a high-dimensional panel setting.
"
2102.10429,2025-05-01,"Taylor's Theorem and Mean Value Theorem for Random Functions and Random
  Variables","  This study addresses the often-overlooked issue of measurability at
intermediate points when applying Taylor's theorems to random functions and
random vectors (e.g., likelihood functions with respect to estimators) in
statistics. Classical Taylor-related theorems were originally developed for
deterministic settings. Consequently, they do not directly extend to stochastic
functions and variables and do not inherently guarantee the measurability of
intermediate points. In statistical contexts, applying these theorems without
properly accounting for randomness can lead to analyses that lack well-defined
probabilistic interpretations. Elementary approaches, such as pointwise
constructions, are insufficient for handling random quantities and establishing
measurable intermediate points. Moreover, some statistical literature has
implicitly disregarded this issue, often neglecting the stochastic nature of
the problem and assuming that intermediate points are measurable. To address
this gap, we develop multivariate Taylor's and mean value theorems tailored for
random functions and random variables under mild assumptions. We provide
illustrative examples demonstrating the applicability of our results to
commonly used statistical methods, including maximum likelihood estimation,
$M$-estimation, and profile estimation. Our findings contribute a rigorous
foundation for the applications of Taylor expansions in statistics.
"
2102.10909,2021-03-10,Optimal Transport of Information,"  We study the general problem of Bayesian persuasion (optimal information
design) with continuous actions and continuous state space in arbitrary
dimensions. First, we show that with a finite signal space, the optimal
information design is always given by a partition. Second, we take the limit of
an infinite signal space and characterize the solution in terms of a
Monge-Kantorovich optimal transport problem with an endogenous information
transport cost. We use our novel approach to: 1. Derive necessary and
sufficient conditions for optimality based on Bregman divergences for
non-convex functions. 2. Compute exact bounds for the Hausdorff dimension of
the support of an optimal policy. 3. Derive a non-linear, second-order partial
differential equation whose solutions correspond to regular optimal policies.
We illustrate the power of our approach by providing explicit solutions to
several non-linear, multidimensional Bayesian persuasion problems.
"
2102.12225,2022-08-22,"Valid Instrumental Variables Selection Methods using Negative Control
  Outcomes and Constructing Efficient Estimator","  In observational studies, instrumental variable (IV) methods are commonly
applied when there exists some unmeasured covariates. In Mendelian
Randomization (MR), constructing an allele score by using many single
nucleotide polymorphisms (SNPs) is often implemented; however, there are risks
estimating biased causal effects by including some invalid IVs. Invalid IVs are
candidates of IVs associated with some unobserved variables. To solve this
problem, we propose a novel strategy in this paper: using Negative Control
Outcomes (NCOs) as auxiliary variables. By using NCOs, we can essentialy select
only valid IVs and exclude invalid IVs without any information of IV
candidates. We also propose the new two-step estimating procedure and prove the
semiparametric efficiency. We demonstrate the superior performance of the
proposed estimator compared with existing estimators via simulation studies.
"
2102.12290,2023-12-04,"Smooth Online Parameter Estimation for time varying VAR models with
  application to rat's LFP data","  Multivariate time series data appear often as realizations of non-stationary
processes where the covariance matrix or spectral matrix smoothly evolve over
time. Most of the current approaches estimate the time-varying spectral
properties only retrospectively - that is, after the entire data has been
observed. Retrospective estimation is a major limitation in many adaptive
control applications where it is important to estimate these properties and
detect changes in the system as they happen in real-time. One major obstacle in
online estimation is the computational cost due to the high-dimensionality of
the parameters. Existing methods such as the Kalman filter or local least
squares are feasible. However, they are not always suitable because they
provide noisy estimates and can become prohibitively costly as the dimension of
the time series increases. In our brain signal application, it is critical to
develop a robust method that can estimate, in real-time, the properties of the
underlying stochastic process, in particular, the spectral brain connectivity
measures. For these reasons we propose a new smooth online parameter estimation
approach (SOPE) that has the ability to control for the smoothness of the
estimates with a reasonable computational complexity. Consequently, the models
are fit in real-time even for high dimensional time series. We demonstrate that
our proposed SOPE approach is as good as the Kalman filter in terms of
mean-squared error for small dimensions. However, unlike the Kalman filter, the
SOPE has lower computational cost and hence scalable for higher dimensions.
Finally, we apply the SOPE method to a rat's local field potential data during
a hippocampus-dependent sequence-memory task. As demonstrated in the video, the
proposed SOPE method is able to capture the dynamics of the connectivity as the
rat performs the sequence of non-spatial working memory tasks.
"
2102.12328,2021-02-25,"Bridging Breiman's Brook: From Algorithmic Modeling to Statistical
  Learning","  In 2001, Leo Breiman wrote of a divide between ""data modeling"" and
""algorithmic modeling"" cultures. Twenty years later this division feels far
more ephemeral, both in terms of assigning individuals to camps, and in terms
of intellectual boundaries. We argue that this is largely due to the ""data
modelers"" incorporating algorithmic methods into their toolbox, particularly
driven by recent developments in the statistical understanding of Breiman's own
Random Forest methods. While this can be simplistically described as ""Breiman
won"", these same developments also expose the limitations of the
prediction-first philosophy that he espoused, making careful statistical
analysis all the more important. This paper outlines these exciting recent
developments in the random forest literature which, in our view, occurred as a
result of a necessary blending of the two ways of thinking Breiman originally
described. We also ask what areas statistics and statisticians might currently
overlook.
"
2102.12600,2021-02-26,"Constructing Evacuation Evolution Patterns and Decisions Using Mobile
  Device Location Data: A Case Study of Hurricane Irma","  Understanding individuals' behavior during hurricane evacuation is of
paramount importance for local, state, and government agencies hoping to be
prepared for natural disasters. Complexities involved with human
decision-making procedures and lack of data for such disasters are the main
reasons that make hurricane evacuation studies challenging. In this paper, we
utilized a large mobile phone Location-Based Services (LBS) data to construct
the evacuation pattern during the landfall of Hurricane Irma. By employing our
proposed framework on more than 11 billion mobile phone location sightings, we
were able to capture the evacuation decision of 807,623 smartphone users who
were living within the state of Florida. We studied users' evacuation
decisions, departure and reentry date distribution, and destination choice. In
addition to these decisions, we empirically examined the influence of
evacuation order and low-lying residential areas on individuals' evacuation
decisions. Our analysis revealed that 57.92% of people living in mandatory
evacuation zones evacuated their residences while this ratio was 32.98% and
33.68% for people living in areas with no evacuation order and voluntary
evacuation order, respectively. Moreover, our analysis revealed the importance
of the individuals' mobility behavior in modeling the evacuation decision
choice. Historical mobility behavior information such as number of trips taken
by each individual and the spatial area covered by individuals' location
trajectory estimated significant in our choice model and improve the overall
accuracy of the model significantly.
"
2103.00117,2021-03-09,"Online High-Dimensional Change-Point Detection using Topological Data
  Analysis","  Topological Data Analysis (TDA) is a rapidly growing field, which studies
methods for learning underlying topological structures present in complex data
representations. TDA methods have found recent success in extracting useful
geometric structures for a wide range of applications, including protein
classification, neuroscience, and time-series analysis. However, in many such
applications, one is also interested in sequentially detecting changes in this
topological structure. We propose a new method called Persistence Diagram based
Change-Point (PD-CP), which tackles this problem by integrating the widely-used
persistence diagrams in TDA with recent developments in nonparametric
change-point detection. The key novelty in PD-CP is that it leverages the
distribution of points on persistence diagrams for online detection of
topological changes. We demonstrate the effectiveness of PD-CP in an
application to solar flare monitoring.
"
2103.01658,2021-10-04,Minimizing Information Leakage of Abrupt Changes in Stochastic Systems,"  This work investigates the problem of analyzing privacy of abrupt changes for
general Markov processes. These processes may be affected by changes, or
exogenous signals, that need to remain private. Privacy refers to the
disclosure of information of these changes through observations of the
underlying Markov chain. In contrast to previous work on privacy, we study the
problem for an online sequence of data. We use theoretical tools from optimal
detection theory to motivate a definition of online privacy based on the
average amount of information per observation of the stochastic system in
consideration. Two cases are considered: the full-information case, where the
eavesdropper measures all but the signals that indicate a change, and the
limited-information case, where the eavesdropper only measures the state of the
Markov process. For both cases, we provide ways to derive privacy upper-bounds
and compute policies that attain a higher privacy level. It turns out that the
problem of computing privacy-aware policies is concave, and we conclude with
some examples and numerical simulations for both cases.
"
2103.03910,2021-03-09,"Health and Demographic Surveillance Systems and the 2030 Agenda:
  Sustainable Development Goals","  The health and demographic surveillance system (HDSS) is an old method for
intensively monitoring a population to assess the effects of healthcare or
other population-level interventions - often clinical trials. The strengths of
HDSS include very detailed descriptions of whole populations with frequent
updates. This often provides long time series of accurate population and health
indicators for the HDSS study population. The primary weakness of HDSS is that
the data describe only the HDSS study population and cannot be generalized
beyond that.
  The 2030 agenda is the ecosystem of activities - many including
population-level monitoring - that relate to the United Nations (UN)
Sustainable Development Goals (SDG). With respect to the 2030 agenda, HDSS can
contribute by: continuing to conduct cause-and-effect studies; contributing to
data triangulation or amalgamation initiatives; characterizing the bias in and
calibrating 'big data'; and contributing more to the rapid training of
data-oriented professionals, especially in the population and health fields.
"
2103.04139,2021-03-09,visTree: Visualization of Subgroups for a Decision Tree,"  Decision trees are flexible prediction models which are constructed to
quantify outcome-covariate relationships and characterize relevant population
subgroups. However, the standard graphical representation of fitted decision
trees highlights individual split points, and hence is suboptimal for
visualizing defined subgroups. In this paper, we present a novel visual
representation of decision trees which shifts the primary focus to
characterizing subgroups, both in terms of their defining covariates and their
outcome distribution. We implement our method in the \texttt{visTree} package,
which builds on the toolkit and infrastructure provided by the
\texttt{partykit} package and enables the visualization to be applied to varied
decision trees. Individual functions are demonstrated using data from the Box
Lunch study [French et al., 2014], a randomized trial to evaluate the effect of
exposure to different lunch sizes on energy intake and body weight among
working adults.
"
2103.05689,2023-05-24,Design Principles for Data Analysis,"  The data science revolution has led to an increased interest in the practice
of data analysis. While much has been written about statistical thinking, a
complementary form of thinking that appears in the practice of data analysis is
design thinking -- the problem-solving process to understand the people for
whom a product is being designed. For a given problem, there can be significant
or subtle differences in how a data analyst (or producer of a data analysis)
constructs, creates, or designs a data analysis, including differences in the
choice of methods, tooling, and workflow. These choices can affect the data
analysis products themselves and the experience of the consumer of the data
analysis. Therefore, the role of a producer can be thought of as designing the
data analysis with a set of design principles. Here, we introduce design
principles for data analysis and describe how they can be mapped to data
analyses in a quantitative, objective and informative manner. We also provide
empirical evidence of variation of principles within and between both producers
and consumers of data analyses. Our work leads to two insights: it suggests a
formal mechanism to describe data analyses based on the design principles for
data analysis, and it provides a framework to teach students how to build data
analyses using formal design principles.
"
2103.07424,2021-03-15,Measuring Reciprocity in a Directed Preferential Attachment Network,"  Empirical studies show that online social networks have not only in- and
out-degree distributions with Pareto-like tails but also a high proportion of
reciprocal edges. A classical directed preferential attachment (PA) model
generates in- and out-degree distribution with power-law tails, but theoretical
properties of the reciprocity feature in this model have not yet been studied.
We derive the asymptotic results on the number of reciprocal edges between two
fixed nodes, as well as the proportion of reciprocal edges in the entire PA
network. We see that with certain choices of parameters, the proportion of
reciprocal edges in a directed PA network is close to 0, which differs from the
empirical observation. This points out one potential problem of fitting a
classical PA model to a given network dataset with high reciprocity and
indicates alternative models need to be considered.
"
2103.07746,2022-08-05,"A Simulation Study Evaluating Phase I Clinical Trial Designs for
  Combinational Agents","  Nowadays, more and more clinical trials choose combinational agents as the
intervention to achieve better therapeutic responses. However, dose-finding for
combinational agents is much more complicated than single agent as the full
order of combination dose toxicity is unknown. Therefore, regular phase I
designs are not able to identify the maximum tolerated dose (MTD) of
combinational agents. Motivated by such needs, plenty of novel phase I clinical
trial designs for combinational agents were proposed. With so many available
designs, research that compare their performances, explore parameters' impacts,
and provide recommendations is very limited. Therefore, we conducted a
simulation study to evaluate multiple phase I designs that proposed to identify
single MTD for combinational agents under various scenarios. We also explored
influences of different design parameters. In the end, we summarized the pros
and cons of each design, and provided a general guideline in design selection.
"
2103.10463,2021-03-22,Two-sided confidence interval of a binomial proportion: how to choose?,"  Introduction: estimation of confidence intervals (CIs) of binomial
proportions has been reviewed more than once but the directional
interpretation, distinguishing the overestimation from the underestimation, was
neglected while the sample size and theoretical proportion variances from
experiment to experiment have not been formally taken in account. Herein, we
define and apply new evaluation criteria, then give recommendations for the
practical use of these CIs.
  Materials & methods: Google Scholar was used for bibliographic research.
Evaluation criteria were (i) one-sided conditional errors, (ii) one-sided local
average errors assuming a random theoretical proportion and (iii) expected
half-widths of CIs.
  Results: Wald's CI did not control any of the risks, even when the expected
number of successes reached 32. The likelihood ratio CI had a better balance
than the logistic Wald CI. The Clopper-Pearson mid-P CI controlled well
one-sided local average errors whereas the simple Clopper-Pearson CI was
strictly conservative on both one-sided conditional errors. The percentile and
basic bootstrap CIs had the same bias order as Wald's CI whereas the
studentized CIs and BCa, modified for discrete bootstrap distributions, were
less biased but not as efficient as the parametric methods. The half-widths of
CIs mirrored local average errors.
  Conclusion: we recommend using the Clopper-Pearson mid-P CI for the
estimation of a proportion except for observed-theoretical proportion
comparison under controlled experimental conditions in which the
Clopper-Pearson CI may be better.
"
2103.11147,2021-03-23,A unified approach for covariance matrix estimation under Stein loss,"  In this paper, we address the problem of estimating a covariance matrix of a
multivariate Gaussian distribution, relative to a Stein loss function, from a
decision theoretic point of view. We investigate the case where the covariance
matrix is invertible and the case when it is non--invertible in a unified
approach.
"
2103.11309,2021-07-22,"Branching out into Structural Identifiability Analysis with Maple:
  Interactive Exploration of Uncontrolled Linear Time-Invariant Structures","  Suppose we wish to predict the behaviour of a physical system. We may choose
to represent the system by model structure $S$ (a set of related mathematical
models defined by parametric relationships between system variables), and a
parameter set $\Theta$. Each parameter vector in $\Theta$ is associated with a
completely specified model in $S$. We use $S$ with system observations in
estimating the ""true"" (unknown) parameter vector. Inconveniently, multiple
parameter vectors may cause $S$ to approximate the data equally well. If we
cannot distinguish between such alternatives, and these lead to dissimilar
predictions, we cannot confidently use $S$ in decision making. This result may
render efforts in data collection and modelling fruitless. This outcome occurs
when $S$ lacks the property of structural global identifiability (SGI).
Fortunately, we can test various classes of structures for SGI prior to data
collection. A non-SGI result may guide changes to our structure or experimental
design towards obtaining a better outcome. We aim to assist the testing of
structures for SGI through bespoke Maple 2020 procedures. We consider
continuous-time, uncontrolled, linear time-invariant state-space structures.
Here, the time evolution of the state-variable vector ${\bf x}$ is modelled by
a system of constant-coefficient, ordinary differential equations. We utilise
the ""transfer function"" approach, which is also applicable to the
""compartmental"" subclass (mass is conserved). Our use of Maple's ""Explore""
enables an interactive consideration of a parent structure and its variants,
obtained as the user changes which components of ${\bf x}$ are observed, or
have non-zero initial conditions. Such changes may influence the information
content of the idealised output available for the SGI test, and hence, its
result. Our approach may inform the interactive analysis of structures from
other classes.
"
2103.12581,2021-03-24,"The case for balanced hypothesis tests and equal-tailed confidence
  intervals","  Introduction: there is an ongoing debate about directional inference of
two-sided hypothesis tests for which some authors argue that rejecting $\theta
= \theta_0$ does not allow to conclude that $\theta > \theta_0$ or $\theta <
\theta_0$ but only that $\theta \neq \theta_0$, while others argue that this is
a minor error without practical consequence.
  Discussion: new elements are brought to the debate. It is shown that the
directional interpretation of some non-directional hypothesis tests about
Receiver Operating Characteristic (ROC) and survival curves may lead to
inflated type III error rates with a probability of concluding that a
difference exists in the opposite side of the actual difference that can reach
50% in the worst case. Some of the issues of directional tests also apply to
two-sided confidence intervals (CIs). It is shown that equal-tailed CIs should
be preferred to shortest CIs. New assessment criteria of two-sided CIs and
hypothesis tests are proposed to provide a reliable directional interpretation:
partial left-sided and right-sided $\alpha$ error rates for hypothesis tests,
probabilities of overestimation and underestimation $\alpha_L$ and $\alpha_U$
and interval half-widths for two-sided CIs.
  Conclusion: two-sided CIs and two-sided tests are interpreted directionally.
This implies that directional interpretation be taken in account in the
development and evaluation of confidence intervals and tests.
"
2103.13521,2022-05-10,"Conditions and Assumptions for Constraint-based Causal Structure
  Learning","  We formalize constraint-based structure learning of the ""true"" causal graph
from observed data when unobserved variables are also existent. We provide
conditions for a ""natural"" family of constraint-based structure-learning
algorithms that output graphs that are Markov equivalent to the causal graph.
Under the faithfulness assumption, this natural family contains all exact
structure-learning algorithms. We also provide a set of assumptions, under
which any natural structure-learning algorithm outputs Markov equivalent graphs
to the causal graph. These assumptions can be thought of as a relaxation of
faithfulness, and most of them can be directly tested from (the underlying
distribution) of the data, particularly when one focuses on structural causal
models. We specialize the definitions and results for structural causal models.
"
2103.15678,2021-03-30,"Inference in the stochastic Cox-Ingersol-Ross diffusion process with
  continuous sampling: Computational aspects and simulation","  In this paper, we consider a stochastic model based on the Cox- Ingersoll-
Ross model (CIR). The stochastic model is parameterized analytically by
applying It\^o's calculus and the trend functions of the proposed process is
calculated. The parameter estimators are then derived by means of two
procedures: the first is used to estimate the parameters in the drift
coefficient by the maximum likelihood (ML) method, based on continuous
sampling, and the second procedure approximates the diffusion coefficient by
two methods. Finally, a simulation of the process is presented. Thus, a typical
simulated trajectory of the process and its estimators is obtained.
"
2103.15704,2021-04-07,"Are Multilevel functional models the next step in sports biomechanics
  and wearable technology? A case study of Knee Biomechanics patterns in
  typical training sessions of recreational runners","  This paper illustrates how multilevel functional models can detect and
characterize biomechanical changes along different sport training sessions. Our
analysis focuses on the relevant cases to identify differences in knee
biomechanics in recreational runners during low and high-intensity exercise
sessions with the same energy expenditure by recording $20$ steps. To do so, we
review the existing literature of multilevel models, and then, we propose a new
hypothesis test to look at the changes between different levels of the
multilevel model as low and high-intensity training sessions. We also evaluate
the reliability of measures recorded in three-dimension knee angles from the
functional intra-class correlation coefficient (ICC) obtained from the
decomposition performed with the multilevel funcional model taking into account
$20$ measures recorded in each test. The results show that there are no
statistically significant differences between the two modes of exercise.
However, we have to be careful with the conclusions since, as we have shown,
human gait-patterns are very individual and heterogeneous between groups of
athletes, and other alternatives to the p-value may be more appropriate to
detect statistical differences in biomechanical changes in this context.
"
2103.16004,2021-08-05,A Review of Containerization for Interactive and Reproducible Analysis,"  In recent decades the analysis of data has become increasingly computational.
Correspondingly, this has changed how scientific and statistical work is
shared. For example, it is now commonplace for underlying analysis code and
data to be proffered alongside journal publications and conference talks.
Unfortunately, sharing code faces several challenges. First, it is often
difficult to take code from one computer and run it on another. Code
configuration, version, and dependency issues often make this challenging.
Secondly, even if the code runs, it is often hard to understand or interact
with the analysis. This makes it difficult to assess the code and its findings,
for example, in a peer review process. In this review we describe the
combination of two computing technologies that help make analyses shareable,
interactive, and completely reproducible. These technologies are (1) analysis
containerization, which leverages virtualization to fully encapsulate analysis,
data, code and dependencies into an interactive and shareable format, and (2)
code notebooks, a literate programming format for interacting with analyses.
The fusion of these two technologies offers significant advantages over using
either individually. This review surveys how the combination enhances the
accessibility and reproducibility of code, analyses, and ideas.
"
2104.00777,2021-04-05,"Impact of climate change on West Nile virus distribution in South
  America","  West Nile virus (WNV) is a vector-borne pathogen of global relevance and is
currently the most widely distributed flavivirus of encephalitis worldwide.
This virus infects birds, humans, horses, and other mammals, and its
transmission cycle occurs in urban and rural areas. Climate conditions have
direct and indirect impacts on vector abundance and virus dynamics within the
mosquito. The significance of environmental variables as drivers in WNV
epidemiology is increasing under the current climate change scenario. In this
study, we used a machine learning algorithm to model WNV distributions in South
America. Our model evaluated eight environmental variables (type of biome,
annual temperature, seasonality of temperature, daytime temperature variation,
thermal amplitude, seasonality of precipitation, annual rainfall, and
elevation) for their contribution to the occurrence of WNV since its
introduction in South America (2004). Our results showed that environmental
variables can directly alter the occurrence of WNV, with lower precipitation
and higher temperatures associated with increased virus incidence. High-risk
areas may be modified in the coming years, becoming more evident with high
greenhouse gas emission levels. Countries such as Bolivia and Paraguay will be
greatly affected, drastically changing their current WNV distribution. Several
Brazilian areas will also increase the likelihood of presenting WNV, mainly in
the Northeast and Midwest regions and the Pantanal biome. The Galapagos Islands
will also probably increase their geographic range suitable for WNV occurrence.
It is necessary to develop preventive policies to minimize potential WNV
infection in humans and enhance active epidemiological surveillance in birds,
humans, and other mammals before it becomes a more significant public health
problem in South America.
"
2104.01107,2021-04-05,Geodesic B-Score for Improved Assessment of Knee Osteoarthritis,"  Three-dimensional medical imaging enables detailed understanding of
osteoarthritis structural status. However, there remains a vast need for
automatic, thus, reader-independent measures that provide reliable assessment
of subject-specific clinical outcomes. To this end, we derive a consistent
generalization of the recently proposed B-score to Riemannian shape spaces. We
further present an algorithmic treatment yielding simple, yet efficient
computations allowing for analysis of large shape populations with several
thousand samples. Our intrinsic formulation exhibits improved discrimination
ability over its Euclidean counterpart, which we demonstrate for predictive
validity on assessing risks of total knee replacement. This result highlights
the potential of the geodesic B-score to enable improved personalized
assessment and stratification for interventions.
"
2104.01114,2021-07-15,"The general conformable fractional grey system model and its
  applications","  Grey system theory is an important mathematical tool for describing uncertain
information in the real world. It has been used to solve the uncertainty
problems specially caused by lack of information. As a novel theory, the theory
can deal with various fields and plays an important role in modeling the small
sample problems. But many modeling mechanisms of grey system need to be
answered, such as why grey accumulation can be successfully applied to grey
prediction model? What is the key role of grey accumulation? Some scholars have
already given answers to a certain extent. In this paper, we explain the role
from the perspective of complex networks. Further, we propose generalized
conformable accumulation and difference, and clarify its physical meaning in
the grey model. We use our newly proposed fractional accumulation and
difference to our generalized conformable fractional grey model, or GCFGM(1,1),
and employ practical cases to verify that GCFGM(1,1) has higher accuracy
compared to traditional models.
"
2104.01165,2022-01-21,"Distributional data analysis of accelerometer data from the NHANES
  database using nonparametric survey regression models","  Accelerometers enable an objective measurement of physical activity levels
among groups of individuals in free-living environments, providing
high-resolution detail about physical activity changes at different time
scales. Current approaches used in the literature for analyzing such data
typically employ summary measures such as total inactivity time or
compositional metrics. However, at the conceptual level, these methods have the
potential disadvantage of discarding important information from recorded data
when calculating these summaries and metrics since these typically depend on
cut-offs related to exercise intensity zones chosen subjectively or even
arbitrarily. Furthermore, much of the data collected in these studies follow
complex survey designs. Then, using specific estimation strategies adapted to a
particular sampling mechanism is mandatory. The aim of this paper is two-fold.
First, a new functional representation of a distributional nature accelerometer
data is introduced to build a complete individualized profile of each subject's
physical activity levels. Second, we extend two nonparametric functional
regression models, kernel smoothing and kernel ridge regression, to handle
survey data and obtain reliable conclusions about the influence of physical
activity in the different analyses performed in the complex sampling design
NHANES cohort and so, show representation advantages.
"
2104.03735,2022-09-07,Sugar and Stops in Drivers with Insulin-Dependent Type 1 Diabetes,"  Diabetes is a major public health challenge worldwide. Abnormal physiology in
diabetes, particularly hypoglycemia, can cause driver impairments that affect
safe driving. While diabetes driver safety has been previously researched, few
studies link real-time physiologic changes in drivers with diabetes to
objective real-world driver safety, particularly at high-risk areas like
intersections. To address this, we investigated the role of acute physiologic
changes in drivers with type 1 diabetes mellitus (T1DM) on safe stopping at
stop intersections. 18 T1DM drivers (21-52 years, mean = 31.2 years) and 14
controls (21-55 years, mean = 33.4 years) participated in a 4-week naturalistic
driving study. At induction, each participant's vehicle was fitted with a
camera and sensor system to collect driving data. Video was processed with
computer vision algorithms detecting traffic elements. Stop intersections were
geolocated with clustering methods, state intersection databases, and manual
review. Videos showing driver stop intersection approaches were extracted and
manually reviewed to classify stopping behavior (full, rolling, and no stop)
and intersection traffic characteristics. Mixed-effects logistic regression
models determined how diabetes driver stopping safety (safe vs. unsafe stop)
was affected by 1) disease and 2) at-risk, acute physiology (hypo- and
hyperglycemia). Diabetes drivers who were acutely hyperglycemic had 2.37
increased odds of unsafe stopping (95% CI: 1.26-4.47, p = 0.008) compared to
those with normal physiology. Acute hypoglycemia did not associate with unsafe
stopping (p = 0.537), however the lower frequency of hypoglycemia (vs.
hyperglycemia) warrants a larger sample of drivers to investigate this effect.
Critically, presence of diabetes alone did not associate with unsafe stopping,
underscoring the need to evaluate driver physiology in licensing guidelines.
"
2104.04228,2021-12-21,Deepest Voting: a new way of electing,"  This article aims to present a unified framework for grading-based voting
processes. The idea is to represent the grades of each voter on d candidates as
a point in R^d and to define the winner of the vote using the deepest point of
the scatter plot. The deepest point is obtained by the maximization of a depth
function. Universality, unanimity, and neutrality properties are proved to be
satisfied. Monotonicity and Independence to Irrelevant Alternatives are also
studied. It is shown that usual voting processes correspond to specific choices
of depth functions. Finally, some basic paradoxes are explored for these voting
processes.
"
2104.06495,2021-04-15,"Interpreting the outcomes of research assessments: a geometrical
  approach","  Research evaluations and comparison of the assessments of academic
institutions (scientific areas, departments, universities etc.) are among the
major issues in recent years in higher education systems. One method, followed
by some national evaluation agencies, is to assess the research quality by the
evaluation of a limited number of publications in a way that each publication
is rated among $n$ classes. This method produces, for each institution, a
distribution of the publications in the $n$ classes. In this paper we introduce
a natural geometric way to compare these assessments by introducing an ad hoc
distance from the performance of an institution to the best possible achievable
assessment. Moreover, to avoid the methodological error of comparing
non-homogeneous institutions, we introduce a {\em geometric score} based on
such a distance. The latter represents the probability that an ideal
institution, with the same configuration as the one under evaluation, performs
worst. We apply our method, based on the geometric score, to rank, in two
specific scientific areas, the Italian universities using the results of the
evaluation exercise VQR 2011-2014.
"
2104.07212,2021-04-16,Discussion of `A Gibbs sampler for a class of random convex polytopes',"  This is a contribution for the discussion on ""A Gibbs sampler for a class of
random convex polytopes"" by Pierre E. Jacob, Ruobin Gong, Paul T. Edlefsen and
Arthur P. Dempster to appear in the Journal of American Statistical
Association.
"
2104.08016,2021-04-21,"A Review of the State-of-the-Art on Tours for Dynamic Visualization of
  High-dimensional Data","  This article discusses a high-dimensional visualization technique called the
tour, which can be used to view data in more than three dimensions. We review
the theory and history behind the technique, as well as modern software
developments and applications of the tour that are being found across the
sciences and machine learning.
"
2104.10125,2021-04-21,"Bisecting for selecting: using a Laplacian eigenmaps clustering approach
  to create the new European football Super League","  We use European football performance data to select teams to form the
proposed European football Super League, using only unsupervised techniques. We
first used random forest regression to select important variables predicting
goal difference, which we used to calculate the Euclidian distances between
teams. Creating a Laplacian eigenmap, we bisected the Fielder vector to
identify the five major European football leagues' natural clusters. Our
results showed how an unsupervised approach could successfully identify four
clusters based on five basic performance metrics: shots, shots on target, shots
conceded, possession, and pass success. The top two clusters identify those
teams who dominate their respective leagues and are the best candidates to
create the most competitive elite super league.
"
2104.11547,2021-08-30,Transitional Conditional Independence,"  We develope the framework of transitional conditional independence. For this
we introduce transition probability spaces and transitional random variables.
These constructions will generalize, strengthen and unify previous notions of
(conditional) random variables and non-stochastic variables, (extended)
stochastic conditional independence and some form of functional conditional
independence. Transitional conditional independence is asymmetric in general
and it will be shown that it satisfies all desired relevance relations in terms
of left and right versions of the separoid rules, except symmetry, on standard,
analytic and universal measurable spaces. As a preparation we prove a
disintegration theorem for transition probabilities, i.e. the existence and
essential uniqueness of (regular) conditional Markov kernels, on those spaces.
Transitional conditional independence will be able to express classical
statistical concepts like sufficiency, adequacy and ancillarity. As an
application, we will then show how transitional conditional independence can be
used to prove a directed global Markov property for causal graphical models
that allow for non-stochastic input variables in strong generality. This will
then also allow us to show the main rules of causal/do-calculus, relating
observational and interventional distributions, in such measure theoretic
generality.
"
2104.11766,2021-04-27,"A very short guide to IOI: A general framework for statistical inference
  summarised","  Integrated organic inference (IOI) is discussed in a concise and informal way
with the aim that the reader is given the gist of what this approach to
statistical inference is about as well as given pointers to further reading.
"
2104.12060,2021-04-27,"Contraction of a quasi-Bayesian model with shrinkage priors in precision
  matrix estimation","  Currently several Bayesian approaches are available to estimate large sparse
precision matrices, including Bayesian graphical Lasso (Wang, 2012), Bayesian
structure learning (Banerjee and Ghosal, 2015), and graphical horseshoe (Li et
al., 2019). Although these methods have exhibited nice empirical performances,
in general they are computationally expensive. Moreover, we have limited
knowledge about the theoretical properties, e.g., posterior contraction rate,
of graphical Bayesian Lasso and graphical horseshoe. In this paper, we propose
a new method that integrates some commonly used continuous shrinkage priors
into a quasi-Bayesian framework featured by a pseudo-likelihood. Under mild
conditions, we establish an optimal posterior contraction rate for the proposed
method. Compared to existing approaches, our method has two main advantages.
First, our method is computationally more efficient while achieving similar
error rate; second, our framework is more amenable to theoretical analysis.
Extensive simulation experiments and the analysis on a real data set are
supportive of our theoretical results.
"
2104.12919,2023-03-24,"A Comprehensive Survey of Inverse Uncertainty Quantification of Physical
  Model Parameters in Nuclear System Thermal-Hydraulics Codes","  Uncertainty Quantification (UQ) is an essential step in computational model
validation because assessment of the model accuracy requires a concrete,
quantifiable measure of uncertainty in the model predictions. The concept of UQ
in the nuclear community generally means forward UQ (FUQ), in which the
information flow is from the inputs to the outputs. Inverse UQ (IUQ), in which
the information flow is from the model outputs and experimental data to the
inputs, is an equally important component of UQ but has been significantly
underrated until recently. FUQ requires knowledge in the input uncertainties
which has been specified by expert opinion or user self-evaluation. IUQ is
defined as the process to inversely quantify the input uncertainties based on
experimental data. This review paper aims to provide a comprehensive and
comparative discussion of the major aspects of the IUQ methodologies that have
been used on the physical models in system thermal-hydraulics codes. IUQ
methods can be categorized by three main groups: frequentist (deterministic),
Bayesian (probabilistic), and empirical (design-of-experiments). We used eight
metrics to evaluate an IUQ method, including solidity, complexity,
accessibility, independence, flexibility, comprehensiveness, transparency, and
tractability. Twelve IUQ methods are reviewed, compared, and evaluated based on
these eight metrics. Such comparative evaluation will provide a good guidance
for users to select a proper IUQ method based on the IUQ problem under
investigation.
"
2104.13440,2021-04-29,Changepoint detection in random coefficient autoregressive models,"  We propose a family of CUSUM-based statistics to detect the presence of
changepoints in the deterministic part of the autoregressive parameter in a
Random Coefficient AutoRegressive (RCA) sequence. In order to ensure the
ability to detect breaks at sample endpoints, we thoroughly study weighted
CUSUM statistics, analysing the asymptotics for virtually all possible weighing
schemes, including the standardised CUSUM process (for which we derive a
Darling-Erdos theorem) and even heavier weights (studying the so-called R\'enyi
statistics). Our results are valid irrespective of whether the sequence is
stationary or not, and no prior knowledge of stationarity or lack thereof is
required. Technically, our results require strong approximations which, in the
nonstationary case, are entirely new. Similarly, we allow for
heteroskedasticity of unknown form in both the error term and in the stochastic
part of the autoregressive coefficient, proposing a family of test statistics
which are robust to heteroskedasticity, without requiring any prior knowledge
as to the presence or type thereof. Simulations show that our procedures work
very well in finite samples. We complement our theory with applications to
financial, economic and epidemiological time series.
"
2104.14708,2021-08-03,"The potential stickiness of pandemic-induced behavior changes in the
  United States","  Human behavior is notoriously difficult to change, but a disruption of the
magnitude of the COVID-19 pandemic has the potential to bring about long-term
behavioral changes. During the pandemic, people have been forced to experience
new ways of interacting, working, learning, shopping, traveling, and eating
meals. A critical question going forward is how these experiences have actually
changed preferences and habits in ways that might persist after the pandemic
ends. Many observers have suggested theories about what the future will bring,
but concrete evidence has been lacking. We present evidence on how much U.S.
adults expect their own post-pandemic choices to differ from their pre-pandemic
lifestyles in the areas of telecommuting, restaurant patronage, air travel,
online shopping, transit use, car commuting, uptake of walking and biking, and
home location. The analysis is based on a nationally-representative survey
dataset collected between July and October 2020. Key findings include that the
new normal will feature a doubling of telecommuting, reduced air travel, and
improved quality of life for some.
"
2105.02727,2021-07-26,Each student with her/his own data: understanding sampling distributions,"  Sampling distribution, a foundational concept in statistics, is difficult to
understand, since we usually have only one realization of the estimator of
interest. In this work, we present an innovative method for helping university
students understand the variability of an estimator. In our approach, each
student uses a different data set, getting diverse estimations. Then, sharing
the results, we can empirically study the sampling distribution. After some
""handmade"" experiences, we have built a web page to deliver a personalized data
set for each student. Through this web page, we can also reformulate the role
of the student in the classroom, inviting him/her to became an active player,
submitting the solution to different problems and checking whether they are
correct. In this work we present a recent experience in such direction.
"
2105.03497,2021-05-11,"Probabilistic Modeling of Hurricane Wind-Induced Damage in
  Infrastructure Systems","  This paper presents a modeling approach for probabilistic estimation of
hurricane wind-induced damage to infrastructural assets. In our approach, we
employ a Nonhomogeneous Poisson Process (NHPP) model for estimating
spatially-varying probability distributions of damage as a function of
hurricane wind field velocities. Specifically, we consider a physically-based,
quadratic NHPP model for failures of overhead assets in electricity
distribution systems. The wind field velocities are provided by Forecasts of
Hurricanes using Large-Ensemble Outputs (FHLO), a framework for generating
probabilistic hurricane forecasts. We use FHLO in conjunction with the NHPP
model, such that the hurricane forecast uncertainties represented by FHLO are
accounted for in estimating the probability distributions of damage.
Furthermore, we evaluate the spatial variability and extent of hurricane damage
under key wind field parameters (intensity, size, and asymmetries). By applying
our approach to prediction of power outages (loss-of-service) in northwestern
Florida due to Hurricane Michael (2018), we demonstrate a statistically
significant relationship between outage rate and failure rate. Finally, we
formulate parametric models that relate total damage and financial losses to
the hurricane parameters of intensity and size. Overall, this paper's findings
suggest that our approach is well-suited to jointly account for spatial
variability and forecast uncertainty in the damage estimates, and is readily
applicable to prediction of system loss-of-service due to the damage.
"
2105.03557,2022-10-04,Comparative analysis of the original and amplitude permutations,"  The original and amplitude permutations are two basic ordinal patterns;
however, their relationship has received little attention. This paper compares
the original and amplitude permutations used to characterize vector structures.
To accurately convey the vector structure, we modify indexes of equal values in
the permutations to be the same ones in each group of equalities. Comparative
analysis suggests that the amplitude permutation, comprising the positions of
the original values in the reordered vector, directly reflects the vector's
temporal structure, whereas the original permutation, consisting of the indexes
of reorganized values in the original vector, conveys the structural pattern of
the reorganized vector. Moreover, we clarify the association of the original
and amplitude permutations with timeand amplitude-symmetric vectors, thus
contributing to the fields of symbolic analysis, topological data analysis, and
so on.
"
2105.04296,2021-08-23,Phase transition in a power-law uniform hypergraph,"  We propose a power-law $m$-uniform random hypergraph on $n$ vertexes. In this
hypergraph, each vertex is independently assigned a random weight from a
power-law distribution with exponent $\alpha\in(0,\infty)$ and the hyperedge
probabilities are defined as functions of the random weights. We characterize
the number of hyperedge and the number of loose 2-cycle. There is a phase
transition phenomenon for the number of hyperedge at $\alpha=1$. Interestingly,
for the number of loose 2-cycle, phase transition occurs at both $\alpha=1$ and
$\alpha=2$.
"
2105.06324,2021-05-14,Perspective on Data Science,"  The field of data science currently enjoys a broad definition that includes a
wide array of activities which borrow from many other established fields of
study. Having such a vague characterization of a field in the early stages
might be natural, but over time maintaining such a broad definition becomes
unwieldy and impedes progress. In particular, the teaching of data science is
hampered by the seeming need to cover many different points of interest. Data
scientists must ultimately identify the core of the field by determining what
makes the field unique and what it means to develop new knowledge in data
science. In this review we attempt to distill some core ideas from data science
by focusing on the iterative process of data analysis and develop some
generalizations from past experience. Generalizations of this nature could form
the basis of a theory of data science and would serve to unify and scale the
teaching of data science to large audiences.
"
2105.06966,2021-05-17,"A Spatio-Temporal Model for Predicting Wind Speeds in Southern
  California","  The share of wind power in fuel mixes worldwide has increased considerably.
The main ingredient when deriving wind power predictions are wind speed data;
the closer to the wind farms, the better they forecast the power supply. The
current paper proposes a hybrid model for predicting wind speeds at convenient
locations. It is then applied to Southern California power price area. We build
random fields with time series of gridded historical forecasts and actual wind
speed observations. We estimate with ordinary kriging the spatial variability
of the temporal parameters and derive predictions. The advantages of this work
are twofold: (1) an accurate daily wind speed forecast at any location in the
area and (2) a general method applicable to other markets.
"
2105.07315,2021-05-18,"Comments on ""Two Cultures"": What have changed over 20 years?","  Twenty years ago Breiman (2001) called to our attention a significant
cultural division in modeling and data analysis between the stochastic data
models and the algorithmic models. Out of his deep concern that the statistical
community was so deeply and ""almost exclusively"" committed to the former,
Breiman warned that we were losing our abilities to solve many real-world
problems. Breiman was not the first, and certainly not the only statistician,
to sound the alarm; we may refer to none other than John Tukey who wrote almost
60 years ago ""data analysis is intrinsically an empirical science."" However,
the bluntness and timeliness of Breiman's article made it uniquely influential.
It prepared us for the data science era and encouraged a new generation of
statisticians to embrace a more broadly defined discipline. Some might argue
that ""The cultural division between these two statistical learning frameworks
has been growing at a steady pace in recent years"", to quote Mukhopadhyay and
Wang (2020). In this commentary, we focus on some of the positive changes over
the past 20 years and offer an optimistic outlook for our profession.
"
2105.08927,2021-05-20,"Digital competency of educators in the virtual learning environment: a
  structural equation modeling analysis","  This study integrates the educators digital competency (DC), as an individual
characteristic construct of the task-technology fit (TTF) theory, to examine a
better fit between Moodle using and teaching task, and to investigate its
effect on both Moodles utilization and their task performance. For assessing
our proposed hypotheses, an online survey was conducted with 238 teaching staff
from different departments of universities in Malaysia. Using Structural
Equation Modelling (SEM), our analysis revealed that all the proposed
components (i.e., technology literacy, knowledge deepening, presentation
skills, and professional skills) of digital competency significantly influenced
the TTF. The Task-Technology Fit was also found as an influential construct,
which positively and significantly affected both Moodles utilization and
teachers task performance. Besides, Moodles utilization was confirmed to be a
substantial determinant of the performance impact. In the end, this study
included limitations and future directions based on how the study's
contribution can support academics and practitioners for assessing and
understanding what particular components of digital competency impact TTF,
which in turn may influence the systems utilization and performance impact.
"
2105.09347,2021-05-21,An Introduction to DoSStoolkit,"  We describe a series of interactive, student-developed, self-paced, modules
for learning R. We detail the components of this resource, and the pedagogical
underpinning. We discuss the development of this resource, and avenues for
future work. Our resource is available as an R package: DoSStoolkit.
"
2105.10210,2022-05-18,Bayesian Uncertainty Quantification of Local Volatility Model,"  Local volatility is an important quantity in option pricing, portfolio
hedging, and risk management. It is not directly observable from the market;
hence calibrations of local volatility models are necessary using observable
market data. Unlike most existing point-estimate methods, we cast the
large-scale nonlinear inverse problem into the Bayesian framework, yielding a
posterior distribution of the local volatility, which naturally quantifies its
uncertainty. This extra uncertainty information enables traders and risk
managers to make better decisions. To alleviate the computational cost, we
apply Karhunen--L\`oeve expansion to reduce the dimensionality of the Gaussian
Process prior for local volatility. A modified two-stage adaptive Metropolis
algorithm is applied to sample the posterior probability distribution, which
further reduces computational burdens caused by repetitive numerical forward
option pricing model solver and time of heuristic tuning. We demonstrate our
methodology with both synthetic and market data.
"
2106.00006,2021-11-23,"Review of Low Voltage Load Forecasting: Methods, Applications, and
  Recommendations","  The increased digitalisation and monitoring of the energy system opens up
numerous opportunities to decarbonise the energy system. Applications on low
voltage, local networks, such as community energy markets and smart storage
will facilitate decarbonisation, but they will require advanced control and
management. Reliable forecasting will be a necessary component of many of these
systems to anticipate key features and uncertainties. Despite this urgent need,
there has not yet been an extensive investigation into the current
state-of-the-art of low voltage level forecasts, other than at the smart meter
level. This paper aims to provide a comprehensive overview of the landscape,
current approaches, core applications, challenges and recommendations. Another
aim of this paper is to facilitate the continued improvement and advancement in
this area. To this end, the paper also surveys some of the most relevant and
promising trends. It establishes an open, community-driven list of the known
low voltage level open datasets to encourage further research and development.
"
2106.00758,2022-08-24,Statistical issues in Serial Killer Nurse cases,"  We study statistical aspects of the case of the British nurse Ben Geen,
convicted of 2 counts of murder and 15 of grievous bodily harm following events
at Horton General Hospital (in the town of Banbury, Oxfordshire, UK) during
December 2013-February 2014. We draw attention to parallels with the cases of
nurses Lucia de Berk (the Netherlands) and Daniela Poggiali (Italy), in both of
which an initial conviction for multiple murders of patients was overturned
after reopening of the case. We pay most attention to the investigative
processes by which data, and not just statistical data, is generated; namely,
the identification of past cases in which the nurse under suspicion might have
been involved. We argue that the investigation and prosecution of such cases is
vulnerable to many cognitive biases and errors of reasoning about uncertainty,
complicated by the fact that fact-finders have to determine not only whether a
particular person was guilty of certain crimes, but whether any crimes were
committed by anybody at all. The paper includes some new statistical findings
on the Ben Geen case and suggests further avenues for investigation. The
experiences recounted here have contributed to the writing of the hand-book
Green et al. (2022), Healthcare Serial Killer or Coincidence? Statistical
Issues in Investigation of Suspected Medical Misconduct, commissioned by the
Royal Statistical Society, Statistics and the Law section. Submitted to MDPI
Laws. This version: 5 August, 2022.
"
2106.03178,2021-06-08,Path-specific Effects Based on Information Accounts of Causality,"  Path-specific effects in mediation analysis provide a useful tool for
fairness analysis, which is mostly based on nested counterfactuals. However,
the dictum ``no causation without manipulation'' implies that path-specific
effects might be induced by certain interventions. This paper proposes a new
path intervention inspired by information accounts of causality, and develops
the corresponding intervention diagrams and $\pi$-formula. Compared with the
interventionist approach of Robins et al.(2020) based on nested
counterfactuals, our proposed path intervention method explicitly describes the
manipulation in structural causal model with a simple information transferring
interpretation, and does not require the non-existence of recanting witness to
identify path-specific effects. Hence, it could serve useful communications and
theoretical focus for mediation analysis.
"
2106.04253,2023-01-10,"A likelihood based sensitivity analysis for publication bias on summary
  ROC in meta-analysis of diagnostic test accuracy","  In meta-analysis of diagnostic test accuracy, summary receiver operating
characteristic (SROC) is a recommended method to summarize the discriminant
capacity of a diagnostic test in the presence of study-specific cutoff values
and the area under the SROC (SAUC) gives the aggregate measure of test
accuracy. SROC or SAUC can be estimated by bivariate modelling of pairs of
sensitivity and specificity over the primary diagnostic studies. However,
publication bias is a major threat to the validity of estimates in
meta-analysis. To address this issue, we propose to adopt sensitivity analysis
to make an objective inference for the impact of publication bias on SROC or
SAUC. We extend Copas likelihood based sensitivity analysis to the bivariate
normal model used for meta-analysis of diagnostic test accuracy to evaluate how
much SROC or SAUC would change with different selection probabilities under
several selective publication mechanisms dependent on sensitivity and/or
specificity. The selection probability is modelled by a selection function on
$t$-type statistic for the linear combination of logit-transformed sensitivity
and specificity, allowing the selective publication of each study to be
influenced by the cutoff-dependent $p$-value for sensitivity, specificity, or
diagnostic odds ratio. By embedding the selection function into the bivariate
normal model, the conditional likelihood is proposed and the bias-corrected
SROC or SAUC can be estimated by maximizing the likelihood. We illustrate the
proposed sensitivity analysis by reanalyzing a meta-analysis of test accuracy
for intravascular device related infection. Simulation studies are conducted to
investigate the performance of proposed methods.
"
2106.04433,2022-06-16,"Singhing with Confidence: Visualising the Performance of Confidence
  Structures","  Confidence intervals are an established means of portraying uncertainty about
an inferred parameter and can be generated through the use of confidence
distributions. For a confidence distribution to be ideal, it must maintain
frequentist coverage of the true parameter. This can be represented for a
precise distribution by adherence to a cumulative unit uniform distribution,
referred to here as a Singh plot. This manuscript extends this to imprecise
confidence structures with bounds around the uniform distribution, and
describes how deviations convey information regarding the characteristics of
confidence structures designed for inference and prediction. This quick visual
representation, in a manner similar to ROC curves, aids the development of
robust structures and methods that make use of confidence. A demonstration of
the utility of Singh plots is provided with an assessment of the coverage of
the ProUCL Chebyshev upper confidence limit estimator for the mean of an
unknown distribution.
"
2106.04997,2023-02-27,ergm 4: New features,"  The ergm package supports the statistical analysis and simulation of network
data. It anchors the statnet suite of packages for network analysis in R
introduced in a special issue in Journal of Statistical Software in 2008. This
article provides an overview of the new functionality in the 2021 release of
ergm version 4. These include more flexible handling of nodal covariates, term
operators that extend and simplify model specification, new models for networks
with valued edges, improved handling of constraints on the sample space of
networks, and estimation with missing edge data. We also identify the new
packages in the statnet suite that extend ergm's functionality to other network
data types and structural features and the robust set of online resources that
support the statnet development process and applications.
"
2106.07053,2021-06-15,Convex Sparse Blind Deconvolution,"  In the blind deconvolution problem, we observe the convolution of an unknown
filter and unknown signal and attempt to reconstruct the filter and signal. The
problem seems impossible in general, since there are seemingly many more
unknowns than knowns . Nevertheless, this problem arises in many application
fields; and empirically, some of these fields have had success using heuristic
methods -- even economically very important ones, in wireless communications
and oil exploration. Today's fashionable heuristic formulations pose non-convex
optimization problems which are then attacked heuristically as well. The fact
that blind deconvolution can be solved under some repeatable and
naturally-occurring circumstances poses a theoretical puzzle.
  To bridge the gulf between reported successes and theory's limited
understanding, we exhibit a convex optimization problem that -- assuming signal
sparsity -- can convert a crude approximation to the true filter into a
high-accuracy recovery of the true filter. Our proposed formulation is based on
L1 minimization of inverse filter outputs. We give sharp guarantees on
performance of the minimizer assuming sparsity of signal, showing that our
proposal precisely recovers the true inverse filter, up to shift and rescaling.
There is a sparsity/initial accuracy tradeoff: the less accurate the initial
approximation, the greater we rely on sparsity to enable exact recovery. To our
knowledge this is the first reported tradeoff of this kind. We consider it
surprising that this tradeoff is independent of dimension.
  We also develop finite-$N$ guarantees, for highly accurate reconstruction
under $N\geq O(k \log(k) )$ with high probability. We further show stable
approximation when the true inverse filter is infinitely long and extend our
guarantees to the case where the observations are contaminated by stochastic or
adversarial noise.
"
2106.07587,2021-12-30,"Limited-Information Maximum Likelihood based Model Selection Procedures
  for Binary Outcomes","  Unmeasured covariates constitute one of the important problems in causal
inference. Even if there are some unmeasured covariates, some instrumental
variable methods such as a two-stage residual inclusion (2SRI) estimator, or a
limited-information maximum likelihood (LIML) estimator can obtain an unbiased
estimate for causal effects despite there being nonlinear outcomes such as
binary outcomes; however, it requires that we specify not only a correct
outcome model but also a correct treatment model. Therefore, detecting correct
models is an important process. In this paper, we propose two model selection
procedures: AIC-type and BIC-type, and confirm their properties. The proposed
model selection procedures are based on a LIML estimator. We prove that a
proposed BIC-type model selection procedure has model selection consistency,
and confirm their properties of the proposed model selection procedures through
simulation datasets.
"
2106.11209,2022-03-08,"Facilitating team-based data science: lessons learned from the DSC-WAV
  project","  While coursework provides undergraduate data science students with some
relevant analytic skills, many are not given the rich experiences with data and
computing they need to be successful in the workplace. Additionally, students
often have limited exposure to team-based data science and the principles and
tools of collaboration that are encountered outside of school. In this paper,
we describe the DSC-WAV program, an NSF-funded data science workforce
development project in which teams of undergraduate sophomores and juniors work
with a local non-profit organization on a data-focused problem. To help
students develop a sense of agency and improve confidence in their technical
and non-technical data science skills, the project promoted a team-based
approach to data science, adopting several processes and tools intended to
facilitate this collaboration. Evidence from the project evaluation, including
participant survey and interview data, is presented to document the degree to
which the project was successful in engaging students in team-based data
science, and how the project changed the students' perceptions of their
technical and non-technical skills. We also examine opportunities for
improvement and offer insight to other data science educators who may want to
implement a similar team-based approach to data science projects at their own
institutions.
"
2106.12180,2022-10-19,"Changepoint Detection: An Analysis of the Central England Temperature
  Series","  This paper presents a statistical analysis of structural changes in the
Central England temperature series, one of the longest surface temperature
records available. A changepoint analysis is performed to detect abrupt
changes, which can be regarded as a preliminary step before further analysis is
conducted to identify the causes of the changes (e.g., artificial,
human-induced or natural variability). Regression models with structural
breaks, including mean and trend shifts, are fitted to the series and compared
via two commonly used multiple changepoint penalized likelihood criteria that
balance model fit quality (as measured by likelihood) against parsimony
considerations. Our changepoint model fits, with independent and short-memory
errors, are also compared with a different class of models termed long-memory
models that have been previously used by other authors to describe persistence
features in temperature series. In the end, the optimal model is judged to be
one containing a changepoint in the late 1980s, with a transition to an
intensified warming regime. This timing and warming conclusion is consistent
across changepoint models compared in this analysis. The variability of the
series is not found to be significantly changing, and shift features are judged
to be more plausible than either short- or long-memory autocorrelations. The
final proposed model is one including trend-shifts (both intercept and slope
parameters) with independent errors. The analysis serves as a walk-through
tutorial of different changepoint techniques, illustrating what can be
statistically inferred.
"
2106.13564,2021-06-28,Extreme event propagation using counterfactual theory and vine copulas,"  Understanding multivariate extreme events play a crucial role in managing the
risks of complex systems since extremes are governed by their own mechanisms.
Conditional on a given variable exceeding a high threshold (e.g.\ traffic
intensity), knowing which high-impact quantities (e.g\ air pollutant levels)
are the most likely to be extreme in the future is key. This article
investigates the contribution of marginal extreme events on future extreme
events of related quantities. We propose an Extreme Event Propagation framework
to maximise counterfactual causation probabilities between a known cause and
future high-impact quantities. Extreme value theory provides a tool for
modelling upper tails whilst vine copulas are a flexible device for capturing a
large variety of joint extremal behaviours. We optimise for the probabilities
of causation and apply our framework to a London road traffic and air
pollutants dataset. We replicate documented atmospheric mechanisms beyond
linear relationships. This provides a new tool for quantifying the propagation
of extremes in a large variety of applications.
"
2106.15127,2021-11-04,Evolving-Graph Gaussian Processes,"  Graph Gaussian Processes (GGPs) provide a data-efficient solution on graph
structured domains. Existing approaches have focused on static structures,
whereas many real graph data represent a dynamic structure, limiting the
applications of GGPs. To overcome this we propose evolving-Graph Gaussian
Processes (e-GGPs). The proposed method is capable of learning the transition
function of graph vertices over time with a neighbourhood kernel to model the
connectivity and interaction changes between vertices. We assess the
performance of our method on time-series regression problems where graphs
evolve over time. We demonstrate the benefits of e-GGPs over static graph
Gaussian Process approaches.
"
2107.01060,2022-11-02,Projected Least-Squares Quantum Process Tomography,"  We propose and investigate a new method of quantum process tomography (QPT)
which we call projected least squares (PLS). In short, PLS consists of first
computing the least-squares estimator of the Choi matrix of an unknown channel,
and subsequently projecting it onto the convex set of Choi matrices. We
consider four experimental setups including direct QPT with Pauli eigenvectors
as input and Pauli measurements, and ancilla-assisted QPT with mutually
unbiased bases (MUB) measurements. In each case, we provide a closed form
solution for the least-squares estimator of the Choi matrix. We propose a
novel, two-step method for projecting these estimators onto the set of matrices
representing physical quantum channels, and a fast numerical implementation in
the form of the hyperplane intersection projection algorithm. We provide
rigorous, non-asymptotic concentration bounds, sampling complexities and
confidence regions for the Frobenius and trace-norm error of the estimators.
For the Frobenius error, the bounds are linear in the rank of the Choi matrix,
and for low ranks, they improve the error rates of the least squares estimator
by a factor $d^2$, where $d$ is the system dimension. We illustrate the method
with numerical experiments involving channels on systems with up to 7 qubits,
and find that PLS has highly competitive accuracy and computational
tractability.
"
2107.02522,2022-01-11,A nonBayesian view of Hempel's paradox of the ravens,"  In Hempel's paradox of the ravens, seeing a red pencil is considered as
supporting evidence that all ravens are black. Also known as the Paradox of
Confirmation, the paradox and its many resolutions indicate that we cannot
underestimate the logical and statistical elements needed in the assessment of
evidence in support of a hypothesis. Most of the previous analyses of the
paradox are within the Bayesian framework. These analyses and Hempel himself
generally accept the paradoxical conclusion; it feels paradoxical supposedly
because the amount of evidence is extremely small. Here I describe a
nonBayesian analysis of various statistical models with an accompanying
likelihood-based reasoning. The analysis shows that the paradox feels
paradoxical because there are natural models where observing a red pencil has
no relevance to the color of ravens. In general the value of the evidence
depends crucially on the sampling scheme and on the assumption about the
underlying parameters of the relevant model.
"
2107.02537,2021-07-07,"Approximations to ultimate ruin probabilities with a Wienner process
  perturbation","  In this paper, we adapt the classic Cram\'er-Lundberg collective risk theory
model to a perturbed model by adding a Wiener process to the compound Poisson
process, which can be used to incorporate premium income uncertainty, interest
rate fluctuations and changes in the number of policyholders. Our study is part
of a Master dissertation, our aim is to make a short overview and present
additionally some new approximation methods for the infinite time ruin
probabilities for the perturbed risk model. We present four different
approximation methods for the perturbed risk model. The first method is based
on iterative upper and lower approximations to the maximal aggregate loss
distribution. The second method relies on a four-moment exponential De Vylder
approximation. The third method is based on the first-order Pad\'e
approximation of the Renyi and De Vylder approximations. The last method is the
second order Pad\'e-Ramsay approximation. These are generated by fitting one,
two, three or four moments of the claim amount distribution, which greatly
generalizes the approximations. We test the precision of approximations using a
combination of light and heavy tailed distributions for the individual claim
amount. We assess the ultimate ruin probability and present numerical results
for the exponential, gamma, and mixed exponential claim distributions,
demonstrating the high accuracy of these four methods. Analytical and numerical
methods are used to highlight the practical implications of our findings.
"
2107.03249,2021-07-26,"Patient-reported outcomes in the context of the benefit assessment in
  Germany","  Since the 2011 Act on the Reform of the Market for Medicinal Products,
benefit dossiers are submitted by pharmaceutical companies to facilitate the
Health Technology Assessment (HTA) appraisals in Germany. The Institute for
Quality and Efficiency in Health Care conducts the added benefit assessment
following their General Methods Paper, which was updated November 5, 2020. This
White Paper is dedicated to patient-reported outcomes (PRO) to highlight their
importance for the added benefit assessment. We focus on methodological aspects
but consider also other relevant requirements and challenges, which are
demanded by G-BA and IQWiG. The following topics will be presented and
discussed: 1. Role of PRO in HTA decision making exemplary to benefit
assessment in Germany 2. Guidances of PRO evaluations 3. PRO Estimand framework
4. Perception and requirements for PRO within the German benefit assessment 5.
Validity of instrument 6. Response thresholds for assessing clinical relevance
of PRO 7. PRO endpoints / outcome measures / operationalization 8. Missing PRO
data 9. PRO after treatment discontinuation This White Paper aims to provide
deeper insights about new requirements concerning PRO evaluations for HTA
decision making in Germany, highlight points to consider that should inform
global development in terms of study planning and frame the requirements also
in the context of global recommendations and guidelines. We also aim to enhance
the understanding of the complexity when preparing the benefit dossier and
promote further scientific discussions where appropriate.
"
2107.05145,2021-07-13,Discovery of Bayes' Table at Tunbridge Wells,"  In 1755 Thomas Bayes expressed an interest in the problem of combining
repeated measurements of the location of a star. Bayes described a tandem
set-up of a ball thrown on a table, followed by repeated throws of a second
ball. Bayes' table has long been taken as a billiard table, for which there is
no evidence. We report the discovery of Bayes' table, a bowling green located
half a km uphill (SE) from the meeting house where Bayes served as minister for
two decades. Bayes' drawing shows a rectangular space marked off in yards,
which allows calculation of an interval measurement of uncertainty. The Bayes
rule interval from 2.5% to 97.5% is from 0.56 - 0.42 = 0.12 perches equivalent
to 0.61 m. The discovery of Bayes' table establishes the physical basis for
Bayes' symmetrical probability model, a fixed parameter binomial ({\theta} =
0.5). The discovery establishes Bayes as the founder of statistical science,
defined as the application of mathematics to scientific measurement.
"
2107.06223,2021-07-14,"Impact of heat waves and cold spells on cause-specific mortality in the
  city of Sao Paulo, Brazil","  The impact of heat waves and cold spells on mortality has become a major
public health problem worldwide, especially among older adults living in low-
to middle-income countries. This study aimed to investigate the effects of heat
waves and cold spells under different definitions on cause-specific mortality
among people aged 65 years and over in Sao Paulo from 2006 to 2015. A
quasi-Poisson generalized linear model with a distributed lag model was used to
investigate the association between cause-specific mortality and extreme air
temperature events. To evaluate the effects of the intensity under different
durations, we considered 12 heat wave and nine cold spell definitions. Our
results showed an increase in cause-specific deaths related to heat waves and
cold spells under several definitions. The highest risk of death related to
heat waves was identified mostly at higher temperature thresholds with longer
events. We verified that men were more vulnerable to die from an ischemic
stroke on heat waves and cold spells days than women, while women presented a
higher risk of dying from ischemic heart diseases during cold spells and tended
to have a higher risk of chronic obstructive pulmonary disease than men.
Identification of heat wave- and cold spell-related mortality is important for
the development and promotion of public health measures.
"
2107.13894,2021-07-30,Inference in heavy-tailed non-stationary multivariate time series,"  We study inference on the common stochastic trends in a non-stationary,
$N$-variate time series $y_{t}$, in the possible presence of heavy tails. We
propose a novel methodology which does not require any knowledge or estimation
of the tail index, or even knowledge as to whether certain moments (such as the
variance) exist or not, and develop an estimator of the number of stochastic
trends $m$ based on the eigenvalues of the sample second moment matrix of
$y_{t}$. We study the rates of such eigenvalues, showing that the first $m$
ones diverge, as the sample size $T$ passes to infinity, at a rate faster by
$O\left(T \right)$ than the remaining $N-m$ ones, irrespective of the tail
index. We thus exploit this eigen-gap by constructing, for each eigenvalue, a
test statistic which diverges to positive infinity or drifts to zero according
to whether the relevant eigenvalue belongs to the set of the first $m$
eigenvalues or not. We then construct a randomised statistic based on this,
using it as part of a sequential testing procedure, ensuring consistency of the
resulting estimator of $m$. We also discuss an estimator of the common trends
based on principal components and show that, up to a an invertible linear
transformation, such estimator is consistent in the sense that the estimation
error is of smaller order than the trend itself. Finally, we also consider the
case in which we relax the standard assumption of \textit{i.i.d.} innovations,
by allowing for heterogeneity of a very general form in the scale of the
innovations. A Monte Carlo study shows that the proposed estimator for $m$
performs particularly well, even in samples of small size. We complete the
paper by presenting four illustrative applications covering commodity prices,
interest rates data, long run PPP and cryptocurrency markets.
"
2107.14055,2021-07-30,Profit and loss manipulations by online trading brokers,"  Online trading has attracted millions of people around the world. In March
2021, it was reported there were 18 million accounts from just one broker.
Historically, manipulation in financial markets is considered to be
fraudulently influencing share, currency pairs or any other indices prices.
This article introduces the idea that online trading platform technical issues
can be considered as brokers manipulation to control traders profit and loss.
More importantly it shows these technical issues are the contributing factors
of the 82% risk of retail traders losing money. We identify trading platform
technical issues of one of the world's leading online trading providers and
calculate retail traders losses caused by these issues. To do this, we
independently record each trade details using the REST API response provided by
the broker. We show traders log activity files is the only way to assess any
suspected profit or loss manipulation by the broker. Therefore, it is essential
for any retail trader to have access to their log files. We compare our
findings with broker's Trustpilot customer reviews. We illustrate how traders'
profit and loss can be negatively affected by broker's platform technical
issues such as not being able to close profitable trades, closing trades with
delays, disappearance of trades, disappearance of profit from clients
statements, profit and loss discrepancies, stop loss not being triggered, stop
loss or limit order triggered too early. Although regulatory bodies try to
ensure that consumers get a fair deal, these attempts are hugely insufficient
in protecting retail traders. Therefore, regulatory bodies such as the FCA
should take these technical issues seriously and not rely on brokers' internal
investigations, because under any other circumstances, these platform
manipulations would be considered as crimes and connivingly misappropriating
funds.
"
2108.02727,2023-11-17,"Signatures, Lipschitz-free spaces, and paths of persistence diagrams","  Paths of persistence diagrams provide a summary of the dynamic topological
structure of a one-parameter family of metric spaces. These summaries can be
used to study and characterize the dynamic shape of data such as swarming
behavior in multi-agent systems, time-varying fMRI scans from neuroscience, and
time-dependent scalar fields in hydrodynamics. While persistence diagrams can
provide a powerful topological summary of data, the standard space of
persistence diagrams lacks the sufficient algebraic and analytic structure
required for many theoretical and computational analyses. We enrich the space
of persistence diagrams by isometrically embedding it into a Lipschitz-free
space, a Banach space built from a universal construction. We utilize the
Banach space structure to define bounded variation paths of persistence
diagrams, which can be studied using the path signature, a
reparametrization-invariant characterization of paths valued in a Banach space.
The signature is universal and characteristic which allows us to theoretically
characterize measures on the space of paths and motivates its use in the
context of kernel methods. However, kernel methods often require a feature map
into a Hilbert space, so we introduce the moment map, a stable and injective
feature map for static persistence diagrams, and compose it with the discrete
path signature, producing a computable feature map into a Hilbert space.
Finally, we demonstrate the efficacy of our methods by applying this to a
parameter estimation problem for a 3D model of swarming behavior.
"
2108.02836,2021-08-09,"Discussion on ""Bayesian Regression Tree Models for Causal Inference:
  Regularization, Confounding, and Heterogeneous Effects"" by Hahn, Murray and
  Carvalho","  Hahn et al. (2020) offers an extensive study to explicate and evaluate the
performance of the BCF model in different settings and provides a detailed
discussion about its utility in causal inference. It is a welcomed addition to
the causal machine learning literature. I will emphasize the contribution of
the BCF model to the field of causal inference through discussions on two
topics: 1) the difference between the PS in the BCF model and the Bayesian PS
in a Bayesian updating approach, 2) an alternative exposition of the role of
the PS in outcome modeling based methods for the estimation of causal effects.
I will conclude with comments on avenues for future research involving BCF that
will be important and much needed in the era of Big data.
"
2108.03510,2022-10-11,An educator's perspective of the tidyverse,"  Computing makes up a large and growing component of data science and
statistics courses. Many of those courses, especially when taught by faculty
who are statisticians by training, teach R as the programming language. A
number of instructors have opted to build much of their teaching around use of
the tidyverse. The tidyverse, in the words of its developers, ""is a collection
of R packages that share a high-level design philosophy and low-level grammar
and data structures, so that learning one package makes it easier to learn the
next"". These shared principles have led to the widespread adoption of the
tidyverse ecosystem. A large part of this usage is because the tidyverse tools
have been intentionally designed to ease the learning process and make it
easier for users to learn new functions as they engage with additional pieces
of the larger ecosystem. Moreover, the functionality offered by the packages
within the tidyverse spans the entire data science cycle, which includes data
import, visualisation, wrangling, modeling, and communication. We believe the
tidyverse provides an effective and efficient pathway for undergraduate
students at all levels and majors to gain computational skills and thinking
needed throughout the data science cycle. In this paper, we introduce the
tidyverse from an educator's perspective. We provide a brief introduction to
the tidyverse, demonstrate how foundational statistics and data science tasks
are accomplished with the tidyverse, and discuss the strengths of the
tidyverse, particularly in the context of teaching and learning.
"
2108.03971,2022-01-19,"Lottery paradox, DNA evidence and other stories: How to accept uncertain
  statements","  I think we can agree that dealing with uncertainty is not easy. Probability
is the main tool for dealing with uncertainty, and we know there are many
probability-related puzzles and paradoxes. Here I describe a rather
idiosyncratic selection that highlights the problem of accepting uncertain
statements. Without going into a formal decision theory, there are simple
intuitive rational bases for doing that, for instance based on high probability
alone. The lottery paradox shows the logical problem of accepting uncertain
statements based on high probability. The DNA evidence story is an example of
the use probabilistic reasoning in court, where philosophical differences
between the schools of inference -- the frequentist, Bayesian and likelihood
schools -- lead to substantial differences in the quantification of evidence.
"
2108.04068,2022-03-09,"On the role of data, statistics and decisions in a pandemic","  A pandemic poses particular challenges to decision-making because of the need
to continuously adapt decisions to rapidly changing evidence and available
data. For example, which countermeasures are appropriate at a particular stage
of the pandemic? How can the severity of the pandemic be measured? What is the
effect of vaccination in the population and which groups should be vaccinated
first? The process of decision-making starts with data collection and modeling
and continues to the dissemination of results and the subsequent decisions
taken. The goal of this paper is to give an overview of this process and to
provide recommendations for the different steps from a statistical perspective.
In particular, we discuss a range of modeling techniques including
mathematical, statistical and decision-analytic models along with their
applications in the COVID-19 context. With this overview, we aim to foster the
understanding of the goals of these modeling approaches and the specific data
requirements that are essential for the interpretation of results and for
successful interdisciplinary collaborations. A special focus is on the role
played by data in these different models, and we incorporate into the
discussion the importance of statistical literacy, and of effective
dissemination and communication of findings.
"
2108.04396,2021-08-11,"An examination of the generalised pooled binomial distribution and its
  information properties","  This paper examines the statistical properties of a distributional form that
arises from pooled testing for the prevalence of a binary outcome. Our base
distribution is a two-parameter distribution using a prevalence and excess
intensity parameter; the latter is included to allow for a dilution or
intensification effect with larger pools. We also examine a generalised form of
the distribution where pools have covariate information that affects the
prevalence through a linked linear form. We study the general pooled binomial
distribution in its own right and as a special case of broader forms of
binomial GLMs using the complementary log-log link function. We examine the
information function and show the information content of individual sample
items. We demonstrate that pooling reduces information content of sample units
and we give simple heuristics for choosing an ""optimal"" pool size for testing.
We derive the form of the log-likelihood function and its derivatives and give
results for maximum likelihood estimation. We also discuss diagnostic testing
of the positive pool probabilities, including testing for
intensification/dilution in the testing mechanism. We illustrate the use of
this distribution by applying it to pooled testing data on virus prevalence in
a mosquito population.
"
2108.04903,2021-08-30,Rank Energy Statistics in the Context of Change Point Detection,"  In this paper, I propose a general procedure for multivariate
distribution-free nonparametric testing derived from the concept of ranks that
are based upon measure transportation in the context of multiple change point
analysis. I will use this algorithm to estimate both the number of change
points and their locations within an observed multivariate time series. In this
paper, the change point problem is observed in a general setting in which both
the given distribution and number of change points are unknown, rather than
assume the observed time series follows a specific distribution or contains
only one change point as many works in this area of study assume. The intention
of this is to develop a technique for accurately identifying the changes in a
distribution while making as few suppositions as possible. The rank energy
statistic used here is based on energy statistics and has the potential to
detect any change in a distribution. I present the properties of this new
algorithm, which can be used to analyze various datasets, including
hierarchical clustering, testing multivariate normality, gene selection, and
microarray data analysis. This algorithm has also been implemented in the R
package recp, which is available on GitHub.
"
2108.05100,2024-02-20,"Which Type of Statistical Uncertainty Helps Evidence-Based Policymaking?
  An Insight from a Survey Experiment in Ireland","  Which type of statistical uncertainty -- statistical (in)significance with a
p-value, or a Bayesian probability -- enables people to see the continuous
nature of uncertainty more clearly in a policymaking context? An original
survey experiment used a hypothetical scenario, where participants from Ireland
were asked whether to introduce a new bus line to reduce traffic jams, given a
research report estimating its effectiveness. The treatments were uncertainty
information: statistical significance with a p-value of 2%, statistical
insignificance with a p-value of 25%, the 95% probability that the estimate is
correct, and the 68% probability that the estimate is correct. In the case of
lower uncertainty, both significance and Bayesian frameworks resulted in a
large proportion of participants adopting the policy (0.82 and 0.91
respectively). In the case of higher uncertainty, the significance framework
led a much smaller proportion of participants to adopt the policy (0.39 against
0.83). The findings suggest participants saw the continuous nature of
uncertainty more clearly in the Bayesian framework than in the significance
framework.
"
2108.07937,2021-08-24,"The Generalized Gamma distribution as a useful RND under Heston's
  stochastic volatility model","  Following Boukai (2021) we present the Generalized Gamma (GG) distribution as
a possible RND for modeling European options prices under Heston's (1993)
stochastic volatility (SV) model. This distribution is seen as especially
useful in situations in which the spot's price follows a negatively skewed
distribution and hence, Black-Scholes based (i.e. the log-normal distribution)
modeling is largely inapt. We apply the GG distribution as RND to modeling
current market option data on three large market-index ETFs, namely the SPY,
IWM and QQQ as well as on the TLT (an ETF that tracks an index of long term US
Treasury bonds). The current option chain of each of the three market-index
ETFs shows of a pronounced skew of their volatility `smile' which indicates a
likely distortion in the Black-Scholes modeling of such option data. Reflective
of entirely different market expectations, this distortion appears not to exist
in the TLT option data. We provide a thorough modeling of the available option
data we have on each ETF (with the October 15, 2021 expiration) based on the GG
distribution and compared it to the option pricing and RND modeling obtained
directly from a well-calibrated Heston's (1993) SV model (both theoretically
and empirically, using Monte-Carlo simulations of the spot's price). All three
market-index ETFs exhibit negatively skewed distributions which are
well-matched with those derived under the GG distribution as RND. The
inadequacy of the Black-Scholes modeling in such instances which involve
negatively skewed distribution is further illustrated by its impact on the
hedging factor, delta, and the immediate implications to the retail trader. In
contrast, for the TLT ETF, which exhibits no such distortion to the volatility
`smile', the three pricing models (i.e. Heston's, Black-Scholes and Generalized
Gamma) appear to yield similar results.
"
2108.13147,2024-11-11,Functional Data Representation with Merge Trees,"  In this paper we face the problem of representation of functional data with
the tools of algebraic topology. We represent functions by means of merge
trees, which, like the more commonly used persistence diagrams, are invariant
under homeomorphic reparametrizations of the functions they represent, thus
allowing for a statistical analysis which is indifferent to functional
misalignment. We consider a recently defined metric for merge trees and we
prove some theoretical results related to its specific implementation when
merge trees represent functions, establishing also a class of consistent
estimators with convergence rates. To showcase the good properties of our
topological approach to functional data analysis, we test it on the Aneurisk65
dataset replicating, from our different perspective, the supervised
classification analysis which contributed to make this dataset a benchmark for
methods dealing with misaligned functional data. In the Appendix we provide an
extensive comparison between merge trees and persistence diagrams, highlighting
similarities and differences, which can guide the analyst in choosing between
the two representations.
"
2108.13575,2021-09-01,"Comments on The clinical meaningfulness of a treatment's effect on a
  time-to-event variable","  Some years ago, Snapinn and Jiang[1] considered the interpretation and
pitfalls of absolute versus relative treatment effect measures in analyses of
time-to-event outcomes. Through specific examples and analytical considerations
based solely on the exponential and the Weibull distributions they reach two
conclusions: 1) that the commonly used criteria for clinical effectiveness, the
ARR (Absolute Risk Reduction) and the median (survival time) difference (MD)
directly contradict each other and 2) cost-effectiveness depends only the
hazard ratio(HR) and the shape parameter (in the Weibull case) but not the
overall baseline risk of the population. Though provocative, the first
conclusion does not apply to either the two special cases considered or even
more generally, while the second conclusion is strictly correct only for the
exponential case. Therefore, the implication inferred by the authors i.e. all
measures of absolute treatment effect are of little value compared with the
relative measure of the hazard ratio, is not of general validity and hence both
absolute and relative measures should continue to be used when appraising
clinical evidence.
"
2109.00171,2021-09-02,"A generalized bootstrap procedure of the standard error and confidence
  interval estimation for inverse probability of treatment weighting","  The inverse probability of treatment weighting (IPTW) approach is commonly
used in propensity score analysis to infer causal effects in regression models.
Due to oversized IPTW weights and errors associated with propensity score
estimation, the IPTW approach can underestimate the standard error of causal
effect. To remediate this, bootstrap standard errors have been recommended to
replace the IPTW standard error, but the ordinary bootstrap (OB) procedure
might still result in underestimation of the standard error because of its
inefficient sampling algorithm and un-stabilized weights. In this paper, we
develop a generalized bootstrap (GB) procedure for estimating the standard
error of the IPTW approach. Compared with the OB procedure, the GB procedure
has much lower risk of underestimating the standard error and is more efficient
for both point and standard error estimates. The GB procedure also has smaller
risk of standard error underestimation than the ordinary bootstrap procedure
with trimmed weights, with comparable efficiencies. We demonstrate the
effectiveness of the GB procedure via a simulation study and a dataset from the
National Educational Longitudinal Study-1988 (NELS-88).
"
2109.00378,2021-09-02,"A truncated mean-parameterised Conway-Maxwell-Poisson model for the
  analysis of Test match bowlers","  Assessing the relative merits of sportsmen and women whose careers took place
far apart in time via a suitable statistical model is a complex task as any
comparison is compromised by fundamental changes to the sport and society and
often handicapped by the popularity of inappropriate traditional metrics. In
this work we focus on cricket and the ranking of Test match bowlers using
bowling data from the first Test in 1877 onwards. A truncated,
mean-parameterised Conway-Maxwell-Poisson model is developed to handle the
under- and overdispersed nature of the data, which are in the form of small
counts, and to extract the innate ability of individual bowlers. Inferences are
made using a Bayesian approach by deploying a Markov Chain Monte Carlo
algorithm to obtain parameter estimates and confidence intervals. The model
offers a good fit and indicates that the commonly used bowling average is a
flawed measure.
"
2109.00848,2024-07-23,"The Current State of Undergraduate Bayesian Education and
  Recommendations for the Future","  As a result of the increased emphasis on mis- and over-use of $p$-values in
scientific research and the rise in popularity of Bayesian statistics, Bayesian
education is becoming more important at the undergraduate level. With the
advances in computing tools, Bayesian statistics is also becoming more
accessible for the undergraduates. This study focuses on analyzing Bayesian
courses for the undergraduates. We explored whether an undergraduate Bayesian
course is offered in our sample of 152 high-ranking research universities and
liberal arts colleges. For each identified Bayesian course, we examined how it
fits into the institution's undergraduate curricula, such as majors and
prerequisites. Through a series of course syllabi analyses, we explored the
topics covered and their popularity in these courses, and the adopted teaching
and learning tools, such as software. This paper presents our findings on the
current practices of teaching full Bayesian courses at the undergraduate level.
Based on our findings, we provide recommendations for programs that may
consider offering Bayesian courses to their students.
"
2109.03027,2025-01-06,Statistical analysis of locally parameterized shapes,"  The alignment of shapes has been a crucial step in statistical shape
analysis, for example, in calculating mean shape, detecting locational
differences between two shape populations, and classification. Procrustes
alignment is the most commonly used method and state of the art. In this work,
we uncover that alignment might seriously affect the statistical analysis. For
example, alignment can induce false shape differences and lead to misleading
results and interpretations. We propose a novel hierarchical shape
parameterization based on local coordinate systems. The local parameterized
shapes are translation and rotation invariant. Thus, the inherent alignment
problems from the commonly used global coordinate system for shape
representation can be avoided using this parameterization. The new
parameterization is also superior for shape deformation and simulation. The
method's power is demonstrated on the hypothesis testing of simulated data as
well as the left hippocampi of patients with Parkinson's disease and controls.
"
2109.03365,2023-05-03,"SIHR: Statistical Inference in High-Dimensional Linear and Logistic
  Regression Models","  We introduce the R package \CRANpkg{SIHR} for statistical inference in
high-dimensional generalized linear models with continuous and binary outcomes.
The package provides functionalities for constructing confidence intervals and
performing hypothesis tests for low-dimensional objectives in both one-sample
and two-sample regression settings. We illustrate the usage of \CRANpkg{SIHR}
through numerical examples and present real data applications to demonstrate
the package's performance and practicality.
"
2109.04227,2022-12-27,"Evaluation of imputation techniques with varying percentage of missing
  data","  Missing data is a common problem which has consistently plagued statisticians
and applied analytical researchers. While replacement methods like mean-based
or hot deck imputation have been well researched, emerging imputation
techniques enabled through improved computational resources have had limited
formal assessment. This study formally considers five more recently developed
imputation methods: Amelia, Mice, mi, Hmisc and missForest - compares their
performances using RMSE against actual values and against the well-established
mean-based replacement approach. The RMSE measure was consolidated by method
using a ranking approach. Our results indicate that the missForest algorithm
performed best and the mi algorithm performed worst.
"
2109.05978,2021-09-14,RWP+: A New Random Waypoint Model for High-Speed Mobility,"  In this letter, we emulate real-world statistics for mobility patterns on
road systems. We then propose modifications to the assumptions of the random
waypoint (RWP) model to better represent high-mobility profiles. We call the
model under our new framework as RWP+. Specifically, we show that the lengths
of the transitions which constitute a trip, are best represented by a lognormal
distribution, and that the velocities are best described by a linear
combination of normal distributions with different mean values. Compared to the
assumptions used in the literature for mobile cellular networks, our modeling
provides mobility metrics, such as handoff rates, that better characterize
actual emulated trips from the collected statistics.
"
2109.06559,2023-01-11,Bayesian model-based outlier detection in network meta-analysis,"  In a network meta-analysis, some of the collected studies may deviate
markedly from the others, for example having very unusual effect sizes. These
deviating studies can be regarded as outlying with respect to the rest of the
network and can be influential on the pooled results. Thus, it could be
inappropriate to synthesize those studies without further investigation. In
this paper, we propose two Bayesian methods to detect outliers in a network
meta-analysis via: (a) a mean-shifted outlier model and (b), posterior
predictive p-values constructed from ad-hoc discrepancy measures. The former
method uses Bayes factors to formally test each study against outliers while
the latter provides a score of outlyingness for each study in the network,
which allows to numerically quantify the uncertainty associated with being
outlier. Furthermore, we present a simple method based on informative priors as
part of the network meta-analysis model to down-weight the detected outliers.
We conduct extensive simulations to evaluate the effectiveness of the proposed
methodology while comparing it to some alternative, available outlier
diagnostic tools. Two real networks of interventions are then used to
demonstrate our methods in practice.
"
2109.07978,2021-09-17,On variable selection in joint modeling of mean and dispersion,"  The joint modeling of mean and dispersion (JMMD) provides an efficient method
to obtain useful models for the mean and dispersion, especially in problems of
robust design experiments. However, in the literature on JMMD there are few
works dedicated to variable selection and this theme is still a challenge. In
this article, we propose a procedure for selecting variables in JMMD, based on
hypothesis testing and the quality of the model's fit. A criterion for checking
the goodness of fit is used, in each iteration of the selection process, as a
filter for choosing the terms that will be evaluated by a hypothesis test.
Three types of criteria were considered for checking the quality of the model
fit in our variable selection procedure. The criteria used were: the extended
Akaike information criterion, the corrected Akaike information criterion and a
specific criterion for the JMMD, proposed by us, a type of extended adjusted
coefficient of determination. Simulation studies were carried out to verify the
efficiency of our variable selection procedure. In all situations considered,
the proposed procedure proved to be effective and quite satisfactory. The
variable selection process was applied to a real example from an industrial
experiment.
"
2109.08242,2021-09-20,Trading styles and long-run variance of asset prices,"  Trading styles can be classified into either trend-following or
mean-reverting. If the net trading style is trend-following the traded asset is
more likely to move in the same direction it moved previously (the opposite is
true if the net style is mean-reverting). The result of this is to introduce
positive (or negative) correlations into the time series. We here explore the
effect of these correlations on the long-run variance of the series through
probabilistic models designed to explicitly capture the direction of trading.
Our theoretical insights suggests that relative to random walk models of asset
prices the long-run variance is increased under trend-following strategies and
can actually be reduced under mean-reversal conditions. We apply these models
to some of the largest US stocks by market capitalisation as well as
high-frequency EUR/USD data and show that in both these settings, the ability
to predict the asset price is generally increased relative to a random walk.
"
2109.10869,2021-09-23,"Interactive Probing of Multivariate Time Series Prediction Models: A
  Case of Freight Rate Analysis","  We present an interactive probing tool to create, modify and analyze what-if
scenarios for multivariate time series models. The solution is applied to
freight trading, where analysts can carry out sensitivity analysis on freight
rates by changing demand and supply-related econometric variables and observing
their resultant effects on freight indexes. We utilize various visualization
techniques to enable intuitive scenario creation, alteration, and comprehension
of time series inputs and model predictions. Our tool proved to be useful to
the industry practitioners, demonstrated by a case study where freight traders
are given hypothetical market scenarios and successfully generated quantitative
freight index projection with confidence.
"
2109.13686,2021-09-29,"Preemptive Two-stage Goal-Programming Formulation of a Strict Version of
  the Unbounded Knapsack Problem with Bounded Weights","  The unbounded knapsack problem with bounded weights is a variant of the
well-studied variant of the traditional binary knapsack problem; key changes
being the relaxation of the binary constraint and allowing the unit weights of
each item to fall within a range. In this paper, we formulate a variant of this
problem, which we call the strict unbounded knapsack problem with bounded
weights, by replacing the inequality constraint on the total weight with an
equality. We show that this problem can be decomposed into a two-stage,
pre-emptive goal programming problem, with the first stage being a
2-dimensional knapsack problem and the second being either a linear feasibility
program (per canonical formulation) or simply a linearly-constrained program in
the general case. This reformulation is shown to be equivalent to the original
formulation but allows the use of well-studied, efficient algorithms for
multidimensional knapsack problems. In addition, it separates the modeling
effort around what to put in the knapsack from considerations around what unit
weight one should assign to each item type, providing substantially more
flexibility to the modeler without adding complexity to the choice of knapsack
configuration. Finally, we show that for the feasibility version of the second
stage, one can immediately get a feasible solution to the first stage solution.
"
2110.02363,2022-05-09,Bernoulli Sums: The only random variables that count,"  A novel multinomial theorem for commutative idempotents is shown to lead to
new results about the moments, central moments, factorial moments, and their
generating functions for any random variable $X = \sum_{i} Y_i $ expressible as
a sum of Bernoulli indicator random variables $Y_i$. The resulting expressions
are functions of the expectation of products of the Bernoulli indicator random
variables. These results are used to derive novel expressions for the various
moments in a large number of familiar examples and classic problems including:
the binomial, hypergeometric, Poisson limit of the binomial, Poisson,
Conway-Maxwell-Poisson binomial, Ideal Soliton, and Benford distributions as
well as the empty urns and the matching problems. Other general results include
expressions for the moments of an arbitrary count random variable in terms of
its upper tail probabilities.
"
2110.06355,2024-07-23,"Framework for Accessible and Inclusive Teaching Materials for Statistics
  and Data Science Courses","  Despite rapid growth in the data science workforce, people of color, women,
those with disabilities, and others remain underrepresented in, underserved by,
and sometimes excluded from the field. This pattern prevents equal opportunity
for individuals, while also creating products and policies that perpetuate
inequality. Thus, for statistics and data science educators of the next
generation, accessibility and inclusion should be of utmost importance in our
programs and courses. In this paper, we discuss how we developed an
accessibility and inclusion framework, hence a structure for holding ourselves
accountable to these principles, for the writing of a statistics textbook. We
share our experiences in setting accessibility and inclusion goals, the tools
we used to achieve these goals, and recommendations for other educators. We
provide examples for instructors that can be implemented in their own courses.
"
2110.06445,2022-09-15,Generating MCMC proposals by randomly rotating the regular simplex,"  We present the simplicial sampler, a class of parallel MCMC methods that
generate and choose from multiple proposals at each iteration. The algorithm's
multiproposal randomly rotates a simplex connected to the current Markov chain
state in a way that inherently preserves symmetry between proposals. As a
result, the simplicial sampler leads to a simplified acceptance step: it simply
chooses from among the simplex nodes with probability proportional to their
target density values. We also investigate a multivariate Gaussian-based
symmetric multiproposal mechanism and prove that it also enjoys the same
simplified acceptance step. This insight leads to significant theoretical and
practical speedups. While both algorithms enjoy natural parallelizability, we
show that conventional implementations are sufficient to confer efficiency
gains across an array of dimensions and a number of target distributions.
"
2110.11156,2022-07-06,"DMS, AE, DAA: methods and applications of adaptive time series model
  selection, ensemble, and financial evaluation","  We introduce three adaptive time series learning methods, called Dynamic
Model Selection (DMS), Adaptive Ensemble (AE), and Dynamic Asset Allocation
(DAA). The methods respectively handle model selection, ensembling, and
contextual evaluation in financial time series. Empirically, we use the methods
to forecast the returns of four key indices in the US market, incorporating
information from the VIX and Yield curves. We present financial applications of
the learning results, including fully-automated portfolios and dynamic hedging
strategies. The strategies strongly outperform long-only benchmarks over our
testing period, spanning from Q4 2015 to the end of 2021. The key outputs of
the learning methods are interpreted during the 2020 market crash.
"
2110.15012,2021-10-29,On rereading Savage,"  If we accept Savage's set of axioms, then all uncertainties must be treated
like ordinary probability. Savage espoused subjective probability, allowing,
for example, the probability of Donald Trump's re-election. But Savage's
probability also covers the objective version, such as the probability of heads
in a fair toss of a coin. In other words, there is no distinction between
objective and subjective probability. Savage's system has great theoretical
implications; for example, prior probabilities can be elicited from subjective
preferences, and then get updated by objective evidence, a learning step that
forms the basis of Bayesian computations. Non-Bayesians have generally refused
to accept the subjective aspect of probability or to allow priors in formal
statistical modelling. As demanded, for example, by the late Dennis Lindley,
since Bayesian probability is axiomatic, it is the non-Bayesians' duty to point
out which axioms are not acceptable to them. This is not a simple request,
since the Bayesian axioms are not commonly covered in our professional
training, even in the Bayesian statistics courses. So our aim is to provide a
readable exposition the Bayesian axioms from a close rereading Savage's classic
book.
"
2111.00104,2021-11-02,"Principal Component Pursuit for Pattern Identification in Environmental
  Mixtures","  Environmental health researchers often aim to identify sources/behaviors that
give rise to potentially harmful exposures. We adapted principal component
pursuit (PCP)-a robust technique for dimensionality reduction in computer
vision and signal processing-to identify patterns in environmental mixtures.
PCP decomposes the exposure mixture into a low-rank matrix containing
consistent exposure patterns across pollutants and a sparse matrix isolating
unique exposure events. We adapted PCP to accommodate non-negative and missing
data, and values below a given limit of detection (LOD). We simulated data to
represent environmental mixtures of two sizes with increasing proportions <LOD
and three noise structures. We compared PCP-LOD to principal component analysis
(PCA) to evaluate performance. We next applied PCP-LOD to a mixture of 21
persistent organic pollutants (POPs) measured in 1,000 U.S. adults from the
2001-2002 National Health and Nutrition Examination Survey. We applied singular
value decomposition to the estimated low-rank matrix to characterize the
patterns. PCP-LOD recovered the true number of patterns through
cross-validation for all simulations; based on an a priori specified criterion,
PCA recovered the true number of patterns in 32% of simulations. PCP-LOD
achieved lower relative predictive error than PCA for all simulated datasets
with up to 50% of the data <LOD. When 75% of values were <LOD, PCP-LOD
outperformed PCA only when noise was low. In the POP mixture, PCP-LOD
identified a rank-three underlying structure and separated 6% of values as
unique events. One pattern represented comprehensive exposure to all POPs. The
other patterns grouped chemicals based on known structure and toxicity. PCP-LOD
serves as a useful tool to express multi-dimensional exposures as consistent
patterns that, if found to be related to adverse health, are amenable to
targeted interventions.
"
2111.09279,2021-11-18,A revision to the theory of organic fiducial inference,"  A principle is modified that underlies the theory of organic fiducial
inference as this theory was presented in an earlier paper. This modification,
which is arguably a natural one to make, allows Bayesian inference to sometimes
have a minor role within the theory in question and, as a consequence, allows
more information from the data to be incorporated into the way a full
conditional fiducial density is defined in certain cases. The new version of
the principle concerned is applied to examples that were analysed previously
using the older version of this principle.
"
2111.10191,2022-07-21,History and Nature of the Jeffreys-Lindley Paradox,"  The Jeffreys-Lindley paradox exposes a rift between Bayesian and frequentist
hypothesis testing that strikes at the heart of statistical inference. Contrary
to what most current literature suggests, the paradox was central to the
Bayesian testing methodology developed by Sir Harold Jeffreys in the late
1930s. Jeffreys showed that the evidence against a point-null hypothesis
$\mathcal{H}_0$ scales with $\sqrt{n}$ and repeatedly argued that it would
therefore be mistaken to set a threshold for rejecting $\mathcal{H}_0$ at a
constant multiple of the standard error. Here we summarize Jeffreys's early
work on the paradox and clarify his reasons for including the $\sqrt{n}$ term.
The prior distribution is seen to play a crucial role; by implicitly correcting
for selection, small parameter values are identified as relatively surprising
under $\mathcal{H}_1$. We highlight the general nature of the paradox by
presenting both a fully frequentist and a fully Bayesian version. We also
demonstrate that the paradox does not depend on assigning prior mass to a point
hypothesis, as is commonly believed.
"
2111.12267,2021-11-25,The Practical Scope of the Central Limit Theorem,"  The \textit{Central Limit Theorem (CLT)} is at the heart of a great deal of
applied problem-solving in statistics and data science, but the theorem is
silent on an important implementation issue: \textit{how much data do you need
for the CLT to give accurate answers to practical questions?} Here we examine
several approaches to addressing this issue -- along the way reviewing the
history of this problem over the last 290 years -- and we illustrate the
calculations with case-studies from finite-population sampling and gambling. A
variety of surprises emerge.
"
2112.02950,2021-12-07,"Bayesian Estimation Approach for Linear Regression Models with Linear
  Inequality Restrictions","  Univariate and multivariate general linear regression models, subject to
linear inequality constraints, arise in many scientific applications. The
linear inequality restrictions on model parameters are often available from
phenomenological knowledge and motivated by machine learning applications of
high-consequence engineering systems (Agrell, 2019; Veiga and Marrel, 2012).
Some studies on the multiple linear models consider known linear combinations
of the regression coefficient parameters restricted between upper and lower
bounds. In the present paper, we consider both univariate and multivariate
general linear models subjected to this kind of linear restrictions. So far,
research on univariate cases based on Bayesian methods is all under the
condition that the coefficient matrix of the linear restrictions is a square
matrix of full rank. This condition is not, however, always feasible. Another
difficulty arises at the estimation step by implementing the Gibbs algorithm,
which exhibits, in most cases, slow convergence. This paper presents a Bayesian
method to estimate the regression parameters when the matrix of the constraints
providing the set of linear inequality restrictions undergoes no condition. For
the multivariate case, our Bayesian method estimates the regression parameters
when the number of the constrains is less than the number of the regression
coefficients in each multiple linear models. We examine the efficiency of our
Bayesian method through simulation studies for both univariate and multivariate
regressions. After that, we illustrate that the convergence of our algorithm is
relatively faster than the previous methods. Finally, we use our approach to
analyze two real datasets.
"
2112.07737,2024-05-30,The Importance of Discussing Assumptions when Teaching Bootstrapping,"  Bootstrapping and other resampling methods are increasingly appearing in the
textbooks and curricula of courses that introduce undergraduate students to
statistical methods. In order to teach the bootstrap well, students and
instructors need to be aware of the assumptions behind these intervals. In this
article we discuss important assumptions about simple non-parametric bootstrap
intervals and their corresponding hypothesis tests. We present simulations that
instructors can use to help students understand some of the assumptions behind
these methods. The simulations will be especially relevant to instructors who
desire to increase accessibility for students from non-mathematical
backgrounds, including those with math anxiety.
"
2112.10073,2022-04-05,"Temporal and spectral governing dynamics of Australian hydrological
  streamflow time series","  We use new and established methodologies in multivariate time series analysis
to study the dynamics of 414 Australian hydrological stations' streamflow.
First, we analyze our collection of time series in the temporal domain, and
compare the similarity in hydrological stations' candidate trajectories. Then,
we introduce a Whittle Likelihood-based optimization framework to study the
collective similarity in periodic phenomena among our collection of stations.
Having identified noteworthy similarity in the temporal and spectral domains,
we introduce an algorithmic procedure to estimate a governing hydrological
streamflow process across Australia. To determine the stability of such
behaviours over time, we then study the evolution of the governing dynamics and
underlying time series with time-varying applications of principal components
analysis (PCA) and spectral analysis.
"
2112.10996,2021-12-22,"Efficient Estimation of the Maximal Association between Multiple
  Predictors and a Survival Outcome","  This paper develops a new approach to post-selection inference for screening
high-dimensional predictors of survival outcomes. Post-selection inference for
right-censored outcome data has been investigated in the literature, but much
remains to be done to make the methods both reliable and
computationally-scalable in high-dimensions. Machine learning tools are
commonly used to provide {\it predictions} of survival outcomes, but the
estimated effect of a selected predictor suffers from confirmation bias unless
the selection is taken into account. The new approach involves construction of
semi-parametrically efficient estimators of the linear association between the
predictors and the survival outcome, which are used to build a test statistic
for detecting the presence of an association between any of the predictors and
the outcome. Further, a stabilization technique reminiscent of bagging allows a
normal calibration for the resulting test statistic, which enables the
construction of confidence intervals for the maximal association between
predictors and the outcome and also greatly reduces computational cost.
Theoretical results show that this testing procedure is valid even when the
number of predictors grows superpolynomially with sample size, and our
simulations support that this asymptotic guarantee is indicative the
performance of the test at moderate sample sizes. The new approach is applied
to the problem of identifying patterns in viral gene expression associated with
the potency of an antiviral drug.
"
2112.11079,2023-12-12,Data fission: splitting a single data point,"  Suppose we observe a random vector $X$ from some distribution $P$ in a known
family with unknown parameters. We ask the following question: when is it
possible to split $X$ into two parts $f(X)$ and $g(X)$ such that neither part
is sufficient to reconstruct $X$ by itself, but both together can recover $X$
fully, and the joint distribution of $(f(X),g(X))$ is tractable? As one
example, if $X=(X_1,\dots,X_n)$ and $P$ is a product distribution, then for any
$m<n$, we can split the sample to define $f(X)=(X_1,\dots,X_m)$ and
$g(X)=(X_{m+1},\dots,X_n)$. Rasines and Young (2022) offers an alternative
approach that uses additive Gaussian noise -- this enables post-selection
inference in finite samples for Gaussian distributed data and asymptotically
when errors are non-Gaussian. In this paper, we offer a more general
methodology for achieving such a split in finite samples by borrowing ideas
from Bayesian inference to yield a (frequentist) solution that can be viewed as
a continuous analog of data splitting. We call our method data fission, as an
alternative to data splitting, data carving and p-value masking. We exemplify
the method on a few prototypical applications, such as post-selection inference
for trend filtering and other regression problems.
"
2112.12067,2021-12-24,"Tutorial on principal component analysis, with applications in R","  This tutorial reviews the main steps of the principal component analysis of a
multivariate data set and its subsequent dimensional reduction on the grounds
of identified dominant principal components. The underlying computations are
demonstrated and performed by means of a script written in the statistical
software package R.
"
2112.12442,2021-12-24,A generalised matching distribution for the problem of coincidences,"  This paper examines the classical matching distribution arising in the
""problem of coincidences"". We generalise the classical matching distribution
with a preliminary round of allocation where items are correctly matched with
some fixed probability, and remaining non-matched items are allocated using
simple random sampling without replacement. Our generalised matching
distribution is a convolution of the classical matching distribution and the
binomial distribution. We examine the properties of this latter distribution
and show how its probability functions can be computes. We also show how to use
the distribution for matching tests and inferences of matching ability.
"
2112.13829,2022-09-20,Source Reconstruction for Spatio-Temporal Physical Statistical Models,"  In many applications, a signal is deformed by well-understood dynamics before
it can be measured. For example, when a pollutant enters a river, it
immediately begins dispersing, flowing, settling, and reacting. If the
pollutant enters at a single point, its concentration can be measured before it
enters the complex dynamics of the river system. However, in the case of a
non-point source pollutant, it is not clear how to efficiently measure its
source. One possibility is to record concentration measurements in the river,
but this signal is masked by the fluid dynamics of the river. Specifically,
concentration is governed by the advection-diffusion-reaction PDE, with an
unknown source term. We propose a method to statistically reconstruct a source
term from these PDE-deformed measurements. Our method is general and applies to
any linear PDE. This method has important applications in the study of
environmental DNA and non-point source pollution.
"
2112.15258,2024-12-30,"Random cohort effects and age groups dependency structure for mortality
  modelling and forecasting: Mixed-effects time-series model approach","  There have been significant efforts devoted to solving the longevity risk
given that a continuous growth in population ageing has become a severe issue
for many developed countries over the past few decades. The Cairns-Blake-Dowd
(CBD) model, which incorporates cohort effects parameters in its parsimonious
design, is one of the most well-known approaches for mortality modelling at
higher ages and longevity risk. This article proposes a novel mixed-effects
time-series approach for mortality modelling and forecasting with
considerations of age groups dependence and random cohort effects parameters.
The proposed model can disclose more mortality data information and provide a
natural quantification of the model parameters uncertainties with no
pre-specified constraint required for estimating the cohort effects parameters.
The abilities of the proposed approach are demonstrated through two
applications with empirical male and female mortality data. The proposed
approach shows remarkable improvements in terms of forecast accuracy compared
to the CBD model in the short-, mid-and long-term forecasting using mortality
data of several developed countries in the numerical examples.
"
2201.01132,2022-01-05,"A Multivariate Dependence Analysis for Electricity Prices, Demand and
  Renewable Energy Sources","  This paper examines the dependence between electricity prices, demand, and
renewable energy sources by means of a multivariate copula model {while
studying Germany, the widest studied market in Europe}. The inter-dependencies
are investigated in-depth and monitored over time, with particular emphasis on
the tail behavior. To this end, suitable tail dependence measures are
introduced to take into account a multivariate extreme scenario appropriately
identified {through the} Kendall's distribution function. The empirical
evidence demonstrates a strong association between electricity prices,
renewable energy sources, and demand within a day and over the studied years.
Hence, this analysis provides guidance for further and different incentives for
promoting green energy generation while considering the time-varying
dependencies of the involved variables
"
2201.02705,2022-01-11,"Tisane: Authoring Statistical Models via Formal Reasoning from
  Conceptual and Data Relationships","  Proper statistical modeling incorporates domain theory about how concepts
relate and details of how data were measured. However, data analysts currently
lack tool support for recording and reasoning about domain assumptions, data
collection, and modeling choices in an integrated manner, leading to mistakes
that can compromise scientific validity. For instance, generalized linear
mixed-effects models (GLMMs) help answer complex research questions, but
omitting random effects impairs the generalizability of results. To address
this need, we present Tisane, a mixed-initiative system for authoring
generalized linear models with and without mixed-effects. Tisane introduces a
study design specification language for expressing and asking questions about
relationships between variables. Tisane contributes an interactive compilation
process that represents relationships in a graph, infers candidate statistical
models, and asks follow-up questions to disambiguate user queries to construct
a valid model. In case studies with three researchers, we find that Tisane
helps them focus on their goals and assumptions while avoiding past mistakes.
"
2201.03512,2022-01-11,"SMLE: An R Package for Joint Feature Screening in Ultrahigh-dimensional
  GLMs","  The sparsity-restricted maximum likelihood estimator (SMLE) has received
considerable attention for feature screening in ultrahigh-dimensional
regression. SMLE is a computationally convenient method that naturally
incorporates the joint effects among features in the screening process. We
develop a publicly available R package SMLE, which provides a user-friendly
environment to carry out SMLE in generalized linear models. In particular, the
package includes functions to conduct SMLE-screening and post-screening
selection using SMLE with popular selection criteria such as AIC and (extended)
BIC. The package gives users the flexibility in controlling a series of
screening parameters and accommodates both numerical and categorical feature
input. The usage of the package is illustrated on extensive numerical examples,
where the promising performance of SMLE is well observed.
"
2201.04982,2023-12-08,An empirical exploration of the diversified R ecosystem,"  Born in the late 20s, R is one of the most popular software for statistical
computing and graphics. With the development of information technology and the
advent of the big data era, great changes have taken place in the R ecosystem.
Based on the meta information of the Comprehensive R Archive Network (CRAN) and
the bibliometric data of literature citing R, we discovered that while R is
initiated by statistics, its development is benefited greatly from computer
science and the main user group in academics come from various disciplines such
as agricultural science, biological science, environmental science and medical
science. In addition, we displayed the collaboration patterns among R
developers and analyze the possible effects of collaboration in the R
community.
"
2201.05486,2022-10-12,"A computational framework for modelling infectious disease policy based
  on age and household structure with applications to the COVID-19 pandemic","  The widespread, and in many countries unprecedented, use of
non-pharmaceutical interventions (NPIs) during the COVID-19 pandemic has
highlighted the need for mathematical models which can estimate the impact of
these measures while accounting for the highly heterogeneous risk profile of
COVID-19. Models accounting either for age structure or the household structure
necessary to explicitly model many NPIs are commonly used in infectious disease
modelling, but models incorporating both levels of structure present
substantial computational and mathematical challenges due to their high
dimensionality. Here we present a modelling framework for the spread of an
epidemic that includes explicit representation of age structure and household
structure. Our model is formulated in terms of tractable systems of ordinary
differential equations for which we provide an open-source Python
implementation. Such tractability leads to significant benefits for model
calibration, exhaustive evaluation of possible parameter values, and
interpretability of results. We demonstrate the flexibility of our model
through four policy case studies, where we quantify the likely benefits of the
following measures which were either considered or implemented in the UK during
the current COVID-19 pandemic: control of within- and between-household mixing
through NPIs; formation of support bubbles during lockdown periods;
out-of-household isolation (OOHI); and temporary relaxation of NPIs during
holiday periods. Our ordinary differential equation formulation and associated
analysis demonstrate that multiple dimensions of risk stratification and social
structure can be incorporated into infectious disease models without
sacrificing mathematical tractability. This model and its software
implementation expand the range of tools available to infectious disease policy
analysts.
"
2201.10610,2022-01-27,"Extending compositional data analysis from a graph signal processing
  perspective","  Traditional methods for the analysis of compositional data consider the
log-ratios between all different pairs of variables with equal weight,
typically in the form of aggregated contributions. This is not meaningful in
contexts where it is known that a relationship only exists between very
specific variables (e.g.~for metabolomic pathways), while for other pairs a
relationship does not exist. Modeling absence or presence of relationships is
done in graph theory, where the vertices represent the variables, and the
connections refer to relations. This paper links compositional data analysis
with graph signal processing, and it extends the Aitchison geometry to a
setting where only selected log-ratios can be considered. The presented
framework retains the desirable properties of scale invariance and
compositional coherence. An additional extension to include absolute
information is readily made. Examples from bioinformatics and geochemistry
underline the usefulness of thisapproach in comparison to standard methods for
compositional data analysis.
"
2201.11454,2022-01-28,Estimating the Capacities of Function-as-a-Service Functions,"  Serverless computing is a cloud computing paradigm that allows developers to
focus exclusively on business logic as cloud service providers manage resource
management tasks. Serverless applications follow this model, where the
application is decomposed into a set of fine-grained Function-as-a-Service
(FaaS) functions. However, the obscurities of the underlying system
infrastructure and dependencies between FaaS functions within the application
pose a challenge for estimating the performance of FaaS functions. To
characterize the performance of a FaaS function that is relevant for the user,
we define Function Capacity (FC) as the maximal number of concurrent
invocations the function can serve in a time without violating the
Service-Level Objective (SLO).
  The paper addresses the challenge of quantifying the FC individually for each
FaaS function within a serverless application. This challenge is addressed by
sandboxing a FaaS function and building its performance model. To this end, we
develop FnCapacitor - an end-to-end automated Function Capacity estimation
tool. We demonstrate the functioning of our tool on Google Cloud Functions
(GCF) and AWS Lambda. FnCapacitor estimates the FCs on different deployment
configurations (allocated memory & maximum function instances) by conducting
time-framed load tests and building various models using statistical: linear,
ridge, and polynomial regression, and Deep Neural Network (DNN) methods on the
acquired performance data. Our evaluation of different FaaS functions shows
relatively accurate predictions, with an accuracy greater than 75% using DNN
for both cloud providers.
"
2201.12318,2022-02-14,"Effect of Measurement Errors on the Multivariate CUSUM CoDa Control
  Chart for the Manufacturing Process","  Control charts, one of the main tools in Statistical Process Control (SPC),
have been widely adopted in manufacturing sectors as an effective strategy for
malfunction detection throughout the previous decades. Measurement errors
(M.E's) are involved in the quality characteristic of interest. The authors
explored the impact of a linear covariate error model on the multivariate
cumulative sum (CUSUM) control charts for a specific kind of data known as
compositional data(CoDa). The average run length ARL is used to assess the
performance of the proposed chart. The results indicate that M.E's
significantly affects the multivariate CUSUM-CoDa control charts. The authors
have used the Markov chain method to study the impact of different involved
parameters using four different cases for the variance-covariance matrix (i.e.
uncorrelated with equal variances, negatively correlated with equal variances,
uncorrelated with unequal variances, positively correlated with unequal
variances). The authors concluded that the ARL of the multivariate CUSUM-CoDa
chart increase with an increase in the value of error variance-covariance
matrix, while the ARL decreases with an increase in the subgroup size m or the
constant powering b. For the implementation of the proposal, two illustrated
examples have been reported for multivariate CUSUM-CoDa control charts in the
presence of M.E's. One deals with the manufacturing process of uncoated aspirin
tablets, and the other is based on monitoring machines in the muesli
manufacturing process.
"
2201.12960,2024-12-31,"Teaching modeling in introductory statistics: A comparison of formula
  and tidyverse syntaxes","  There are many pedagogical considerations for incorporating programming into
a statistics course. When using the programming language R, one consideration
is the particular R syntax that will be used. This paper reports on a
head-to-head comparison run in a pair of introductory statistics labs, one
conducted fully in the formula syntax, the other in tidyverse. Analysis of pre-
and post-survey data show minimal differences between the two labs, with
students reporting a positive experience regardless of section. Analysis of
data from YouTube and RStudio Cloud show interesting distinctions. The formula
section appeared to watch a larger proportion of pre-lab YouTube videos, but
spend less time computing on RStudio Cloud. Conversely, the tidyverse section
watched a smaller proportion of the videos and spent more time computing.
Analysis of lab materials showed tidyverse labs tended to be slightly longer in
terms of lines in the provided RMarkdown materials and minutes of the
associated YouTube videos. The tidyverse labs exposed students to more distinct
R functions, but reused functions more frequently. Both labs relied on a
relatively small vocabulary of consistent functions, which can provide a
starting point for instructors interested in teaching introductory statistics
in R. The instructor experience of teaching in the two syntaxes diverged
primarily when discussing relationships between categorical variables, as well
as when working with summary statistics for numeric variables. This work
provides additional evidence for instructors looking to choose between syntaxes
for introductory statistics teaching.
"
2202.00877,2022-02-03,"Efficient Volatility Estimation for L\'evy Processes with Jumps of
  Unbounded Variation","  Statistical inference for stochastic processes based on high-frequency
observations has been an active research area for more than a decade. One of
the most well-known and widely studied problems is that of estimation of the
quadratic variation of the continuous component of an It\^o semimartingale with
jumps. Several rate- and variance-efficient estimators have been proposed in
the literature when the jump component is of bounded variation. However, to
date, very few methods can deal with jumps of unbounded variation. By
developing new high-order expansions of the truncated moments of a L\'evy
process, we construct a new rate- and variance-efficient estimator for a class
of L\'evy processes of unbounded variation, whose small jumps behave like those
of a stable L\'evy process with Blumenthal-Getoor index less than $8/5$. The
proposed method is based on a two-step debiasing procedure for the truncated
realized quadratic variation of the process. Our Monte Carlo experiments
indicate that the method outperforms other efficient alternatives in the
literature in the setting covered by our theoretical framework.
"
2202.02650,2022-07-21,"Efficient Privacy Preserving Logistic Regression for Horizontally
  Distributed Data","  Internet of Things devices are expanding rapidly and generating huge amount
of data. There is an increasing need to explore data collected from these
devices. Collaborative learning provides a strategic solution for the Internet
of Things settings but also raises public concern over data privacy. In recent
years, large amount of privacy preserving techniques have been developed based
on secure multi-party computation and differential privacy. A major challenge
of collaborative learning is to balance disclosure risk and data utility while
maintaining high computation efficiency. In this paper, we proposed privacy
preserving logistic regression model using matrix encryption approach. The
secure scheme is resilient to chosen plaintext attack, known plaintext attack,
and collusion attack that could compromise any agencies in the collaborative
learning. Encrypted model estimate is decrypted to provide true model results
with no accuracy degradation. Verification phase is implemented to examine
dishonest behavior among agencies. Experimental evaluations demonstrate fast
convergence rate and high efficiency of proposed scheme.
"
2202.03072,2022-02-08,Mathematical models of confirmation bias,"  Confirmation bias is a cognitive bias that adversely affects management
decisions, and mathematical modelling is an aid to its detailed understanding.
Bias in opinion update about the value of a parameter is modelled here assuming
that observations are discounted depending on their distance from prior
opinion. The models allow belief persistence, attitude polarization, and the
irrational primacy effect to be explored. A general framework for exploring
large-sample properties of these models is given, and an attempt made to
classify the models. An interesting result is that in some models the influence
of an observation always increases with distance from the prior opinion,
whereas in others observations greatly at odds with prior opinion are given
very little weight. The models could be useful to those exploring these
phenomena in detail.
"
2202.03652,2023-03-24,Real-time privacy preserving disease diagnosis using ECG signal,"  The rapid development in Internet of Medical Things (IoMT) boosts the
opportunity for real-time health monitoring using various data types such as
electroencephalography (EEG) and electrocardiography (ECG). Security issues
have significantly impeded the e-healthcare system implementation. Three
important challenges for privacy preserving system need to be addressed:
accurate diagnosis, privacy protection without compromising accuracy, and
computation efficiency. It is essential to guarantee prediction accuracy since
disease diagnosis is strongly related to health and life. By implementing
matrix encryption method, we propose a real-time disease diagnosis scheme using
support vector machine (SVM). A biomedical signal provided by the client is
diagnosed such that the server does not get any information about the signal as
well as the final result of the diagnosis while the proposed scheme also
achieves confidentiality of the SVM classifier and the server's medical data.
The proposed scheme has no accuracy degradation. Experiments on real-world data
illustrate the high efficiency of the proposed scheme. It takes less than 1
second to derive the disease diagnosis result using a device with 4Gb RAMs,
suggesting the feasibility to implement real-time privacy preserving health
monitoring.
"
2202.03819,2022-02-09,Mr. Bayes and the classics: a suggested interpretation,"  The main hypothesis about Thomas Bayes's intentions to write his famous Essay
on probability is that he wanted to refute the arguments of David Hume against
the reliability of the occurrence of miracles, published in 1748. In this paper
we argue that it was not Bayes's intention to rebut Hume but that his interest
on the ""inverse problem"" came about as result of his study of the second
edition of Abraham De Moivre's book, The Doctrine of Chances, published in
1738.
  A possible communication of Bayes's breakthrough might have annoyed De
Moivre, leading to a response written for Bayes in the third edition of De
Moivre's book, published in 1756. Among other points, the response claims that
De Moivre was the first to solve the mentioned inverse problem. Under this
perspective Richard Price's letter, written as preface to Bayes's essay, has a
new interpretation, appearing also as a defense of Bayes premiership on a
satisfactory or proper solution.
"
2202.05876,2022-02-15,Group testing via residuation and partial geometries,"  The motivation for this paper comes from the ongoing SARS-CoV-2 Pandemic. Its
goal is to present a previously neglected approach to non-adaptive group
testing and describes it in terms of residuated pairs on partially ordered
sets. Our investigation has the advantage, as it naturally yields an efficient
decision scheme (decoder) for any given testing scheme. This decoder allows to
detect a large amount of infection patterns. Apart from this, we devise a
construction of good group testing schemes that are based on incidence matrices
of finite partial linear spaces. The key idea is to exploit the structure of
these matrices and make them available as test matrices for group testing.
These matrices may generally be tailored for different estimated disease
prevalence levels. As an example, we discuss the group testing schemes based on
generalized quadrangles. In the context at hand, we state our results only for
the error-free case so far. An extension to a noisy scenario is desirable and
will be treated in a subsequent account on the topic.
"
2202.06762,2022-02-15,"Temporal Properties of Vaccine Effectiveness Measures in Presence of
  Multiple Pathogen Variants and Multiple Vaccines","  Vaccine effectiveness (VE) is typically defined as incidence rate ratio,
cumulative-risk ratio, or odds ratio. The VE based on incidence rate ratio is
known to be time-invariant over the study period for leaky action vaccines and,
the VE based on cumulative-risk ratio is time-invariant for all-or-none action
vaccines. Consequently, these VE measures are recommended as appropriate
measures of VE for leaky and all-or-none vaccines, respectively. However, in
diseases with multiple pathogen variants and multiple vaccines, investigators
may also be interested in variant-specific VE of a vaccine, the relative VE of
a vaccine against two variants, or the relative VE of different vaccines
against a given variant. In this multi-variant and multi-vaccine scenario, the
temporal properties of the aforementioned VE measures have not been studied
entirely yet. Furthermore, no general-purpose sample size calculator is
available for either studies that intend to estimate variant-specific VE or
relative VE. As a solution, we define variant-specific and relative VE measures
while accounting for multiple competing pathogen variants. We then propose a
generic mode of action for all-or-none vaccines in a multi-variant setting.
Subsequently, we evaluate the conditions and the extent to which various VE
measures can be time-varying. We show that every VE measure is time-varying for
all-or-none action vaccines in a multi-variant and multi-vaccine scenario. For
leaky vaccines, we show that all measures other than those based on incidence
rate ratios are time-varying. We discuss the practical implications of these
results on VE studies in the context of the commonly used cohort, cumulative
case-control, and test-negative study designs. Lastly, for the multi-variant
and multi-vaccine scenario, we implement sample size calculations for both
variant-specific and relative VE in an R package and an online calculator.
"
2202.06779,2022-02-15,"Modelling and forecasting patient recruitment in clinical trials with
  patients' dropout","  This paper focuses on statistical modelling and prediction of patient
recruitment in clinical trials accounting for patients dropout. The recruitment
model is based on a Poisson-gamma model introduced by Anisimov and Fedorov
(2007), where the patients arrive at different centres according to Poisson
processes with rates viewed as gamma-distributed random variables. Each patient
can drop the study during some screening period. Managing the dropout process
is of a major importance but data related to dropout are rarely correctly
collected. In this paper, a few models of dropout are proposed. The technique
for estimating parameters and predicting the number of recruited patients over
time and the recruitment time is developed. Simulation results confirm the
applicability of the technique and thus, the necessity to account for patients
dropout at the stage of forecasting recruitment in clinical trials.
"
2202.07389,2022-10-11,Spam four ways: Making sense of text data,"  The world is full of text data, yet text analytics has not traditionally
played a large part in statistics education. We consider four different ways to
provide students with opportunities to explore whether email messages are
unwanted correspondence (spam). Text from subject lines are used to identify
features that can be used in classification. The approaches include use of a
Model Eliciting Activity, exploration with CODAP, modeling with a specially
designed Shiny app, and coding more sophisticated analyses using R. The
approaches vary in their use of technology and code but all share the common
goal of using data to make better decisions and assessment of the accuracy of
those decisions.
"
2202.09326,2022-02-21,Edge coherence in multiplex networks,"  This paper introduces a nonparametric framework for the setting where
multiple networks are observed on the same set of nodes, also known as
multiplex networks. Our objective is to provide a simple parameterization which
explicitly captures linear dependence between the different layers of networks.
For non-Euclidean observations, such as shapes and graphs, the notion of
""linear"" must be defined appropriately. Taking inspiration from the
representation of stochastic processes and the analogy of the multivariate
spectral representation of a stochastic process with joint exchangeability of
Bernoulli arrays, we introduce the notion of edge coherence as a measure of
linear dependence in the graph limit space. Edge coherence is defined for pairs
of edges from any two network layers and is the key novel parameter. We
illustrate the utility of our approach by eliciting simple models such as a
correlated stochastic blockmodel and a correlated inhomogeneous graph limit
model.
"
2202.09504,2024-07-23,Tools and Recommendations for Reproducible Teaching,"  It is recommended that teacher-scholars of data science adopt reproducible
workflows in their research as scholars and teach reproducible workflows to
their students. In this paper, we propose a third dimension to reproducibility
practices and recommend that regardless of whether they teach reproducibility
in their courses or not, data science instructors adopt reproducible workflows
for their own teaching. We consider computational reproducibility,
documentation, and openness as three pillars of reproducible teaching
framework. We share tools, examples, and recommendations for the three pillars.
"
2202.11591,2022-08-11,"Exploring the effects of activity-preserving time dilation on the
  dynamic interplay of airborne contagion processes and temporal networks using
  an interaction-driven model","  Contacts' temporal ordering and dynamics are crucial for understanding the
transmission of infectious diseases. We introduce an interaction-driven model
of an airborne disease over contact networks. We demonstrate our
interaction-driven contagion model, instantiated for COVID-19, over
history-maintaining random temporal networks and real-world contacts. We use it
to evaluate temporal, spatiotemporal, and spatial social distancing policies.
We find that a spatial distancing policy is mainly beneficial at the early
stages of a disease.
  We then continue to evaluate temporal social distancing, that is, timeline
dilation that maintains the activity potential. We expand our model to consider
the exposure to viral load, which we correlate with meetings' duration. Using
real-life contact data, we demonstrate the beneficial effect of timeline
dilation on overall infection rates.
  Our results demonstrate that given the same transmission level, there is a
decrease in the disease's infection rate and overall prevalence under timeline
dilation conditions. We further show that slow-spreading pathogens (i.e.,
require more prolonged exposure to infect) spread roughly at the same rate as
fast-spreading ones in highly active communities. This is surprising since
slower pathogens follow paths that include longer meetings, while faster
pathogens can potentially follow paths that include shorter meetings, which are
more common. Our results demonstrate that the temporal dynamics of a community
have a more significant effect on the spread of the disease than the
characteristics of the spreading processes.
"
2202.12480,2022-02-28,"Revisiting the secondary climate attributes for transportation
  infrastructure management: A Redux and Update for 2020","  Environmental conditions in various regions can have a severely negative
impact on the longevity and durability of the civil engineering
infrastructures. In 2018, a published paper used 1971 to 2010 NOAA data from
the contiguous United States to examine the temporal changes in secondary
climate attributes (freeze-thaw cycles and freeze index) using the climate
normals from two time windows, 1971-2000 and 1981-2010. That paper investigated
whether there have been statistically significant changes in climate attribute
levels across the two time windows, and used GIS-based interpolation methods to
develop isarithmic maps of the climate attributes to facilitate their
interpretation and application. This paper updates that study. In this study,
we use NOAA climatic data from 1991 to 2020 to construct continuous surface
maps showing the values of the primary and secondary climate attributes at the
48 continental states. The new maps provide an updated picture of the freeze
index values and freeze-thaw cycles for the most recent climate condition.
These new values provide a better picture of the freezing season
characteristics of the United States, and will provide information necessary
for better winter maintenance procedures and infrastructure design to
accommodate regional climate differences.
"
2203.01290,2022-12-29,"Effect of congestion avoidance due to congestion information provision
  on optimizing agent dynamics on an endogenous star network topology","  The importance of fundamental research on network topologies is widely
acknowledged. This study aims to elucidate the effect of congestion avoidance
of agents given congestion information on optimizing traffic in a network
topology. We investigated stochastic traffic networks in a star topology with a
central node connected to isolated secondary nodes with different preferences.
Each agent at the central node selects a secondary node by referring to the
declining preferences based on the congestion rate of the secondary nodes. We
examined two scenarios: 1) Each agent can repeatedly visit the central and
secondary nodes. 2) Each agent can access each secondary node only once. For
1), we investigated the uniformity of the agent distribution in a stationary
state, and for 2), we measured the travel time for all agents visiting all
nodes. When agents repeatedly visit central and other nodes, the uniformity of
agent distribution has been found to show three types of nonlinear dependence
on the increase in nodes. We found that multivariate statistics describe these
characteristic dependences well, suggesting that the balance between the
equalization of network usage by avoiding congestion and the covariance caused
by mutual referral to congestion information determines the uniformity. We
discovered that congestion-avoidance linearizes the travel time, which
increases exponentially with the number of nodes, notwithstanding the degree of
reference to the congestion information. Consequently, we successfully
described the optimization effect of congestion-avoidance on the collective
dynamics of agents in star topologies. Our findings are useful in many areas of
network science.
"
2203.04679,2022-12-20,"Assessing and mitigating systematic errors in forest attribute maps
  utilizing harvester and airborne laser scanning data","  Cut-to-length harvesters collect useful information for modeling
relationships between forest attributes and airborne laser scanning (ALS) data.
However, harvesters operate in mature forests, which may introduce selection
biases that can result in systematic errors in harvester data-based forest
attribute maps. We fitted regression models (harvester models) for volume (V),
height (HL), stem frequency (N), above-ground biomass, basal area, and
quadratic mean diameter (QMD) using harvester and ALS data. Performances of the
harvester models were evaluated using national forest inventory plots in an 8.7
Mha study area. We estimated biases of large-area synthetic estimators and
compared efficiencies of model-assisted (MA) estimators with field data-based
direct estimators. The harvester models performed better in productive than
unproductive forests, but systematic errors occurred in both. The use of MA
estimators resulted in efficiency gains that were largest for HL (relative
efficiency, RE=6.0) and smallest for QMD (RE=1.5). The bias of the synthetic
estimator was largest for N (39%) and smallest for V (1%). The latter was due
to an overestimation of deciduous and an underestimation of spruce forests that
by chance balanced. We conclude that a probability sample of reference
observations may be required to ensure the unbiasedness of estimators utilizing
harvester data.
"
2203.10706,2022-03-22,Predicting Cricket Outcomes using Bayesian Priors,"  This research has developed a statistical modeling procedure to predict
outcomes of future cricket tournaments. Proposed model provides an insight into
the application of stratified survey sampling to the team selection pattern by
incorporating individual players' performance history coupled with Bayesian
priors not only against a particular opposition but also against any cricket
playing nation - full member of International Cricket Council (ICC). A case
study for the next ICC cricket world cup 2023 in India is provided, predictions
are obtained for all participating teams against one another, and simulation
results are discussed. The proposed statistical model is tested on 2020 Indian
Premier League (IPL) season. The model predicted the top three finishers of IPL
2020 correctly, including the winners of the tournament, Mumbai Indians, and
other positions with reasonable accuracy. The method can predict probabilities
of winning for each participating team. This method can be extended to other
cricket tournaments as well.
"
2203.14526,2022-03-29,Non-iterative Gaussianization,"  In this work, we propose a non-iterative Gaussian transformation strategy
based on copula function, which doesn't require some commonly seen restrictive
assumptions in the previous studies such as the elliptically symmetric
distribution assumption and the linear independent component analysis
assumption. Theoretical properties guarantee the proposed strategy can exactly
transfer any random variable vector with a continuous multivariate distribution
to a variable vector that follows a multivariate Gaussian distribution.
Simulation studies also demonstrate the outperformance of such a strategy
compared to some other methods like Box-Cox Gaussianization and radial
Gaussianization. An application for probability density estimation for image
synthesis is also shown.
"
2203.17262,2022-04-01,Length L-function for Network-Constrained Point Data,"  Network constrained points are referred to as points restricted to road
networks, such as taxi pick up and drop off locations. A significant pattern of
network constrained points is referred to as an aggregation; e.g., the
aggregation of pick up points may indicate a high taxi demand in a particular
area. Although the network K function using the shortest path network distance
has been proposed to detect point aggregation, its statistical unit is still
radius based. R neighborhood, in particular, has inconsistent network length
owing to the complex configuration of road networks which cause unfair counts
and identification errors in networks (e.g., the length of the r neighborhood
located at an intersection is longer than that on straight roads, which may
include more points). In this study, we derived the length L function for
network constrained points to identify the aggregation by designing a novel
neighborhood as the statistical unit; the total length of this is consistent
throughout the network. Compared to the network K function, our method can
detect a true to life aggregation scale, identify the aggregation with higher
network density, as well as identify the aggregations that the network K
function cannot. We validated our method using taxi trips pick up location data
within Zhongguancun Area in Beijing, analyzing differences in maximal
aggregation between workdays and weekends to understand taxi demand in the
morning and evening peak.
"
2204.01540,2022-12-08,Teaching for large-scale Reproducibility Verification,"  We describe a unique environment in which undergraduate students from various
STEM and social science disciplines are trained in data provenance and
reproducible methods, and then apply that knowledge to real, conditionally
accepted manuscripts and associated replication packages. We describe in detail
the recruitment, training, and regular activities. While the activity is not
part of a regular curriculum, the skills and knowledge taught through explicit
training of reproducible methods and principles, and reinforced through
repeated application in a real-life workflow, contribute to the education of
these undergraduate students, and prepare them for post-graduation jobs and
further studies.
"
2204.02231,2022-04-06,"Causal inference: critical developments, past and future","  Causality is a subject of philosophical debate and a central scientific issue
with a long history. In the statistical domain, the study of cause and effect
based on the notion of `fairness' in comparisons dates back several hundred
years, and yet statistical concepts and developments that form the area of
causal inference are only decades old. In this paper, we review core tenets and
methods of causal inference and key developments in the history of the field.
We highlight connections with traditional `associational' statistical methods,
including estimating equations and semiparametric theory, and point to current
topics of active research in this crucial area of our field.
"
2204.02344,2023-06-19,Bayesian Quantile Regression for Longitudinal Count Data,"  This work introduces Bayesian quantile regression modeling framework for the
analysis of longitudinal count data. In this model, the response variable is
not continuous and hence an artificial smoothing of counts is incorporated. The
Bayesian implementation utilizes the normal-exponential mixture representation
of the asymmetric Laplace distribution for the response variable. An efficient
Gibbs sampling algorithm is derived for fitting the model to the data. The
model is illustrated through simulation studies and implemented in an
application drawn from neurology. Model comparison demonstrates the practical
utility of the proposed model.
"
2204.02686,2022-05-10,The loss value of multilinear regression,"  Determinant formulas are presented for: a certain positive semidefinite,
hermitian matrix; the loss value of multilinear regression; the multiple linear
regression coefficient.
"
2204.05313,2022-09-20,Six Statistical Senses,"  This article proposes a set of categories, each one representing a particular
distillation of important statistical ideas. Each category is labeled a ""sense""
because we think of these as essential in helping every statistical mind
connect in constructive and insightful ways with statistical theory,
methodologies, and computation, toward the ultimate goal of building
statistical phronesis. The illustration of each sense with statistical
principles and methods provides a sensical tour of the conceptual landscape of
statistics, as a leading discipline in the data science ecosystem.
"
2204.05530,2022-04-13,Computational Statistics and Data Science in the Twenty-first Century,"  Data science has arrived, and computational statistics is its engine. As the
scale and complexity of scientific and industrial data grow, the discipline of
computational statistics assumes an increasingly central role among the
statistical sciences. An explosion in the range of real-world applications
means the development of more and more specialized computational methods, but
five Core Challenges remain. We provide a high-level introduction to
computational statistics by focusing on its central challenges, present recent
model-specific advances and preach the ever-increasing role of non-sequential
computational paradigms such as multi-core, many-core and quantum computing.
Data science is bringing major changes to computational statistics, and these
changes will shape the trajectory of the discipline in the 21st century.
"
2204.07903,2022-04-19,"Comment on ""The statistics wars and intellectual conflicts of interest""
  by D. Mayo","  While P-values are widely abused, they are a useful tool for many purposes;
banning them is analogous to banning scalpels because most people do not know
how to perform surgery. Many reported P-values are not genuine P-values, for a
variety of reasons. Perhaps the most widespread and pernicious problem is the
Type III error of testing a statistical hypothesis that has little or no
connection to the scientific hypothesis.
"
2204.08603,2022-04-20,"Minimizing Fleet Size and Improving Bike Allocation of Bike Sharing
  under Future Uncertainty","  As a rapidly expanding service, bike sharing is facing severe problems of
bike over-supply and demand fluctuation in many Chinese cities. This study
develops a large-scale method to determine the minimum fleet size under
uncertainty, based on the bike sharing data of millions of trips in Nanjing. It
is found that the algorithm of minimizing fleet size under the
incomplete-information scenario is effective in handling future uncertainty.
For a dockless bike sharing system, supplying 14.5% of the original fleet could
meet 96.8% of trip demands. Meanwhile, the results suggest that providing a
integrated service platform that integrates multiple companies can
significantly reduce the total fleet size by 44.6%. Moreover, in view of the
COVID-19 pandemic, this study proposes a social distancing policy that
maintains a suitable usage interval. These findings provide useful insights for
improving the resource efficiency and operational service of bike sharing and
shared mobility.
"
2204.09790,2022-04-22,Wrapped Distributions on homogeneous Riemannian manifolds,"  We provide a general framework for constructing probability distributions on
Riemannian manifolds, taking advantage of area-preserving maps and isometries.
Control over distributions' properties, such as parameters, symmetry and
modality yield a family of flexible distributions that are straightforward to
sample from, suitable for use within Monte Carlo algorithms and latent variable
models, such as autoencoders. As an illustration, we empirically validate our
approach by utilizing our proposed distributions within a variational
autoencoder and a latent space network model. Finally, we take advantage of the
generalized description of this framework to posit questions for future work.
"
2204.10159,2022-04-22,"Physical, subjective and analogical probability","  The aim of this paper is to show that the concept of probability is best
understood by dividing this concept into two different types of probability,
namely physical probability and analogical probability. Loosely speaking, a
physical probability is a probability that applies to the outcomes of an
experiment that have been judged as being equally likely on the basis of
physical symmetry. Physical probabilities are arguably in some sense
'objective' and possess all the standard properties of the concept of
probability. On the other hand, an analogical probability is defined by making
an analogy between the uncertainty surrounding an event of interest and the
uncertainty surrounding an event that has a physical probability. Analogical
probabilities are undeniably subjective probabilities and are not obliged to
have all the standard mathematical properties possessed by physical
probabilities, e.g. they may not have the property of additivity or obey the
standard definition of conditional probability. Nevertheless, analogical
probabilities have extra properties, which are not possessed by physical
probabilities, that assist in their direct elicitation, general derivation,
comparison and justification. More specifically, these properties facilitate
the application of analogical probability to real-world problems that can not
be adequately resolved by using only physical probability, e.g. probabilistic
inference about hypotheses on the basis of observed data. Careful definitions
are given of the concepts that are introduced and, where appropriate, examples
of the application of these concepts are presented for additional clarity.
"
2204.10525,2022-04-25,"Research on spatial information transmission efficiency and capability
  of safe evacuation signs","  As an indispensable spatial direction information indicator for emergency
evacuation, the spatial relationship between safety evacuation signs and
evacuees will affect the response time of evacuees and the evacuation
efficiency. This paper takes 2 kinds of common safety evacuation signs,
hangtag-type and embedded, as the research object and designs space direction
information transmission efficiency and capability simulation experiment and
fire drill, the efficiency and capability of spatial direction information
transmission of safety evacuation signs are studied. The results show that the
space angle of the hangtag-type safety evacuation sign is inversely
proportional to the information transmission efficiency and capability of the
space direction, and the fire drill also confirms this conclusion. When the
spatial angle of the embedded safety evacuation sign is 5{\deg}, the spatial
direction information transmission efficiency and capability increase.
Simultaneously, the average escape time of the participants in the fire drill
was lower, and the percentage of choosing unfamiliarity exports increased. The
evolution of spatial angle has no significant effect on the intention of the
response of subjects of different genders; when choosing the direction, males
are more easily affected by the change of spatial angle than females; the
confidence level of females' choice is more easily affected by spatial angle.
In addition, according to the research results, the corresponding
three-dimensional structure safety evacuation signs are designed. The
functional structure of the safety evacuation signs is perfected, which can
effectively improve the efficiency of fire emergency evacuation.
"
2204.11623,2022-12-01,"A Simulation-Optimization Framework To Improve The Organ Transplantation
  Offering System","  We propose a simulation-optimization-based methodology to improve the way
that organ transplant offers are made to potential recipients. Our policy can
be applied to all types of organs, is implemented starting at the local level,
is flexible with respect to simultaneous offers of an organ to multiple
patients, and takes into account the quality of the organs under consideration.
We describe in detail our simulation-optimization procedure and how it uses
underlying real-world transplant data to inform the decision-making process. We
present results using our liver and kidney models, where we show that, under
our policy recommendations, more organs are utilized and the required times to
allocate the organs are reduced -- sometimes dramatically.
"
2204.11777,2022-04-26,"Bayesian estimation of in-game home team win probability for college
  basketball","  Two new Bayesian methods for estimating and predicting in-game home team win
probabilities are proposed. The first method has a prior that adjusts as a
function of lead differential and time elapsed. The second is an adjusted
version of the first, where the adjustment is a linear combination of the
Bayesian estimator with a time-weighted pre-game win probability. The proposed
methods are compared to existing methods, showing the new methods perform
better for both estimation and prediction. The utility is illustrated via an
application to the 2016 NCAA Division 1 Championship game.
"
2204.12933,2022-04-28,High-Frequency-Based Volatility Model with Network Structure,"  This paper introduces one new multivariate volatility model that can
accommodate an appropriately defined network structure based on low-frequency
and high-frequency data. The model reduces the number of unknown parameters and
the computational complexity substantially. The model parameterization and
iterative multistep-ahead forecasts are discussed and the targeting
reparameterization is also presented. Quasi-likelihood functions for parameter
estimation are proposed and their asymptotic properties are established. A
series of simulation experiments are carried out to assess the performance of
the estimation in finite samples. An empirical example is demonstrated that the
proposed model outperforms the network GARCH model, with the gains being
particularly significant at short forecast horizons.
"
2204.13815,2023-05-05,Controlling for Latent Confounding with Triple Proxies,"  We present new results for nonparametric identification of causal effects
using noisy proxies for unobserved confounders. Our approach builds on the
results of \citet{Hu2008} who tackle the problem of general measurement error.
We call this the `triple proxy' approach because it requires three proxies that
are jointly independent conditional on unobservables. We consider three
different choices for the third proxy: it may be an outcome, a vector of
treatments, or a collection of auxiliary variables. We compare to an
alternative identification strategy introduced by \citet{Miao2018a} in which
causal effects are identified using two conditionally independent proxies. We
refer to this as the `double proxy' approach. The triple proxy approach
identifies objects that are not identified by the double proxy approach,
including some that capture the variation in average treatment effects between
strata of the unobservables. Moreover, the conditional independence assumptions
in the double and triple proxy approaches are non-nested.
"
2205.01279,2022-05-04,Complementary Goodness of Fit Procedure for Crash Frequency Models,"  This paper presents a new procedure for evaluating the goodness of fit of
Generalized Linear Models (GLM) estimated with Roadway Departure (RwD) crash
frequency data for the State of Hawaii on two-lane two-way (TLTW) state roads.
The procedure is analyzed using ten years of RwD crash data (including all
severity levels) and roadway characteristics (e.g., traffic, geometry, and
inventory databases) that can be aggregated at the section level. The three
estimation methods evaluated using the proposed procedure include: Negative
Binomial (NB), Zero-Inflated Negative Binomial (ZINB), and Generalized Linear
Mixed Model-Negative Binomial (GLMM-NB). The procedure shows that the three
methodologies can provide very good fits in terms of the distributions of
crashes within narrow ranges of the predicted mean frequency of crashes and in
terms of observed vs. predicted average crash frequencies for those data
segments. The proposed procedure complements other statistics such as Akaike
Information Criterion, Bayesian Information Criterion, and Log-likelihood used
for model selection. It is consistent with those statistics for models without
random effects, but it diverges for GLMM-NB models. The procedure can aid model
selection by providing a clear visualization of the fit of crash frequency
models and allowing the computation of a pseudo R2 similar the one used in
linear regression. It is recommended to evaluate its use for evaluating the
trade-off between the number of random effects in GLMM-NB models and their
goodness of fit using more appropriate datasets that do not lead to convergence
problems.
"
2205.02829,2023-01-30,"Foundations for NLP-assisted formative assessment feedback for
  short-answer tasks in large-enrollment classes","  Research suggests ""write-to-learn"" tasks improve learning outcomes, yet
constructed-response methods of formative assessment become unwieldy with large
class sizes. This study evaluates natural language processing algorithms to
assist this aim. Six short-answer tasks completed by 1,935 students were scored
by several human raters, using a detailed rubric, and an algorithm. Results
indicate substantial inter-rater agreement using quadratic weighted kappa for
rater pairs (each QWK > 0.74) and group consensus (Fleiss Kappa = 0.68).
Additionally, intra-rater agreement was estimated for one rater who had scored
178 responses seven years prior (QWK = 0.89). With compelling rater agreement,
the study then pilots cluster analysis of response text toward enabling
instructors to ascribe meaning to clusters as a means for scalable formative
assessment.
"
2205.03343,2023-04-03,Far from Asymptopia,"  Inference from limited data requires a notion of measure on parameter space,
most explicit in the Bayesian framework as a prior. Here we demonstrate that
Jeffreys prior, the best-known uninformative choice, introduces enormous bias
when applied to typical scientific models. Such models have a relevant
effective dimensionality much smaller than the number of microscopic
parameters. Because Jeffreys prior treats all microscopic parameters equally,
it is from uniform when projected onto the sub-space of relevant parameters,
due to variations in the local co-volume of irrelevant directions. We present
results on a principled choice of measure which avoids this issue, leading to
unbiased inference in complex models. This optimal prior depends on the
quantity of data to be gathered, and approaches Jeffreys prior in the
asymptotic limit. However, this limit cannot be justified without an impossibly
large amount of data, exponential in the number of microscopic parameters.
"
2205.04571,2024-10-15,Adjust Pearson's $r$ to Measure Arbitrary Monotone Dependence,"  Pearson's r, the most widely-used correlation coefficient, is traditionally
regarded as exclusively capturing linear dependence, leading to its
discouragement in contexts involving nonlinear relationships. However, recent
research challenges this notion, suggesting that Pearson's r should not be
ruled out a priori for measuring nonlinear monotone relationships. Pearson's r
is essentially a scaled covariance, rooted in the renowned Cauchy-Schwarz
Inequality. Our findings reveal that different scaling bounds yield
coefficients with different capture ranges, and interestingly, tighter bounds
actually expand these ranges. We derive a tighter inequality than
Cauchy-Schwarz Inequality, leverage it to refine Pearson's r, and propose a new
correlation coefficient, i.e., rearrangement correlation. This coefficient is
able to capture arbitrary monotone relationships, both linear and nonlinear
ones. It reverts to Pearson's r in linear scenarios. Simulation experiments and
real-life investigations show that the rearrangement correlation is more
accurate in measuring nonlinear monotone dependence than the three classical
correlation coefficients, and other recently proposed dependence measures.
"
2205.05608,2022-05-12,"Examining the role of context in statistical literacy outcomes using an
  isomorphic assessment instrument","  The central role of statistical literacy has been discussed extensively,
emphasizing its importance as a learning outcome and in promoting a citizenry
capable of interacting with the world in an informed and critical manner. Our
work contributes to the growing literature on assessing and improving people's
statistical literacy vis-a-vis contexts important in their professional and
personal lives. We consider the measurement of contextualized statistics
literacy - statistical literacy as applied to relevant contexts. We discuss the
development of an isomorphic instrument modifying an existing assessment,
design of a pilot study, and results which conclude that 1) the isomorphic
assessment has comparable psychometric properties, and 2) test takers have
lower statistical literacy scores on an assessment that incorporates relevant
contexts.
"
2205.06417,2022-05-16,"A Journey from Wild to Textbook Data to Reproducibly Refresh the Wages
  Data from the National Longitudinal Survey of Youth Database","  Textbook data is essential for teaching statistics and data science methods
because they are clean, allowing the instructor to focus on methodology.
Ideally textbook data sets are refreshed regularly, especially when they are
subsets taken from an on-going data collection. It is also important to use
contemporary data for teaching, to imbue the sense that the methodology is
relevant today. This paper describes the trials and tribulations of refreshing
a textbook data set on wages, extracted from the National Longitudinal Survey
of Youth (NLSY79) in the early 1990s. The data is useful for teaching modeling
and exploratory analysis of longitudinal data. Subsets of NLSY79, including the
wages data, can be found in supplementary files from numerous textbooks and
research articles. The NLSY79 database has been continuously updated through to
2018, so new records are available. Here we describe our journey to refresh the
wages data, and document the process so that the data can be regularly updated
into the future. Our journey was difficult because the steps and decisions
taken to get from the raw data to the wages textbook subset have not been
clearly articulated. We have been diligent to provide a reproducible workflow
for others to follow, which also hopefully inspires more attempts at refreshing
data for teaching. Three new data sets and the code to produce them are
provided in the open source R package called `yowie`.
"
2205.06694,2023-03-06,On the use of a local $\hat{R}$ to improve MCMC convergence diagnostic,"  Diagnosing convergence of Markov chain Monte Carlo is crucial and remains an
essentially unsolved problem. Among the most popular methods, the potential
scale reduction factor, commonly named $\hat{R}$, is an indicator that monitors
the convergence of output chains to a target distribution, based on a
comparison of the between- and within-variances. Several improvements have been
suggested since its introduction in the 90s. Here, we aim at better
understanding the $\hat{R}$ behavior by proposing a localized version that
focuses on quantiles of the target distribution. This new version relies on key
theoretical properties of the associated population value. It naturally leads
to proposing a new indicator $\hat{R}_\infty$, which is shown to allow both for
localizing the Markov chain Monte Carlo convergence in different quantiles of
the target distribution, and at the same time for handling some convergence
issues not detected by other $\hat{R}$ versions.
"
2205.08144,2022-05-18,BayesMix: Bayesian Mixture Models in C++,"  We describe BayesMix, a C++ library for MCMC posterior simulation for general
Bayesian mixture models. The goal of BayesMix is to provide a self-contained
ecosystem to perform inference for mixture models to computer scientists,
statisticians and practitioners. The key idea of this library is extensibility,
as we wish the users to easily adapt our software to their specific Bayesian
mixture models. In addition to the several models and MCMC algorithms for
posterior inference included in the library, new users with little familiarity
on mixture models and the related MCMC algorithms can extend our library with
minimal coding effort. Our library is computationally very efficient when
compared to competitor software. Examples show that the typical code runtimes
are from two to 25 times faster than competitors for data dimension from one to
ten. Our library is publicly available on Github at
https://github.com/bayesmix-dev/bayesmix/.
"
2205.11026,2023-07-10,"Three principles for modernizing an undergraduate regression analysis
  course","  As data have become more prevalent in academia, industry, and daily life, it
is imperative that undergraduate students are equipped with the skills needed
to analyze data in the modern environment. In recent years there has been a lot
of work innovating introductory statistics courses and developing introductory
data science courses; however, there has been less work beyond the first
course. This paper describes innovations to Regression Analysis taught at Duke
University, a course focused on application that serves a diverse undergraduate
student population of statistics and data science majors along with non-majors.
Three principles guiding the modernization of the course are presented with
details about how these principles align with the necessary skills of practice
outlined in recent statistics and data science curriculum guidelines. The paper
includes pedagogical strategies, motivated by the innovations in introductory
courses, that make it feasible to implement skills for the practice of modern
statistics and data science alongside fundamental statistical concepts. The
paper concludes with the impact of these changes, challenges, and next steps
for the course. Portions of in-class activities and assignments are included in
the paper, with full sample assignments and resources for finding data in the
supplemental materials.
"
2205.12478,2022-05-26,"Visualising Multilevel Regression and Poststratification: Alternatives
  to the Current Practice","  Surveys provide important evidence for policymaking, decision-making, and
understanding of society. However, conducting the large surveys required to
provide subpopulation level estimates is expensive and time-consuming.
Multilevel Regression and Poststratification (MRP) is a promising method to
provide reliable estimates for subpopulations from surveys without the amount
of data needed for reliable direct estimates. Graphical displays have been
widely used to communicate and diagnose MRP estimates. However, there have been
few studies on how visualisation should be performed in this field.
Accordingly, this study examines the current practice of MRP visualisation
using a systematic literature review. This study also applies MRP to estimate
the Trump vote share in the U.S. 2016 presidential election using the
Cooperative Congressional Election Study (CCES) data to illustrate the
implication of current visualisation practices and explore alternatives for
improvement. We find that uncertainty is not often displayed in the current
practice, despite its importance for survey inference. The choropleth map is
the most frequently used to display MRP estimates even though it only shows
point estimates and could hinder the information conveyed. Using various
graphical representations, we show that visualisation with uncertainty can
illustrate the effect of different model specifications on the estimation
result. In addition, this study also proposes a visualisation strategy to also
take the bias-variance trade-off into account when evaluating MRP models.
"
2205.14016,2022-05-30,The paradoxical nature of easily improvable evidence,"  Established frameworks to understand problems with reproducibility in science
begin with the relationship between our understanding of the prior probability
of a claim and the statistical certainty that should be demanded of it, and
explore the ways in which independent investigations, biases in study design
and publication bias interact with these considerations.
  We propose a complementary perspective; namely, that to improve
reproducibility in science, our interpretation of the persuasiveness of
evidence (e.g., statistical significance thresholds) should be responsive to
our understanding of the effort that would be required to improve that
evidence. We will quantify this notion in some formal settings. Indeed, we will
demonstrate that even simplistic models of evidence publication can exhibit an
improvable evidence paradox, where the publication of easily improvable
evidence in favor of a claim can best seen as evidence the claim is false.
"
2205.14360,2022-05-31,"A discrete analogue of Terrell's characterization of rectangular
  distributions","  George R. Terrell (1983, {Ann. Probab., vol. 11(3), pp. 823--826) showed that
the Pearson coefficient of correlation of an ordered pair from a random sample
of size two is at most one-half, and the equality is attained only for
rectangular (uniform over some interval) distributions.
  In the present note it is proved that the same is true for the discrete case,
in the sense that the correlation coefficient attains its maximal value only
for discrete rectangular (uniform over some finite lattice) distributions.
  MSC: Primary 60E15; 62E10; Secondary 62G30.
  Key words and phrases: discrete rectangular distribution; order statistics;
Hahn polynomials; Pearson coefficient of correlation.
"
2206.00590,2023-07-10,"A validation of the short-form classroom community scale for
  undergraduate mathematics and statistics students","  This study examines Cho and Demmans Epp's short-form adaptation of Rovai's
well-known Classroom Community Scale (CCS-SF) as a measure of classroom
community among introductory undergraduate math and statistics students. A
series of statistical analyses were conducted to investigate the validity of
the CCS-SF for this new population. Data were collected from 351 students
enrolled in 21 online classes, offered for credit in Fall 2020 and Spring 2021
at a private university in the United States. Further confirmatory analysis was
conducted with data from 128 undergraduates enrolled in 13 in-person and hybrid
classes, offered for credit in Fall 2021 at the same institution. Following
Rovai's original 20-item CCS, the 8-item CCS-SF yields two interpretable
factors, connectedness and learning. This study confirms the two-factor
structure of the CCS-SF, and concludes that it is a valid measure of classroom
community among undergraduate students enrolled in remote, hybrid, and
in-person introductory mathematics and statistics courses.
"
2206.01703,2022-06-06,Interactive Exploration of Large Dendrograms with Prototypes,"  Hierarchical clustering is one of the standard methods taught for identifying
and exploring the underlying structures that may be present within a data set.
Students are shown examples in which the dendrogram, a visual representation of
the hierarchical clustering, reveals a clear clustering structure. However, in
practice, data analysts today frequently encounter data sets whose large scale
undermines the usefulness of the dendrogram as a visualization tool. Densely
packed branches obscure structure, and overlapping labels are impossible to
read. In this paper we present a new workflow for performing hierarchical
clustering via the R package called protoshiny that aims to restore
hierarchical clustering to its former role of being an effective and versatile
visualization tool. Our proposal leverages interactivity combined with the
ability to label internal nodes in a dendrogram with a representative data
point (called a prototype). After presenting the workflow, we provide three
case studies to demonstrate its utility.
"
2206.06219,2022-09-28,"Making Sense of Dependence: Efficient Black-box Explanations Using
  Dependence Measure","  This paper presents a new efficient black-box attribution method based on
Hilbert-Schmidt Independence Criterion (HSIC), a dependence measure based on
Reproducing Kernel Hilbert Spaces (RKHS). HSIC measures the dependence between
regions of an input image and the output of a model based on kernel embeddings
of distributions. It thus provides explanations enriched by RKHS representation
capabilities. HSIC can be estimated very efficiently, significantly reducing
the computational cost compared to other black-box attribution methods. Our
experiments show that HSIC is up to 8 times faster than the previous best
black-box attribution methods while being as faithful. Indeed, we improve or
match the state-of-the-art of both black-box and white-box attribution methods
for several fidelity metrics on Imagenet with various recent model
architectures. Importantly, we show that these advances can be transposed to
efficiently and faithfully explain object detection models such as YOLOv4.
Finally, we extend the traditional attribution methods by proposing a new
kernel enabling an ANOVA-like orthogonal decomposition of importance scores
based on HSIC, allowing us to evaluate not only the importance of each image
patch but also the importance of their pairwise interactions. Our
implementation is available at
https://github.com/paulnovello/HSIC-Attribution-Method.
"
2206.06498,2022-06-15,"Fast Computation of Highly G-optimal Exact Designs via Particle Swarm
  Optimization","  Computing proposed exact $G$-optimal designs for response surface models is a
difficult computation that has received incremental improvements via algorithm
development in the last two-decades. These optimal designs have not been
considered widely in applications in part due to the difficulty and cost
involved with computing them. Three primary algorithms for constructing exact
$G$-optimal designs are presented in the literature: the coordinate exchange
(CEXCH), a genetic algorithm (GA), and the relatively new $G$-optimal via
$I_\lambda$-optimality algorithm ($G(I_\lambda)$-CEXCH) which was developed in
part to address large computational cost. Particle swarm optimization (PSO) has
achieved widespread use in many applications, but to date, its broad-scale
success notwithstanding, has seen relatively few applications in optimal design
problems. In this paper we develop an extension of PSO to adapt it to the
optimal design problem. We then employ PSO to generate optimal designs for
several scenarios covering $K = 1, 2, 3, 4, 5$ design factors, which are common
experimental sizes in industrial experiments. We compare these results to all
$G$-optimal designs published in last two decades of literature. Published
$G$-optimal designs generated by GA for $K=1, 2, 3$ factors have stood
unchallenged for 14 years. We demonstrate that PSO has found improved
$G$-optimal designs for these scenarios, and it does this with comparable
computational cost to the state-of-the-art algorithm $G(I_\lambda)$-CEXCH.
Further, we show that PSO is able to produce equal or better $G$-optimal
designs for $K= 4, 5$ factors than those currently known. These results suggest
that PSO is superior to existing approaches for efficiently generating highly
$G$-optimal designs.
"
2206.07532,2022-12-15,Current state and prospects of R-packages for the design of experiments,"  Re-running an experiment is generally costly and, in some cases, impossible
due to limited resources; therefore, the design of an experiment plays a
critical role in increasing the quality of experimental data. In this paper, we
describe the current state of R-packages for the design of experiments through
an exploratory data analysis of package downloads, package metadata, and a
comparison of characteristics with other topics. We observed that experimental
designs in practice appear to be sufficiently manufactured by a small number of
packages, and the development of experimental designs often occurs in silos. We
also discuss the interface designs of widely utilized R packages in the field
of experimental design and discuss their future prospects for advancing the
field in practice.
"
2206.08376,2022-06-20,"Process, Population, and Sample: the Researcher's Interest","  A case is made that researchers are interested in studying processes. Often
the inferences they are interested in making are about the process and its
associated population. On other occasions, a researcher may be interested in
making an inference about the collection of individuals the process has
generated. We will call the statistical methods employed by the researcher to
make such inferences about the process/population ``estimation methods.'' The
statistical methods used in making an inference about the collection of
individuals generated we call ``prediction methods.'' Methods for obtaining
interval estimates of a parameter and prediction intervals for a statistic are
given. The analytical and enumerative methods discussed in Deming (1953) are
simply estimation and prediction methods, respectively.
"
2206.08649,2022-06-20,"On the probability of invalidating a causal inference due to limited
  external validity","  External validity is often questionable in empirical research, especially in
randomized experiments due to the trade-off between internal validity and
external validity. To quantify the robustness of external validity, one must
first conceptualize the gap between a sample that is fully representative of
the target population (i.e., the ideal sample) and the observed sample. Drawing
on Frank & Min (2007) and Frank et al. (2013), I define such gap as the
unobserved sample and intend to quantify its relationship with the null
hypothesis statistical testing (NHST) in this study. The probability of
invalidating a causal inference due to limited external validity, i.e., the
PEV, is the probability of failing to reject the null hypothesis based on the
ideal sample provided the null hypothesis has been rejected based on the
observed sample. This study illustrates the guideline and the procedure of
evaluating external validity with the PEV through an empirical example (i.e.,
Borman et al. (2008)). Specifically, one would be able to locate the threshold
of the unobserved sample statistic that would make the PEV higher than a
desired value and use this information to characterize the unobserved sample
that would render external validity of the research in question less robust.
The PEV is shown to be linked to statistical power when the NHST is thought to
be based on the ideal sample.
"
2206.09287,2022-06-22,"Approximate Bayesian Inference for the Interaction Types 1, 2, 3 and 4
  with Application in Disease Mapping","  We address in this paper a new approach for fitting spatiotemporal models
with application in disease mapping using the interaction types 1,2,3, and 4.
When we account for the spatiotemporal interactions in disease-mapping models,
inference becomes more useful in revealing unknown patterns in the data.
However, when the number of locations and/or the number of time points is
large, the inference gets computationally challenging due to the high number of
required constraints necessary for inference, and this holds for various
inference architectures including Markov chain Monte Carlo (MCMC) and
Integrated Nested Laplace Approximations (INLA). We re-formulate INLA approach
based on dense matrices to fit the intrinsic spatiotemporal models with the
four interaction types and account for the sum-to-zero constraints, and discuss
how the new approach can be implemented in a high-performance computing
framework. The computing time using the new approach does not depend on the
number of constraints and can reach a 40-fold faster speed compared to INLA in
realistic scenarios. This approach is verified by a simulation study and a real
data application, and it is implemented in the R package INLAPLUS and the
Python header function: inla1234().
"
2206.10689,2022-09-07,"Analysis of Hydrogen Production Costs across the United States and over
  the next 30 years","  Hydrogen can play an important role for decarbonization. While hydrogen is
usually produced through SMR, it can also be produced through water
electrolysis which is cleaner. The relative cost and carbon intensity of
hydrogen production through SMR and electrolysis vary throughout the United
States because of differences in the grid. While many hydrogen cost models
exist, no regional hydrogen study has been conducted across the US. We studied
how the Levelized Cost of Hydrogen (LCOH) and carbon intensity for producing
hydrogen vary across the US. We looked at electrolysis technologies (Alkaline,
PEM, and SOEC) and compared them to SMR. In 2020, SMR with 90 percent CCUS has
a lower average LCOH and carbon intensity for hydrogen production than
electrolysis by SOEC. For states with cleaner grids, hydrogen produced through
SOEC has a lower carbon intensity than hydrogen produced using SMR with 90
percent CCUS. Washington has one of the lowest carbon footprints and the lowest
LCOH to produce hydrogen through electrolysis (alkaline). We predict that the
LCOH for hydrogen production will be 3.2 USD per kg for Alkaline, 3.1 USD per
kg for PEM, and 2.6 USD per kg for SOEC by 2050 with constant electricity
prices. These projected LCOHs are still higher than the LCOH for hydrogen
produced through SMR with 90 percent CCUS. If electricity costs decrease to 2c
per kWh, we expect to reach cost-parity with SMR with 90 percent CCUS. The
results suggest that significant investment in decarbonizing the grid and
lowering the cost of electricity needs to be made to make electrolysis more
competitive compared to SMR.
"
2206.13053,2023-03-09,Run and Frequency quotas for q-binary trials,"  We study the distributions of waiting times in variations of the $q$-sooner
and later waiting time problem. One variation imposes length and frequency
quotas on the runs of successes and failures. Another case considers binary
trials for which the probability of ones is geometrically varying. We also
study the distributions of Longest run under the same variations. The main
theorems are sooner and later waiting time problem and the joint distribution
of the length of the longest success and longest failure runs when a run and
frequency quotas imposed on runs of successes and failure. In the present work,
we consider a sequence of independent binary $(0-1)$ trials with not
necessarily identical distributed with probability of ones varying according to
a geometric rule. Exact formulae for the distributions obtained by means of
enumerative combinatorics.
"
2206.14166,2023-01-02,Modified entropies as the origin of generalized uncertainty principles,"  The Heisenberg uncertainty principle is known to be connected to the entropic
uncertainty principle. This correspondence is obtained employing a Gaussian
probability distribution for wave functions associated to the Shannon entropy.
Independently, due to quantum gravity effects the Heisenberg uncertainty
principle has been extended to a Generalized Uncertainty Principle (GUP). In
this work, we show that GUP has been derived from considering non-extensive
entropies, proposed by one of us. We found that the deformation parameters
associated with $S_{+}$ and $S_-$ entropies are negative and positive
respectively. This allows us to explore various possibilities in the search of
physical implications. We conclude that non-extensive statistics constitutes a
signature of quantum gravity.
"
2207.01400,2023-11-23,"Cost-Efficient Fixed-Width Confidence Intervals for the Difference of
  Two Bernoulli Proportions","  We study properties of confidence intervals (CIs) for the difference of two
Bernoulli distributions' success parameters, $p_x - p_y$, in the case where the
goal is to obtain a CI of a given half-width while minimizing sampling costs
when the observation costs may be different between the two distributions.
Assuming that we are provided with preliminary estimates of the success
parameters, we propose three different methods for constructing fixed-width
CIs: (i) a two-stage sampling procedure, (ii) a sequential method that carries
out sampling in batches, and (iii) an $\ell$-stage ""look-ahead"" procedure. We
use Monte Carlo simulation to show that, under diverse success probability and
observation cost scenarios, our proposed algorithms obtain significant cost
savings versus their baseline counterparts (up to 50\% for the two-stage
procedure, up to 15\% for the sequential methods). Furthermore, for the battery
of scenarios under study, our sequential-batches and $\ell$-stage ""look-ahead""
procedures approximately obtain the nominal coverage while also meeting the
desired width requirement. Our sequential-batching method turned out to be more
efficient than the ""look-ahead"" method from a computational standpoint, with
average running times at least an order-of-magnitude faster over all the
scenarios tested.
"
2207.02003,2022-07-06,On General Weighted Extropy of Ranked Set Sampling,"  In the past six years, a considerable attention has been given to the extropy
measure proposed by Lad et al. (2015). Weighted Extropy of Ranked Set Sampling
was studied and compared with simple random sampling by Qiu et al. (2022). The
general weighted extropy and some results related to it are introduced in this
paper. We provide general weighted extropy of ranked set sampling. We also
studied characterization results, stochastic comparison and monotone properties
of general weighted extropy.
"
2207.08785,2022-07-19,The Inference Framework,"  The following three sections and appendices are taken from my thesis ""The
Foundations of Inference and its Application to Fundamental Physics"" from 2021,
in which I construct a theory of entropic inference from first principles. The
majority of these chapters are not original, but are a collection of various
sources through the history of the subject. The first section deals with
deductive reasoning, which is inference in the presence of complete
information. The second section expands on the deductive system by constructing
a theory of inductive inference, a theory of probabilities, which is inference
in the presence of incomplete information. Finally, section three develops a
means of updating these probabilities in the presence of new information that
comes in the form of constraints.
"
2207.08882,2023-09-15,Sharp hypotheses and organic fiducial inference,"  A fundamental class of inferential problems are those characterised by there
having been a substantial degree of pre-data (or prior) belief that the value
of a model parameter $\theta_j$ was equal or lay close to a specified value
$\theta^{*}_j$, which may, for example, be the value that indicates the absence
of a treatment effect or the lack of correlation between two variables. This
paper puts forward a generally applicable 'push-button' solution to problems of
this type that circumvents the severe difficulties that arise when attempting
to apply standard methods of inference, including the Bayesian method, to such
problems. Usually the only input of major note that is required from the user
in implementing this solution is the assignment of a pre-data or prior
probability to the hypothesis that the parameter $\theta_j$ lies in a narrow
interval $[\theta_{j0},\theta_{j1}]$ that is assumed to contain the value of
interest $\theta^{*}_j$. On the other hand, the end result that is achieved by
applying this method is, conveniently, a joint post-data distribution over all
the parameters $\theta_1,\theta_2,\ldots,\theta_k$ of the model concerned. The
proposed method is constructed by naturally combining a simple Bayesian
argument with an approach to inference called organic fiducial inference that
was developed in a number of earlier papers. To begin with, the main
theoretical arguments underlying this combined Bayesian and fiducial method are
presented and discussed in detail. Various applications and useful extensions
of this methodology are then outlined in the latter part of the paper. The
examples that are considered are made relevant to the analysis of clinical
trial data where appropriate.
"
2207.11561,2022-07-26,"Alternative approaches for analysing repeated measures data that are
  missing not at random","  We consider studies where multiple measures on an outcome variable are
collected over time, but some subjects drop out before the end of follow up.
Analyses of such data often proceed under either a 'last observation carried
forward' or 'missing at random' assumption. We consider two alternative
strategies for identification; the first is closely related to the
difference-in-differences methodology in the causal inference literature. The
second enables correction for violations of the parallel trend assumption, so
long as one has access to a valid 'bespoke instrumental variable'. These are
compared with existing approaches, first conceptually and then in an analysis
of data from the Framingham Heart Study.
"
2207.13522,2022-07-28,"Model-Free, Monotone Invariant and Computationally Efficient Feature
  Screening with Data-adaptive Threshold","  Feature screening for ultrahigh-dimension, in general, proceeds with two
essential steps. The first step is measuring and ranking the marginal
dependence between response and covariates, and the second is determining the
threshold. We develop a new screening procedure, called SIT-BY procedure, that
possesses appealing statistical properties in both steps. By employing sliced
independence estimates in the measuring and ranking stage, our proposed
procedure requires no model assumptions, remains invariant to monotone
transformation, and achieves almost linear computation complexity. Inspired by
false discovery rate (FDR) control procedures, we offer a data-adaptive
threshold benefit from the asymptotic normality of test statistics. Under
moderate conditions, we demonstrate that our procedure can asymptotically
control the FDR while maintaining the sure screening property. We investigate
the finite sample performance of our proposed procedure via extensive
simulations and an application to genome-wide dataset.
"
2208.01752,2022-08-04,"InsightiGen: a versatile tool to generate insight for an academic
  systematic literature review","  A comprehensive literature review has always been an essential first step of
every meaningful research. In recent years, however, the availability of a vast
amount of information in both open-access and subscription-based literature in
every field has made it difficult, if not impossible, to be certain about the
comprehensiveness of one's survey. This subsequently can lead to reviewers'
questioning of the novelties of the research directions proposed, regardless of
the quality of the actual work presented. In this situation, statistics derived
from the published literature data can provide valuable quantitative and visual
information about research trends, knowledge gaps, and research networks and
hubs in different fields. Our tool provides an automatic and rapid way of
generating insight for systematic reviews in any research area.
"
2208.02484,2023-09-06,Customs Import Declaration Datasets,"  Given the huge volume of cross-border flows, effective and efficient control
of trade becomes more crucial in protecting people and society from illicit
trade. However, limited accessibility of the transaction-level trade datasets
hinders the progress of open research, and lots of customs administrations have
not benefited from the recent progress in data-based risk management. In this
paper, we introduce an import declaration dataset to facilitate the
collaboration between domain experts in customs administrations and researchers
from diverse domains, such as data science and machine learning. The dataset
contains 54,000 artificially generated trades with 22 key attributes, and it is
synthesized with conditional tabular GAN while maintaining correlated features.
Synthetic data has several advantages. First, releasing the dataset is free
from restrictions that do not allow disclosing the original import data. The
fabrication step minimizes the possible identity risk which may exist in trade
statistics. Second, the published data follow a similar distribution to the
source data so that it can be used in various downstream tasks. Hence, our
dataset can be used as a benchmark for testing the performance of any
classification algorithm. With the provision of data and its generation
process, we open baseline codes for fraud detection tasks, as we empirically
show that more advanced algorithms can better detect fraud.
"
2208.02565,2024-07-23,"Teaching Visual Accessibility in Introductory Data Science Classes with
  Multi-Modal Data Representations","  Although there are various ways to represent data patterns and models,
visualization has been primarily taught in many data science courses for its
efficiency. Such vision-dependent output may cause critical barriers against
those who are blind and visually impaired and people with learning
disabilities. We argue that instructors need to teach multiple data
representation methods so that all students can produce data products that are
more accessible. In this paper, we argue that accessibility should be taught as
early as the introductory course as part of the data science curriculum so that
regardless of whether learners major in data science or not, they can have
foundational exposure to accessibility. As data science educators who teach
accessibility as part of our lower-division courses in two different
institutions, we share specific examples that can be utilized by other data
science instructors.
"
2208.06543,2023-09-29,"Size Matters: The Use and Misuse of Statistical Significance in Discrete
  Choice Models in the Transportation Academic Literature","  In this paper we review the academic transportation literature published
between 2014 and 2018 to evaluate where the field stands regarding the use and
misuse of statistical significance in empirical analysis, with a focus on
discrete choice models. Our results show that 39% of studies explained model
results exclusively based on the sign of the coefficient, 67% of studies did
not distinguish statistical significance from economic, policy or scientific
significance in their conclusions, and none of the reviewed studies considered
the statistical power of the tests. Based on these results we put forth a set
of recommendations aimed at shifting the focus away from statistical
significance towards proper and comprehensive assessment of effect magnitudes
and other policy relevant quantities.
"
2208.09174,2022-11-15,"Atomist or Holist? A Diagnosis and Vision for More Productive
  Interdisciplinary AI Ethics Dialogue","  In response to growing recognition of the social impact of new AI-based
technologies, major AI and ML conferences and journals now encourage or require
papers to include ethics impact statements and undergo ethics reviews. This
move has sparked heated debate concerning the role of ethics in AI research, at
times devolving into name-calling and threats of ""cancellation."" We diagnose
this conflict as one between atomist and holist ideologies. Among other things,
atomists believe facts are and should be kept separate from values, while
holists believe facts and values are and should be inextricable from one
another. With the goal of reducing disciplinary polarization, we draw on
numerous philosophical and historical sources to describe each ideology's core
beliefs and assumptions. Finally, we call on atomists and holists within the
ever-expanding data science community to exhibit greater empathy during ethical
disagreements and propose four targeted strategies to ensure AI research
benefits society.
"
2208.09638,2024-07-30,"Optimal Pre-Analysis Plans: Statistical Decisions Subject to
  Implementability","  What is the purpose of pre-analysis plans, and how should they be designed?
We model the interaction between an agent who analyzes data and a principal who
makes a decision based on agent reports. The agent could be the manufacturer of
a new drug, and the principal a regulator deciding whether the drug is
approved. Or the agent could be a researcher submitting a research paper, and
the principal an editor deciding whether it is published. The agent decides
which statistics to report to the principal. The principal cannot verify
whether the analyst reported selectively. Absent a pre-analysis message, if
there are conflicts of interest, then many desirable decision rules cannot be
implemented. Allowing the agent to send a message before seeing the data
increases the set of decision rules that can be implemented, and allows the
principal to leverage agent expertise. The optimal mechanisms that we
characterize require pre-analysis plans. Applying these results to hypothesis
testing, we show that optimal rejection rules pre-register a valid test, and
make worst-case assumptions about unreported statistics. Optimal tests can be
found as a solution to a linear-programming problem.
"
2208.11543,2023-11-07,"Evaluating the Planning and Operational Resilience of Electrical
  Distribution Systems with Distributed Energy Resources using Complex Network
  Theory","  Electrical Distribution Systems are extensively penetrated with Distributed
Energy Resources (DERs) to cater the energy demands with the general perception
that it enhances the system's resilience. However, integration of DERs may
adversely affect the grid operation and affect the system resilience due to
various factors like their intermittent availability, dynamics of weather
conditions, non-linearity, complexity, number of malicious threats, and
improved reliability requirements of consumers. This paper proposes a
methodology to evaluate the planning and operational resilience of power
distribution systems under extreme events and determines the withstand
capability of the electrical network. The proposed framework is developed by
effectively employing the complex network theory. Correlated networks for
undesirable configurations are developed from the time series data of active
power monitored at nodes of the electrical network. For these correlated
networks, computed the network parameters such as clustering coefficient,
assortative coefficient, average degree and power law exponent for the
anticipation; and percolation threshold for the determination of the network
withstand capability under extreme conditions. The proposed methodology is also
suitable for identifying the hosting capacity of solar panels in the system
while maintaining resilience under different unfavourable conditions and
identifying the most critical nodes of the system that could drive the system
into non-resilience. This framework is demonstrated on IEEE 123 node test
feeder by generating active power time-series data for a variety of electrical
conditions using simulation software, GridLAB-D. The percolation threshold
resulted as an effective metric for the determination of the planning and
operational resilience of the power distribution system.
"
2208.12443,2022-08-29,"Race and ethnicity data for first, middle, and last names","  We provide the largest compiled publicly available dictionaries of first,
middle, and last names for the purpose of imputing race and ethnicity using,
for example, Bayesian Improved Surname Geocoding (BISG). The dictionaries are
based on the voter files of six Southern states that collect self-reported
racial data upon voter registration. Our data cover a much larger scope of
names than any comparable dataset, containing roughly one million first names,
1.1 million middle names, and 1.4 million surnames. Individuals are categorized
into five mutually exclusive racial and ethnic groups -- White, Black,
Hispanic, Asian, and Other -- and racial/ethnic counts by name are provided for
every name in each dictionary. Counts can then be normalized row-wise or
column-wise to obtain conditional probabilities of race given name or name
given race. These conditional probabilities can then be deployed for imputation
in a data analytic task for which ground truth racial and ethnic data is not
available.
"
2208.13382,2022-08-30,"A Bayesian nonparametric approach for causal inference with multiple
  mediators","  Mediation analysis with contemporaneously observed multiple mediators is an
important area of causal inference. Recent approaches for multiple mediators
are often based on parametric models and thus may suffer from model
misspecification. Also, much of the existing literature either only allow
estimation of the joint mediation effect, or, estimate the joint mediation
effect as the sum of individual mediator effects, which often is not a
reasonable assumption. In this paper, we propose a methodology which overcomes
the two aforementioned drawbacks. Our method is based on a novel Bayesian
nonparametric (BNP) approach, wherein the joint distribution of the observed
data (outcome, mediators, treatment, and confounders) is modeled flexibly using
an enriched Dirichlet process mixture with three levels: the first level
characterizing the conditional distribution of the outcome given the mediators,
treatment and the confounders, the second level corresponding to the
conditional distribution of each of the mediators given the treatment and the
confounders, and the third level corresponding to the distribution of the
treatment and the confounders. We use standardization (g-computation) to
compute causal mediation effects under three uncheckable assumptions that allow
identification of the individual and joint mediation effects. The efficacy of
our proposed method is demonstrated with simulations. We apply our proposed
method to analyze data from a study of Ventilator-associated Pneumonia (VAP)
co-infected patients, where the effect of the abundance of Pseudomonas on VAP
infection is suspected to be mediated through antibiotics.
"
2208.14502,2023-01-11,"Flickering emergences: The question of locality in information-theoretic
  approaches to emergence","  ""Emergence"", the phenomenon where a complex system displays properties,
behaviours, or dynamics not trivially reducible to its constituent elements, is
one of the defining properties of complex systems. Recently, there has been a
concerted effort to formally define emergence using the mathematical framework
of information theory, which proposes that emergence can be understood in terms
of how the states of wholes and parts collectively disclose information about
the system's collective future. In this paper, we show how a common,
foundational component of information-theoretic approaches to emergence implies
an inherent instability to emergent properties, which we call flickering
emergence. A system may on average display a meaningful emergent property (be
it an informative coarse-graining, or higher-order synergy), but for particular
configurations, that emergent property falls apart and becomes misinformative.
We show existence proofs that flickering emergence occurs in two different
frameworks (one based on coarse-graining and another based on multivariate
information decomposition) and argue that any approach based on temporal mutual
information will display it. Finally, we argue that flickering emergence should
not be a disqualifying property of any model of emergence, but that it should
be accounted for when attempting to theorize about how emergence relates to
practical models of the natural world.
"
2209.00562,2022-09-02,"Model Transparency and Interpretability : Survey and Application to the
  Insurance Industry","  The use of models, even if efficient, must be accompanied by an understanding
at all levels of the process that transforms data (upstream and downstream).
Thus, needs increase to define the relationships between individual data and
the choice that an algorithm could make based on its analysis (e.g. the
recommendation of one product or one promotional offer, or an insurance rate
representative of the risk). Model users must ensure that models do not
discriminate and that it is also possible to explain their results. This paper
introduces the importance of model interpretation and tackles the notion of
model transparency. Within an insurance context, it specifically illustrates
how some tools can be used to enforce the control of actuarial models that can
nowadays leverage on machine learning. On a simple example of loss frequency
estimation in car insurance, we show the interest of some interpretability
methods to adapt explanation to the target audience.
"
2209.00636,2022-09-02,Testing for the Important Components of Posterior Predictive Variance,"  We give a decomposition of the posterior predictive variance using the law of
total variance and conditioning on a finite dimensional discrete random
variable. This random variable summarizes various features of modeling that are
used to form the prediction for a future outcome. Then, we test which terms in
this decomposition are small enough to ignore. This allows us identify which of
the discrete random variables are most important to prediction intervals. The
terms in the decomposition admit interpretations based on conditional means and
variances and are analogous to the terms in a Cochran's theorem decomposition
of squared error often used in analysis of variance. Thus, the modeling
features are treated as factors in completely randomized design. In cases where
there are multiple decompositions we suggest choosing the one that that gives
the best predictive coverage with the smallest variance.
"
2209.01289,2022-09-07,"elhmc: An R Package for Hamiltonian Monte Carlo Sampling in Bayesian
  Empirical Likelihood","  In this article, we describe a {\tt R} package for sampling from an empirical
likelihood-based posterior using a Hamiltonian Monte Carlo method. Empirical
likelihood-based methodologies have been used in Bayesian modeling of many
problems of interest in recent times. This semiparametric procedure can easily
combine the flexibility of a non-parametric distribution estimator together
with the interpretability of a parametric model. The model is specified by
estimating equations-based constraints. Drawing an inference from a Bayesian
empirical likelihood (BayesEL) posterior is challenging. The likelihood is
computed numerically, so no closed expression of the posterior exists.
Moreover, for any sample of finite size, the support of the likelihood is
non-convex, which hinders the fast mixing of many Markov Chain Monte Carlo
(MCMC) procedures. It has been recently shown that using the properties of the
gradient of log empirical likelihood, one can devise an efficient Hamiltonian
Monte Carlo (HMC) algorithm to sample from a BayesEL posterior.
  The package requires the user to specify only the estimating equations, the
prior, and their respective gradients. An MCMC sample drawn from the BayesEL
posterior of the parameters, with various details required by the user is
obtained.
"
2209.03009,2022-09-08,"Biblio-Analysis of Cohort Intelligence (CI) Algorithm and its allied
  applications from Scopus and Web of Science Perspective","  Cohort Intelligence or CI is one of its kind of novel optimization algorithm.
Since its inception, in a very short span it is applied successfully in various
domains and its results are observed to be effectual in contrast to algorithm
of its kind. Till date, there is no such type of bibliometric analysis carried
out on CI and its related applications. So, this research paper in a way will
be an ice breaker for those who want to take up CI to a new level. In this
research papers, CI publications available in Scopus are analyzed through
graphs, networked diagrams about authors, source titles, keywords over the
years, journals over the time. In a way this bibliometric paper showcase CI,
its applications and detail outs systematic review in terms its bibliometric
details.
"
2209.05648,2023-04-14,"Noise Dynamics of Quantum Annealers: Estimating the Effective Noise
  Using Idle Qubits","  Quantum annealing is a type of analog computation that aims to use quantum
mechanical fluctuations in search of optimal solutions of QUBO (quadratic
unconstrained binary optimization) or, equivalently, Ising problems. Since
NP-hard problems can in general be mapped to Ising and QUBO formulations, the
quantum annealing paradigm has the potential to help solve various NP-hard
problems. Current quantum annealers, such as those manufactured by D-Wave
Systems, Inc., have various practical limitations including the size (number of
qubits) of the problem that can be solved, the qubit connectivity, and error
due to the environment or system calibration, which can reduce the quality of
the solutions. Typically, for an arbitrary problem instance, the corresponding
QUBO (or Ising) structure will not natively embed onto the available qubit
architecture on the quantum chip. Thus, in these cases, a minor embedding of
the problem structure onto the device is necessary. However, minor embeddings
on these devices do not always make use of the full sparse chip hardware graph,
and a large portion of the available qubits stay unused during quantum
annealing. In this work, we embed a disjoint random QUBO on the unused parts of
the chip alongside the QUBO to be solved, which acts as an indicator of the
solution quality of the device over time. Using experiments on three different
D-Wave quantum annealers, we demonstrate that (i) long term trends in solution
quality exist on the D-Wave device, and (ii) the unused qubits can be used to
measure the current level of noise of the quantum system.
"
2209.07698,2023-01-25,Hitting a prime in 2.43 dice rolls (on average),"  What is the number of rolls of fair 6-sided dice until the first time the
total sum of all rolls is a prime? We compute the expectation and the variance
of this random variable up to an additive error of less than 10^{-4}. This is a
solution to a puzzle suggested by DasGupta (2017) in the Bulletin of the
Institute of Mathematical Statistics, where the published solution is
incomplete. The proof is simple, combining a basic dynamic programming
algorithm with a quick Matlab computation and basic facts about the
distribution of primes.
"
2209.09299,2022-12-13,"Finite- and Large- Sample Inference for Model and Coefficients in
  High-dimensional Linear Regression with Repro Samples","  In this paper, we present a new and effective simulation-based approach to
conduct both finite- and large-sample inference for high-dimensional linear
regression models. This approach is developed under the so-called repro samples
framework, in which we conduct statistical inference by creating and studying
the behavior of artificial samples that are obtained by mimicking the sampling
mechanism of the data. We obtain confidence sets for (a) the true model
corresponding to the nonzero coefficients, (b) a single or any collection of
regression coefficients, and (c) both the model and regression coefficients
jointly. We also extend our approaches to drawing inferences on functions of
the regression coefficients. The proposed approach fills in two major gaps in
the high-dimensional regression literature: (1) lack of effective approaches to
address model selection uncertainty and provide valid inference for the
underlying true model; (2) lack of effective inference approaches that
guarantee finite-sample performances. We provide both finite-sample and
asymptotic results to theoretically guarantee the performances of the proposed
methods. In addition, our numerical results demonstrate that the proposed
methods are valid and achieve better coverage with smaller confidence sets than
the existing state-of-art approaches, such as debiasing and bootstrap
approaches.
"
2209.10599,2023-03-29,"Population heterogeneity in the fractional master equation, ensemble
  self-reinforcement and strong memory effects","  We formulate a fractional master equation in continuous time with random
transition probabilities across the population of random walkers such that the
effective underlying random walk exhibits ensemble self-reinforcement. The
population heterogeneity generates a random walk with conditional transition
probabilities that increase with the number of steps taken previously
(self-reinforcement). Through this, we establish the connection between random
walks with a heterogeneous ensemble and those with strong memory where the
transition probability depends on the entire history of steps. We find the
ensemble averaged solution of the fractional master equation through
subordination involving the fractional Poisson process counting the number of
steps at a given time and the underlying discrete random walk with
self-reinforcement. We also find the exact solution for the variance which
exhibits superdiffusion even as the fractional exponent tends to 1.
"
2210.00972,2022-10-04,Predictive density estimators with integrated $L_1$ loss,"  This paper addresses the problem of an efficient predictive density
estimation for the density $q(\|y-\theta\|^2)$ of $Y$ based on $X \sim
p(\|x-\theta\|^2)$ for $y, x, \theta \in \mathbb{R}^d$. The chosen criteria are
integrated $L_1$ loss given by $L(\theta, \hat{q}) \, =\, \int_{\mathbb{R}^d}
\big|\hat{q}(y)- q(\|y-\theta\|^2) \big| \, dy$, and the associated frequentist
risk, for $\theta \in \Theta$. For absolutely continuous and strictly
decreasing $q$, we establish the inevitability of scale expansion improvements
$\hat{q}_c(y;X)\,=\, \frac{1}{c^d} q\big(\|y-X\|^2/c^2 \big) $ over the plug-in
density $\hat{q}_1$, for a subset of values $c \in (1,c_0)$. The finding is
universal with respect to $p,q$, and $d \geq 2$, and extended to loss functions
$\gamma \big(L(\theta, \hat{q} ) \big)$ with strictly increasing $\gamma$. The
finding is also extended to include scale expansion improvements of more
general plug-in densities $q(\|y-\hat{\theta}(X)\|^2 \big)$, when the parameter
space $\Theta$ is a compact subset of $\mathbb{R}^d$. Numerical analyses
illustrative of the dominance findings are presented and commented upon. As a
complement, we demonstrate that the unimodal assumption on $q$ is necessary
with a detailed analysis of cases where the distribution of $Y|\theta$ is
uniformly distributed on a ball centered about $\theta$. In such cases, we
provide a univariate ($d=1$) example where the best equivariant estimator is a
plug-in estimator, and we obtain cases (for $d=1,3$) where the plug-in density
$\hat{q}_1$ is optimal among all $\hat{q}_c$.
"
2210.03283,2022-10-21,Design Amortization for Bayesian Optimal Experimental Design,"  Bayesian optimal experimental design is a sub-field of statistics focused on
developing methods to make efficient use of experimental resources. Any
potential design is evaluated in terms of a utility function, such as the
(theoretically well-justified) expected information gain (EIG); unfortunately
however, under most circumstances the EIG is intractable to evaluate. In this
work we build off of successful variational approaches, which optimize a
parameterized variational model with respect to bounds on the EIG. Past work
focused on learning a new variational model from scratch for each new design
considered. Here we present a novel neural architecture that allows
experimenters to optimize a single variational model that can estimate the EIG
for potentially infinitely many designs. To further improve computational
efficiency, we also propose to train the variational model on a significantly
cheaper-to-evaluate lower bound, and show empirically that the resulting model
provides an excellent guide for more accurate, but expensive to evaluate bounds
on the EIG. We demonstrate the effectiveness of our technique on generalized
linear models, a class of statistical models that is widely used in the
analysis of controlled experiments. Experiments show that our method is able to
greatly improve accuracy over existing approximation strategies, and achieve
these results with far better sample efficiency.
"
2210.03617,2024-03-19,"Type $1$, $2$, $3$ and $4$ $q$-negative binomial distribution of order
  $k$","  We study the distributions of waiting times in variations of the negative
binomial distribution of order $k$. One variation apply different enumeration
scheme on the runs of successes. Another case considers binary trials for which
the probability of ones is geometrically varying. We investigate the exact
distribution of the waiting time for the $r$-th occurrence of success run of a
specified length (non-overlapping, overlapping, at least, exactly,
$\ell$-overlapping) in a $q$-sequence of binary trials. The main theorems are
Type $1$, $2$, $3$ and $4$ $q$-negative binomial distribution of order $k$ and
$q$-negative binomial distribution of order $k$ in the $\ell$-overlapping case.
In the present work, we consider a sequence of independent binary zero and one
trials with not necessarily identical distribution with the probability of ones
varying according to a geometric rule. Exact formulae for the distributions
obtained by means of enumerative combinatorics.
"
2210.03991,2023-08-29,Fostering better coding practices for data scientists,"  Many data science students and practitioners don't see the value in making
time to learn and adopt good coding practices as long as the code ""works"".
However, code standards are an important part of modern data science practice,
and they play an essential role in the development of data acumen. Good coding
practices lead to more reliable code and save more time than they cost, making
them important even for beginners. We believe that principled coding is vital
for quality data science practice. To effectively instill these practices
within academic programs, instructors and programs need to begin establishing
these practices early, to reinforce them often, and to hold themselves to a
higher standard while guiding students. We describe key aspects of good coding
practices for data science, illustrating with examples in R and in Python,
though similar standards are applicable to other software environments.
Practical coding guidelines are organized into a top ten list.
"
2210.04521,2022-10-21,"On Success runs of a fixed length defined on a $q$-sequence of binary
  trials","  We study the exact distributions of runs of a fixed length in variation which
considers binary trials for which the probability of ones is geometrically
varying. The random variable $E_{n,k}$ denote the number of success runs of a
fixed length $k$, $1\leq k \leq n$.
  Theorem 3.1 gives an closed expression for the probability mass function
(PMF) of the Type4 $q$-binomial distribution of order $k$. Theorem 3.2 and
Corollary 3.1 gives an recursive expression for the probability mass function
(PMF) of the Type4 $q$-binomial distribution of order $k$. The probability
generating function and moments of random variable $E_{n,k}$ are obtained as a
recursive expression. We address the parameter estimation in the distribution
of $E_{n,k}$ by numerical techniques. In the present work, we consider a
sequence of independent binary zero and one trials with not necessarily
identical distribution with the probability of ones varying according to a
geometric rule. Exact and recursive formulae for the distribution obtained by
means of enumerative combinatorics.
"
2210.08797,2022-10-18,"Recurrence algorithms of waiting time for the success run of length $k$
  in relation to generalized Fibonacci sequences","  Let $V(k)$ denote the waiting time, the number of trials needed to get a
consecutive $k$ ones. We propose recurrence algorithms for the probability
distribution function (pdf) and the probability generating function (pgf) of
$V(k)$ in sequences of independent and Markov dependent Bernoulli trials using
generalized Fibonacci sequences of order $k$. Maximum likelihood estimation
(MLE) methods for the probability distributions are presented in both cases
with simulation examples.
"
2210.10535,2023-03-28,"Stability of Entropic Wasserstein Barycenters and application to random
  geometric graphs","  As interest in graph data has grown in recent years, the computation of
various geometric tools has become essential. In some area such as mesh
processing, they often rely on the computation of geodesics and shortest paths
in discretized manifolds. A recent example of such a tool is the computation of
Wasserstein barycenters (WB), a very general notion of barycenters derived from
the theory of Optimal Transport, and their entropic-regularized variant. In
this paper, we examine how WBs on discretized meshes relate to the geometry of
the underlying manifold. We first provide a generic stability result with
respect to the input cost matrices. We then apply this result to random
geometric graphs on manifolds, whose shortest paths converge to geodesics,
hence proving the consistency of WBs computed on discretized shapes.
"
2210.11593,2022-10-24,"Linear mixed model vs two-stage methods: Developing prognostic models of
  diabetic kidney disease progression","  Identifying prognostic factors for disease progression is a cornerstone of
medical research. Repeated assessments of a marker outcome are often used to
evaluate disease progression, and the primary research question is to identify
factors associated with the longitudinal trajectory of this marker. Our work is
motivated by diabetic kidney disease (DKD), where serial measures of estimated
glomerular filtration rate (eGFR) are the longitudinal measure of kidney
function, and there is notable interest in identifying factors, such as
metabolites, that are prognostic for DKD progression. Linear mixed models (LMM)
with serial marker outcomes (e.g., eGFR) are a standard approach for prognostic
model development, namely by evaluating the time and prognostic factor (e.g.,
metabolite) interaction. However, two-stage methods that first estimate
individual-specific eGFR slopes, and then use these as outcomes in a regression
framework with metabolites as predictors are easy to interpret and implement
for applied researchers. Herein, we compared the LMM and two-stage methods, in
terms of bias and mean squared error via analytic methods and simulations,
allowing for irregularly spaced measures and missingness. Our findings provide
novel insights into when two-stage methods are suitable longitudinal prognostic
modeling alternatives to the LMM. Notably, our findings generalize to other
disease studies.
"
2210.12528,2023-02-06,Data science transfer pathways from associate's to bachelor's programs,"  A substantial fraction of students who complete their college education at a
public university in the United States begin their journey at one of the 935
public two-year colleges. While the number of four-year colleges offering
bachelor's degrees in data science continues to increase, data science
instruction at many two-year colleges lags behind. A major impediment is the
relative paucity of introductory data science courses that serve multiple
student audiences and can easily transfer. In addition, the lack of pre-defined
transfer pathways (or articulation agreements) for data science creates a
growing disconnect that leaves students who want to study data science at a
disadvantage. We describe opportunities and barriers to data science transfer
pathways. Five points of curricular friction merit attention: 1) a first course
in data science, 2) a second course in data science, 3) a course in scientific
computing, data science workflow, and/or reproducible computing, 4) lab
sciences, and 5) navigating communication, ethics, and application domain
requirements in the context of general education and liberal arts course
mappings. We catalog existing transfer pathways, efforts to align curricula
across institutions, obstacles to overcome with minimally-disruptive solutions,
and approaches to foster these pathways. Improvements in these areas are
critically important to ensure that a broad and diverse set of students are
able to engage and succeed in undergraduate data science programs.
"
2210.15044,2022-10-28,"Generative modeling of the enteric nervous system employing point
  pattern analysis and graph construction","  We describe a generative network model of the architecture of the enteric
nervous system (ENS) in the colon employing data from images of human and mouse
tissue samples obtained through confocal microscopy. Our models combine spatial
point pattern analysis with graph generation to characterize the spatial and
topological properties of the ganglia (clusters of neurons and glial cells),
the inter-ganglionic connections, and the neuronal organization within the
ganglia. We employ a hybrid hardcore-Strauss process for spatial patterns and a
planar random graph generation for constructing the spatially embedded network.
We show that our generative model may be helpful in both basic and
translational studies, and it is sufficiently expressive to model the ENS
architecture of individuals who vary in age and health status. Increased
understanding of the ENS connectome will enable the use of neuromodulation
strategies in treatment and clarify anatomic diagnostic criteria for people
with bowel motility disorders.
"
2210.16350,2022-11-01,"A Comparison of Reproducibility Guidelines and Its Implications on
  Undergraduate Statistical Education","  In this paper, we replicated a Bayesian educational research project, which
explores the association between broadband access and online course enrollment
in the US. We summarized key findings from our replication and compared them
with the original project. Based on my replication experience, we aim to
demonstrate the challenges of research reproduction, even when codes and data
are shared openly and the quality of the materials on GitHub are high.
Moreover, we investigate the implicit presumptions of the researchers' level of
knowledge and discuss how such presumptions may add difficulty to the
reproduction of scientific research. Finally, we hope this article sheds light
on the design of reproducibility criterion and opens up a space to explore what
should be taught in undergraduate statistics education.
"
2210.17405,2024-06-25,Exact and Approximate Conformal Inference for Multi-Output Regression,"  It is common in machine learning to estimate a response $y$ given covariate
information $x$. However, these predictions alone do not quantify any
uncertainty associated with said predictions. One way to overcome this
deficiency is with conformal inference methods, which construct a set
containing the unobserved response $y$ with a prescribed probability.
Unfortunately, even with a one-dimensional response, conformal inference is
computationally expensive despite recent encouraging advances. In this paper,
we explore multi-output regression, delivering exact derivations of conformal
inference $p$-values when the predictive model can be described as a linear
function of $y$. Additionally, we propose \texttt{unionCP} and a multivariate
extension of \texttt{rootCP} as efficient ways of approximating the conformal
prediction region for a wide array of multi-output predictors, both linear and
nonlinear, while preserving computational advantages. We also provide both
theoretical and empirical evidence of the effectiveness of these methods using
both real-world and simulated data.
"
2211.00774,2024-06-27,On distribution of runs and patterns in four state trials,"  From a mathematical and statistical point of view, a segment of a DNA strand
can be viewed as a sequence of four-state (A, C, G, T) trials. We consider
distributions of runs and patterns related to run lengths of multi-state
sequences, especially for four states (A, B, C, D). Let $X_{1}, X_{2}, \ldots$
be a sequence of four state i.i.d.\ trials taking values in the set
$\mathscr{S}=\{A,\ B,\ C,\ D\}$ of four symbols with probability $P(A)=P_{a}$,
$P(B)=P_{b}$, $P(C)=P_{c}$ and $P(D)=P_{d},$ respectively. In this paper, we
obtain exact formulae for the probability distribution function for runs of B's
the discrete distribution of order $k$, longest run statistics, shortest run
statistics, waiting time distribution and the distribution of run lengths.
"
2211.00938,2023-07-19,Genomics Data Analysis via Spectral Shape and Topology,"  Mapper, a topological algorithm, is frequently used as an exploratory tool to
build a graphical representation of data. This representation can help to gain
a better understanding of the intrinsic shape of high-dimensional genomic data
and to retain information that may be lost using standard dimension-reduction
algorithms. We propose a novel workflow to process and analyze RNA-seq data
from tumor and healthy subjects integrating Mapper and differential gene
expression. Precisely, we show that a Gaussian mixture approximation method can
be used to produce graphical structures that successfully separate tumor and
healthy subjects, and produce two subgroups of tumor subjects. A further
analysis using DESeq2, a popular tool for the detection of differentially
expressed genes, shows that these two subgroups of tumor cells bear two
distinct gene regulations, suggesting two discrete paths for forming lung
cancer, which could not be highlighted by other popular clustering methods,
including t-SNE. Although Mapper shows promise in analyzing high-dimensional
data, building tools to statistically analyze Mapper graphical structures is
limited in the existing literature. In this paper, we develop a scoring method
using heat kernel signatures that provides an empirical setting for statistical
inferences such as hypothesis testing, sensitivity analysis, and correlation
analysis.
"
2211.01688,2022-11-04,Nearly tight universal bounds for the binomial tail probabilities,"  We derive simple but nearly tight upper and lower bounds for the binomial
lower tail probability (with straightforward generalization to the upper tail
probability) that apply to the whole parameter regime. These bounds are easy to
compute and are tight within a constant factor of $89/44$. Moreover, they are
asymptotically tight in the regimes of large deviation and moderate deviation.
By virtue of a surprising connection with Ramanujan's equation, we also provide
strong evidences suggesting that the lower bound is tight within a factor of
$1.26434$. It may even be regarded as the natural lower bound, given its
simplicity and appealing properties. Our bounds significantly outperform the
familiar Chernoff bound and reverse Chernoff bounds known in the literature and
may find applications in various research areas.
"
2211.03060,2022-11-08,An exposition of possibility and probability,"  This paper considers the notion of possible events which are insignificant in
probabilistic analysis (i.e. events that have zero probability). The paper
discusses the method of modal logic based on ""possible worlds"" and discusses a
mathematical framework for the concepts of possibility, impossibility and
certainty that are sometimes (incorrectly) thought to be defined with respect
to probability. The relationship between possibility and probability is
explored for general probability spaces and for refinements of these spaces
conditional on other events, with particular focus on the properties of events
having zero probability. We derive conditions under which possibility and
significance diverge and conditions under which they can be reconciled as
equivalent ideas within certain contexts. We also apply this analysis to
discuss issues in possibility and probability in the multinomial model.
"
2211.03119,2022-11-08,The Second Competition on Spatial Statistics for Large Datasets,"  In the last few decades, the size of spatial and spatio-temporal datasets in
many research areas has rapidly increased with the development of data
collection technologies. As a result, classical statistical methods in spatial
statistics are facing computational challenges. For example, the kriging
predictor in geostatistics becomes prohibitive on traditional hardware
architectures for large datasets as it requires high computing power and memory
footprint when dealing with large dense matrix operations. Over the years,
various approximation methods have been proposed to address such computational
issues, however, the community lacks a holistic process to assess their
approximation efficiency. To provide a fair assessment, in 2021, we organized
the first competition on spatial statistics for large datasets, generated by
our {\em ExaGeoStat} software, and asked participants to report the results of
estimation and prediction. Thanks to its widely acknowledged success and at the
request of many participants, we organized the second competition in 2022
focusing on predictions for more complex spatial and spatio-temporal processes,
including univariate nonstationary spatial processes, univariate stationary
space-time processes, and bivariate stationary spatial processes. In this
paper, we describe in detail the data generation procedure and make the
valuable datasets publicly available for a wider adoption. Then, we review the
submitted methods from fourteen teams worldwide, analyze the competition
outcomes, and assess the performance of each team.
"
2211.07258,2023-10-06,"Inconsistency identification in network meta-analysis via stochastic
  search variable selection","  The reliability of the results of network meta-analysis (NMA) lies in the
plausibility of key assumption of transitivity. This assumption implies that
the effect modifiers' distribution is similar across treatment comparisons.
Transitivity is statistically manifested through the consistency assumption
which suggests that direct and indirect evidence are in agreement. Several
methods have been suggested to evaluate consistency. A popular approach
suggests adding inconsistency factors to the NMA model. We follow a different
direction by describing each inconsistency factor with a candidate covariate
whose choice relies on variable selection techniques. Our proposed method,
Stochastic Search Inconsistency Factor Selection (SSIFS), evaluates the
consistency assumption both locally and globally, by applying the stochastic
search variable selection method to determine whether the inconsistency factors
should be included in the model. The posterior inclusion probability of each
inconsistency factor quantifies how likely is a specific comparison to be
inconsistent. We use posterior model odds or the median probability model to
decide on the importance of inconsistency factors. Differences between direct
and indirect evidence can be incorporated into the inconsistency detection
process. A key point of our proposed approach is the construction of a
reasonable ""informative"" prior concerning network consistency. The prior is
based on the elicitation of information derived historical data from 201
published network meta-analyses. The performance of our proposed method is
evaluated in two published network meta-analyses. The proposed methodology is
publicly available in an R package called ssifs, developed and maintained by
the authors of this work.
"
2211.08637,2024-06-11,"Near-peer mentoring in data science: Two experiences at Stanford
  University","  Universities have been expanding the data science programs for undergraduate
students, with the simultaneous goal of reaching and retaining students from
underrepresented groups in the data science workforce. The set of new programs
also offer opportunities to involve graduate students, fostering their growth
as future leaders in data science education. We describe two programs that use
the near peer mentoring structure to provide pathways for graduate students to
develop teaching and mentoring skills, while providing research and learning
opportunities for undergraduate students from diverse backgrounds. In the Data
Science for Social Good Summer program, graduate students mentor a group of
undergraduate fellows as they tackle a data science project with positive
social impact. In the Inclusive Mentoring in Data Science course, graduate
students participate in workshops on effective and inclusive mentorship
strategies. In an experiential learning framework, they are paired with
undergraduate students from non-R1 schools, who they mentor through weekly
one-on-one on-line meetings. These initiatives offer a prototype of future
programs that serve the dual goal of providing both hands-on mentoring
experience for graduate students and research opportunities for undergraduate
students, in a high-touch inclusive and encouraging environment.
"
2211.11675,2022-11-22,Moment Propagation,"  We introduce and develop moment propagation for approximate Bayesian
inference. This method can be viewed as a variance correction for mean field
variational Bayes which tends to underestimate posterior variances. Focusing on
the case where the model is described by two sets of parameter vectors, we
develop moment propagation algorithms for linear regression, multivariate
normal, and probit regression models. We show for the probit regression model
that moment propagation empirically performs reasonably well for several
benchmark datasets. Finally, we discuss theoretical gaps and future extensions.
In the supplementary material we show heuristically why moment propagation
leads to appropriate posterior variance estimation, for the linear regression
and multivariate normal models we show precisely why mean field variational
Bayes underestimates certain moments, and prove that our moment propagation
algorithm recovers the exact marginal posterior distributions for all
parameters, and for probit regression we show that moment propagation provides
asymptotically correct posterior means and covariance estimates.
"
2211.11965,2024-02-19,"Predicting adverse outcomes following catheter ablation treatment for
  atrial fibrillation","  Objective: To develop prognostic survival models for predicting adverse
outcomes after catheter ablation treatment for non-valvular atrial fibrillation
(AF).
  Methods: We used a linked dataset including hospital administrative data,
prescription medicine claims, emergency department presentations, and death
registrations of patients in New South Wales, Australia. The cohort included
patients who received catheter ablation for AF. Traditional and deep survival
models were trained to predict major bleeding events and a composite of heart
failure, stroke, cardiac arrest, and death.
  Results: Out of a total of 3285 patients in the cohort, 177 (5.3%)
experienced the composite outcome (heart failure, stroke, cardiac arrest,
death) and 167 (5.1%) experienced major bleeding events after catheter ablation
treatment. Models predicting the composite outcome had high risk discrimination
accuracy, with the best model having a concordance index > 0.79 at the
evaluated time horizons. Models for predicting major bleeding events had poor
risk discrimination performance, with all models having a concordance index <
0.66. The most impactful features for the models predicting higher risk were
comorbidities indicative of poor health, older age, and therapies commonly used
in sicker patients to treat heart failure and AF.
  Conclusions: Diagnosis and medication history did not contain sufficient
information for precise risk prediction of experiencing major bleeding events.
The models for predicting the composite outcome have the potential to enable
clinicians to identify and manage high-risk patients following catheter
ablation proactively. Future research is needed to validate the usefulness of
these models in clinical practice.
"
2211.12687,2023-10-27,"Elastic Functional Changepoint Detection of Climate Impacts from
  Localized Sources","  Detecting changepoints in functional data has become an important problem as
interest in monitoring of climate phenomenon has increased, where the data is
functional in nature. The observed data often contains both amplitude
($y$-axis) and phase ($x$-axis) variability. If not accounted for properly,
true changepoints may be undetected, and the estimated underlying mean change
functions will be incorrect. In this paper, an elastic functional changepoint
method is developed which properly accounts for these types of variability. The
method can detect amplitude and phase changepoints which current methods in the
literature do not, as they focus solely on the amplitude changepoint. This
method can easily be implemented using the functions directly or can be
computed via functional principal component analysis to ease the computational
burden. We apply the method and its non-elastic competitors to both simulated
data and observed data to show its efficiency in handling data with phase
variation with both amplitude and phase changepoints. We use the method to
evaluate potential changes in stratospheric temperature due to the eruption of
Mt.\ Pinatubo in the Philippines in June 1991. Using an epidemic changepoint
model, we find evidence of a increase in stratospheric temperature during a
period that contains the immediate aftermath of Mt.\ Pinatubo, with most
detected changepoints occurring in the tropics as expected.
"
2211.14556,2022-11-29,"Multiple imputation for logistic regression models: incorporating an
  interaction","  Background: Multiple imputation is often used to reduce bias and gain
efficiency when there is missing data. The most appropriate imputation method
depends on the model the analyst is interested in fitting. Several imputation
approaches have been proposed for when this model is a logistic regression
model with an interaction term that contains a binary partially observed
variable; however, it is not clear which performs best under certain parameter
settings. Methods: Using 1000 simulations, each with 10,000 observations, under
six data-generating mechanisms (DGM), we investigate the performance of four
methods: (i) 'passive imputation', (ii) 'just another variable' (JAV), (iii)
'stratify-impute-append' (SIA), and (iv) 'substantive model compatible fully
conditional specifica-tion' (SMCFCS). The application of each method is shown
in an empirical example using England-based cancer registry data. Results:
SMCFCS and SIA showed the least biased estimate of the coefficients for the
fully, and partially, observed variable and the interaction term. SMCFCS and
SIA showed good coverage and low relative error for all DGMs. SMCFCS had a
large bias when there was a low prevalence of the fully observed variable in
the interaction. SIA performed poorly when the fully observed variable in the
interaction had a continuous underlying form. Conclusion: SMCFCS and SIA give
consistent estimation for logistic regression models with an interaction term
when data are missing at random, and either can be used in most analyses.
SMCFCS performed better than SIA when the fully observed variable in the
interaction had an underlying continuous form. Researchers should be cautious
when using SMCFCS when there is a low prevalence of the fully observed variable
in the interaction.
"
2211.14602,2023-07-26,A historical view on the maximum entropy,"  How to find unknown distributions is questioned in many pieces of research.
There are several ways to figure them out, but the main question is which acts
more reasonably than others. In this paper, we focus on the maximum entropy
principle as a suitable method of discovering the unknown distribution, which
recommends some prior information based on the available data set. We explain
its features by reviewing some papers. Furthermore, we recommend some articles
to study around the generalized maximum entropy issue, which is more suitable
when autocorrelation data exists. Then, we list the beneficial features of the
maximum entropy as a result. Finally, some disadvantages of entropy are
expressed to have a complete look at the maximum entropy principle, and we list
its drawbacks as the final step.
"
2211.16171,2023-04-04,"Learning to forecast: The probabilistic time series forecasting
  challenge","  We report on a course project in which students submit weekly probabilistic
forecasts of two weather variables and one financial variable. This real-time
format allows students to engage in practical forecasting, which requires a
diverse set of skills in data science and applied statistics. We describe the
context and aims of the course, and discuss design parameters like the
selection of target variables, the forecast submission process, the evaluation
of forecast performance, and the feedback provided to students. Furthermore, we
describe empirical properties of students' probabilistic forecasts, as well as
some lessons learned on our part.
"
2211.16975,2022-12-01,The Infinity of Randomness,"  This work starts from definition of randomness, the results of algorithmic
randomness are analyzed from the perspective of application. Then, the source
and nature of randomness is explored, and the relationship between infinity and
randomness is found. The properties of randomness are summarized from the
perspective of interaction between systems, that is, the set composed of
sequences generated by randomness has the property of asymptotic completeness.
Finally, the importance of randomness in AI research is emphasized.
"
2212.00219,2024-01-22,Are you using test log-likelihood correctly?,"  Test log-likelihood is commonly used to compare different models of the same
data or different approximate inference algorithms for fitting the same
probabilistic model. We present simple examples demonstrating how comparisons
based on test log-likelihood can contradict comparisons according to other
objectives. Specifically, our examples show that (i) approximate Bayesian
inference algorithms that attain higher test log-likelihoods need not also
yield more accurate posterior approximations and (ii) conclusions about
forecast accuracy based on test log-likelihood comparisons may not agree with
conclusions based on root mean squared error.
"
2212.02020,2022-12-21,"A framework to determine micro-level population figures using spatially
  disaggregated population estimates","  About half of the world population already live in urban areas. It is
projected that by 2050, approximately 70% of the world population will live in
cities. In addition to this, most developing countries do not have reliable
population census figures, and periodic population censuses are extremely
resource expensive. In Africa's most populous country, Nigeria, for instance,
the last decennial census was conducted in 2006. The relevance of near-accurate
population figures at the local levels cannot be overemphasized for a broad
range of applications by government agencies and non-governmental
organizations, including the planning and delivery of services, estimating
populations at risk of hazards or infectious diseases, and disaster relief
operations. Using GRID3 (Geo-Referenced Infrastructure and Demographic Data for
Development) high-resolution spatially disaggregated population data estimates,
this study proposed a framework for aggregating population figures at micro
levels within a larger geographic jurisdiction. Python, QGIS, and machine
learning techniques were used for data visualization, spatial analysis, and
zonal statistics. Lagos Island, Nigeria was used as a case study to demonstrate
how to obtain a more precise population estimate at the lowest administrative
jurisdiction and eliminate ambiguity caused by antithetical parameters in the
calculations. We also demonstrated how the framework can be used as a benchmark
for estimating the carrying capacities of urban basic services like healthcare,
housing, sanitary facilities, education, water etc. The proposed framework
would help urban planners and government agencies to plan and manage cities
better using more accurate data.
"
2212.04587,2023-02-15,Parameter Estimation with Maximal Updated Densities,"  A recently developed measure-theoretic framework solves a stochastic inverse
problem (SIP) for models where uncertainties in model output data are
predominantly due to aleatoric (i.e., irreducible) uncertainties in model
inputs (i.e., parameters). The subsequent inferential target is a distribution
on parameters. Another type of inverse problem is to quantify uncertainties in
estimates of ""true"" parameter values under the assumption that such
uncertainties should be reduced as more data are incorporated into the problem,
i.e., the uncertainty is considered epistemic. A major contribution of this
work is the formulation and solution of such a parameter identification problem
(PIP) within the measure-theoretic framework developed for the SIP. The
approach is novel in that it utilizes a solution to a stochastic forward
problem (SFP) to update an initial density only in the parameter directions
informed by the model output data. In other words, this method performs
""selective regularization"" only in the parameter directions not informed by
data. The solution is defined by a maximal updated density (MUD) point where
the updated density defines the measure-theoretic solution to the PIP. Another
significant contribution of this work is the full theory of existence and
uniqueness of MUD points for linear maps with Gaussian distributions.
Data-constructed Quantity of Interest (QoI) maps are also presented and
analyzed for solving the PIP within this measure-theoretic framework as a means
of reducing uncertainties in the MUD estimate. We conclude with a demonstration
of the general applicability of the method on two problems involving either
spatial or temporal data for estimating uncertain model parameters.
"
2301.00046,2023-09-26,A Bayesian treatment of the German tank problem,"  The German tank problem has an interesting historical background and is an
engaging problem of statistical estimation for the classroom. The objective is
to estimate the size of a population of tanks inscribed with sequential serial
numbers, from a random sample. In this tutorial article, we outline the
Bayesian approach to the German tank problem, (i) whose solution assigns a
probability to each tank population size, thereby quantifying uncertainty, and
(ii) which provides an opportunity to incorporate prior information and/or
beliefs about the tank population size into the solution. We illustrate with an
example. Finally, we survey problems in other contexts that resemble the German
tank problem.
"
2301.01251,2023-05-26,"Introducing Variational Inference in Statistics and Data Science
  Curriculum","  Probabilistic models such as logistic regression, Bayesian classification,
neural networks, and models for natural language processing, are increasingly
more present in both undergraduate and graduate statistics and data science
curricula due to their wide range of applications. In this paper, we present a
one-week course module for studnets in advanced undergraduate and applied
graduate courses on variational inference, a popular optimization-based
approach for approximate inference with probabilistic models. Our proposed
module is guided by active learning principles: In addition to lecture
materials on variational inference, we provide an accompanying class activity,
an \texttt{R shiny} app, and guided labs based on real data applications of
logistic regression and clustering documents using Latent Dirichlet Allocation
with \texttt{R} code. The main goal of our module is to expose students to a
method that facilitates statistical modeling and inference with large datasets.
Using our proposed module as a foundation, instructors can adopt and adapt it
to introduce more realistic case studies and applications in data science,
Bayesian statistics, multivariate analysis, and statistical machine learning
courses.
"
2301.01373,2023-01-05,Covariate-guided Bayesian mixture model for multivariate time series,"  With rapid development of techniques to measure brain activity and structure,
statistical methods for analyzing modern brain-imaging play an important role
in the advancement of science. Imaging data that measure brain function are
usually multivariate time series and are heterogeneous across both imaging
sources and subjects, which lead to various statistical and computational
challenges. In this paper, we propose a group-based method to cluster a
collection of multivariate time series via a Bayesian mixture of smoothing
splines. Our method assumes each multivariate time series is a mixture of
multiple components with different mixing weights. Time-independent covariates
are assumed to be associated with the mixture components and are incorporated
via logistic weights of a mixture-of-experts model. We formulate this approach
under a fully Bayesian framework using Gibbs sampling where the number of
components is selected based on a deviance information criterion. The proposed
method is compared to existing methods via simulation studies and is applied to
a study on functional near-infrared spectroscopy (fNIRS), which aims to
understand infant emotional reactivity and recovery from stress. The results
reveal distinct patterns of brain activity, as well as associations between
these patterns and selected covariates.
"
2301.01693,2023-01-18,Mortality modeling at old-age: a mixture model approach,"  This paper presents a novel approach for modeling mortality rates above age
70 by proposing a mixture-based model. This model is compared to four other
widely used models: the Beard, Gompertz, Makeham, and Perks models. Our model
can capture the complex behavior of mortality rates at all ages, providing a
more accurate representation of the data.
  To evaluate the performance of our model, we applied it to two countries with
different data quality: Japan and Brazil. Our results show that the proposed
model outperforms the other models in both countries, particularly in Japan
where it obtained an absolute mean percentage error of less than 7%, while the
other models presented values greater than 30%. This highlights the ability of
our model to adapt to different data quality and country-specific mortality
patterns.
  In summary, this paper presents a mixture-based model that captures the
behavior of mortality rates at all ages and outperforms other widely used
models in both high- and low-quality data settings. This model can improve
mortality prediction and inform public health policy.
"
2301.02478,2023-09-25,"Divergence vs. Decision P-values: A Distinction Worth Making in Theory
  and Keeping in Practice","  There are two distinct definitions of 'P-value' for evaluating a proposed
hypothesis or model for the process generating an observed dataset. The
original definition starts with a measure of the divergence of the dataset from
what was expected under the model, such as a sum of squares or a deviance
statistic. A P-value is then the ordinal location of the measure in a reference
distribution computed from the model and the data, and is treated as a
unit-scaled index of compatibility between the data and the model. In the other
definition, a P-value is a random variable on the unit interval whose
realizations can be compared to a cutoff alpha to generate a decision rule with
known error rates under the model and specific alternatives. It is commonly
assumed that realizations of such decision P-values always correspond to
divergence P-values. But this need not be so: Decision P-values can violate
intuitive single-sample coherence criteria where divergence P-values do not. It
is thus argued that divergence and decision P-values should be carefully
distinguished in teaching, and that divergence P-values are the relevant choice
when the analysis goal is to summarize evidence rather than implement a
decision rule.
"
2301.03350,2023-01-10,mRpostman: An IMAP Client for R,"  Internet Message Access Protocol (IMAP) clients are a common feature in
several programming languages. Despite having some packages for electronic
messages retrieval, the R language, until recently, lacked a broader solution,
capable of coping with different IMAP servers and providing a wide spectrum of
features. mRpostman covers most of the IMAP 4rev1 functionalities by
implementing tools for message searching, selective fetching of message
attributes, mailbox management, attachment extraction, and several other IMAP
features that can be executed in virtually any mail provider. By doing so, it
enables users to perform data analysis based on e-mail content. The goal of
this article is to showcase the toolkit provided with the mRpostman package, to
describe its key features and provide some application examples.
"
2301.05298,2023-01-16,"Open Case Studies: Statistics and Data Science Education through
  Real-World Applications","  With unprecedented and growing interest in data science education, there are
limited educator materials that provide meaningful opportunities for learners
to practice statistical thinking, as defined by Wild and Pfannkuch (1999), with
messy data addressing real-world challenges. As a solution, Nolan and Speed
(1999) advocated for bringing applications to the forefront in undergraduate
statistics curriculum with the use of in-depth case studies to encourage and
develop statistical thinking in the classroom. Limitations to this approach
include the significant time investment required to develop a case study --
namely, to select a motivating question and to create an illustrative data
analysis -- and the domain expertise needed. As a result, case studies based on
realistic challenges, not toy examples, are scarce. To address this, we
developed the Open Case Studies (https://www.opencasestudies.org) project,
which offers a new statistical and data science education case study model.
This educational resource provides self-contained, multimodal, peer-reviewed,
and open-source guides (or case studies) from real-world examples for active
experiences of complete data analyses. We developed an educator's guide
describing how to most effectively use the case studies, how to modify and
adapt components of the case studies in the classroom, and how to contribute
new case studies. (https://www.opencasestudies.org/OCS_Guide).
"
2301.05507,2023-01-16,Correlation-Based And-Operations Can Be Copulas: A Proof,"  In many practical situations, we know the probabilities $a$ and $b$ of two
events $A$ and $B$, and we want to estimate the joint probability ${\rm
Prob}(A\,\&\,B)$. The algorithm that estimates the joint probability based on
the known values $a$ and $b$ is called an and-operation. An important case when
such a reconstruction is possible is when we know the correlation between $A$
and $B$; we call the resulting and-operation correlation-based. On the other
hand, in statistics, there is a widely used class of and-operations known as
copulas. Empirical evidence seems to indicate that the correlation-based
and-operation derived in https://doi.org/10.1007/978-3-031-08971-8_64 is a
copula, but until now, no proof of this statement was available. In this paper,
we provide such a proof.
"
2301.07685,2023-01-19,"Factors other than climate change are currently more important in
  predicting how well fruit farms are doing financially","  Machine learning and statistical modeling methods were used to analyze the
impact of climate change on financial wellbeing of fruit farmers in Tunisia and
Chile. The analysis was based on face to face interviews with 801 farmers.
Three research questions were investigated. First, whether climate change
impacts had an effect on how well the farm was doing financially. Second, if
climate change was not influential, what factors were important for predicting
financial wellbeing of the farm. And third, ascertain whether observed effects
on the financial wellbeing of the farm were a result of interactions between
predictor variables. This is the first report directly comparing climate change
with other factors potentially impacting financial wellbeing of farms. Certain
climate change factors, namely increases in temperature and reductions in
precipitation, can regionally impact self-perceived financial wellbeing of
fruit farmers. Specifically, increases in temperature and reduction in
precipitation can have a measurable negative impact on the financial wellbeing
of farms in Chile. This effect is less pronounced in Tunisia. Climate impact
differences were observed within Chile but not in Tunisia. However, climate
change is only of minor importance for predicting farm financial wellbeing,
especially for farms already doing financially well. Factors that are more
important, mainly in Tunisia, included trust in information sources and prior
farm ownership. Other important factors include farm size, water management
systems used and diversity of fruit crops grown. Moreover, some of the
important factors identified differed between farms doing and not doing well
financially. Interactions between factors may improve or worsen farm financial
wellbeing.
"
2301.07711,2023-01-20,Matlab routines for centrality in directed acyclic graphs,"  New Matlab functions for network centrality are introduced. Instead of the
mean distance, the generalized mean distance is used. If closer relationships
are prioritized, this closeness measure is also defined for unconnected graphs.
Instead of distance to all nodes, distance to selected nodes is considered.
Besides the vertical in- and out-closeness measures, horizontal cross-closeness
is proposed.
"
2301.09442,2023-01-24,"Sharing information across patient subgroups to draw conclusions from
  sparse treatment networks","  Network meta-analysis (NMA) usually provides estimates of the relative
effects with the highest possible precision. However, sparse networks with few
available studies and limited direct evidence can arise, threatening the
robustness and reliability of NMA estimates. In these cases, the limited amount
of available information can hamper the formal evaluation of the underlying NMA
assumptions of transitivity and consistency. In addition, NMA estimates from
sparse networks are expected to be imprecise and possibly biased as they rely
on large sample approximations which are invalid in the absence of sufficient
data. We propose a Bayesian framework that allows sharing of information
between two networks that pertain to different population subgroups.
Specifically, we use the results from a subgroup with a lot of direct evidence
(a dense network) to construct informative priors for the relative effects in
the target subgroup (a sparse network). This is a two-stage approach where at
the first stage we extrapolate the results of the dense network to those
expected from the sparse network. This takes place by using a modified
hierarchical NMA model where we add a location parameter that shifts the
distribution of the relative effects to make them applicable to the target
population. At the second stage, these extrapolated results are used as prior
information for the sparse network. We illustrate our approach through a
motivating example of psychiatric patients. Our approach results in more
precise and robust estimates of the relative effects and can adequately inform
clinical practice in presence of sparse networks.
"
2302.01476,2023-02-06,"Challenges and Successes of Emergency Online Teaching in Statistics
  Courses","  As the COVID-19 pandemic took hold in early months of 2020, education at all
levels was pushed to emergency fully remote, online formats. This emergency
shift affected all aspects of teaching and learning with very little notice and
often with limited resources. Educators were required to convert entire courses
online and shift to remote instructional approaches practically overnight.
Students found themselves enrolled in online courses without choice and
struggling to adjust to their new learning environments. This article
highlights some of the challenges and successes of teaching emergency online
undergraduate statistics courses. In particular, we discuss challenges and
successes related to (1) technology, (2) classroom community and feedback, and
(3) student-content engagement. We also reflect on the opportunity to continue
to enhance and enrich the learning experiences of our students by utilizing
some of the lessons learned from emergency online teaching as new permanent
online statistics courses are developed and/or moved back into the classroom.
"
2302.05705,2023-02-14,"A practically efficient fixed-pivot selection algorithm and its
  extensible MATLAB suite","  Many statistical problems and applications require repeated computation of
order statistics, such as the median, but most statistical and programming
environments do not offer in their main distribution linear selection
algorithms. We introduce one, formally equivalent to quickselect, which keeps
the position of the pivot fixed. This makes the implementation simpler and much
practical compared with the best known solutions. It also enables an ""oracular""
pivot position option that can reduce a lot the convergence time of certain
statistical applications. We have extended the algorithm to weighted
percentiles such as the weighted median, applicable to data associated with
varying precision measurements, image filtering, descriptive statistics like
the medcouple and for combining multiple predictors in boosting algorithms. We
provide the new functions in MATLAB, C and R. We have packaged them in a broad
MATLAB toolbox addressing robust statistical methods, many of which can be now
optimised by means of efficient (weighted) selections.
"
2302.06075,2023-02-14,"A Graphical Point Process Framework for Understanding Removal Effects in
  Multi-Touch Attribution","  Marketers employ various online advertising channels to reach customers, and
they are particularly interested in attribution for measuring the degree to
which individual touchpoints contribute to an eventual conversion. The
availability of individual customer-level path-to-purchase data and the
increasing number of online marketing channels and types of touchpoints bring
new challenges to this fundamental problem. We aim to tackle the attribution
problem with finer granularity by conducting attribution at the path level. To
this end, we develop a novel graphical point process framework to study the
direct conversion effects and the full relational structure among numerous
types of touchpoints simultaneously. Utilizing the temporal point process of
conversion and the graphical structure, we further propose graphical
attribution methods to allocate proper path-level conversion credit, called the
attribution score, to individual touchpoints or corresponding channels for each
customer's path to purchase. Our proposed attribution methods consider the
attribution score as the removal effect, and we use the rigorous probabilistic
definition to derive two types of removal effects. We examine the performance
of our proposed methods in extensive simulation studies and compare their
performance with commonly used attribution models. We also demonstrate the
performance of the proposed methods in a real-world attribution application.
"
2302.08256,2023-02-23,"Motivation literally. Construction and expression of educational
  aspirations on Parcoursup","  This paper analyses the framing and expression of French high school
students' aspirations. It sheds new light on the inequalities in tracking
between academic versus technological and vocational track. Through the
analysis of a national survey and a corpus of cover letters written by
applicants for a sociology degree, it shows that, due to the lack of means,
teachers mainly have two types of guidance support strategies.Teachers use to
target and concentrate their supporting on ``good students'' in vocational
tracks, while, in academic tracks, they delegate some steps of the tracking
procedures to families. These different strategies have effects on the way high
school students internalise school prescriptions and restitute them in cover
letters. Through the close support they benefit from teachers, ``good
students'' in vocational tracks strongly internalise the instructions and their
place in the school hierarchy. In academic tracks, students' expression of the
aspirations is much more dependent of their familial capital.
"
2302.08724,2023-10-20,Piecewise Deterministic Markov Processes for Bayesian Neural Networks,"  Inference on modern Bayesian Neural Networks (BNNs) often relies on a
variational inference treatment, imposing violated assumptions of independence
and the form of the posterior. Traditional MCMC approaches avoid these
assumptions at the cost of increased computation due to its incompatibility to
subsampling of the likelihood. New Piecewise Deterministic Markov Process
(PDMP) samplers permit subsampling, though introduce a model specific
inhomogenous Poisson Process (IPPs) which is difficult to sample from. This
work introduces a new generic and adaptive thinning scheme for sampling from
these IPPs, and demonstrates how this approach can accelerate the application
of PDMPs for inference in BNNs. Experimentation illustrates how inference with
these methods is computationally feasible, can improve predictive accuracy,
MCMC mixing performance, and provide informative uncertainty measurements when
compared against other approximate inference schemes.
"
2302.11536,2023-02-23,Against normality testing,"  I reject the following null hypothesis: {H0: your data are normal}. Such
drastic decision is motivated by theoretical reasons, and applies to your
current data, the past ones, and the future ones. While this situation may
appear embarrassing, it does not invalidate any of your results. Moreover, it
allows to save time and energy that are currently spent in vain by performing
the following unnecessary tasks: (i) carrying out normality tests; (ii)
pretending to do something if normality is rejected; and (iii) arguing about
normality with Referee #2.
"
2303.03134,2023-03-07,The Matrix-variate Dirichlet Averages and Its Applications,"  This paper is about Dirichlet averages in the matrix-variate case or averages
of functions over the Dirichlet measure in the complex domain. The classical
power mean contains the harmonic mean, arithmetic mean and geometric mean
(Hardy, Littlewood and Polya), which is generalized to $y$-mean by deFinetti
and hypergeometric mean by Carlson, see the references herein. Carlson's
hypergeometric mean is to average a scalar function over a real scalar variable
type-1 Dirichlet measure and this in the current literature is known as
Dirichlet average of that function. The idea is examined when there is a type-1
or type-2 Dirichlet density in the complex domain. Averages of several
functions are computed in such Dirichlet densities in the complex domain.
Dirichlet measures are defined when the matrices are Hermitian positive
definite. Some applications are also discussed.
"
2303.08282,2023-06-16,"Reimagining Doctoral Training in Statistics: Is There a Role for a
  Professional Doctorate?","  Modern demands of the statistics profession call for reimagining statistics
training. The discipline needs to attract and develop students who are
effective as real-world problem solvers, interdisciplinary collaborators,
communicators, leaders, and teachers. Demand for statistics professionals with
broad technical and non-technical skills has grown in a variety of settings,
but especially in business and industry. Academic curricula, though, remain
primarily oriented around a narrow, technical conception of statistics.
Advanced graduate-level training essentially is limited to research doctorate
(PhD) programs which tend to prioritize theoretical and methodological research
over development of effective applied statisticians. Other professions, such as
those of physicians and surgeons, have training oriented around a professional
doctorate, as opposed to a research doctorate. The statistics profession should
consider not only changes to PhD curricula, but also the potential for a
professional doctorate, drawing ideas from the curricula of other professional
degrees such as the MD.
"
2303.10792,2023-03-24,Successful and sustainable undergraduate research in data science,"  Undergraduate research experiences hold many potential benefits. Students can
learn about new areas opening up previously unknown paths in academia and
industry. The hands-on experience often provides a deeper understanding of what
science, research, and data analysis is and, importantly, is not. While
numerous studies have provided information about the benefits and challenges of
undergraduate research, many still find it difficult to start an undergraduate
research group. Here, we provide a roadmap and resources to help faculty of all
levels create and sustain an undergraduate research group in quantitative areas
such as statistics, informatics, and data science. While we focus on
undergraduate research in data science, many of the recommendations may be
generally useful to research mentoring of all levels and other domains.
"
2303.11830,2023-03-22,Why must every data scientist be a Platonist,"  Data scientists are not mathematicians, but they make heavy use of
mathematics in their daily work. While mathematicians can study a mathematical
object which is inaccessible to our five senses, data scientists must deal with
real-world data which are observable to us. This fine line suggests that a data
scientist's philosophical position on mathematics might have a nontrivial
impact on their work. By examining how different philosophical views of
mathematics affect the interpretation of the basic model assumption in data
science, we arrive at the conclusion that a data scientist, who uses modern
probabilistic and statistical tools, must be a Platonist.
"
2303.12922,2023-04-10,Revisiting the Fragility of Influence Functions,"  In the last few years, many works have tried to explain the predictions of
deep learning models. Few methods, however, have been proposed to verify the
accuracy or faithfulness of these explanations. Recently, influence functions,
which is a method that approximates the effect that leave-one-out training has
on the loss function, has been shown to be fragile. The proposed reason for
their fragility remains unclear. Although previous work suggests the use of
regularization to increase robustness, this does not hold in all cases. In this
work, we seek to investigate the experiments performed in the prior work in an
effort to understand the underlying mechanisms of influence function fragility.
First, we verify influence functions using procedures from the literature under
conditions where the convexity assumptions of influence functions are met.
Then, we relax these assumptions and study the effects of non-convexity by
using deeper models and more complex datasets. Here, we analyze the key metrics
and procedures that are used to validate influence functions. Our results
indicate that the validation procedures may cause the observed fragility.
"
2303.15220,2023-07-31,Soft Skills Centrality in Graduate Studies Offerings,"  Is it possible to measure how critical soft skills like leadership or
teamwork are from the viewpoint of graduate studies offerings? This paper
provides a conceptual and methodological framework that introduces the concept
of a bipartite network as a practical way to estimate the importance of soft
skills as socio-emotional abilities trained in graduate studies. We examined
230 graduate programs offered by 49 higher education institutions in Colombia
to estimate the empirical importance of soft skills from the viewpoint of
graduate studies offerings. The results show that: a) graduate programs in
Colombia share 31 soft skills in their intended learning outcomes; b) the
centrality of these skills varies as a function of the graduate program,
although this variation was not statistically significant; and c) while most
central soft skills tend to be those related to creativity (i.e., creation or
generation of ideas or projects), leadership (to lead or teamwork), and
analytical orientation (e.g., evaluating situations and solving problems), less
central were those related to empathy (i.e., understanding others and
acknowledgment of others), ethical thinking, and critical thinking, posing the
question if too much emphasis on most visible skills might imply an unbalance
in the opportunities to enhancing other soft skills such as ethical thinking.
"
2303.16126,2023-11-13,The Value of Information and Circular Settings,"  We present a universal concept for the Value of Information (VoI), based on
the works of Claude Shannon's and Ruslan Stratonovich that can take into
account very general preferences of the agents and results in a single number.
As such it is convenient for applications and also has desirable properties for
decision theory and demand analysis. The Shannon/Stratonovich VoI concept is
compared to alternatives and applied in examples. In particular we apply the
concept to a circular spatial structure well known from many economic models
and allow for various economic transport costs.
"
2303.17425,2023-07-27,"A possibility-theoretic solution to Basu's Bayesian--frequentist via
  media","  Basu's via media is what he referred to as the middle road between the
Bayesian and frequentist poles. He seemed skeptical that a suitable via media
could be found, but I disagree. My basic claim is that the likelihood alone
can't reliably support probabilistic inference, and I justify this by
considering a technical trap that Basu stepped in concerning interpretation of
the likelihood. While reliable probabilistic inference is out of reach, it
turns out that reliable possibilistic inference is not. I lay out my proposed
possibility-theoretic solution to Basu's via media and I investigate how the
flexibility afforded by my imprecise-probabilistic solution can be leveraged to
achieve the likelihood principle (or something close to it).
"
2303.17788,2024-10-15,"Exploratory analysis of injury severity under different levels of
  driving automation (SAE Level 2-5) using multi-source data","  Vehicles equipped with automated driving capabilities have shown potential to
improve safety and operations. Advanced driver assistance systems (ADAS) and
automated driving systems (ADS) have been widely developed to support vehicular
automation. Although the studies on the injury severity outcomes that involve
automated vehicles are ongoing, there is limited research investigating the
difference between injury severity outcomes for the ADAS and ADS equipped
vehicles. To ensure a comprehensive analysis, a multi-source dataset that
includes 1,001 ADAS crashes (SAE Level 2 vehicles) and 548 ADS crashes (SAE
Level 4 vehicles) is used. Two random parameters multinomial logit models with
heterogeneity in the means of random parameters are considered to gain a better
understanding of the variables impacting the crash injury severity outcomes for
the ADAS (SAE Level 2) and ADS (SAE Level 4) vehicles. It was found that while
67 percent of crashes involving the ADAS equipped vehicles in the dataset took
place on a highway, 94 percent of crashes involving ADS took place in more
urban settings. The model estimation results also reveal that the weather
indicator, driver type indicator, differences in the system sophistication that
are captured by both manufacture year and high/low mileage as well as rear and
front contact indicators all play a role in the crash injury severity outcomes.
The results offer an exploratory assessment of safety performance of the ADAS
and ADS equipped vehicles using the real-world data and can be used by the
manufacturers and other stakeholders to dictate the direction of their
deployment and usage.
"
2304.00149,2023-04-04,"Using online student focus groups in the development of new educational
  resources","  Educational resources, such as web apps and self-directed tutorials, have
become popular tools for teaching and active learning. Ideally, students - the
intended users of these resources - should be involved in the resource
development stage. However, in practice students often only interact with fully
developed resources, when it might be too late to incorporate changes. Previous
work has addressed this by involving students in the development of new
resources via in-person focus groups and interviews. In these, the resource
developers observe students interacting with the resource. This allows
developers to incorporate their observations and students' direct feedback into
further development of the resource. However, as a result of the COVID-19
pandemic, carrying out in-person focus groups became infeasible due to social
distancing restrictions. Instead, online meetings and classes became
ubiquitous. In this work, we describe a fully-online methodology to evaluate
new resources in development. Specifically, our methodology consists of
carrying out student focus groups via online video conferencing software. We
assessed two educational resources for introductory statistics using our
methodology and found that the online setting allowed us to obtain rich,
detailed information from the students. We also found online focus groups to be
more efficient: students and researchers did not need to travel and scheduling
was not restricted by the availability of physical space. Our findings suggest
that online focus groups are an attractive alternative to in-person focus
groups for student assessment of resources in development, even now that
pandemic restrictions are being eased.
"
2304.01141,2023-04-04,Testing for idiosyncratic Treatment Effect Heterogeneity,"  This paper provides asymptotically valid tests for the null hypothesis of no
treatment effect heterogeneity. Importantly, I consider the presence of
heterogeneity that is not explained by observed characteristics, or so-called
idiosyncratic heterogeneity. When examining this heterogeneity, common
statistical tests encounter a nuisance parameter problem in the average
treatment effect which renders the asymptotic distribution of the test
statistic dependent on that parameter. I propose an asymptotically valid test
that circumvents the estimation of that parameter using the empirical
characteristic function. A simulation study illustrates not only the test's
validity but its higher power in rejecting a false null as compared to current
tests. Furthermore, I show the method's usefulness through its application to a
microfinance experiment in Bosnia and Herzegovina. In this experiment and for
outcomes related to loan take-up and self-employment, the tests suggest that
treatment effect heterogeneity does not seem to be completely accounted for by
baseline characteristics. For those outcomes, researchers could potentially try
to collect more baseline characteristics to inspect the remaining treatment
effect heterogeneity, and potentially, improve treatment targeting.
"
2304.01276,2024-07-23,"The Design and Implementation of a Bayesian Data Analysis Lesson for
  Pre-Service Mathematics and Science Teachers","  With the rise of the popularity of Bayesian methods and accessible computer
software, teaching and learning about Bayesian methods are expanding. However,
most educational opportunities are geared toward statistics and data science
students and are less available in the broader STEM fields. In addition, there
are fewer opportunities at the K-12 level. With the indirect aim of introducing
Bayesian methods at the K-12 level, we have developed a Bayesian Data Analysis
activity and implemented it with 35 mathematics and science pre-service
teachers. In this manuscript, we describe the activity, the web app supporting
the activity, and pre-service teachers' perceptions of the activity. Lastly, we
discuss future directions for preparing K-12 teachers in teaching and learning
about Bayesian methods.
"
2304.01789,2023-04-05,Communication of Statistics and Evidence in Times of Crisis,"  This review provides an overview of concepts relating to the communication of
statistical and empirical evidence in times of crisis, with a special focus on
COVID-19. In it, we consider topics relating both to the communication of
numbers -- such as the role of format, context, comparisons, and visualization
-- and the communication of evidence more broadly -- such as evidence quality,
the influence of changes in available evidence, transparency, and repeated
decision making. A central focus is on the communication of the inherent
uncertainties in statistical analysis, especially in rapidly changing
informational environments during crises. We present relevant literature on
these topics and draw connections to the communication of statistics and
empirical evidence during the COVID-19 pandemic and beyond. We finish by
suggesting some considerations for those faced with communicating statistics
and evidence in times of crisis.
"
2304.04249,2023-04-11,"Convergent estimators of variance of a spatial mean in the presence of
  missing observations","  In the geosciences, a recurring problem is one of estimating spatial means of
a physical field using weighted averages of point observations. An important
variant is when individual observations are counted with some probability less
than one. This can occur in different contexts: from missing data to estimating
the statistics across subsamples. In such situations, the spatial mean is a
ratio of random variables, whose statistics involve approximate estimators
derived through series expansion. The present paper considers truncated
estimators of variance of the spatial mean and their general structure in the
presence of missing data. To all orders, the variance estimator depends only on
the first and second moments of the underlying field, and convergence requires
these moments to be finite. Furthermore, convergence occurs if either the
probability of counting individual observations is larger than 1/2 or the
number of point observations is large. In case the point observations are
weighted uniformly, the estimators are easily found using combinatorics and
involve Stirling numbers of the second kind.
"
2304.05189,2023-04-12,Individualized Conformal,"  The problem of individualized prediction can be addressed using variants of
conformal prediction, obtaining the intervals to which the actual values of the
variables of interest belong. Here we present a method based on detecting the
observations that may be relevant for a given question and then using simulated
controls to yield the intervals for the predicted values. This method is shown
to be adaptive and able to detect the presence of latent relevant variables.
"
2304.05740,2024-04-29,"Possibility-theoretic statistical inference offers performance and
  probativeness assurances","  Statisticians are largely focused on developing methods that perform well in
a frequentist sense -- even the Bayesians. But the widely-publicized
replication crisis suggests that these performance guarantees alone are not
enough to instill confidence in scientific discoveries. In addition to reliably
detecting hypotheses that are (in)compatible with data, investigators require
methods that can probe for hypotheses that are actually supported by the data.
In this paper, we demonstrate that valid inferential models (IMs) achieve both
performance and probativeness properties and we offer a powerful new result
that ensures the IM's probing is reliable. We also compare and contrast the
IM's dual performance and probativeness abilities with that of Deborah Mayo's
severe testing framework.
"
2304.06999,2023-09-21,"Finite mixtures in capture-recapture surveys for modelling residency
  patterns in marine wildlife populations","  In this work, the goal is to estimate the abundance of an animal population
using data coming from capture-recapture surveys. We leverage the prior
knowledge about the population's structure to specify a parsimonious finite
mixture model tailored to its behavioral pattern. Inference is carried out
under the Bayesian framework, where we discuss suitable priors' specification
that could alleviate label-switching and non-identifiability issues affecting
finite mixtures. We conduct simulation experiments to show the competitive
advantage of our proposal over less specific alternatives. Finally, the
proposed model is used to estimate the common bottlenose dolphins' population
size at the Tiber River estuary (Mediterranean Sea), using data collected via
photo-identification from 2018 to 2020. Results provide novel insights on the
population's size and structure, and shed light on some of the ecological
processes governing the population dynamics.
"
2304.07005,2023-06-14,"Detector Design and Performance Analysis for Target Detection in
  Subspace Interference","  It is often difficult to obtain sufficient training data for adaptive signal
detection, which is required to calculate the unknown noise covariance matrix.
Additionally, interference is frequently present, which complicates the
detecting issue. We provide a two-step method, termed interference cancellation
before detection (ICBD), to address the issue of signal detection in the
unknown Gaussian noise and subspace interference. The first involves projecting
the test and training data to the interference-orthogonal subspace in order to
suppress the interference. Utilizing traditional adaptive detector design ideas
is the next stage. Due to the smaller dimension of the projected data, the
ICBD-based detectors can function with little training data. The ICBD has two
additional benefits over traditional detectors. Lower computational burden and
proper operation with interference being in the training data are two
additional benefits of ICBD-based detectors over conventional ones. We also
give the statistical properties of the ICBD-based detectors and demonstrate
their equivalence with the traditional ones in the special case of a large
amount of training data containing no interference
"
2304.07709,2023-04-18,"Development of Tools for the Classification of Peer Groups Geographies
  in the Analysis of Health Care Variation","  This dissertation is based on a project co-founded by the Health Market
Quality Program (now Rozetta Institute) and the Australian Institute of Health
and Welfare. The overall objective of this work is to provide a framework and a
tool for classification and clustering of homogeneous geographic areas based on
aggregated population data. Thus, to enable the presentation and reporting of
comparable information of individual units with peers, I develop the
Homogeneity and Location indices to measure respectively the dispersion and
central tendency of a categorical ordinal distribution. The advantages of such
indices include statistical efficiency and a simple presentation of results.
Our approach is founded on the general theory of probability distributions, and
our aim is to provide a natural benchmark for a homogeneity measure in terms of
what is a ""high"" and ""low"" concentration of a probability distribution.
Currently, there is no accepted benchmark that could be used to assess the
homogeneity of a categorical ordinal variable. In this work, the proposed
statistical indices are used to assess the socioeconomic homogeneity of the
commonly used SA3 Australia census geography and analyse the variation of GP
attenders in the metropolitan area of Sydney. The approach can be used to
classify any geographic area and explore variation across any specified
geographical boundaries. The SA3 dataset and scripts (R/Python) to develop
these indices have been made available on my GitHub account:
https://github.com/lpinzari/homogeneity-location-index
"
2304.09460,2024-05-15,"Studying continuous, time-varying, and/or complex exposures using
  longitudinal modified treatment policies","  This tutorial discusses methodology for causal inference using longitudinal
modified treatment policies. This method facilitates the mathematical
formalization, identification, and estimation of many novel parameters, and
mathematically generalizes many commonly used parameters, such as the average
treatment effect. Longitudinal modified treatment policies apply to a wide
variety of exposures, including binary, multivariate, and continuous, and can
accommodate time-varying treatments and confounders, competing risks,
loss-to-follow-up, as well as survival, binary, or continuous outcomes.
Longitudinal modified treatment policies can be seen as an extension of static
and dynamic interventions to involve the natural value of treatment, and, like
dynamic interventions, can be used to define alternative estimands with a
positivity assumption that is more likely to be satisfied than estimands
corresponding to static interventions. This tutorial aims to illustrate several
practical uses of the longitudinal modified treatment policy methodology,
including describing different estimation strategies and their corresponding
advantages and disadvantages. We provide numerous examples of types of research
questions which can be answered using longitudinal modified treatment policies.
We go into more depth with one of these examples--specifically, estimating the
effect of delaying intubation on critically ill COVID-19 patients' mortality.
We demonstrate the use of the open-source R package lmtp to estimate the
effects, and we provide code on https://github.com/kathoffman/lmtp-tutorial.
"
2304.09918,2023-07-11,Smart Sports Predictions via Hybrid Simulation: NBA Case Study,"  Increased data availability has stimulated the interest in studying sports
prediction problems via analytical approaches; in particular, with machine
learning and simulation. We characterize several models that have been proposed
in the literature, all of which suffer from the same drawback: they cannot
incorporate rational decision-making and strategies from teams/players
effectively. We tackle this issue by proposing hybrid simulation logic that
incorporates teams as agents, generalizing the models/methodologies that have
been proposed in the past. We perform a case study on the NBA with two goals:
i) study the quality of predictions when using only one predictive variable,
and ii) study how much historical data should be kept to maximize prediction
accuracy. Results indicate that there is an optimal range of data quantity and
that studying what data and variables to include is of extreme importance.
"
2304.10698,2023-04-24,A new copula regression model for hierarchical data,"  This paper proposes multivariate copula models for hierarchical data. They
account for two types of correlation: one is between variables measured on the
same unit and the other is a correlation between units in the same cluster.
This model is used to carry out copula regression for hierarchical data that
gives cluster specific prediction curves. In the simple case where a cluster
contains two units and where two variables are measured on each one, the new
model is constructed within a D-vine. Then we focus on situations where two
variables are measured on the units of a cluster of arbitrary size. The
proposed copula density has an explicit form; it is expressed in terms of three
copula families. We study the properties of the model; compare it to the linear
mixed model and end with special cases. When the three copula families and the
marginal distributions are normal, the model is equivalent to a normal linear
mixed model with random, cluster specific, intercepts. The method to select the
three copula families and to estimate their parameters are proposed. We perform
a Monte Carlo study of the parameter estimators. A data set on the marks of
students in several school is used to implement the proposed model and to
compare its performance to standard normal mixed linear models.
"
2304.11562,2023-04-25,Pandemic Data Quality Modelling: A Bayesian Approach,"  When pandemics like COVID-19 spread around the world, the rapidly evolving
situation compels officials and executives to take prompt decisions and adapt
policies depending on the current state of the disease. In this context, it is
crucial for policymakers to have always a firm grasp on what is the current
state of the pandemic, and to envision how the number of infections and
possible deaths is going to evolve over the next weeks. However, as in many
other situations involving compulsory registration of sensitive data from
multiple collectors, cases might be reported with errors, often with delays
deferring an up-to-date view of the state of things. Errors in collecting new
cases affect the overall mortality, resulting in excess deaths reported by
official statistics only months later. In this paper, we provide tools for
evaluating the quality of pandemic mortality data. We accomplish this through a
Bayesian approach accounting for the excess mortality pandemics might bring
with respect to the normal level of mortality in the population.
"
2304.12237,2023-04-25,"Using Auxiliary Data to Guide the Recruitment of Sites for Randomized
  Controlled Trials","  Sampling methods such as Stratified Random Sampling can be used to select
representative samples of schools for randomized controlled trials of
educational interventions. However, these methods may still yield external
validity bias when participation by schools is voluntary and participation
decisions are associated with unobserved variables. This paper offers a new
sampling method called Stratified Random Sampling with Quotas. Under this
method, quotas are set to avoid including too many schools of a particular
type, as defined by auxiliary variables that are unobserved in the sampling
frame, but whose population distribution can be estimated from external data.
Our simulations find that when the auxiliary variables affect whether or not a
school participates in the study, quotas set based on those variables reduce
external validity bias. These results suggest that when auxiliary data are
available on strong impact moderators for the target population, these data can
be used to address non-ignorable self-selection by schools into randomized
controlled trials.
"
2304.12305,2023-04-25,"Downscaling Epidemiological Time Series Data for Improving Forecasting
  Accuracy: An Algorithmic Approach","  Data scarcity and discontinuity are common occurrences in the healthcare and
epidemiological dataset and often need help in forming an educative decision
and forecasting the upcoming scenario. Often, these data are stored as
monthly/yearly aggregate where the prevalent forecasting tools like
Autoregressive Integrated Moving Average (ARIMA), Seasonal Autoregressive
Integrated Moving Average (SARIMA), and TBATS often fail to provide
satisfactory results. Artificial data synthesis methods have been proven to be
a powerful tool for tackling these challenges. The paper aims to propose a
downscaling data algorithm based on the underlying distribution. Our findings
show that the synthesized data is in agreement with the original data in terms
of trend, seasonality, and residuals, and the synthesized data provides a
stable foothold for the forecasting tools to generate a much more accurate
forecast of the situation.
"
2304.12482,2025-01-20,Information Theory for Complex Systems Scientists,"  In the 21st century, many of the crucial scientific and technical issues
facing humanity can be understood as problems associated with understanding,
modelling, and ultimately controlling complex systems: systems comprised of a
large number of non-trivially interacting components whose collective behaviour
can be difficult to predict. Information theory, a branch of mathematics
historically associated with questions about encoding and decoding messages,
has emerged as something of a lingua franca for those studying complex systems,
far exceeding its original narrow domain of communication systems engineering.
In the context of complexity science, information theory provides a set of
tools which allow researchers to uncover the statistical and effective
dependencies between interacting components; relationships between systems and
their environment; mereological whole-part relationships; and is sensitive to
non-linearities missed by commonly parametric statistical models.
  In this review, we aim to provide an accessible introduction to the core of
modern information theory, aimed specifically at aspiring (and established)
complex systems scientists. This includes standard measures, such as Shannon
entropy, relative entropy, and mutual information, before building to more
advanced topics, including: information dynamics, measures of statistical
complexity, information decomposition, and effective network inference. In
addition to detailing the formal definitions, in this review we make an effort
to discuss how information theory can be interpreted and develop the intuition
behind abstract concepts like ""entropy,"" in the hope that this will enable
interested readers to understand what information is, and how it is used, at a
more fundamental level.
"
2304.13406,2024-09-26,"Onset of a conceptual outline map to get a hold on the jungle of cluster
  analysis","  The domain of cluster analysis is a meeting point for a very rich
multidisciplinary encounter, with cluster-analytic methods being studied and
developed in discrete mathematics, numerical analysis, statistics, data
analysis, data science, and computer science (including machine learning, data
mining, and knowledge discovery), to name but a few. The other side of the
coin, however, is that the domain suffers from a major accessibility problem as
well as from the fact that it is rife with division across many pretty isolated
islands. As a way out, the present paper offers a thorough and in-depth review
of the clustering domain as a whole under the form of an outline map based on
an overarching conceptual framework and a common language. With this framework
we wish to contribute to structuring the clustering domain, to characterizing
methods that have often been developed and studied in quite different contexts,
to identifying links between methods, and to introducing a frame of reference
for optimally setting up cluster analyses in data-analytic practice.
"
2304.14098,2023-05-01,"Optimal Covariance Cleaning for Heavy-Tailed Distributions: Insights
  from Information Theory","  In optimal covariance cleaning theory, minimizing the Frobenius norm between
the true population covariance matrix and a rotational invariant estimator is a
key step. This estimator can be obtained asymptotically for large covariance
matrices, without knowledge of the true covariance matrix. In this study, we
demonstrate that this minimization problem is equivalent to minimizing the loss
of information between the true population covariance and the rotational
invariant estimator for normal multivariate variables. However, for Student's t
distributions, the minimal Frobenius norm does not necessarily minimize the
information loss in finite-sized matrices. Nevertheless, such deviations vanish
in the asymptotic regime of large matrices, which might extend the
applicability of random matrix theory results to Student's t distributions.
These distributions are characterized by heavy tails and are frequently
encountered in real-world applications such as finance, turbulence, or nuclear
physics. Therefore, our work establishes a connection between statistical
random matrix theory and estimation theory in physics, which is predominantly
based on information theory.
"
2305.00908,2023-08-22,"Estimation of the Impact of COVID-19 Pandemic Lockdowns on Breast Cancer
  Deaths and Costs in Poland using Markovian Monte Carlo Simulation","  This study examines the effect of COVID-19 pandemic and associated lockdowns
on access to crucial diagnostic procedures for breast cancer patients,
including screenings and treatments. To quantify the impact of the lockdowns on
patient outcomes and cost, the study employs a mathematical model of breast
cancer progression. The model includes ten different states that represent
various stages of health and disease, along with the four different stages of
cancer that can be diagnosed or undiagnosed. The study employs a natural
history stochastic model to simulate the progression of breast cancer in
patients. The model includes transition probabilities between states, estimated
using both literature and empirical data. The study utilized a Markov Chain
Monte Carlo simulation to model the natural history of each simulated patient
over a seven-year period from 2019 to 2025. The simulation was repeated 100
times to estimate the variance in outcome variables. The study found that the
COVID-19 pandemic and associated lockdowns caused a significant increase in
breast cancer costs, with an average rise of 172.5 million PLN (95% CI [82.4,
262.6]) and an additional 1005 breast cancer deaths (95% CI [426, 1584]) in
Poland during the simulated period. While these results are preliminary, they
highlight the potential harmful impact of lockdowns on breast cancer treatment
outcomes and costs.
"
2305.01518,2023-05-03,Defining Replicability of Prediction Rules,"  In this article I propose an approach for defining replicability for
prediction rules. Motivated by a recent NAS report, I start from the
perspective that replicability is obtaining consistent results across studies
suitable to address the same prediction question, each of which has obtained
its own data. I then discuss concept and issues in defining key elements of
this statement. I focus specifically on the meaning of ""consistent results"" in
typical utilization contexts, and propose a multi-agent framework for defining
replicability, in which agents are neither partners nor adversaries. I recover
some of the prevalent practical approaches as special cases. I hope to provide
guidance for a more systematic assessment of replicability in machine learning.
"
2305.03108,2023-05-08,The Saltbox-Roof Probability Distribution,"  The saltbox-roof parametric probability distribution is a special case of the
triangular distribution, where only one side is truncated. Here it is presented
as a single and independent distribution, where the explicit equations are
defined for its probability density--, the cumulative distribution--, and the
inverse of the cumulative distribution (quantile--) functions as also its
random generator. Four parameters are necessary to define it: the lower and the
upper limits, the mode, and a shape parameter. Also, the saltbox-roof
distribution degenerates into the uniform distribution, into a kind of a
trapezoidal distribution and into other special cases of the general triangular
distribution, all of them which are related to the domain of the shape
parameter within the mode. The mean, median, and the variance are also here
expressed by explicit equations. The function equations have been verified with
theorems of truncated distributions. Three application examples are exposed.
"
2305.03205,2024-08-21,"Risk management in the use of published statistical results for policy
  decisions","  Statistical inferential results generally come with a measure of reliability
for decision-making purposes. For a policy implementer, the value of
implementing published policy research depends critically upon this
reliability. For a policy researcher, the value of policy implementation may
depend weakly or not at all upon the policy's outcome. Some researchers might
benefit from overstating the reliability of statistical results. Implementers
may find it difficult or impossible to determine whether researchers are
overstating reliability. This information asymmetry between researchers and
implementers can lead to an adverse selection problem where, at best, the full
benefits of a policy are not realized or, at worst, a policy is deemed too
risky to implement at any scale. Researchers can remedy this by guaranteeing
the policy outcome. Researchers can overcome their own risk aversion and wealth
constraints by exchanging risks with other researchers or offering only partial
insurance. The problem and remedy are illustrated using a confidence interval
for the success probability of a binomial policy outcome.
"
2305.04386,2023-10-19,Inferring Local Structure from Pairwise Correlations,"  To construct models of large, multivariate complex systems, such as those in
biology, one needs to constrain which variables are allowed to interact. This
can be viewed as detecting ""local"" structures among the variables. In the
context of a simple toy model of 2D natural and synthetic images, we show that
pairwise correlations between the variables -- even when severely undersampled
-- provide enough information to recover local relations, including the
dimensionality of the data, and to reconstruct arrangement of pixels in fully
scrambled images. This proves to be successful even though higher order
interaction structures are present in our data. We build intuition behind the
success, which we hope might contribute to modeling complex, multivariate
systems and to explaining the success of modern attention-based machine
learning approaches.
"
2305.04479,2023-11-15,Axiomatization of Interventional Probability Distributions,"  Causal intervention is an essential tool in causal inference. It is
axiomatized under the rules of do-calculus in the case of structure causal
models. We provide simple axiomatizations for families of probability
distributions to be different types of interventional distributions. Our
axiomatizations neatly lead to a simple and clear theory of causality that has
several advantages: it does not need to make use of any modeling assumptions
such as those imposed by structural causal models; it only relies on
interventions on single variables; it includes most cases with latent variables
and causal cycles; and more importantly, it does not assume the existence of an
underlying true causal graph as we do not take it as the primitive object--in
fact, a causal graph is derived as a by-product of our theory. We show that,
under our axiomatizations, the intervened distributions are Markovian to the
defined intervened causal graphs, and an observed joint probability
distribution is Markovian to the obtained causal graph; these results are
consistent with the case of structural causal models, and as a result, the
existing theory of causal inference applies. We also show that a large class of
natural structural causal models satisfy the theory presented here. We note
that the aim of this paper is axiomatization of interventional families, which
is subtly different from ""causal modeling.""
"
2305.04486,2023-05-09,Measurable Taylor's Theorem: An Elementary Proof,"  The Taylor expansion is a widely used and powerful tool in all branches of
Mathematics, both pure and applied. In Probability and Mathematical Statistics,
however, a stronger version of Taylor's classical theorem is often needed, but
only tacitly assumed. In this note, we provide an elementary proof of this
measurable Taylor's theorem, which guarantees that the interpolating point in
the Lagrange form of the remainder can be chosen to depend measurably on the
independent variable.
"
2305.06420,2023-05-12,"A distribution-free change-point monitoring scheme in high-dimensional
  settings with application to industrial image surveillance","  Existing monitoring tools for multivariate data are often asymptotically
distribution-free, computationally intensive, or require a large stretch of
stable data. Many of these methods are not applicable to 'high dimension, low
sample size' scenarios. With rapid technological advancement, high-dimensional
data has become omnipresent in industrial applications. We propose a
distribution-free change point monitoring method applicable to high dimensional
data. Through an extensive simulation study, performance comparison has been
done for different parameter values, under different multivariate distributions
with complex dependence structures. The proposed method is robust and efficient
in detecting change points under a wide range of shifts in the process
distribution. A real-life application illustrated with the help of
high-dimensional image surveillance dataset.
"
2305.08041,2023-05-16,"Over-Measurement Paradox: Suspension of Thermonuclear Research Center
  and Need to Update Standards","  In general, the more measurements we perform, the more information we gain
about the system and thus, the more adequate decisions we will be able to make.
However, in situations when we perform measurements to check for safety, the
situation is sometimes opposite: the more additional measurements we perform
beyond what is required, the worse the decisions will be: namely, the higher
the chance that a perfectly safe system will be erroneously classified as
unsafe and therefore, unnecessary additional features will be added to the
system design. This is not just a theoretical possibility: exactly this
phenomenon is one of the reasons why the construction of a world-wide
thermonuclear research center has been suspended. In this paper, we show that
the reason for this paradox is in the way the safety standards are formulated
now -- what was a right formulation when sensors were much more expensive is no
longer adequate now when sensors and measurements are much cheaper. We also
propose how to modify the safety standards so as to avoid this paradox and make
sure that additional measurements always lead to better solutions.
"
2305.10335,2023-05-18,The Geometry of Chi-Square Degrees of Freedom,"  In this paper, we state and prove a simple geometric interpretation of the
degrees of freedom of a chi-square distribution. The general geometric idea
goes back at least to Fisher in the 1920s, but the exact result does not appear
to have been explicitly stated or proved prior to this paper.
"
2305.12006,2024-04-03,Comparison of open-source software for producing directed acyclic graphs,"  Many software packages have been developed to assist researchers in drawing
directed acyclic graphs (DAGs), each with unique functionality and usability.
We examine five of the most common software to generate DAGs: TikZ, DAGitty,
ggdag, dagR, and igraph. For each package, we provide a general description of
the its background, analysis and visualization capabilities, and
user-friendliness. Additionally in order to compare packages, we produce two
DAGs in each software, the first featuring a simple confounding structure,
while the second includes a more complex structure with three confounders and a
mediator. We provide recommendations for when to use each software depending on
the user's needs.
"
2305.14478,2023-07-06,"Reproducibility and Transparency versus Privacy and Confidentiality:
  Reflections from a Data Editor","  Transparency and reproducibility are often seen in opposition to privacy and
confidentiality. Data that need to be kept confidential are seen as an
impediment to reproducibility, and privacy would seem to inhibit transparency.
I bring a more nuanced view to the discussion, and show, using examples from
over 1,000 reproducibility assessments, that confidential data can very well be
used in reproducible and transparent research. The key insight is that access
to most confidential data, while tedious, is open to hundreds if not thousands
of researchers. In cases where few researchers can consider accessing such data
in the future, reproducibility services, such as those provided by some
journals, can provide some evidence for effective reproducibility even when the
same data may not be available for future research.
"
2305.18366,2023-12-21,"Using maximum weighted likelihood to derive Lehmer and H\""older mean
  families","  In this paper, we establish the links between the Lehmer and H\""older mean
families and maximum weighted likelihood estimator. Considering the regular
one-parameter exponential family of probability density functions, we show that
the maximum weighted likelihood of the parameter is a generalized weighted mean
family from which Lehmer and H\""older mean families are derived. Some of the
outcomes obtained provide a probabilistic interpretation of these mean families
and could therefore broaden their uses in various applications.
"
2306.01890,2024-10-14,"Mixed-type Distance Shrinkage and Selection for Clustering via Kernel
  Metric Learning","  Distance-based clustering and classification are widely used in various
fields to group mixed numeric and categorical data. In many algorithms, a
predefined distance measurement is used to cluster data points based on their
dissimilarity. While there exist numerous distance-based measures for data with
pure numerical attributes and several ordered and unordered categorical
metrics, an efficient and accurate distance for mixed-type data that utilizes
the continuous and discrete properties simulatenously is an open problem. Many
metrics convert numerical attributes to categorical ones or vice versa. They
handle the data points as a single attribute type or calculate a distance
between each attribute separately and add them up. We propose a metric called
KDSUM that uses mixed kernels to measure dissimilarity, with cross-validated
optimal bandwidth selection. We demonstrate that KDSUM is a shrinkage method
from existing mixed-type metrics to a uniform dissimilarity metric, and
improves clustering accuracy when utilized in existing distance-based
clustering algorithms on simulated and real-world datasets containing
continuous-only, categorical-only, and mixed-type data.
"
2306.03981,2024-12-03,Developing an Index of National Research Capacity,"  Public managers lack feedback on the effectiveness of public investments,
policies, and programs instituted to build and use research capacity. Numerous
reports rank countries on global performance on innovation and competitiveness,
but the highly globalized data does not distinguish country contributions from
global ones. We suggest improving upon global reports by removing globalized
measures and combining a reliable set of national indicators into an index. We
factor analyze 14 variables for 172 countries from 2013 to 2021. Two factors
emerge, one for raw or core research capacity and the other indicating the
wider context of governance. Analysis shows convergent validity within the two
factors and divergent validity between them. Nations rank differently between
capacity, governance context, and the product of the two. Ranks also vary as a
function of the chosen aggregation method. Finally, as a test of the predictive
validity of the capacity index, a regression analysis was implemented
predicting national citation strength. Policymakers and analysts may find
stronger feedback from this approach to quantifying national research strength.
"
2306.04430,2025-01-22,"Evaluating the impact of outcome delay on the efficiency of two-arm
  group-sequential trials","  Adaptive designs(AD) are a broad class of trial designs that allow preplanned
modifications based on patient data providing improved efficiency and
flexibility. However, a delay in observing the primary outcome variable can
harm this added efficiency. In this paper, we aim to ascertain the size of such
outcome delay that results in the realised efficiency gains of ADs becoming
negligible compared to classical fixed sample RCTs. We measure the impact of
delay by developing formulae for the no. of overruns in 2 arm GSDs with normal
data, assuming different recruitment models. The efficiency of a GSD is usually
measured in terms of the expected sample size (ESS), with GSDs generally
reducing the ESS compared to a standard RCT. Our formulae measures the
efficiency gain from a GSD in terms of ESS reduction that is lost due to delay.
We assess whether careful choice of design (e.g., altering the spacing of the
IAs) can help recover the benefits of GSDs in presence of delay. We also
analyse the efficiency of GSDs with respect to time to complete the trial.
Comparing the expected efficiency gains, with and without consideration of
delay, it is evident GSDs suffer considerable losses due to delay. Even a small
delay can have a significant impact on the trial's efficiency. In contrast,
even in the presence of substantial delay, a GSD will have a smaller expected
time to trial completion in comparison to a simple RCT. Greater efficiency is
lost with increase in the no. of stages. The timing of IAs also can impact the
efficiency of a GSDs with delay. Particularly, for unequally spaced IAs,
conducting IAs too early in the trial can be harmful for the design with delay.
"
2306.06698,2023-06-13,On the Confidence Intervals in Bioequivalence Studies,"  A bioequivalence study is a type of clinical trial designed to compare the
biological equivalence of two different formulations of a drug. Such studies
are typically conducted in controlled clinical settings with human subjects,
who are randomly assigned to receive two formulations. The two formulations are
then compared with respect to their pharmacokinetic profiles, which encompass
the absorption, distribution, metabolism, and elimination of the drug. Under
the guidance from Food and Drug Administration (FDA), for a size-$\alpha$
bioequivalence test, the standard approach is to construct a $100(1-2\alpha)\%$
confidence interval and verify if the confidence interval falls with the
critical region. In this work, we clarify that $100(1-2\alpha)\%$ confidence
interval approach for bioequivalence testing yields a size-$\alpha$ test only
when the two one-sided tests in TOST are ``equal-tailed''. Furthermore, a
$100(1-\alpha)\%$ confidence interval approach is also discussed in the
bioequivalence study.
"
2306.08840,2023-06-16,"The role of discretization scales in causal inference with
  continuous-time treatment","  There are well-established methods for identifying the causal effect of a
time-varying treatment applied at discrete time points. However, in the real
world, many treatments are continuous or have a finer time scale than the one
used for measurement or analysis. While researchers have investigated the
discrepancies between estimates under varying discretization scales using
simulations and empirical data, it is still unclear how the choice of
discretization scale affects causal inference. To address this gap, we present
a framework to understand how discretization scales impact the properties of
causal inferences about the effect of a time-varying treatment. We introduce
the concept of ""identification bias"", which is the difference between the
causal estimand for a continuous-time treatment and the purported estimand of a
discretized version of the treatment. We show that this bias can persist even
with an infinite number of longitudinal treatment-outcome trajectories. We
specifically examine the identification problem in a class of linear stochastic
continuous-time data-generating processes and demonstrate the identification
bias of the g-formula in this context. Our findings indicate that
discretization bias can significantly impact empirical analysis, especially
when there are limited repeated measurements. Therefore, we recommend that
researchers carefully consider the choice of discretization scale and perform
sensitivity analysis to address this bias. We also propose a simple and
heuristic quantitative measure for sensitivity concerning discretization and
suggest that researchers report this measure along with point and interval
estimates in their work. By doing so, researchers can better understand and
address the potential impact of discretization bias on causal inference.
"
2306.09882,2024-02-01,"Uncertainty Quantification via Spatial-Temporal Tweedie Model for
  Zero-inflated and Long-tail Travel Demand Prediction","  Understanding Origin-Destination (O-D) travel demand is crucial for
transportation management. However, traditional spatial-temporal deep learning
models grapple with addressing the sparse and long-tail characteristics in
high-resolution O-D matrices and quantifying prediction uncertainty. This
dilemma arises from the numerous zeros and over-dispersed demand patterns
within these matrices, which challenge the Gaussian assumption inherent to
deterministic deep learning models. To address these challenges, we propose a
novel approach: the Spatial-Temporal Tweedie Graph Neural Network (STTD). The
STTD introduces the Tweedie distribution as a compelling alternative to the
traditional 'zero-inflated' model and leverages spatial and temporal embeddings
to parameterize travel demand distributions. Our evaluations using real-world
datasets highlight STTD's superiority in providing accurate predictions and
precise confidence intervals, particularly in high-resolution scenarios.
"
2306.10663,2023-08-10,Index-mixed copulas,"  The class of index-mixed copulas is introduced and its properties are
investigated. Index-mixed copulas are constructed from given base copulas and a
random index vector, and show a rather remarkable degree of analytical
tractability. The analytical form of the copula and, if it exists, its density
are derived. As the construction is based on a stochastic representation,
sampling algorithms can be given. Properties investigated include bivariate and
trivariate margins, mixtures of index-mixed copulas, symmetries such as radial
symmetry and exchangeability, tail dependence, measures of concordance such as
Blomqvist's beta, Spearman's rho or Kendall's tau and concordance orderings.
Examples and illustrations are provided, and applications to the distribution
of sums of dependent random variables as well as the stress testing of general
dependence structures are given. A particularly interesting feature of
index-mixed copulas is that they allow one to provide a revealing
interpretation of the well-known family of Eyraud-Farlie-Gumbel-Morgenstern
(EFGM) copulas. Through the lens of index-mixing, one can explain why EFGM
copulas can only model a limited range of concordance and are tail independent,
for example. Index-mixed copulas do not suffer from such restrictions while
remaining analytically tractable.
"
2306.11906,2023-06-22,"Statistical thinking in simulation design: a continuing conversation on
  the balancing intercept problem","  Epidemiologists have a growing interest in employing computational approaches
to solve analytic problems, with simulation being arguably the most accessible
among all approaches. While previous literature discussed the utility of
simulation and demonstrated how to carry out them, few have focused on
connecting underlying statistical concepts to these simulation approaches,
creating gaps between theory and application. Based on the recent series of
discussions on the balancing intercept, we explain the growing complexity when
generalizing the balancing intercept to a wider class of simulations and revise
the closed-form equation for the balancing intercept under assumptions. The
discussion can broadly inform the future design of more complex simulations and
emphasize the importance of applying statistical thinking in the new era of
computational science.
"
2306.12200,2023-06-22,Using R for teaching and research,"  R is a language and environment for statistical computing and graphics, which
provides a wide variety of statistical tools (modeling, statistical testing,
time series analysis, classification problems, machine learning, ...), together
with amazing graphical techniques and the great advantage that it is highly
extensible. Nowadays, there is no doubt that it is the software par excellence
in statistical courses for any level, for theoretical and applied subjects
alike. Besides, it has become an almost essential tool for every research work
that involves any kind of analysis or data visualization. Furthermore, it is
one of the most employed programming languages for general purposes. The goal
of this work is helping to share ideas and resources to improve teaching and/or
research using the statistical software R. We will cover its benefits, show how
to get started and where to locate specific resources, and will make
interesting recommendations for using R, according to our experience. For the
classroom we will develop a curricular and assessment infrastructure to support
both dissemination and evaluation, while for research we will offer a broader
approach to quantitative studies that provides an excellent support for work in
science and technology.
"
2306.15380,2023-06-29,"Multivariate Rank-Based Analysis of Multiple Endpoints in Clinical
  Trials: A Global Test Approach","  Clinical trials often involve the assessment of multiple endpoints to
comprehensively evaluate the efficacy and safety of interventions. In the work,
we consider a global nonparametric testing procedure based on multivariate rank
for the analysis of multiple endpoints in clinical trials. Unlike other
existing approaches that rely on pairwise comparisons for each individual
endpoint, the proposed method directly incorporates the multivariate ranks of
the observations. By considering the joint ranking of all endpoints, the
proposed approach provides robustness against diverse data distributions and
censoring mechanisms commonly encountered in clinical trials. Through extensive
simulations, we demonstrate the superior performance of the multivariate
rank-based approach in controlling type I error and achieving higher power
compared to existing rank-based methods. The simulations illustrate the
advantages of leveraging multivariate ranks and highlight the robustness of the
approach in various settings. The proposed method offers an effective tool for
the analysis of multiple endpoints in clinical trials, enhancing the
reliability and efficiency of outcome evaluations.
"
2306.17547,2023-07-03,"Spaces of innovation and venture formation: the case of biotech in the
  United Kingdom","  Patents serve as valuable indicators of innovation and provide insights into
the spaces of innovation and venture formation within geographic regions. In
this study, we utilise patent data to examine the dynamics of innovation and
venture formation in the biotech sector across the United Kingdom (UK). By
analysing patents, we identify key regions that drive biotech innovation in the
UK. Our findings highlight the crucial role of biotech incubators in
facilitating knowledge exchange between scientific research and industry.
However, we observe that the incubators themselves do not significantly
contribute to the diversity of innovations which might be due to the underlying
effect of geographic proximity on the influences and impact of the patents.
These insights contribute to our understanding of the historical development
and future prospects of the biotech sector in the UK, emphasising the
importance of promoting innovation diversity and fostering inclusive enterprise
for achieving equitable economic growth.
"
2307.00025,2023-07-04,Extending the Bayesian Framework from Information to Action,"  In this review, we examine an extended Bayesian inference method and its
relation to biological information processing. We discuss the idea of combining
two modes of Bayesian inference. The first is the standard Bayesian inference,
which contracts probability space. The second is its inverse, which extends and
enriches the probability space of latent and observable variables. Their
combination has been observed that, greatly, facilitates discovery. Moreover,
this dual search during the updating process elucidates a crucial difference
between biological and artificial information processing. The latter is
restricted due to nonlinearities, while the former utilizes it. This duality is
ubiquitous in biological information process dynamics (`flee-or-fight',
`explore-or-exploit' etc.) as is the role of fractality and chaos in its
underlying nonequilibrium, nonlinear dynamics. We also propose a new
experimental set up that stems from testing these ideas.
"
2307.00590,2024-12-18,Orderings of extremes among dependent extended Weibull random variables,"  In this work, we consider two sets of dependent variables
$\{X_{1},\ldots,X_{n}\}$ and $\{Y_{1},\ldots,Y_{n}\}$, where $X_{i}\sim
EW(\alpha_{i},\lambda_{i},k_{i})$ and $Y_{i}\sim EW(\beta_{i},\mu_{i},l_{i})$,
for $i=1,\ldots, n$, which are coupled by Archimedean copulas having different
generators. Also, let $N_{1}$ and $N_{2}$ be two non-negative integer-valued
random variables, independent of $X_{i}'$s and $Y_{i}'$s, respectively. We then
establish different inequalities between two extremes, namely, $X_{1:n}$ and
$Y_{1:n}$ and $X_{n:n}$ and $Y_{n:n}$, in terms of the usual stochastic, star,
Lorenz, hazard rate, reversed hazard rate and dispersive orders. We also
establish some ordering results between $X_{1:{N_{1}}}$ and $Y_{1:{N_{2}}}$ and
$X_{{N_{1}}:{N_{1}}}$ and $Y_{{N_{2}}:{N_{2}}}$ in terms of the usual
stochastic order. Several examples and counterexamples are presented for
illustrating all the results established here. Some of the results here extend
the existing results of Barmalzan et al. (2020).
"
2307.04634,2023-07-11,Toward optimal placement of spatial sensors,"  This paper addresses the challenges of optimally placing a finite number of
sensors to detect Poisson-distributed targets in a bounded domain. We seek to
rigorously account for uncertainty in the target arrival model throughout the
problem. Sensor locations are selected to maximize the probability that no
targets are missed. While this objective function is well-suited to
applications where failure to detect targets is highly undesirable, it does not
lead to a computationally efficient optimization problem. We propose an
approximation of the objective function that is non-negative, submodular, and
monotone and for which greedy selection of sensor locations works well. We also
characterize the gap between the desired objective function and our
approximation. For numerical illustrations, we consider the case of the
detection of ship traffic using sensors mounted on the seafloor.
"
2307.05876,2023-08-06,"Multiple Correspondence and Proportional Analysis of Vaccination Rate
  Among Healthcare Personnel of MINSA","  DataProAnalytica is a powerful application for analyzing vaccination data in
health care professionals. Through visualizations and multiple correspondence
analysis, it uncovers meaningful relationships between variables and
categories. The results provide valuable information for improving vaccination
strategies. While there are limitations, the potential of DataProAnalytica to
improve accuracy and functionality makes it a promising tool for future
research and decision making in any other research topic.
"
2307.06293,2023-08-06,"Peru Mining: Analysis and Forecast of Mining Production in Peru Using
  Time Series and Data Science Techniques","  Peruvian mining plays a crucial role in the country's economy, being one of
the main producers and exporters of minerals worldwide. In this project, an
application was developed in RStudio that utilizes statistical analysis and
time series modeling techniques to understand and forecast mineral extraction
in different departments of Peru. The application includes an interactive map
that allows users to explore Peruvian geography and obtain detailed statistics
by clicking on each department. Additionally, bar charts, pie charts, and
frequency polygons were implemented to visualize and analyze the data. Using
the ARIMA model, predictions were made on the future extraction of minerals,
enabling informed decision-making in planning and resource management within
the mining sector. The application provides an interactive and accessible tool
to explore the Peruvian mining industry, comprehend trends, and make accurate
forecasts. These predictions for 2027 in total annual production are as
follows: Copper = 2,694,957 MT, Gold = 72,817.47 kg Fine, Zinc = 1,369,649 MT,
Silver = 3,083,036 MT, Lead = 255,443 MT, Iron = 15,776,609 MT, Tin = 29,542
MT, Molybdenum = 35,044.66 MT, and Cadmium = 724 MT. These predictions, based
on historical data, provide valuable information for strategic decision-making
and contribute to the sustainable development of the mining industry in Peru.
"
2307.07042,2023-07-17,Bayesian Analysis of Beta Autoregressive Moving Average Models,"  This work presents a Bayesian approach for the estimation of Beta
Autoregressive Moving Average ($\beta$ARMA) models. We discuss standard choice
for the prior distributions and employ a Hamiltonian Monte Carlo algorithm to
sample from the posterior. We propose a method to approach the problem of unit
roots in the model's systematic component. We then present a series of Monte
Carlo simulations to evaluate the performance of this Bayesian approach. In
addition to parameter estimation, we evaluate the proposed approach to verify
the presence of unit roots in the model's systematic component and study prior
sensitivity. An empirical application is presented to exemplify the usefulness
of the method. In the application, we compare the fitted Bayesian and
frequentist approaches in terms of their out-of-sample forecasting
capabilities.
"
2307.07058,2023-08-06,"Evaluation of Active Affiliates to the SIS Multidimensional Analysis in
  R Shiny","  This article presents a study that uses multiple linear regression analysis
to examine the factors influencing the number of people affiliated with
different insurance plans within the Comprehensive Health Insurance (SIS)
system in Peru.The study highlights the importance of multiple linear
regression analysis in understanding the factors that affect SIS Comprehensive
Health Insurance affiliates. It also showcases the value of utilizing
interactive tools like RShiny to enhance data analysis, providing a dynamic and
participatory experience for researchers and users interested in the subject.To
facilitate the analysis and visualization of SIS-related data, the researchers
developed an interactive application using RShiny. This tool allows for the
easy loading, visualization, and analysis of data in a user-friendly and
practical manner. By providing an interactive platform, users can effectively
explore and understand the factors that impact SIS affiliates.The results of
the analysis indicate that the selected variables have a significant positive
influence on the total number of affiliates. This suggests that the specific
insurance plan examined in this study has a favorable effect on the enrollment
of individuals in SIS. Additionally, the data shows a linear trend, supporting
the use of a linear regression model to describe this relationship.
  Active affiliates,Comprehensive health insurance SIS,Data
Visualization,Multiple Linear Regression Analysis,RShiny
"
2307.07571,2023-08-06,Prediction of breast cancer with 98% accuracy,"  Abstract Cancer is a tumor that affects people worldwide, with a higher
incidence in females but not excluding males. It ranks among the top five
deadliest types of cancer, particularly prevalent in less developed countries
with deficient healthcare programs. Finding the best algorithm for effective
breast cancer prediction with minimal error is crucial. In this scientific
article, we employed the SMOTE method in conjunction with the R package Shiny
to enhance the algorithms and improve prediction accuracy. We classified the
tumor types as benign and malignant (B/M). Various algorithms were analyzed
using a Kaggle dataset, and our study identified the superior algorithm as
logistic regression. We evaluated algorithm performance using confusion
matrices to visualize results and the ROC Curve to obtain a comprehensive
measure of performance. Additionally, we calculated precision by dividing the
number of correct predictions by the total predictions Keywords Breast cancer,
Smote, Benign, Malignant.
"
2307.09489,2023-07-20,J. B. S. Haldane's Rule of Succession,"  After Bayes, the oldest Bayesian account of enumerative induction is given by
Laplace's so-called rule of succession: if all $n$ observed instances of a
phenomenon to date exhibit a given character, the probability that the next
instance of that phenomenon will also exhibit the character is
$\frac{n+1}{n+2}$. Laplace's rule however has the apparently counterintuitive
mathematical consequence that the corresponding ""universal generalization""
(every future observation of this type will also exhibit that character) has
zero probability. In 1932, the British scientist J. B. S. Haldane proposed an
alternative rule giving a universal generalization the positive probability
$\frac{n+1}{n+2} \times \frac{n+3}{n+2}$. A year later Harold Jeffreys proposed
essentially the same rule in the case of a finite population. A related variant
rule results in a predictive probability of $\frac{n+1}{n+2} \times
\frac{n+4}{n+3}$. These arguably elegant adjustments of the original Laplacean
form have the advantage that they give predictions better aligned with
intuition and common sense. In this paper we discuss J. B. S. Haldane's rule
and its variants, placing them in their historical context, and relating them
to subsequent philosophical discussions.
"
2307.11056,2023-08-06,"DataXploreFines: Generalized Data for Informed Decision, Making, An
  Interactive Shiny Application for Data Analysis and Visualization","  This article presents DataXploreFines, an innovative Shiny application that
revolutionizes data exploration, analysis, and visualization. The application
offers functionalities for data loading, management, summarization, basic
graphs, advanced analysis, and contact. Users can upload their datasets in
popular formats like CSV or Excel, explore the data structure, perform
manipulations, and obtain statistical summaries. DataXploreFines provides a
wide range of interactive visualizations, including histograms, scatter plots,
bar charts, and line graphs, enabling users to identify patterns and trends.
Additionally, the application offers statistical tools such as time series
analysis using ARIMA and SARIMA models, forecasting, and Ljung-Box statistic.
Its user-friendly interface empowers individuals from various domains,
including beginners in statistics, to make informed decisions.
"
2307.11239,2023-07-24,Edgewise outliers of network indexed signals,"  We consider models for network indexed multivariate data involving a
dependence between variables as well as across graph nodes.
  In the framework of these models, we focus on outliers detection and
introduce the concept of edgewise outliers. For this purpose, we first derive
the distribution of some sums of squares, in particular squared Mahalanobis
distances that can be used to fix detection rules and thresholds for outlier
detection. We then propose a robust version of the deterministic MCD algorithm
that we call edgewise MCD. An application on simulated data shows the interest
of taking the dependence structure into account. We also illustrate the utility
of the proposed method with a real data set.
"
2307.12296,2023-08-06,"Comparative analysis using classification methods versus early stage
  diabetes","  In this research work, a comparative analysis was carried out using
classification methods such as: Discriminant Analysis and Logistic Regression
to subsequently predict whether a person may have the presence of early stage
diabetes. For this purpose, use was made of a database of the UC IRVINE
platform of the year 2020 where specific variables that influence diabetes were
used for a better result. Likewise in terms of methodology, the corresponding
analysis was performed for each of the 3 classification methods and then take
them to a comparative table and analyze the results obtained. Finally we can
add that the majority of the studies carried out applying the classification
methods to the diseases can be clearly seen that there is a certain attachment
and more use of the logistic regression classification method, on the other
hand, in the results we could see significant differences in terms of the 2
classification methods that were applied, which was valuable information for
later drawing final conclusions.
"
2307.12828,2024-04-09,"Stochastic Degree Sequence Model with Edge Constraints (SDSM-EC) for
  Backbone Extraction","  It is common to use the projection of a bipartite network to measure a
unipartite network of interest. For example, scientific collaboration networks
are often measured using a co-authorship network, which is the projection of a
bipartite author-paper network. Caution is required when interpreting the edge
weights that appear in such projections. However, backbone models offer a
solution by providing a formal statistical method for evaluating when an edge
in a projection is statistically significantly strong. In this paper, we
propose an extension to the existing Stochastic Degree Sequence Model (SDSM)
that allows the null model to include edge constraints (EC) such as prohibited
edges. We demonstrate the new SDSM-EC in toy data and empirical data on young
children's' play interactions, illustrating how it correctly omits noisy edges
from the backbone.
"
2307.12844,2023-08-06,"CATASTROAGRI -- Interactive data analysis and visualization application
  with a future projection for catastrophic agricultural insurance","  CATASTROAGRI is an application developed to load, analyze and interactively
visualize relevant data on catastrophic agricultural insurance. It also focuses
on the analysis of an ARIMA (0,1,1) (0,1,1) model to identify and estimate
patterns in the agricultural data of the Puno Region, it presents a decreasing
trend because there is a significant relationship between successive values of
the time series, We can also state that it is not stationary because the mean
and variance do not remain constant over time and the series has periods, and
it is observed that the cases are decreasing and increasing over the years,
especially the amount to indemnify due to the behavior of the climate in the
highlands. The results of the analysis show that agricultural insurance plays
an important role in protecting farmers against losses caused by adverse
climatic events. The importance of concentrating resources and indemnities on
the most affected crops and in the provinces with the highest agricultural
production is emphasized. The results of the users' evaluation showed a high
level of satisfaction, as well as ease of use.
"
2307.13748,2023-08-06,Time Series Analysis Applied to Notifications of Work Accidents,"  Time series analysis applied to occupational accident reports is a powerful
tool for understanding the evolution of occupational accidents over time. It
provides valuable information to make informed decisions. In this study, data
from reports of work accidents collected from the MINISTRY OF LABOR AND
EMPLOYMENT PROMOTION. MTPE were analyzed by time series. Significant patterns
and trends in accident reporting have been identified, leading to more
effective prevention strategies and better health and safety management.
"
2307.14371,2023-08-06,"Prediction of depression status in college students using a Naive Bayes
  classifier based machine learning model","  This study presents a machine learning model based on the Naive Bayes
classifier for predicting the level of depression in university students, the
objective was to improve prediction accuracy using a machine learning model
involving 70% training data and 30% validation data based on the Naive Bayes
classifier, the collected data includes factors associated with depression from
519 university students, the results showed an accuracy of 78.03%, high
sensitivity in detecting positive cases of depression, especially at moderate
and severe levels, and significant specificity in correctly classifying
negative cases, these findings highlight the effectiveness of the model in
early detection and treatment of depression, benefiting vulnerable sectors and
contributing to the improvement of mental health in the student population.
"
2307.16035,2023-09-11,Binary classification based Monte Carlo simulation,"  Acceptance-rejection (AR), Independent Metropolis Hastings (IMH) or
importance sampling (IS) Monte Carlo (MC) simulation algorithms all involve
computing ratios of probability density functions (pdfs). On the other hand,
classifiers discriminate labeled samples produced by a mixture of two
distributions and can be used for approximating the ratio of the two
corresponding pdfs.This bridge between simulation and classification enables us
to propose pdf-free versions of pdf-ratio-based simulation algorithms, where
the ratio is replaced by a surrogate function computed via a classifier. From a
probabilistic modeling perspective, our procedure involves a structured energy
based model which can easily be trained and is compatible with the classical
samplers.
"
2307.16048,2025-05-20,"Structural restrictions in local causal discovery: identifying direct
  causes of a target variable","  We consider the problem of learning a set of direct causes of a target
variable from an observational joint distribution. Learning directed acyclic
graphs (DAGs) that represent the causal structure is a fundamental problem in
science. Several results are known when the full DAG is identifiable from the
distribution, such as assuming a nonlinear Gaussian data-generating process.
Here, we are only interested in identifying the direct causes of one target
variable (local causal structure), not the full DAG. This allows us to relax
the identifiability assumptions and develop possibly faster and more robust
algorithms. In contrast to the Invariance Causal Prediction framework, we only
assume that we observe one environment without any interventions. We discuss
different assumptions for the data-generating process of the target variable
under which the set of direct causes is identifiable from the distribution.
While doing so, we put essentially no assumptions on the variables other than
the target variable. In addition to the novel identifiability results, we
provide two practical algorithms for estimating the direct causes from a finite
random sample and demonstrate their effectiveness on several benchmark and real
datasets.
"
2307.16744,2023-08-01,"A One-Parameter Diagnostic Classification Model with Familiar
  Measurement Properties","  Diagnostic classification models (DCMs) are psychometric models designed to
classify examinees according to their proficiency or non-proficiency of
specified latent characteristics. These models are well-suited for providing
diagnostic and actionable feedback to support formative assessment efforts.
Several DCMs have been developed and applied in different settings. This study
proposes a DCM with functional form similar to the 1-parameter logistic item
response theory model. Using data from a large-scale mathematics education
research study, we demonstrate that the proposed DCM has measurement properties
akin to the Rasch and 1-parameter logistic item response theory models,
including test score sufficiency, item-free and person-free measurement, and
invariant item and person ordering. We discuss the implications and limitations
of these developments, as well as directions for future research.
"
2308.01198,2024-10-08,"Analyzing the Reporting Error of Public Transport Trips in the Danish
  National Travel Survey Using Smart Card Data","  Household travel surveys have been used for decades to collect individuals
and households' travel behavior. However, self-reported surveys are subject to
recall bias, as respondents might struggle to recall and report their
activities accurately. This study examines the time reporting error of public
transit users in a nationwide household travel survey by matching, at the
individual level, five consecutive years of data from two sources, namely the
Danish National Travel Survey (TU) and the Danish Smart Card system
(Rejsekort). Survey respondents are matched with travel cards from the
Rejsekort data solely based on the respondents' declared spatiotemporal travel
behavior. Approximately, 70% of the respondents were successfully matched with
Rejsekort travel cards. The findings reveal a median time reporting error of
11.34 minutes, with an Interquartile Range of 28.14 minutes. Furthermore, a
statistical analysis was performed to explore the relationships between the
survey respondents' reporting error and their socio-economic and demographic
characteristics. The results indicate that females and respondents with a fixed
schedule are in general more accurate than males and respondents with a
flexible schedule in reporting their times of travel. Moreover, trips reported
during weekdays or via the internet displayed higher accuracies compared to
trips reported during weekends and holidays or via telephone interviews. This
disaggregated analysis provides valuable insights that could help in improving
the design and analysis of travel surveys, as well accounting for reporting
errors/biases in travel survey-based applications. Furthermore, it offers
valuable insights underlying the psychology of travel recall by survey
respondents.
"
2308.08004,2023-08-17,"The Mastery Rubric for Statistics and Data Science: promoting coherence
  and consistency in data science education and training","  Consensus based publications of both competencies and undergraduate
curriculum guidance documents targeting data science instruction for higher
education have recently been published. Recommendations for curriculum features
from diverse sources may not result in consistent training across programs. A
Mastery Rubric was developed that prioritizes the promotion and documentation
of formal growth as well as the development of independence needed for the 13
requisite knowledge, skills, and abilities for professional practice in
statistics and data science, SDS. The Mastery Rubric, MR, driven curriculum can
emphasize computation, statistics, or a third discipline in which the other
would be deployed or, all three can be featured. The MR SDS supports each of
these program structures while promoting consistency with international,
consensus based, curricular recommendations for statistics and data science,
and allows 'statistics', 'data science', and 'statistics and data science'
curricula to consistently educate students with a focus on increasing learners
independence. The Mastery Rubric construct integrates findings from the
learning sciences, cognitive and educational psychology, to support teachers
and students through the learning enterprise. The MR SDS will support higher
education as well as the interests of business, government, and academic work
force development, bringing a consistent framework to address challenges that
exist for a domain that is claimed to be both an independent discipline and
part of other disciplines, including computer science, engineering, and
statistics. The MR-SDS can be used for development or revision of an evaluable
curriculum that will reliably support the preparation of early e.g.,
undergraduate degree programs, middle e.g., upskilling and training programs,
and late e.g., doctoral level training practitioners.
"
2308.09045,2023-08-21,The Lindy Effect,"  The Lindy effect is a statistical tendency for things with longer pasts
behind them to have longer futures ahead. It has been experimentally confirmed
to apply to some categories, but not others, raising questions about when it is
applicable and why. I shed some light on these questions by examining the
mathematical properties required for the effect and generating mechanisms that
can produce them. While the Lindy effect is often thought to require a
declining hazard rate, I show that it arises very naturally even in cases with
constant (or increasing) hazard rates -- so long as there is a probability
distribution over the size of that rate. One implication is that even things
which are becoming less robust over time can display the Lindy effect.
"
2308.13061,2023-08-28,Spatial and Spatiotemporal Volatility Models: A Review,"  Spatial and spatiotemporal volatility models are a class of models designed
to capture spatial dependence in the volatility of spatial and spatiotemporal
data. Spatial dependence in the volatility may arise due to spatial spillovers
among locations; that is, if two locations are in close proximity, they can
exhibit similar volatilities. In this paper, we aim to provide a comprehensive
review of the recent literature on spatial and spatiotemporal volatility
models. We first briefly review time series volatility models and their
multivariate extensions to motivate their spatial and spatiotemporal
counterparts. We then review various spatial and spatiotemporal volatility
specifications proposed in the literature along with their underlying
motivations and estimation strategies. Through this analysis, we effectively
compare all models and provide practical recommendations for their appropriate
usage. We highlight possible extensions and conclude by outlining directions
for future research.
"
2308.14671,2023-08-29,"A generalized Bayesian stochastic block model for microbiome community
  detection","  Advances in next-generation sequencing technology have enabled the
high-throughput profiling of metagenomes and accelerated the microbiome study.
Recently, there has been a rise in quantitative studies that aim to decipher
the microbiome co-occurrence network and its underlying community structure
based on metagenomic sequence data. Uncovering the complex microbiome community
structure is essential to understanding the role of the microbiome in disease
progression and susceptibility. Taxonomic abundance data generated from
metagenomic sequencing technologies are high-dimensional and compositional,
suffering from uneven sampling depth, over-dispersion, and zero-inflation.
These characteristics often challenge the reliability of the current methods
for microbiome community detection. To this end, we propose a Bayesian
stochastic block model to study the microbiome co-occurrence network based on
the recently developed modified centered-log ratio transformation tailored for
microbiome data analysis. Our model allows us to incorporate taxonomic tree
information using a Markov random field prior. The model parameters are jointly
inferred by using Markov chain Monte Carlo sampling techniques. Our simulation
study showed that the proposed approach performs better than competing methods
even when taxonomic tree information is non-informative. We applied our
approach to a real urinary microbiome dataset from postmenopausal women, the
first time the urinary microbiome co-occurrence network structure has been
studied. In summary, this statistical methodology provides a new tool for
facilitating advanced microbiome studies.
"
2308.16252,2023-09-01,Bayesian questions with frequentist answers,"  The two statistical methods, namely the frequentist and the Bayesian methods,
are both commonly used for probabilistic inference in many scientific
situations. However, it is not straightforward to interpret the result of one
approach in terms of the concepts of the other. In this paper we explore the
possibility of finding a Bayesian significance for the frequentist's main
object of interest, the $p$-value, which is the probability assigned to the
proposition -- which we call the {\it extremity proposition} -- that a
measurement will result in a value that is at least as extreme as the value
that was actually obtained. To make contact with the frequentist language, the
Bayesian can choose to update probabilities based on the {\it extremity
proposition}, which is weaker than the standard Bayesian update proposition,
which uses the actual observed value. We then show that the posterior
probability (or probability density) of a theory is equal to the prior
probability (or probability density) multiplied by the ratio of the $p$-value
for the data obtained, given that theory, to the mean $p$-value -- averaged
over all theories weighted by their prior probabilities. Thus, we provide
frequentist answers to Bayesian questions. Our result is generic -- it does not
rely on restrictive assumptions about the situation under consideration or
specific properties of the likelihoods or the priors.
"
2309.01173,2023-09-06,Logic of subjective probability,"  In this paper I discuss both syntax and semantics of subjective probability.
The semantics determines ways of testing probability statements. Among
important varieties of subjective probabilities are intersubjective
probabilities and impersonal probabilities, and I will argue that well-tested
impersonal probabilities acquire features of objective probabilities.
Jeffreys's law, my next topic, states that two successful probability
forecasters must issue forecasts that are close to each other, thus supporting
the idea of objective probabilities. Finally, I will discuss connections
between subjective and frequentist probability.
"
2309.02234,2023-10-03,"Potential Outcomes and Decision Theoretic Foundations for Statistical
  Causality: Response to Richardson and Robins","  I thank Thomas Richardson and James Robins for their discussion of my paper,
and discuss the similarities and differences between their approach to causal
modelling, based on single world intervention graphs, and my own
decision-theoretic approach.
"
2309.02584,2024-06-04,Multivariate Mat\'ern Models -- A Spectral Approach,"  The classical Mat\'ern model has been a staple in spatial statistics. Novel
data-rich applications in environmental and physical sciences, however, call
for new, flexible vector-valued spatial and space-time models. Therefore, the
extension of the classical Mat\'ern model has been a problem of active
theoretical and methodological interest. In this paper, we offer a new
perspective to extending the Mat\'ern covariance model to the vector-valued
setting. We adopt a spectral, stochastic integral approach, which allows us to
address challenging issues on the validity of the covariance structure and at
the same time to obtain new, flexible, and interpretable models. In particular,
our multivariate extensions of the Mat\'ern model allow for asymmetric
covariance structures. Moreover, the spectral approach provides an essentially
complete flexibility in modeling the local structure of the process. We
establish closed-form representations of the cross-covariances when available,
compare them with existing models, simulate Gaussian instances of these new
processes, and demonstrate estimation of the model's parameters through maximum
likelihood. An application of the new class of multivariate Mat\'ern models to
environmental data indicate their success in capturing inherent
covariance-asymmetry phenomena.
"
2309.06601,2023-09-14,Monograf\'ia de Estad\'istica Bayesiana,"  Course notes about an introduction to Bayesian Statistics. First, an
explanation of the bayesian paradigm is motivated and explained in detail
(first three chapters). Then, a brief introduction to the basics about Decision
Theory in chapter four, which is self contained, with the purpose of
introducing parametrica bayesian inference as a decision problem in chapter
five.
"
2309.06983,2023-09-14,Creating Community in a Data Science Classroom,"  A community is a collection of people who know and care about each other. The
vast majority of college courses are not communities. This is especially true
of statistics and data science courses, both because our classes are larger and
because we are more likely to lecture. However, it is possible to create a
community in your classroom. This article offers an idiosyncratic set of
practices for creating community. I have used these techniques successfully in
first and second semester statistics courses with enrollments ranging from 40
to 120. The key steps are knowing names, cold calling, classroom seating, a
shallow learning curve, Study Halls, Recitations and rotating-one-on-one final
project presentations.
"
2309.07180,2023-09-19,"How do ASA Ethical Guidelines Support U.S. Guidelines for Official
  Statistics?","  In 2022, the American Statistical Association revised its Ethical Guidelines
for Statistical Practice. Originally issued in 1982, these Guidelines describe
responsibilities of the 'ethical statistical practitioner' to their profession,
to their research subjects, as well as to their community of practice. These
guidelines are intended as a framework to assist decision-making by
statisticians working across academic, research, and government environments.
For the first time, the 2022 Guidelines describe the ethical obligations of
organizations and institutions that use statistical practice. This paper
examines alignment between the ASA Ethical Guidelines and other
long-established normative guidelines for US official statistics: the OMB
Statistical Policy Directives 1, 2, and 2a NASEM Principles and Practices, and
the OMB Data Ethics Tenets. Our analyses ask how the recently updated ASA
Ethical Guidelines can support these guidelines for federal statistics and data
science. The analysis uses a form of qualitative content analysis, the
alignment model, to identify patterns of alignment, and potential for tensions,
within and across guidelines. The paper concludes with recommendations to
policy makers when using ethical guidance to establish parameters for policy
change and the administrative and technical controls that necessarily follow.
"
2309.08713,2024-03-13,"How does international guidance for statistical practice align with the
  ASA Ethical Guidelines?","  Gillikin (2017) defines a 'practice standard' as a document to 'define the
way the profession's body of knowledge is ethically translated into day-to-day
activities' (Gillikin 2017, p. 1). Such documents fulfill three objectives:
they 1) define the profession; 2) communicate uniform standards to
stakeholders; and 3) reduce conflicts between personal and professional conduct
(Gillikin, 2017 p. 2). However, there are many guidelines - this is due to
different purposes that guidance writers may have, as well as to the fact that
there are different audiences for the many guidance documents. The existence of
diverse statements do not necessarily make it clear that there are
commonalities; and while some statements are explicitly aspirational,
professionals as well as the public need to know that ethically-trained
practitioners follow accepted practice standards. This paper applies the
methodological approach described in Tractenberg (2023) and demonstrated in
Park and Tractenberg (2023) to study alignment among international guidance for
official statistics, and between these guidance documents and the ASA Ethical
Guidelines for Statistical Practice functioning as an ethical practice standard
(Tractenberg, 2022-A, 2022-B; after Gillikin 2017). In the spirit of exchanging
experiences and lessons learned, we discuss how our findings could inform
closer examination, clarification, and, if beneficial, possible revision of
guidance in the future.
"
2309.11739,2025-04-09,"Classroom Community amid Covid-19: A Mixed-Methods Study of
  Undergraduate Students in Introductory Mathematics and Statistics","  A strong sense of classroom community is associated with many positive
learning outcomes and is a critical contributor to undergraduate students'
persistence in STEM, particularly for women and students of color. This chapter
describes a mixed-methods investigation into the relationship between classroom
community and course attributes in introductory undergraduate mathematics and
statistics courses, mediated by student demographics. The project was motivated
by and conducted amid the Covid-19 pandemic: data were collected from online
courses in the 2021-21 academic year and from hybrid and in-person courses in
the 2021-22 academic year. Quantitative data was gathered from both students
and instructors and analyzed using structural equation modeling. The primary
instrument was the validated Classroom Community Scale - Short Form. These
quantitative results are complemented and contextualized by thematic and
textual analyses of focus group data, gathered using a newly developed protocol
piloted during the 2021-22 academic year. All data comes from a highly
selective private university in the United States. Preliminary practical
implications of the study include the value of synchronous participation in
fostering connectedness and the importance of attending to students' personal
identities in understanding their experiences of belonging.
"
2309.12374,2023-10-13,Rational Aversion to Information,"  Is more information always better? Or are there some situations in which more
information can make us worse off? Good (1967) argues that expected utility
maximizers should always accept more information if the information is
cost-free and relevant. But Good's argument presupposes that you are certain
you will update by conditionalization. If we relax this assumption and allow
agents to be uncertain about updating, these agents can be rationally required
to reject free and relevant information. Since there are good reasons to be
uncertain about updating, rationality can require you to prefer ignorance.
"
2309.12924,2025-01-15,"Automated grading workflows for providing personalized feedback to
  open-ended data science assignments","  Open-ended assignments - such as lab reports and semester-long projects -
provide data science and statistics students with opportunities for developing
communication, critical thinking, and creativity skills. However, providing
grades and formative feedback to open-ended assignments can be very time
consuming and difficult to do consistently across students. In this paper, we
discuss the steps of a typical grading workflow and highlight which steps can
be automated in an approach that we call automated grading workflow. We
illustrate how gradetools, a new R package, implements this approach within
RStudio to facilitate efficient and consistent grading while providing
individualized feedback. By outlining the motivations behind the development of
this package and the considerations underlying its design, we hope this article
will provide data science and statistics educators with ideas for improving
their grading workflows, possibly developing new grading tools or considering
use gradetools as their grading workflow assistant.
"
2309.16961,2024-07-31,"A review of Design of Experiments courses offered to undergraduate
  students at American universities","  Design of Experiments (DoE) is a relevant class to undergraduate students in
the sciences, because it teaches them how to plan, conduct, and analyze
experiments. In the literature on DoE, there are several contributions to its
pedagogy, such as easy-to-use class experiments, virtual experiments, and
software to construct experimental designs. However, there are virtually no
systematic evaluations of the actual DoE pedagogy. To address this issue, we
build the first database of DoE courses offered to undergraduate students in
the United States. The database has records on courses offered from 2019 to
2022 at the best universities in the US News Best National Universities ranking
of 2022. Specifically, it has data on 18 general and content-specific features
of 206 courses. To study the DoE pedagogy, we analyze the database using
descriptive statistics and text mining. Based on our analysis, we provide
instructors with recommendations and teaching material to enhance their DoE
courses. The database and material are included in the supplement of this
article.
"
2310.00002,2023-10-03,"The concept of probability, crisis in statistics, and the unbearable
  lightness of Bayesing","  Education in statistics, the application of statistics in scientific
research, and statistics itself as a scientific discipline are in crisis.
Within science, the main cause of the crisis is the insufficiently clarified
concept of probability. This article aims to separate the concept of
probability which is scientifically based from other concepts that do not have
this characteristic. The scientifically based concept of probability is
Kolmogorovs concept of probability models together with the conditions of their
applicability. Bayesian statistics is based on the subjective concept of
probability, and as such can only have a heuristic value in searching for the
truth, but it cannot and must not replace the truth. The way out of the crisis
should take Kolmogorov and Bayesian analysis as elements, each of which has a
well-defined and limited use. Only together with qualitative analysis and other
types of quantitative analysis, and combined with experiments, they can
contribute to reaching correct conclusions.
"
2310.00004,2023-10-03,Statistics for Machine Learning with Mathematica Applications,"  In recent years, the field of statistics has experienced a surge in interest
and application, largely due to significant advances in computer technology.
This progress has led to remarkable developments in statistics methods and
algorithms, enabling their widespread adoption across various disciplines. Key
areas benefiting from these advancements include machine learning, economics,
finance, geophysics, molecular modeling, computational systems biology,
operations research, and engineering. For example, in machine learning,
statistics forms the foundation for algorithms used in regression,
classification, clustering, and deep learning to analyze vast datasets and make
predictions. Mathematica, among other tools, has played a significant role in
enabling the integration of statistics and computer technology, facilitating
deeper exploration of data-driven insights and groundbreaking discoveries
across diverse domains. With a rich library of functions, Mathematica allows
users to calculate measures of central tendency, dispersion, and correlation,
as well as perform hypothesis testing and estimation. Moreover, it supports
probability distributions, making simulations and probabilistic modeling tasks
more accessible. This monograph presents the main theorems in mathematical
statistics, ranging from basic descriptive statistics to sophisticated
inferential techniques. In addition, we have created, more than 200 manipulates
cover different scenarios in statistics, more than 500 light Mathematica codes
(examples) and 25 programs (procedures) that follow the principles of testing
hypotheses and estimation theory. The code will run as-is with no code from
prior algorithms or third parties required beyond the installation of
Mathematica.
"
2310.00865,2023-10-03,Data Science at the Singularity,"  A purported `AI Singularity' has been in the public eye recently. Mass media
and US national political attention focused on `AI Doom' narratives hawked by
social media influencers. The European Commission is announcing initiatives to
forestall `AI Extinction'. In my opinion, `AI Singularity' is the wrong
narrative for what's happening now; recent happenings signal something else
entirely. Something fundamental to computation-based research really changed in
the last ten years. In certain fields, progress is dramatically more rapid than
previously, as the fields undergo a transition to frictionless reproducibility
(FR). This transition markedly changes the rate of spread of ideas and
practices, affects mindsets, and erases memories of much that came before.
  The emergence of frictionless reproducibility follows from the maturation of
3 data science principles in the last decade. Those principles involve data
sharing, code sharing, and competitive challenges, however implemented in the
particularly strong form of frictionless open services. Empirical Machine
Learning (EML) is todays leading adherent field, and its consequent rapid
changes are responsible for the AI progress we see. Still, other fields can and
do benefit when they adhere to the same principles.
  Many rapid changes from this maturation are misidentified. The advent of FR
in EML generates a steady flow of innovations; this flow stimulates outsider
intuitions that there's an emergent superpower somewhere in AI. This opens the
way for PR to push worrying narratives: not only `AI Extinction', but also the
supposed monopoly of big tech on AI research. The helpful narrative observes
that the superpower of EML is adherence to frictionless reproducibility
practices; these practices are responsible for the striking progress in AI that
we see everywhere.
"
2310.01533,2023-10-04,The fiducial-Bayes fusion: A general theory of statistical inference,"  An overview is presented of a general theory of statistical inference that is
referred to as the fiducial-Bayes fusion. This theory combines organic fiducial
inference and Bayesian inference. The aim is that the reader is given a clear
summary of the conceptual framework of the fiducial-Bayes fusion as well as
pointers to further reading about its more technical aspects. Particular
attention is paid to the issue of how much importance should be attached to the
role of Bayesian inference within this framework. The appendix contains a
substantive example of the application of the theory of the fiducial-Bayes
fusion, which supplements various other examples of the application of this
theory that are referenced in the paper.
"
2310.02444,2024-09-10,Philosophy within Data Science Ethics Courses,"  There is wide agreement that ethical considerations are a valuable aspect of
a data science curriculum, and to that end, many data science programs offer
courses in data science ethics. There are not always, however, explicit
connections between data science ethics and the centuries-old work on ethics
within the discipline of philosophy. Here, we present a framework for bringing
together key data science practices with ethics topics. The ethics topics were
collated from sixteen data science ethics courses with public-facing syllabi
and reading lists. We encourage individuals who are teaching data science
ethics to engage with the philosophical literature and its connection to
current data science practices, which are rife with potentially morally charged
decision points.
"
2310.03933,2023-10-09,On Fractional Spherically Restricted Hyperbolic Diffusion Random Field,"  The paper investigates solutions of the fractional hyperbolic diffusion
equation in its most general form with two fractional derivatives of distinct
orders. The solutions are given as spatial-temporal homogeneous and isotropic
random fields and their spherical restrictions are studied. The spectral
representations of these fields are derived and the associated angular spectrum
is analysed. The obtained mathematical results are illustrated by numerical
examples. In addition, the numerical investigations assess the dependence of
the covariance structure and other properties of these fields on the orders of
fractional derivatives.
"
2310.04153,2025-04-21,"Fair coins tend to land on the same side they started: Evidence from
  350,757 flips","  Many people have flipped coins but few have stopped to ponder the statistical
and physical intricacies of the process. We collected $350{,}757$ coin flips to
test the counterintuitive prediction from a physics model of human coin tossing
developed by Diaconis, Holmes, and Montgomery (DHM; 2007). The model asserts
that when people flip an ordinary coin, it tends to land on the same side it
started -- DHM estimated the probability of a same-side outcome to be about
51\%. Our data lend strong support to this precise prediction: the coins landed
on the same side more often than not, $\text{Pr}(\text{same side}) = 0.508$,
95\% credible interval (CI) [$0.506$, $0.509$], $\text{BF}_{\text{same-side
bias}} = 2359$. Furthermore, the data revealed considerable between-people
variation in the degree of this same-side bias. Our data also confirmed the
generic prediction that when people flip an ordinary coin -- with the initial
side-up randomly determined -- it is equally likely to land heads or tails:
$\text{Pr}(\text{heads}) = 0.500$, 95\% CI [$0.498$, $0.502$],
$\text{BF}_{\text{heads-tails bias}} = 0.182$. Furthermore, this lack of
heads-tails bias does not appear to vary across coins. Additional analyses
revealed that the within-people same-side bias decreased as more coins were
flipped, an effect that is consistent with the possibility that practice makes
people flip coins in a less wobbly fashion. Our data therefore provide strong
evidence that when some (but not all) people flip a fair coin, it tends to land
on the same side it started.
"
2310.11095,2023-11-01,Interview with Adrian Raftery,"  Professor Adrian E. Raftery is the Boeing International Professor of
Statistics and Sociology, and an adjunct professor of Atmospheric Sciences, at
the University of Washington in Seattle. He was born in Dublin, Ireland, and
obtained a B.A. in Mathematics and an M.Sc. in Statistics and Operations
Research at Trinity College Dublin. He obtained a doctorate in mathematical
statistics from the Universit\'e Pierre et Marie Curie under the supervision of
Paul Deheuvels. He was a lecturer in statistics at Trinity College Dublin, and
then an associate and full professor of statistics and sociology at the
University of Washington. He was the founding Director of the Center for
Statistics and Social Sciences.
  Professor Raftery has published over 200 articles in peer-reviewed
statistical, sociological and other journals. His research focuses on Bayesian
model selection and Bayesian model averaging, model-based clustering, inference
for deterministic models, and the development of new statistical methods for
demography, sociology, and the environmental and health sciences.
  He is a member of the United States National Academy of Sciences, a Fellow of
the American Academy of Arts and Sciences, an Honorary Member of the Royal
Irish Academy, a member of the Washington State Academy of Sciences, a Fellow
of the American Statistical Association, a Fellow of the Institute of
Mathematical Statistics, and an elected Member of the Sociological Research
Association. He has won multiple awards for his research. He was Coordinating
and Applications Editor of the Journal of the American Statistical Association
and Editor of Sociological Methodology. He was identified as the world's most
cited researcher in mathematics for the period 1995-2005.
  Thirty-three students have obtained Ph.D.'s working under Raftery's
supervision, of whom 21 hold or have held tenure-track university faculty
positions.
"
2310.11603,2023-11-28,Group sequential two-stage preference designs,"  The two-stage preference design (TSPD) enables the inference for treatment
efficacy while allowing for incorporation of patient preference to treatment.
It can provide unbiased estimates for selection and preference effects, where a
selection effect occurs when patients who prefer one treatment respond
differently than those who prefer another, and a preference effect is the
difference in response caused by an interaction between the patient's
preference and the actual treatment they receive. One potential barrier to
adopting TSPD in practice, however, is the relatively large sample size
required to estimate selection and preference effects with sufficient power. To
address this concern, we propose a group sequential two-stage preference design
(GS-TSPD), which combines TSPD with sequential monitoring for early stopping.
In the GS-TSPD, pre-planned sequential monitoring allows investigators to
conduct repeated hypothesis tests on accumulated data prior to full enrollment
to assess study eligibility for early trial termination without inflating type
I error rates. Thus, the procedure allows investigators to terminate the study
when there is sufficient evidence of treatment, selection, or preference
effects during an interim analysis, thereby reducing the design resource in
expectation. To formalize such a procedure, we verify the independent
increments assumption for testing the selection and preference effects and
apply group sequential stopping boundaries from the approximate sequential
density functions. Simulations are then conducted to investigate the operating
characteristics of our proposed GS-TSPD compared to the traditional TSPD. We
demonstrate the applicability of the design using a study of Hepatitis C
treatment modality.
"
2310.13826,2024-08-20,A p-value for Process Tracing and other N=1 Studies,"  We introduce a method for calculating \(p\)-values when testing causal
theories about a single case, for instance when conducting process tracing. As
in Fisher's (1935) original design, our \(p\)-value indicates how frequently
one would find the same or more favorable evidence while entertaining a rival
theory (the null) for the sake of argument. We use an urn model to represent
the null distribution and calibrate it to privilege false negative errors and
reduce false positive errors. We also present an approach to sensitivity
analysis and for representing the evidentiary weight of different observations.
Our test suits any type of evidence, such as data from interviews and archives,
observed in any combination. We apply our hypothesis test in two studies: a
process tracing classic about the cause of the cholera outburst in Soho (Snow
1855) and a recent process tracing based explanation of the cause of a welfare
policy shift in Uruguay (Rossel, Antia, and Manzi 2023).
"
2310.14726,2023-10-24,"Unraveling the Skillsets of Data Scientists: Text Mining Analysis of
  Dutch University Master Programs in Data Science and Artificial Intelligence","  The growing demand for data scientists in the global labor market and the
Netherlands has led to a rise in data science and artificial intelligence (AI)
master programs offered by universities. However, there is still a lack of
clarity regarding the specific skillsets of data scientists. This study aims to
address this issue by employing Correlated Topic Modeling (CTM) to analyse the
content of 41 master programs offered by seven Dutch universities. We assess
the differences and similarities in the core skills taught by these programs,
determine the subject-specific and general nature of the skills, and provide a
comparison between the different types of universities offering these programs.
Our findings reveal that research, data processing, statistics and ethics are
the predominant skills taught in Dutch data science and AI master programs,
with general universities emphasizing research skills and technical
universities focusing more on IT and electronic skills. This study contributes
to a better understanding of the diverse skillsets of data scientists, which is
essential for employers, universities, and prospective students.
"
2310.14808,2023-10-24,"The evolving of Data Science and the Saudi Arabia case. How much have we
  changed in 13 years?","  A comprehensive examination of data science vocabulary usage over the past 13
years in this work is conducted. The investigation commences with a dataset
comprising 16,018 abstracts that feature the term ""data science"" in either the
title, abstract, or keywords. The study involves the identification of
documents that introduce novel vocabulary and subsequently explores how this
vocabulary has been incorporated into scientific literature. To achieve these
objectives, I employ techniques such as Exploratory Data Analysis, Latent
Semantic Analysis, Latent Dirichlet Analysis, and N-grams Analysis. A
comparison of scientific publications between overall results and those
specific to Saudi Arabia is presented. Based on how the vocabulary is utilized,
representative articles are identified.
"
2310.15179,2023-10-25,"Reducing Uncertainty in Sea-level Rise Prediction: A
  Spatial-variability-aware Approach","  Given multi-model ensemble climate projections, the goal is to accurately and
reliably predict future sea-level rise while lowering the uncertainty. This
problem is important because sea-level rise affects millions of people in
coastal communities and beyond due to climate change's impacts on polar ice
sheets and the ocean. This problem is challenging due to spatial variability
and unknowns such as possible tipping points (e.g., collapse of Greenland or
West Antarctic ice-shelf), climate feedback loops (e.g., clouds, permafrost
thawing), future policy decisions, and human actions. Most existing climate
modeling approaches use the same set of weights globally, during either
regression or deep learning to combine different climate projections. Such
approaches are inadequate when different regions require different weighting
schemes for accurate and reliable sea-level rise predictions. This paper
proposes a zonal regression model which addresses spatial variability and model
inter-dependency. Experimental results show more reliable predictions using the
weights learned via this approach on a regional scale.
"
2310.16813,2024-08-28,Improving the Aggregation and Evaluation of NBA Mock Drafts,"  Many enthusiasts and experts publish forecasts of the order players are
drafted into professional sports leagues, known as mock drafts. Using a novel
dataset of mock drafts for the National Basketball Association (NBA), we
analyze authors' mock draft accuracy over time and ask how we can reasonably
use information from multiple authors. To measure how accurate mock drafts are,
we assume that both mock drafts and the actual draft are ranked lists, and we
propose that rank-biased distance (RBD) of Webber et al. (2010) is the
appropriate error metric for mock draft accuracy. This is because RBD allows
mock drafts to have a different length than the actual draft, accounts for
players not appearing in both lists, and weights errors early in the draft more
than errors later on. We validate that mock drafts, as expected, improve in
accuracy over the course of a season, and that accuracy of the mock drafts
produced right before their drafts is fairly stable across seasons. To be able
to combine information from multiple mock drafts into a single consensus mock
draft, we also propose a ranked-list combination method based on the ideas of
ranked-choice voting. We show that our method provides improved forecasts over
the standard Borda count combination method used for most similar analyses in
sports, and that either combination method provides a more accurate forecast
over time than any single author.
"
2311.00122,2023-11-02,"Statistical Network Analysis: Past, Present, and Future","  This article provides a brief overview of statistical network analysis, a
rapidly evolving field of statistics, which encompasses statistical models,
algorithms, and inferential methods for analyzing data in the form of networks.
Particular emphasis is given to connecting the historical developments in
network science to today's statistical network analysis, and outlining
important new areas for future research.
  This invited article is intended as a book chapter for the volume ""Frontiers
of Statistics and Data Science"" edited by Subhashis Ghoshal and Anindya Roy for
the International Indian Statistical Association Series on Statistics and Data
Science, published by Springer. This review article covers the material from
the short course titled ""Statistical Network Analysis: Past, Present, and
Future"" taught by the author at the Annual Conference of the International
Indian Statistical Association, June 6-10, 2023, at Golden, Colorado.
"
2311.00210,2023-11-02,"Broken Adaptive Ridge Method for Variable Selection in Generalized
  Partly Linear Models with Application to the Coronary Artery Disease Data","  Motivated by the CATHGEN data, we develop a new statistical learning method
for simultaneous variable selection and parameter estimation under the context
of generalized partly linear models for data with high-dimensional covariates.
The method is referred to as the broken adaptive ridge (BAR) estimator, which
is an approximation of the $L_0$-penalized regression by iteratively performing
reweighted squared $L_2$-penalized regression. The generalized partly linear
model extends the generalized linear model by including a non-parametric
component to construct a flexible model for modeling various types of covariate
effects. We employ the Bernstein polynomials as the sieve space to approximate
the non-parametric functions so that our method can be implemented easily using
the existing R packages. Extensive simulation studies suggest that the proposed
method performs better than other commonly used penalty-based variable
selection methods. We apply the method to the CATHGEN data with a binary
response from a coronary artery disease study, which motivated our research,
and obtained new findings in both high-dimensional genetic and low-dimensional
non-genetic covariates.
"
2311.05954,2023-11-13,Directional Gaussian spatial processes for South African wind data,"  Accurate wind pattern modelling is crucial for various applications,
including renewable energy, agriculture, and climate adaptation. In this paper,
we introduce the wrapped Gaussian spatial process (WGSP), as well as the
projected Gaussian spatial process (PGSP) custom-tailored for South Africa's
intricate wind behaviour. Unlike conventional models struggling with the
circular nature of wind direction, the WGSP and PGSP adeptly incorporate
circular statistics to address this challenge. Leveraging historical data
sourced from meteorological stations throughout South Africa, the WGSP and PGSP
significantly increase predictive accuracy while capturing the nuanced spatial
dependencies inherent to wind patterns. The superiority of the PGSP model in
capturing the structural characteristics of the South African wind data is
evident. As opposed to the PGSP, the WGSP model is computationally less
demanding, allows for the use of less informative priors, and its parameters
are more easily interpretable. The implications of this study are far-reaching,
offering potential benefits ranging from the optimisation of renewable energy
systems to the informed decision-making in agriculture and climate adaptation
strategies. The WGSP and PGSP emerge as robust and invaluable tools,
facilitating precise modelling of wind patterns within the dynamic context of
South Africa.
"
2311.09705,2023-11-17,"edibble: An R package to encapsulate elements of experimental designs
  for better planning, management and workflow","  I present an R package called edibble that facilitates the design of
experiments by encapsulating elements of the experiment in a series of
composable functions. This package is an interpretation of ""the grammar of
experimental designs"" by Tanaka (2023) in the R programming language. The main
features of the edibble package are demonstrated, illustrating how it can be
used to create a wide array of experimental designs. The implemented system
aims to encourage cognitive thinking for holistic planning and data management
of experiments in a streamlined workflow. This workflow can increase the
inherent value of experimental data by reducing potential errors or noise with
careful preplanning, as well as, ensuring fit-for-purpose analysis of
experimental data.
"
2311.10101,2023-11-20,Gaussian Differential Privacy on Riemannian Manifolds,"  We develop an advanced approach for extending Gaussian Differential Privacy
(GDP) to general Riemannian manifolds. The concept of GDP stands out as a
prominent privacy definition that strongly warrants extension to manifold
settings, due to its central limit properties. By harnessing the power of the
renowned Bishop-Gromov theorem in geometric analysis, we propose a Riemannian
Gaussian distribution that integrates the Riemannian distance, allowing us to
achieve GDP in Riemannian manifolds with bounded Ricci curvature. To the best
of our knowledge, this work marks the first instance of extending the GDP
framework to accommodate general Riemannian manifolds, encompassing curved
spaces, and circumventing the reliance on tangent space summaries. We provide a
simple algorithm to evaluate the privacy budget $\mu$ on any one-dimensional
manifold and introduce a versatile Markov Chain Monte Carlo (MCMC)-based
algorithm to calculate $\mu$ on any Riemannian manifold with constant
curvature. Through simulations on one of the most prevalent manifolds in
statistics, the unit sphere $S^d$, we demonstrate the superior utility of our
Riemannian Gaussian mechanism in comparison to the previously proposed
Riemannian Laplace mechanism for implementing GDP.
"
2312.00632,2023-12-04,A Conversation with A. Philip Dawid,"  Beginning in the 1970s, Alexander Philip Dawid has been a leading contributor
to the foundations of statistics and especially to the development and
application of Bayesian statistics. He is also known for his work on causality,
especially his notation for conditional independence and his critique of the
overuse of counterfactuals, and for his contributions to forensic statistics.
  Dawid was born in Lancashire, England, on February 1, 1946. His family moved
to London soon afterwards, and he attended the City of London School from 1956
to 1963. He studied mathematics at Cambridge, earning a BA (Bachelor of Arts)
degree in 1966. After earning a Diploma in Mathematical Statistics in the
academic year 1966-1967, he studied for a PhD at Imperial, then at UCL, where
he became a Lecturer in Statistics in 1969. In 1978, he left UCL for a position
as Professor of Statistics in the Department of Mathematics, The City
University, London, where he served as Head of Statistics Section and Director
of the Statistical Laboratory. He returned to the Department of Statistics at
UCL in 1981, serving as Head of Department from 1983 to 1993. He moved to the
University of Cambridge in 2007, becoming Professor of Statistics and Fellow of
Darwin College. He has continued his work in mathematical statistics after
retiring from Cambridge in 2013 and was elected Fellow of the Royal Society in
2018.
"
2312.03139,2023-12-07,A Bayesian Skew-heavy-tailed modelling for loss reserving,"  This paper focuses on modelling loss reserving to pay outstanding claims. As
the amount liable on any given claim is not known until settlement, we propose
a flexible model via heavy-tailed and skewed distributions to deal with
outstanding liabilities. The inference relies on Markov chain Monte Carlo via
Gibbs sampler with adaptive Metropolis algorithm steps allowing for fast
computations and providing efficient algorithms. An illustrative example
emulates a typical dataset based on a runoff triangle and investigates the
properties of the proposed models. Also, a case study is considered and shows
that the proposed model outperforms the usual loss reserving models well
established in the literature in the presence of skewness and heavy tails.
"
2312.04610,2025-03-19,"Data-Driven Semi-Supervised Machine Learning with Safety Indicators for
  Abnormal Driving Behavior Detection","  Detecting abnormal driving behavior is critical for road traffic safety and
the evaluation of drivers' behavior. With the advancement of machine learning
(ML) algorithms and the accumulation of naturalistic driving data, many ML
models have been adopted for abnormal driving behavior detection (also referred
to in this paper as ""anomalies""). Most existing ML-based detectors rely on
(fully) supervised ML methods, which require substantial labeled data. However,
ground truth labels are not always available in the real world, and labeling
large amounts of data is tedious. Thus, there is a need to explore unsupervised
or semi-supervised methods to make the anomaly detection process more feasible
and efficient. To fill this research gap, this study analyzes large-scale
real-world data revealing several abnormal driving behaviors (e.g., sudden
acceleration, rapid lane-changing) and develops a hierarchical extreme learning
machine (HELM)-based semi-supervised ML method using partly labeled data to
detect the identified abnormal driving behaviors. Moreover, previous ML-based
approaches predominantly utilized basic vehicle motion features (such as
velocity and acceleration) to label and detect abnormal driving behaviors,
while this study seeks to introduce event-level safety indicators as input
features for ML models to improve detection performance. Results from extensive
experiments demonstrate the effectiveness of the proposed semi-supervised ML
model with the introduced safety indicators serving as important features. The
proposed semi-supervised ML method outperforms other baseline semi-supervised
or unsupervised methods: for example, it delivers the best accuracy at 99.58%
and the best F1-score at 0.9913. The ablation study further highlights the
significance of safety indicators for advancing the detection performance of
abnormal driving behaviors.
"
2312.04898,2024-12-05,"Quantifying the effectiveness of linear preconditioning in Markov chain
  Monte Carlo","  We study linear preconditioning in Markov chain Monte Carlo. We consider the
class of well-conditioned distributions, for which several mixing time bounds
depend on the condition number $\kappa$. First we show that well-conditioned
distributions exist for which $\kappa$ can be arbitrarily large and yet no
linear preconditioner can reduce it. We then impose two sets of extra
assumptions under which a linear preconditioner can significantly reduce
$\kappa$. For the random walk Metropolis we further provide upper and lower
bounds on the spectral gap with tight $1/\kappa$ dependence. This allows us to
give conditions under which linear preconditioning can provably increase the
gap. We then study popular preconditioners such as the covariance, its diagonal
approximation, the hessian at the mode, and the QR decomposition. We show
conditions under which each of these reduce $\kappa$ to near its minimum. We
also show that the diagonal approach can in fact \textit{increase} the
condition number. This is of interest as diagonal preconditioning is the
default choice in well-known software packages. We conclude with a numerical
study comparing preconditioners in different models, and showing how proper
preconditioning can greatly reduce compute time in Hamiltonian Monte Carlo.
"
2312.08978,2024-10-28,"On the Uplink and Downlink EMF Exposure and Coverage in Dense Cellular
  Networks: A Stochastic Geometry Approach","  Existing studies analyzing electromagnetic field (EMFE) in wireless networks
have primarily considered downlink communications. In the uplink, the EMFE
caused by the user's smartphone is usually the only considered source of
radiation, thereby ignoring contributions caused by other active neighboring
devices. In addition, the network coverage and EMFE are typically analyzed
independently for both the uplink and downlink, while a joint analysis would be
necessary to fully understand the network performance and answer various
questions related to optimal network deployment. This paper bridges these gaps
by presenting an enhanced stochastic geometry framework that includes the above
aspects. The proposed topology features base stations modeled via a homogeneous
Poisson point process. The users active during a same time slot are distributed
according to a mixture of a Mat\'ern cluster process and a Gauss-Poisson
process, featuring groups of users possibly carrying several equipments. In
this paper, we derive the marginal and meta distributions of the downlink and
uplink EMFE and we characterize the uplink to downlink EMFE ratio. Moreover, we
derive joint probability metrics considering the uplink and downlink coverage
and EMFE. These metrics are evaluated in four scenarios considering BS, cluster
and/or intracluster densifications. Our numerical results highlight the
existence of optimal node densities maximizing these joint probabilities.
"
2312.09052,2023-12-15,"Applying Pre-Trained Deep-Learning Model on Wrist Angel Data -- An
  Analysis Plan","  We aim to investigate if we can improve predictions of stress caused by OCD
symptoms using pre-trained models, and present our statistical analysis plan in
this paper. With the methods presented in this plan, we aim to avoid bias from
data knowledge and thereby strengthen our hypotheses and findings. The Wrist
Angel study, which this statistical analysis plan concerns, contains data from
nine participants, between 8 and 17 years old, diagnosed with
obsessive-compulsive disorder (OCD). The data was obtained by an Empatica E4
wristband, which the participants wore during waking hours for 8 weeks. The
purpose of the study is to assess the feasibility of predicting the in-the-wild
OCD events captured during this period. In our analysis, we aim to investigate
if we can improve predictions of stress caused by OCD symptoms, and to do this
we have created a pre-trained model, trained on four open-source data for
stress prediction. We intend to apply this pre-trained model to the Wrist Angel
data by fine-tuning, thereby utilizing transfer learning. The pre-trained model
is a convolutional neural network that uses blood volume pulse, heart rate,
electrodermal activity, and skin temperature as time series windows to predict
OCD events. Furthermore, using accelerometer data, another model filters
physical activity to further improve performance, given that physical activity
is physiologically similar to stress. By evaluating various ways of applying
our model (fine-tuned, non-fine-tuned, pre-trained, non-pre-trained, and with
or without activity classification), we contextualize the problem such that it
can be assessed if transfer learning is a viable strategy in this domain.
"
2312.09796,2023-12-18,Better Foundations for Subjective Probability,"  How do we ascribe subjective probability? In decision theory, this question
is often addressed by representation theorems, going back to Ramsey (1926),
which tell us how to define or measure subjective probability by observable
preferences. However, standard representation theorems make strong rationality
assumptions, in particular expected utility maximization. How do we ascribe
subjective probability to agents which do not satisfy these strong rationality
assumptions? I present a representation theorem with weak rationality
assumptions which can be used to define or measure subjective probability for
partly irrational agents.
"
2312.10870,2024-08-15,Quantiles on global non-positive curvature spaces,"  This paper develops a notion of geometric quantiles on Hadamard spaces, also
known as global non-positive curvature spaces. After providing some definitions
and basic properties, including scaled isometry equivariance and a necessary
condition on the gradient of the quantile loss function at quantiles on
Hadamard manifolds, we investigate asymptotic properties of sample quantiles on
Hadamard manifolds, such as strong consistency and joint asymptotic normality.
We provide a detailed description of how to compute quantiles using a gradient
descent algorithm in hyperbolic space and, in particular, an explicit formula
for the gradient of the quantile loss function, along with experiments using
simulated and real single-cell RNA sequencing data.
"
2312.12149,2023-12-20,Bayesian and minimax estimators of loss,"  We study the problem of loss estimation that involves for an observable $X
\sim f_{\theta}$ the choice of a first-stage estimator $\hat{\gamma}$ of
$\gamma(\theta)$, incurred loss $L=L(\theta, \hat{\gamma})$, and the choice of
a second-stage estimator $\hat{L}$ of $L$. We consider both: (i) a sequential
version where the first-stage estimate and loss are fixed and optimization is
performed at the second-stage level, and (ii) a simultaneous version with a
Rukhin-type loss function designed for the evaluation of $(\hat{\gamma},
\hat{L})$ as an estimator of $(\gamma, L)$.
  We explore various Bayesian solutions and provide minimax estimators for both
situations (i) and (ii). The analysis is carried out for several probability
models, including multivariate normal models $N_d(\theta, \sigma^2 I_d)$ with
both known and unknown $\sigma^2$, Gamma, univariate and multivariate Poisson,
and negative binomial models, and relates to different choices of the
first-stage and second-stage losses. The minimax findings are achieved by
identifying least favourable of sequence of priors and depend critically on
particular Bayesian solution properties, namely situations where the
second-stage estimator $\hat{L}(x)$ is constant as a function of $x$.
"
2312.12645,2023-12-21,"Revisiting the effect of greediness on the efficacy of exchange
  algorithms for generating exact optimal experimental designs","  Coordinate exchange (CEXCH) is a popular algorithm for generating exact
optimal experimental designs. The authors of CEXCH advocated for a highly
greedy implementation - one that exchanges and optimizes single element
coordinates of the design matrix. We revisit the effect of greediness on CEXCHs
efficacy for generating highly efficient designs. We implement the
single-element CEXCH (most greedy), a design-row (medium greedy) optimization
exchange, and particle swarm optimization (PSO; least greedy) on 21 exact
response surface design scenarios, under the $D$- and $I-$criterion, which have
well-known optimal designs that have been reproduced by several researchers. We
found essentially no difference in performance of the most greedy CEXCH and the
medium greedy CEXCH. PSO did exhibit better efficacy for generating $D$-optimal
designs, and for most $I$-optimal designs than CEXCH, but not to a strong
degree under our parametrization. This work suggests that further investigation
of the greediness dimension and its effect on CEXCH efficacy on a wider suite
of models and criterion is warranted.
"
2312.13619,2023-12-22,The many routes to the ubiquitous Bradley-Terry model,"  The rating of items based on pairwise comparisons has been a topic of
statistical investigation for many decades. Numerous approaches have been
proposed. One of the best known is the Bradley-Terry model. This paper seeks to
assemble and explain a variety of motivations for its use. Some are based on
principles or on maximising an objective function; others are derived from
well-known statistical models, or stylised game scenarios. They include both
examples well-known in the literature as well as what are believed to be novel
presentations.
"
2401.01973,2024-12-12,"Facilitating the Integration of Ethical Reasoning into Quantitative
  Courses: Stakeholder Analysis, Ethical Practice Standards, and Case Studies","  Case studies are typically used to teach 'ethics', but in quantitative
courses it can seem distracting, for both instructor and learner, to introduce
a case analysis. Moreover, case analyses are typically focused on issues
relating to people: obtaining consent, dealing with research team members,
and/or potential institutional policy violations. While relevant to some
research, not all students in quantitative courses plan to become researchers,
and ethical practice is an essential topic for students of of mathematics,
statistics, data science, and computing regardless of whether or not the
learner intends to do research. Ethical reasoning is a way of thinking that
requires the individual to assess what they know about a potential ethical
problem (their prerequisite knowledge), and in some cases, how behaviors they
observe, are directed to perform, or have performed, diverge from what they
know to be ethical behavior. Ethical reasoning is a learnable, improvable set
of knowledge, skills, and abilities that enable learners to recognize what they
do and do not know about what constitutes 'ethical practice' of a discipline,
and in some cases, to contemplate alternative decisions about how to first
recognize, and then proceed past, or respond to, such divergences. A
stakeholder analysis is part of prerequisite knowledge, and can be used whether
there is or is not an actual case or situation to react to. In courses with
mainly quantitative content, a stakeholder analysis is a useful tool for
instruction and assessment. It can be used to both integrate authentic ethical
content and encourage careful quantitative thought. It is a mistake to treat
'training in ethical practice' and 'training in responsible conduct of
research' as the same thing. This paper discusses how to introduce ethical
reasoning, stakeholder analysis, and ethical practice standards authentically
in quantitative courses.
"
2401.02467,2024-01-08,ceylon: An R package for plotting the maps of Sri Lanka,"  The rapid evolution in the fields of computer science, data science, and
artificial intelligence has significantly transformed the utilisation of data
for decision-making. Data visualisation plays a critical role in any work that
involves data. Visualising data on maps is frequently encountered in many
fields. Visualising data on maps not only transforms raw data into visually
comprehensible representations but also converts complex spatial information
into simple, understandable form. Locating the data files necessary for map
creation can be a challenging task. Establishing a centralised repository can
alleviate the challenging task of finding shape files, allowing users to
efficiently discover geographic data. The ceylon R package is designed to make
simple feature data related to Sri Lanka's administrative boundaries and rivers
and streams accessible for a diverse range of R users. With straightforward
functionalities, this package allows users to quickly plot and explore
administrative boundaries and rivers and streams in Sri Lanka.
"
2401.03126,2024-07-30,Quotient geometry of bounded or fixed rank correlation matrices,"  This paper studies the quotient geometry of bounded or fixed-rank correlation
matrices. We establish a bijection between the set of bounded-rank correlation
matrices and a quotient set of a spherical product manifold by an orthogonal
group. We show that it forms an orbit space, whose stratification is determined
by the rank of the matrices, and the principal stratum has a compatible
Riemannian quotient manifold structure. We show that any minimizing geodesic in
the orbit space has constant rank on the interior of the segment. We also
develop efficient Riemannian optimization algorithms for computing the distance
and weighted the Frechet mean in the orbit space. Moreover, we examine
geometric properties of the quotient manifold, including horizontal and
vertical spaces, Riemannian metric, injectivity radius, exponential and
logarithmic map, curvature, gradient and Hessian. Finally, we apply our
approach to a functional connectivity study using the Autism Brain Imaging Data
Exchange.
"
2401.03891,2024-01-09,"Radius selection using kernel density estimation for the computation of
  nonlinear measures","  When nonlinear measures are estimated from sampled temporal signals with
finite-length, a radius parameter must be carefully selected to avoid a poor
estimation. These measures are generally derived from the correlation integral
which quantifies the probability of finding neighbors, i.e. pair of points
spaced by less than the radius parameter. While each nonlinear measure comes
with several specific empirical rules to select a radius value, we provide a
systematic selection method. We show that the optimal radius for nonlinear
measures can be approximated by the optimal bandwidth of a Kernel Density
Estimator (KDE) related to the correlation sum. The KDE framework provides
non-parametric tools to approximate a density function from finite samples
(e.g. histograms) and optimal methods to select a smoothing parameter, the
bandwidth (e.g. bin width in histograms). We use results from KDE to derive a
closed-form expression for the optimal radius. The latter is used to compute
the correlation dimension and to construct recurrence plots yielding an
estimate of Kolmogorov-Sinai entropy. We assess our method through numerical
experiments on signals generated by nonlinear systems and experimental
electroencephalographic time series.
"
2401.08622,2024-01-19,"Introduction to probability and statistics: a computational framework of
  randomness","  This text presents an unified approach of probability and statistics in the
pursuit of understanding and computation of randomness in engineering or
physical or social system with prediction with generalizability. Starting from
elementary probability and theory of distributions, the material progresses
towards conceptual and advances in prediction and generalization in statistical
models and large sample theory. We also pay special attention to unified
derivation approach and one-shot proof of each and every probabilistic concept.
Our presentation of intuitive and computation framework of conditional
distribution and probability are strongly influenced by unified patterns of
linear models for regression and for classification. The text ends with a
future note on the unified approximation of the linear models, the generalized
linear models and the discovery models to neural networks and a summarized ML
system.
"
2401.11000,2024-04-08,"Human-Centric and Integrative Lighting Asset Management in Public
  Libraries: Qualitative Insights and Challenges from a Swedish Field Study","  Traditional lighting source reliability evaluations, often covering just half
of a lamp's volume, can misrepresent real-world performance. To overcome these
limitations,adopting advanced asset management strategies for a more holistic
evaluation is crucial. This paper investigates human-centric and integrative
lighting asset management in Swedish public libraries. Through field
observations, interviews, and gap analysis, the study highlights a disparity
between current lighting conditions and stakeholder expectations, with issues
like eye strain suggesting significant improvement potential. We propose a
shift towards more dynamic lighting asset management and reliability
evaluations, emphasizing continuous enhancement and comprehensive training in
human-centric and integrative lighting principles.
"
2401.11096,2024-07-29,"Asymptotic Normality of the Conditional Value-at-Risk based Pickands
  Estimator","  The Pickands estimator for the extreme value index is beneficial due to its
universal consistency, location, and scale invariance, which sets it apart from
other types of estimators. However, similar to many extreme value index
estimators, it is marked by poor asymptotic efficiency. Chen (2021) introduces
a Conditional Value-at-Risk (CVaR)-based Pickands estimator, establishes its
consistency, and demonstrates through simulations that this estimator
significantly reduces mean squared error while preserving its location and
scale invariance. The initial focus of this paper is on demonstrating the weak
convergence of the empirical CVaR in functional space. Subsequently, based on
the established weak convergence, the paper presents the asymptotic normality
of the CVaR-based Pickands estimator. It further supports these theoretical
findings with empirical evidence obtained through simulation studies.
"
2401.15063,2024-01-30,Graph fission and cross-validation,"  We introduce a technique called graph fission which takes in a graph which
potentially contains only one observation per node (whose distribution lies in
a known class) and produces two (or more) independent graphs with the same
node/edge set in a way that splits the original graph's information amongst
them in any desired proportion. Our proposal builds on data fission/thinning, a
method that uses external randomization to create independent copies of an
unstructured dataset. We extend this idea to the graph setting where there may
be latent structure between observations. We demonstrate the utility of this
framework via two applications: inference after structural trend estimation on
graphs and a model selection procedure we term ""graph cross-validation"".
"
2401.17647,2025-01-10,Generative AI for Data Science 101: Coding Without Learning To Code,"  Should one teach coding in a required introductory statistics and data
science class for non-major students? Many professors advise against it,
considering it a distraction from the important and challenging statistical
topics that need to be covered. By contrast, other professors argue that the
ability to interact flexibly with data will inspire students with a lasting
love of the subject and a continued commitment to the material beyond the
introductory course. With the release of large language models that write code,
we saw an opportunity for a middle ground, which we tried in Fall 2023 in a
required introductory data science course in our school's full-time MBA
program. We taught students how to write English prompts to the artificial
intelligence tool Github Copilot that could be turned into R code and executed.
In this short article, we report on our experience using this new approach.
"
2402.00183,2024-05-16,"A review of regularised estimation methods and cross-validation in
  spatiotemporal statistics","  This review article focuses on regularised estimation procedures applicable
to geostatistical and spatial econometric models. These methods are
particularly relevant in the case of big geospatial data for dimensionality
reduction or model selection. To structure the review, we initially consider
the most general case of multivariate spatiotemporal processes (i.e., $g > 1$
dimensions of the spatial domain, a one-dimensional temporal domain, and $q
\geq 1$ random variables). Then, the idea of regularised/penalised estimation
procedures and different choices of shrinkage targets are discussed. Finally,
guided by the elements of a mixed-effects model setup, which allows for a
variety of spatiotemporal models, we show different regularisation procedures
and how they can be used for the analysis of geo-referenced data, e.g. for
selection of relevant regressors, dimensionality reduction of the covariance
matrices, detection of conditionally independent locations, or the estimation
of a full spatial interaction matrix.
"
2402.01112,2024-02-05,"Gerontologic Biostatistics 2.0: Developments over 10+ years in the age
  of data science","  Background: Introduced in 2010, the sub-discipline of gerontologic
biostatistics (GBS) was conceptualized to address the specific challenges in
analyzing data from research studies involving older adults. However, the
evolving technological landscape has catalyzed data science and statistical
advancements since the original GBS publication, greatly expanding the scope of
gerontologic research. There is a need to describe how these advancements
enhance the analysis of multi-modal data and complex phenotypes that are
hallmarks of gerontologic research. Methods: This paper introduces GBS 2.0, an
updated and expanded set of analytical methods reflective of the practice of
gerontologic biostatistics in contemporary and future research. Results: GBS
2.0 topics and relevant software resources include cutting-edge methods in
experimental design; analytical techniques that include adaptations of machine
learning, quantifying deep phenotypic measurements, high-dimensional -omics
analysis; the integration of information from multiple studies, and strategies
to foster reproducibility, replicability, and open science. Discussion: The
methodological topics presented here seek to update and expand GBS. By
facilitating the synthesis of biostatistics and data science in gerontology, we
aim to foster the next generation of gerontologic researchers.
"
2402.02233,2024-02-07,"Malaria incidence and prevalence: An ecological analysis through Six
  Sigma approach","  Malaria is the leading cause of death globally, especially in sub-Saharan
African countries claiming over 400,000 deaths globally each year, underscoring
the critical need for continued efforts to combat this preventable and
treatable disease. The objective of this study is to provide statistical
guidance on the optimal preventive and control measures against malaria. Data
have been collected from reliable sources, such as World Health Organization,
UNICEF, Our World in Data, and STATcompiler. Data were categorized according to
the factors and sub-factors related to deaths caused by malaria. These factors
and sub-factors were determined based on root cause analysis and data sources.
Using JMP 16 Pro software, both linear and multiple linear regression were
conducted to analyze the data. The analyses aimed to establish a linear
relationship between the dependent variable (malaria deaths in the overall
population) and independent variables, such as life expectancy, malaria
prevalence in children, net usage, indoor residual spraying usage, literate
population, and population with inadequate sanitation in each selected sample
country. The statistical analysis revealed that using insecticide treated nets
(ITNs) by children and individuals significantly decreased the death count, as
1,000 individuals sleeping under ITNs could reduce the death count by eight.
Based on the statistical analysis, this study suggests more rigorous research
on the usage of ITNs.
"
2402.07029,2025-03-24,Using Mathlink Cubes to Introduce Data Wrangling with Examples in R,"  This paper explores an innovative approach to teaching data wrangling skills
to students through hands-on activities before transitioning to coding. Data
wrangling, a critical aspect of data analysis, involves cleaning, transforming,
and restructuring data. We introduce the use of a physical tool, mathlink
cubes, to facilitate a tangible understanding of data sets. This approach helps
students grasp the concepts of data wrangling before implementing them in
coding languages such as R. We detail a classroom activity that includes
hands-on tasks paralleling common data wrangling processes such as filtering,
selecting, and mutating, followed by their coding equivalents using R's `dplyr`
package.
"
2402.08135,2024-02-14,"A scalable, synergy-first backbone decomposition of higher-order
  structures in complex systems","  Since its introduction in 2011, the partial information decomposition (PID)
has triggered an explosion of interest in the field of multivariate information
theory and the study of emergent, higher-order (""synergistic"") interactions in
complex systems. Despite its power, however, the PID has a number of
limitations that restrict its general applicability: it scales poorly with
system size and the standard approach to decomposition hinges on a definition
of ""redundancy"", leaving synergy only vaguely defined as ""that information not
redundant."" Other heuristic measures, such as the O-information, have been
introduced, although these measures typically only provided a summary statistic
of redundancy/synergy dominance, rather than direct insight into the synergy
itself. To address this issue, we present an alternative decomposition that is
synergy-first, scales much more gracefully than the PID, and has a
straightforward interpretation. Our approach defines synergy as that
information in a set that would be lost following the minimally invasive
perturbation on any single element. By generalizing this idea to sets of
elements, we construct a totally ordered ""backbone"" of partial synergy atoms
that sweeps systems scales. Our approach starts with entropy, but can be
generalized to the Kullback-Leibler divergence, and by extension, to the total
correlation and the single-target mutual information. Finally, we show that
this approach can be used to decompose higher-order interactions beyond just
information theory: we demonstrate this by showing how synergistic combinations
of pairwise edges in a complex network supports signal communicability and
global integration. We conclude by discussing how this perspective on
synergistic structure (information-based or otherwise) can deepen our
understanding of part-whole relationships in complex systems.
"
2402.10000,2024-02-16,"A computed 95% confidence interval does cover the true value with
  probability 0.95 if epistemically interpreted","  Suppose the lifetime of a large sample of batteries in routine use is
measured. A confidence interval is computed to 394 plus/minus 1.96 times 4.6
days. The standard interpretation is that if we repeatedly draw samples and
compute confidence intervals, about 95% of the intervals will cover the unknown
true lifetime. What can be said about the particular interval 394 plus/minus
1.96 times 4.6 has not been clear. We clarify this by using an epistemic
interpretation of probability. The conclusion is that a realised (computed)
confidence interval covers the parameter with the probability given by the
confidence level is a valid statement, unless there are relevant and
recognisable subsets of the sample.
"
2402.14414,2024-08-07,"The PORTSEA (Portuguese School of Extremes and Applications) and a few
  personal scientific achievements","  The Portuguese School of Extremes and Applications is nowadays well
recognised by the international scientific community, and in my opinion, the
organisation of a NATO Advanced Study Institute on Statistical Extremes and
Applications, which took place at Vimeiro in the summer of 1983, was a landmark
for the international recognition of the group. The dynamic of publication has
been very high and the topics under investigation in the area of Extremes have
been quite diverse. In this article, attention will be paid essentially to some
of the scientific achievements of the author in this field.
"
2402.18313,2024-10-23,"Exploring the impact of gamification on engagement in a statistics
  classroom","  In recent years, the integration of gamification into educational settings
has garnered significant attention as a means to enhance student engagement and
learning outcomes. By leveraging gamified elements such as points and
leaderboards, educators aim to promote active participation, motivation, and
deeper understanding among students. This study investigates the effect of
gamification on student engagement in a flipped statistics classroom
environment. The findings suggest that gamification strategies, when
effectively implemented, can have a positive impact on student motivation and
engagement. This paper concludes with recommendations for educators, potential
challenges such as superficial engagement and demotivation, and future
directions for research to address these challenges and further explore the
potential of gamification in fostering student success.
"
2402.19162,2025-04-10,"A Bayesian approach to uncover local and temporal determinants of
  heterogeneity in repeated cross-sectional health surveys","  In several countries, including Italy, a prominent approach to population
health surveillance involves conducting repeated cross-sectional surveys at
short intervals of time. These surveys gather information on the health status
of individual respondents, including details on their behaviours, risk factors,
and relevant socio-demographic information. While the collected data
undoubtedly provides valuable information, modelling such data presents several
challenges. For instance, in health risk models, it is essential to consider
behavioural information, local and temporal dynamics, and disease
co-occurrence. In response to these challenges, our work proposes a
multivariate temporal logistic model for chronic disease diagnoses at local
level. Linear predictors are modelled using individual risk factor covariates
and a latent individual propensity to diseases. Leveraging a state space
formulation of the model, we construct a framework in which temporal
heterogeneity in regression coefficients is informed by exogenous information
at local level, correspond ing to different contextual risk factors that may
affect the occurrence of chronic diseases in different ways. To explore the
utility and the effectiveness of our method, we analyse behavioural and risk
factor surveillance data collected in Italy (PASSI), which is well-known as a
country characterised by high peculiar administrative, social and territorial
diversities reflected on high variability in morbidity among population
subgroups.
"
2403.00776,2024-03-05,A framework for understanding data science,"  The objective of this research is to provide a framework with which the data
science community can understand, define, and develop data science as a field
of inquiry. The framework is based on the classical reference framework
(axiology, ontology, epistemology, methodology) used for 200 years to define
knowledge discovery paradigms and disciplines in the humanities, sciences,
algorithms, and now data science. I augmented it for automated problem-solving
with (methods, technology, community). The resulting data science reference
framework is used to define the data science knowledge discovery paradigm in
terms of the philosophy of data science addressed in previous papers and the
data science problem-solving paradigm, i.e., the data science method, and the
data science problem-solving workflow, both addressed in this paper. The
framework is a much called for unifying framework for data science as it
contains the components required to define data science. For insights to better
understand data science, this paper uses the framework to define the emerging,
often enigmatic, data science problem-solving paradigm and workflow, and to
compare them with their well-understood scientific counterparts, scientific
problem-solving paradigm and workflow.
"
2403.03387,2025-04-04,"A Systematic Literature Review of Undergraduate Data Science Education
  Research","  The presence of data science has been profound in the scientific community in
almost every discipline. An important part of the data science education
expansion has been at the undergraduate level. We conducted a systematic
literature review to (1) portray current evidence and knowledge gaps in
self-proclaimed undergraduate data science education research and (2) inform
policymakers and the data science education community about what educators may
encounter when searching for literature using the general keyword 'data science
education.' While open-access publications that target a broader audience of
data science educators and include multiple examples of data science programs
and courses are a strength, significant knowledge gaps remain. The
undergraduate data science literature that we identified often lacks empirical
data, research questions and reproducibility. Certain disciplines are less
visible. We recommend that we should (1) cherish data science as an
interdisciplinary field; (2) adopt a consistent set of keywords/terminology to
ensure data science education literature is easily identifiable; (3) prioritize
investments in empirical studies.
"
2403.03862,2024-03-07,"An analysis of the NCAA college football playoff team selections using
  an Elo ratings model","  In December 2023 the Florida State Seminoles became the first Power 5 school
to have an undefeated season and miss selection for the College Football
Playoff. In order to assess this decision, we employed an Elo ratings model to
rank the teams and found that the selection committee's decision was justified
and that Florida State were not one of the four best teams in college football
in that season (ranking only 11th!). We extended this analysis to all other
years of the CFP and found that the top four teams by Elo ratings differ
greatly from the four teams selected in almost every year of the CFP's
existence. Furthermore, we found that there have been more egregious
non-selections including when Alabama was ranked first by Elo ratings in 2022
and were not selected. The analysis suggests that the current criteria are too
subjective and a ratings model should be implemented to provide transparency
for the sport, its teams, and its fans.
"
2403.04914,2024-03-11,"Improving the Equation of Exchange for Cryptoasset Valuation Using
  Empirical Data","  In the evolving domain of cryptocurrency markets, accurate token valuation
remains a critical aspect influencing investment decisions and policy
development. Whilst the prevailing equation of exchange pricing model offers a
quantitative valuation approach based on the interplay between token price,
transaction volume, supply, and either velocity or holding time, it exhibits
intrinsic shortcomings. Specifically, the model may not consistently delineate
the relationship between average token velocity and holding time. This paper
aims to refine this equation, enhancing the depth of insight into token
valuation methodologies.
"
2403.08127,2024-05-01,Guidelines for the Creation of Analysis Ready Data,"  Globally, there is an increased need for guidelines to produce high-quality
data outputs for analysis. No framework currently exists that provides
guidelines for a comprehensive approach to producing analysis ready data (ARD).
Through critically reviewing and summarising current literature, this paper
proposes such guidelines for the creation of ARD. The guidelines proposed in
this paper inform ten steps in the generation of ARD: ethics, project
documentation, data governance, data management, data storage, data discovery
and collection, data cleaning, quality assurance, metadata, and data
dictionary. These steps are illustrated through a substantive case study that
aimed to create ARD for a digital spatial platform: the Australian Child and
Youth Wellbeing Atlas (ACYWA).
"
2403.10987,2024-12-11,"Risk Quadrangle and Robust Optimization Based on Extended
  $\varphi$-Divergence","  The Fundamental Risk Quadrangle (FRQ) is a unified framework linking risk
management, statistical estimation, and optimization. Distributionally robust
optimization (DRO) based on $\varphi$-divergence minimizes the maximal expected
loss, where the maximum is over a $\varphi$-divergence ambiguity set. This
paper introduces the \emph{extended} $\varphi$-divergence and the extended
$\varphi$-divergence quadrangle, which integrates DRO into the FRQ framework.
We derive the primal and dual representations of the quadrangle elements (risk,
deviation, regret, error, and statistic). The dual representation provides an
interpretation of classification, portfolio optimization, and regression as
robust optimization based on the extended $\varphi$-divergence. The primal
representation offers tractable formulations of these robust optimizations as
convex optimization. We provide illustrative examples showing that many common
problems, such as least-squares regression, quantile regression, support vector
machines, and CVaR optimization, fall within this framework. Additionally, we
conduct a case study to visualize the optimal solution of the inner
maximization in robust optimization.
"
2403.12110,2024-09-12,Robust estimations from distribution structures: I. Mean,"  As the most fundamental problem in statistics, robust location estimation has
many prominent solutions, such as the trimmed mean, Winsorized mean, Hodges
Lehmann estimator, Huber M estimator, and median of means. Recent studies
suggest that their maximum biases concerning the mean can be quite different,
but the underlying mechanisms largely remain unclear. This study exploited a
semiparametric method to classify distributions by the asymptotic orderliness
of quantile combinations with varying breakdown points, showing their
interrelations and connections to parametric distributions. Further deductions
explain why the Winsorized mean typically has smaller biases compared to the
trimmed mean; two sequences of semiparametric robust mean estimators emerge,
particularly highlighting the superiority of the median Hodges Lehmann mean.
This article sheds light on the understanding of the common nature of
probability distributions.
"
2403.13812,2024-03-22,"Quantitative Analysis of AI-Generated Texts in Academic Research: A
  Study of AI Presence in Arxiv Submissions using AI Detection Tool","  Many people are interested in ChatGPT since it has become a prominent AIGC
model that provides high-quality responses in various contexts, such as
software development and maintenance. Misuse of ChatGPT might cause significant
issues, particularly in public safety and education, despite its immense
potential. The majority of researchers choose to publish their work on Arxiv.
The effectiveness and originality of future work depend on the ability to
detect AI components in such contributions. To address this need, this study
will analyze a method that can see purposely manufactured content that academic
organizations use to post on Arxiv. For this study, a dataset was created using
physics, mathematics, and computer science articles. Using the newly built
dataset, the following step is to put originality.ai through its paces. The
statistical analysis shows that Originality.ai is very accurate, with a rate of
98%.
"
2403.13819,2024-03-22,"A machine learning approach to predict university enrolment choices
  through students' high school background in Italy","  This paper explores the influence of Italian high school students'
proficiency in mathematics and the Italian language on their university
enrolment choices, specifically focusing on STEM (Science, Technology,
Engineering, and Mathematics) courses. We distinguish between students from
scientific and humanistic backgrounds in high school, providing valuable
insights into their enrolment preferences. Furthermore, we investigate
potential gender differences in response to similar previous educational
choices and achievements. The study employs gradient boosting methodology,
known for its high predicting performance and ability to capture non-linear
relationships within data, and adjusts for variables related to the
socio-demographic characteristics of the students and their previous
educational achievements. Our analysis reveals significant differences in the
enrolment choices based on previous high school achievements. The findings shed
light on the complex interplay of academic proficiency, gender, and high school
background in shaping students' choices regarding university education, with
implications for educational policy and future research endeavours.
"
2403.14883,2024-06-04,Why Name Popularity is a Good Test of Historicity,"  Are name statistics in the Gospels and Acts a good test of historicity? Kamil
Gregor and Brian Blais, in a recent article in The Journal for the Study of the
Historical Jesus, argue that the sample of name occurrences in the Gospels and
Acts is too small to be determinative and that several statistical anomalies
weigh against a positive verdict. Unfortunately, their conclusions result
directly from improper testing and questionable data selection. Chi-squared
goodness-of-fit testing establishes that name occurrences in the Gospels and
Acts fit into their historical context at least as good as those in the works
of Josephus. Additionally, they fit better than occurrences derived from
ancient fictional sources and occurrences from modern, well-researched
historical novels.
"
2403.17018,2024-03-27,"Uncertainty quantification in the Henry problem using the multilevel
  Monte Carlo method","  We investigate the applicability of the well-known multilevel Monte Carlo
(MLMC) method to the class of density-driven flow problems, in particular the
problem of salinisation of coastal aquifers. As a test case, we solve the
uncertain Henry saltwater intrusion problem. Unknown porosity, permeability and
recharge parameters are modelled by using random fields. The classical
deterministic Henry problem is non-linear and time-dependent, and can easily
take several hours of computing time. Uncertain settings require the solution
of multiple realisations of the deterministic problem, and the total
computational cost increases drastically. Instead of computing of hundreds
random realisations, typically the mean value and the variance are computed.
The standard methods such as the Monte Carlo or surrogate-based methods is a
good choice, but they compute all stochastic realisations on the same, often,
very fine mesh. They also do not balance the stochastic and discretisation
errors. These facts motivated us to apply the MLMC method. We demonstrate that
by solving the Henry problem on multi-level spatial and temporal meshes, the
MLMC method reduces the overall computational and storage costs. To reduce the
computing cost further, parallelization is performed in both physical and
stochastic spaces. To solve each deterministic scenario, we run the parallel
multigrid solver ug4 in a black-box fashion.
"
2403.18951,2024-09-12,Robust estimations from distribution structures: V. Non-asymptotic,"  Due to the complexity of order statistics, the finite sample behaviour of
robust statistics is generally not analytically solvable. While the Monte Carlo
method can provide approximate solutions, its convergence rate is typically
very slow, making the computational cost to achieve the desired accuracy
unaffordable for ordinary users. In this paper, we propose an approach
analogous to the Fourier transformation to decompose the finite sample
structure of the uniform distribution. By obtaining sets of sequences that are
consistent with parametric distributions for the first four sample moments, we
can approximate the finite sample behavior of other estimators with
significantly reduced computational costs. This article reveals the underlying
structure of randomness and presents a novel approach to integrate multiple
assumptions.
"
2403.20007,2024-04-01,"Best Subset Solution Path for Linear Dimension Reduction Models using
  Continuous Optimization","  The selection of best variables is a challenging problem in supervised and
unsupervised learning, especially in high dimensional contexts where the number
of variables is usually much larger than the number of observations. In this
paper, we focus on two multivariate statistical methods: principal components
analysis and partial least squares. Both approaches are popular linear
dimension-reduction methods with numerous applications in several fields
including in genomics, biology, environmental science, and engineering. In
particular, these approaches build principal components, new variables that are
combinations of all the original variables. A main drawback of principal
components is the difficulty to interpret them when the number of variables is
large. To define principal components from the most relevant variables, we
propose to cast the best subset solution path method into principal component
analysis and partial least square frameworks. We offer a new alternative by
exploiting a continuous optimization algorithm for best subset solution path.
Empirical studies show the efficacy of our approach for providing the best
subset solution path. The usage of our algorithm is further exposed through the
analysis of two real datasets. The first dataset is analyzed using the
principle component analysis while the analysis of the second dataset is based
on partial least square framework.
"
2404.01902,2024-04-03,"Efficient estimation for a smoothing thin plate spline in a
  two-dimensional space","  Using a deterministic framework allows us to estimate a function with the
purpose of interpolating data in spatial statistics. Radial basis functions are
commonly used for scattered data interpolation in a d-dimensional space,
however, interpolation problems have to deal with dense matrices. For the case
of smoothing thin plate splines, we propose an efficient way to address this
problem by compressing the dense matrix by an hierarchical matrix
($\mathcal{H}$-matrix) and using the conjugate gradient method to solve the
linear system of equations. A simulation study was conducted to assess the
effectiveness of the spatial interpolation method. The results indicated that
employing an $\mathcal{H}$-matrix along with the conjugate gradient method
allows for efficient computations while maintaining a minimal error. We also
provide a sensitivity analysis that covers a range of smoothing and compression
parameter values, along with a Monte Carlo simulation aimed at quantifying
uncertainty in the approximated function. Lastly, we present a comparative
study between the proposed approach and thin plate regression using the ""mgcv""
package of the statistical software R. The comparison results demonstrate
similar interpolation performance between the two methods.
"
2404.02400,2025-05-15,"Improved Semi-Parametric Bounds for Tail Probability and Expected Loss:
  Theory and Applications","  Many management decisions involve accumulated random realizations for which
only the first and second moments of their distribution are available. The
sharp Chebyshev-type bound for the tail probability and Scarf bound for the
expected loss are widely used in this setting. We revisit the tail behavior of
such quantities with a focus on independence. Conventional primal-dual
approaches from optimization are ineffective in this setting. Instead, we use
probabilistic inequalities to derive new bounds and offer new insights. For
non-identical distributions attaining the tail probability bounds, we show that
the extreme values are equidistant regardless of the distributional
differences. For the bound on the expected loss, we show that the impact of
each random variable on the expected sum can be isolated using an extension of
the Korkine identity. We illustrate how these new results open up abundant
practical applications, including improved pricing of product bundles, more
precise option pricing, more efficient insurance design, and better inventory
management. For example, we establish a new solution to the optimal bundling
problem, yielding a 17% uplift in per-bundle profits, and a new solution to the
inventory problem, yielding a 5.6% cost reduction for a model with 20
retailers.
"
2404.03786,2024-04-08,"Non-Parametric Estimation of Multiple Periodic Components in Turkey's
  Electricity Consumption","  Electric generation and consumption are an essential component of
contemporary living, influencing diverse facets of our daily routines,
convenience, and economic progress. There is a high demand for characterizing
the periodic pattern of electricity consumption. VBPBB employs a bandpass
filter aligned to retain the frequency of a PC component and eliminating
interference from other components. This leads to a significant reduction in
the size of bootstrapped confidence intervals. Furthermore, other PC bootstrap
methods preserve one but not multiple periodically correlated components,
resulting in superior performance compared to other methods by providing a more
precise estimation of the sampling distribution for the desired
characteristics. The study of the periodic means of Turkey electricity
consumption using VBPBB is presented and compared with outcomes from
alternative bootstrapping approaches. These findings offer significant evidence
supporting the existence of daily, weekly, and annual PC patterns, along with
information on their timing and confidence intervals for their effects. This
information is valuable for enhancing predictions and preparations for future
responses to electricity consumption.
"
2404.08738,2024-04-26,"Seasonal and Periodic Patterns of PM2.5 in Manhattan using the Variable
  Bandpass Periodic Block Bootstrap","  Air quality is a critical component of environmental health. Monitoring and
analysis of particulate matter with a diameter of 2.5 micrometers or smaller
(PM2.5) plays a pivotal role in understanding air quality changes. This study
focuses on the application of a new bandpass bootstrap approach, termed the
Variable Bandpass Periodic Block Bootstrap (VBPBB), for analyzing time series
data which provides modeled predictions of daily mean PM2.5 concentrations over
16 years in Manhattan, New York, the United States. The VBPBB can be used to
explore periodically correlated (PC) principal components for this daily mean
PM2.5 dataset. This method uses bandpass filters to isolate distinct PC
components from datasets, removing unwanted interference including noise, and
bootstraps the PC components. This preserves the PC structure and permits a
better understanding of the periodic characteristics of time series data. The
results of the VBPBB are compared against outcomes from alternative block
bootstrapping techniques. The findings of this research indicate potential
trends of elevated PM2.5 levels, providing evidence of significant semi-annual
and weekly patterns missed by other methods.
"
2404.13664,2024-04-23,An Investigation into Distance Measures in Cluster Analysis,"  This report provides an exploration of different distance measures that can
be used with the $K$-means algorithm for cluster analysis. Specifically, we
investigate the Mahalanobis distance, and critically assess any benefits it may
have over the more traditional measures of the Euclidean, Manhattan and Maximum
distances. We perform this by first defining the metrics, before considering
their advantages and drawbacks as discussed in literature regarding this area.
We apply these distances, first to some simulated data and then to subsets of
the Dry Bean dataset [1], to explore if there is a better quality detectable
for one metric over the others in these cases. One of the sections is devoted
to analysing the information obtained from ChatGPT in response to prompts
relating to this topic.
"
2404.17271,2024-11-07,"To democratize research with sensitive data, we should make synthetic
  data more accessible","  For over 30 years, synthetic data has been heralded as a promising solution
to make sensitive datasets accessible. However, despite much research effort
and several high-profile use-cases, the widespread adoption of synthetic data
as a tool for open, accessible, reproducible research with sensitive data is
still a distant dream. In this opinion, Erik-Jan van Kesteren, head of the
ODISSEI Social Data Science team, argues that in order to progress towards
widespread adoption of synthetic data as a privacy enhancing technology, the
data science research community should shift focus away from developing better
synthesis methods: instead, it should develop accessible tools, educate peers,
and publish small-scale case studies.
"
2404.19495,2024-05-07,Percentage Coefficient (bp) -- Effect Size Analysis (Theory Paper 1),"  Percentage coefficient (bp) has emerged in recent publications as an
additional and alternative estimator of effect size for regression analysis.
This paper retraces the theory behind the estimator. It's posited that an
estimator must first serve the fundamental function of enabling researchers and
readers to comprehend an estimand, the target of estimation. It may then serve
the instrumental function of enabling researchers and readers to compare two or
more estimands. Defined as the regression coefficient when dependent variable
(DV) and independent variable (IV) are both on conceptual 0-1 percentage
scales, percentage coefficients (bp) feature 1) clearly comprehendible
interpretation and 2) equitable scales for comparison. The coefficient (bp)
serves the two functions effectively and efficiently. It thus serves needs
unserved by other indicators, such as raw coefficient (bw) and standardized
beta.
  Another premise of the functionalist theory is that ""effect"" is not a
monolithic concept. Rather, it is a collection of concepts, each of which
measures a component of the conglomerate called ""effect"", thereby serving a
subfunction. Regression coefficient (b), for example, indicates the unit change
in DV associated with a one-unit increase in IV, thereby measuring one aspect
called unit effect, aka efficiency. Percentage coefficient (bp) indicates the
percentage change in DV associated with a whole scale increase in IV. It is not
meant to be an all-encompassing indicator of an all-encompassing concept, but
rather a comprehendible and comparable indicator of efficiency, a key aspect of
effect.
"
2405.00884,2024-05-16,What's So Hard about the Monty Hall Problem?,"  The Monty Hall problem is notorious for its deceptive simplicity. Although
today it is widely used as a provocative thought experiment to introduce
Bayesian thinking to students of probability, in the not so distant past it was
rejected by established mathematicians. This essay provides some historical
background to the problem and explains why it is considered so
counter-intuitive to many. It is argued that the main barrier to understanding
the problem is the back-grounding of the concept of dependence in probability
theory as it is commonly taught. To demonstrate this, a Bayesian solution is
provided and augmented with a probabilistic graphical model (PGM) inspired by
the work of Pearl (1988, 1998). Although the Bayesian approach produces the
correct answer, without a representation of the dependency structure of events
implied by the problem, the salient fact that motivates the problem's solution
remains hidden.
"
2405.01342,2024-05-03,"Strategies for Rare Population Detection and Sampling: A Methodological
  Approach in Liguria","  Economic policy sciences are constantly investigating the quality of
well-being of broad sections of the population in order to describe the current
interdependence between unequal living conditions, low levels of education and
a lack of integration into society. Such studies are often carried out in the
form of surveys, e.g. as part of the EU-SILC program. If the survey is designed
at national or international level, the results of the study are often used as
a reference by a broad range of public institutions. However, the sampling
strategy per se may not capture enough information to provide an accurate
representation of all population strata. Problems might arise from rare, or
hard-to-sample, populations and the conclusion of the study may be compromised
or unrealistic. We propose here a two-phase methodology to identify rare,
poorly sampled populations and then resample the hard-to-sample strata. We
focused our attention on the 2019 EU-SILC section concerning the Italian region
of Liguria. Methods based on dispersion indices or deep learning were used to
detect rare populations. A multi-frame survey was proposed as the sampling
design. The results showed that factors such as citizenship, material
deprivation and large families are still fundamental characteristics that are
difficult to capture.
"
2405.01904,2024-05-06,"Which Identities Are Mobilized: Towards an automated detection of social
  group appeals in political texts","  This paper proposes a computational text classification strategy to identify
references to social groups in European party manifestos and beyond. Our
methodology uses machine learning techniques, including BERT and large language
models, to capture group-based appeals in texts. We propose to combine
automated identification of social groups using the Mistral-7B-v0.1 Large
Language Model with Embedding Space-based filtering to extend a sample of core
social groups to all social groups mentioned in party manifestos. By applying
this approach to RRP's and mainstream parties' group images in manifestos, we
explore whether electoral dynamics explain similarities in group appeals and
potential convergence or divergence in party strategies. Contrary to
expectations, increasing RRP support or mainstream parties' vote loss does not
necessarily lead to convergence in group appeals. Nonetheless, our methodology
enables mapping similarities in group appeals across time and space in 15
European countries from 1980 to 2021 and can be transferred to other use cases
as well.
"
2405.07102,2025-03-18,"Nested Instrumental Variables Design: Switcher Average Treatment Effect,
  Identification, Efficient Estimation and Generalizability","  Instrumental variables (IV) are a commonly used tool to estimate causal
effects from non-randomized data. An archetype of an IV is a randomized trial
with non-compliance where the randomized treatment assignment serves as an IV
for the non-ignorable treatment received. Under a monotonicity assumption, a
valid IV non-parametrically identifies the average treatment effect among a
non-identified, latent complier subgroup, whose generalizability is often under
debate. In many studies, there could exist multiple versions of an IV, for
instance, different nudges to take the same treatment in different study sites
in a multicentre clinical trial. These different versions of an IV may result
in different compliance rates and offer a unique opportunity to study IV
estimates' generalizability. In this article, we introduce a novel nested IV
assumption and study identification of the average treatment effect among two
latent subgroups: always-compliers and switchers, who are defined based on the
joint potential treatment received under two versions of a binary IV. We derive
the efficient influence function for the SWitcher Average Treatment Effect
(SWATE) under a non-parametric model and propose efficient estimators. We then
propose formal statistical tests of the generalizability of IV estimates under
the nested IV framework. We apply the proposed method to the Prostate, Lung,
Colorectal and Ovarian (PLCO) Cancer Screening Trial and study the causal
effect of colorectal cancer screening and its generalizability.
"
2405.08307,2024-05-15,"Sequential Maximal Updated Density Parameter Estimation for Dynamical
  Systems with Parameter Drift","  We present a novel method for generating sequential parameter estimates and
quantifying epistemic uncertainty in dynamical systems within a data-consistent
(DC) framework. The DC framework differs from traditional Bayesian approaches
due to the incorporation of the push-forward of an initial density, which
performs selective regularization in parameter directions not informed by the
data in the resulting updated density. This extends a previous study that
included the linear Gaussian theory within the DC framework and introduced the
maximal updated density (MUD) estimate as an alternative to both least squares
and maximum a posterior (MAP) estimates. In this work, we introduce algorithms
for operational settings of MUD estimation in real or near-real time where
spatio-temporal datasets arrive in packets to provide updated estimates of
parameters and identify potential parameter drift. Computational diagnostics
within the DC framework prove critical for evaluating (1) the quality of the DC
update and MUD estimate and (2) the detection of parameter value drift. The
algorithms are applied to estimate (1) wind drag parameters in a high-fidelity
storm surge model, (2) thermal diffusivity field for a heat conductivity
problem, and (3) changing infection and incubation rates of an epidemiological
model.
"
2405.08574,2024-05-15,"Predicting Short Response Ratings with Non-Content Related Features: A
  Hierarchical Modeling Approach","  We explore whether the human ratings of open ended responses can be explained
with non-content related features, and if such effects vary across different
mathematics-related items. When scoring is rigorously defined and rooted in a
measurement framework, educators intend that the features of a response which
are indicative of the respondent's level of ability are contributing to scores.
However, we find that features such as response length, a grammar score of the
response, and a metric relating to key phrase frequency are significant
predictors for response ratings. Although our findings are not causally
conclusive, they may propel us to be more critical of he way in which we assess
open ended responses, especially in high stakes scenarios. Educators take great
care to provide unbiased, consistent ratings, but it may be that extraneous
features unrelated to those which were intended to be rated are being
evaluated.
"
2405.09797,2024-05-21,Identification of Single-Treatment Effects in Factorial Experiments,"  Despite their cost, randomized controlled trials (RCTs) are widely regarded
as gold-standard evidence in disciplines ranging from social science to
medicine. In recent decades, researchers have increasingly sought to reduce the
resource burden of repeated RCTs with factorial designs that simultaneously
test multiple hypotheses, e.g. experiments that evaluate the effects of many
medications or products simultaneously. Here I show that when multiple
interventions are randomized in experiments, the effect any single intervention
would have outside the experimental setting is not identified absent heroic
assumptions, even if otherwise perfectly realistic conditions are achieved.
This happens because single-treatment effects involve a counterfactual world
with a single focal intervention, allowing other variables to take their
natural values (which may be confounded or modified by the focal intervention).
In contrast, observational studies and factorial experiments provide
information about potential-outcome distributions with zero and multiple
interventions, respectively. In this paper, I formalize sufficient conditions
for the identifiability of those isolated quantities. I show that researchers
who rely on this type of design have to justify either linearity of functional
forms or -- in the nonparametric case -- specify with Directed Acyclic Graphs
how variables are related in the real world. Finally, I develop nonparametric
sharp bounds -- i.e., maximally informative best-/worst-case estimates
consistent with limited RCT data -- that show when extrapolations about effect
signs are empirically justified. These new results are illustrated with
simulated data.
"
2405.10453,2024-05-20,"Expected Points Above Average: A Novel NBA Player Metric Based on
  Bayesian Hierarchical Modeling","  Team and player evaluation in professional sport is extremely important given
the financial implications of success/failure. It is especially critical to
identify and retain elite shooters in the National Basketball Association
(NBA), one of the premier basketball leagues worldwide because the ultimate
goal of the game is to score more points than one's opponent. To this end we
propose two novel basketball metrics: ""expected points"" for team-based
comparisons and ""expected points above average (EPAA)"" as a player-evaluation
tool. Both metrics leverage posterior samples from Bayesian hierarchical
modeling framework to cluster teams and players based on their shooting
propensities and abilities. We illustrate the concepts for the top 100 shot
takers over the last decade and offer our metric as an additional metric for
evaluating players.
"
2405.11284,2025-03-13,The Logic of Counterfactuals and the Epistemology of Causal Inference,"  The 2021 Nobel Prize in Economics recognized an epistemology of causal
inference based on the Rubin causal model (Rubin 1974), which merits broader
attention in philosophy. This model, in fact, presupposes a logical principle
of counterfactuals, Conditional Excluded Middle (CEM), the locus of a pivotal
debate between Stalnaker (1968) and Lewis (1973) on the semantics of
counterfactuals. Proponents of CEM should recognize that this connection points
to a new argument for CEM -- a Quine-Putnam indispensability argument grounded
in the Nobel-winning applications of the Rubin model in health and social
sciences. To advance the dialectic, I challenge this argument with an updated
Rubin causal model that retains its successes while dispensing with CEM. This
novel approach combines the strengths of the Rubin causal model and a causal
model familiar in philosophy, the causal Bayes net. The takeaway: deductive
logic and inductive inference, often studied in isolation, are deeply
interconnected.
"
2405.17224,2024-05-28,The Epistemology behind Covariate Adjustment,"  It is often asserted that to control for the effects of confounders, one
should include the confounding variables of concern in a statistical model as a
covariate. Conversely, it is also asserted that control can only be concluded
by design, where the results from an analysis can only be interpreted as
evidence of an effect because the design controlled for the cause. To suggest
otherwise is said to be a fallacy of cum hoc ergo propter hoc. Obviously, these
two assertions create a conundrum: How can the effect of confounder be
controlled for with analysis instead of by design without committing cum hoc
ergo propter hoc? The present manuscript answers this conundrum.
"
2405.18232,2024-07-09,Guidelines and Best Practices to Share Deidentified Data and Code,"  In 2022, the Journal of Statistics and Data Science Education (JSDSE)
instituted augmented requirements for authors to post deidentified data and
code underlying their papers. These changes were prompted by an increased focus
on reproducibility and open science (NASEM 2019). A recent review of data
availability practices noted that ""such policies help increase the
reproducibility of the published literature, as well as make a larger body of
data available for reuse and re-analysis"" (PLOS ONE, 2024). JSDSE values
accessibility as it endeavors to share knowledge that can improve educational
approaches to teaching statistics and data science. Because institution,
environment, and students differ across readers of the journal, it is
especially important to facilitate the transfer of a journal article's findings
to new contexts. This process may require digging into more of the details,
including the deidentified data and code. Our goal is to provide our readers
and authors with a review of why the requirements for code and data sharing
were instituted, summarize ongoing trends and developments in open science,
discuss options for data and code sharing, and share advice for authors.
"
2405.20156,2024-05-31,"Scaling up archival text analysis with the blockmodeling of n-gram
  networks -- A case study of Bulgaria's representation in the Osservatore
  Romano (January-May 1877)","  This paper seeks to bridge the gap between archival text analysis and network
analysis by applying network clustering methods to analyze the coverage of
Bulgaria in 123 issues of the newspaper Osservatore Romano published between
January and May 1877. Utilizing optical character recognition and generalized
homogeneity blockmodeling, the study constructs networks of relevant keywords.
Those including the sets Bulgaria and Russia are rather isomorphic and they
largely overlap with those for Germany, Britain, and War. In structural terms,
the blockmodel of the two networks exhibits a clear
core-semiperiphery-periphery structure that reflects relations between concepts
in the newpaper's coverage. The newspaper's lexical choices effectively
delegitimised the Bulgarian national revival, highlighting the influence of the
Holy See on the newspaper's editorial line.
"
2405.20415,2025-01-29,Differentially Private Boxplots,"  Despite the potential of differentially private data visualization to
harmonize data analysis and privacy, research in this area remains
underdeveloped. Boxplots are a widely popular visualization used for
summarizing a dataset and for comparison of multiple datasets. Consequentially,
we introduce a differentially private boxplot. We evaluate its effectiveness
for displaying location, scale, skewness and tails of a given empirical
distribution. In our theoretical exposition, we show that the location and
scale of the boxplot are estimated with optimal sample complexity, and the
skewness and tails are estimated consistently, which is not always the case for
a boxplot naively constructed from a single existing differentially private
quantile algorithm. As a byproduct of this exposition, we introduce several new
results concerning private quantile estimation. In simulations, we show that
this boxplot performs similarly to a non-private boxplot, and it outperforms
the naive boxplot. Additionally, we conduct a real data analysis of Airbnb
listings, which shows that comparable analysis can be achieved through
differentially private boxplot visualization.
"
2405.20601,2024-06-03,Bayesian Nonparametric Quasi Likelihood,"  A recent trend in Bayesian research has been revisiting generalizations of
the likelihood that enable Bayesian inference without requiring the
specification of a model for the data generating mechanism. This paper focuses
on a Bayesian nonparametric extension of Wedderburn's quasi-likelihood, using
Bayesian additive regression trees to model the mean function. Here, the
analyst posits only a structural relationship between the mean and variance of
the outcome. We show that this approach provides a unified, computationally
efficient, framework for extending Bayesian decision tree ensembles to many new
settings, including simplex-valued and heavily heteroskedastic data. We also
introduce Bayesian strategies for inferring the dispersion parameter of the
quasi-likelihood, a task which is complicated by the fact that the
quasi-likelihood itself does not contain information about this parameter;
despite these challenges, we are able to inject updates for the dispersion
parameter into a Markov chain Monte Carlo inference scheme in a way that, in
the parametric setting, leads to a Bernstein-von Mises result for the
stationary distribution of the resulting Markov chain. We illustrate the
utility of our approach on a variety of both synthetic and non-synthetic
datasets.
"
2406.05258,2024-06-11,"Advances in Machine Learning, Statistical Methods, and AI for
  Single-Cell RNA Annotation Using Raw Count Matrices in scRNA-seq Data","  Single-cell RNA sequencing (scRNA-seq) has revolutionized our ability to
analyze gene expression at the resolution of individual cells, providing
unprecedented insights into cellular heterogeneity and complex biological
systems. This paper reviews various advanced computational and machine learning
techniques tailored for the analysis of scRNA-seq data, emphasizing their roles
in different stages of the data processing pipeline.
"
2406.10612,2024-06-18,"Producing treatment hierarchies in network meta-analysis using
  probabilistic models and treatment-choice criteria","  A key output of network meta-analysis (NMA) is the relative ranking of the
treatments; nevertheless, it has attracted a lot of criticism. This is mainly
due to the fact that ranking is an influential output and prone to
over-interpretations even when relative effects imply small differences between
treatments. To date, common ranking methods rely on metrics that lack a
straightforward interpretation, while it is still unclear how to measure their
uncertainty. We introduce a novel framework for estimating treatment
hierarchies in NMA. At first, we formulate a mathematical expression that
defines a treatment choice criterion (TCC) based on clinically important
values. This TCC is applied to the study treatment effects to generate paired
data indicating treatment preferences or ties. Then, we synthesize the paired
data across studies using an extension of the so-called ""Bradley-Terry"" model.
We assign to each treatment a latent variable interpreted as the treatment
""ability"" and we estimate the ability parameters within a regression model.
Higher ability estimates correspond to higher positions in the final ranking.
We further extend our model to adjust for covariates that may affect treatment
selection. We illustrate the proposed approach and compare it with alternatives
in two datasets: a network comparing 18 antidepressants for major depression
and a network comparing 6 antihypertensives for the incidence of diabetes. Our
approach provides a robust and interpretable treatment hierarchy which accounts
for clinically important values and is presented alongside with uncertainty
measures. Overall, the proposed framework offers a novel approach for ranking
in NMA based on concrete criteria and preserves from over-interpretation of
unimportant differences between treatments.
"
2406.11940,2024-06-19,"Model-Based Inference and Experimental Design for Interference Using
  Partial Network Data","  The stable unit treatment value assumption states that the outcome of an
individual is not affected by the treatment statuses of others, however in many
real world applications, treatments can have an effect on many others beyond
the immediately treated. Interference can generically be thought of as mediated
through some network structure. In many empirically relevant situations
however, complete network data (required to adjust for these spillover effects)
are too costly or logistically infeasible to collect. Partially or indirectly
observed network data (e.g., subsamples, aggregated relational data (ARD),
egocentric sampling, or respondent-driven sampling) reduce the logistical and
financial burden of collecting network data, but the statistical properties of
treatment effect adjustments from these design strategies are only beginning to
be explored. In this paper, we present a framework for the estimation and
inference of treatment effect adjustments using partial network data through
the lens of structural causal models. We also illustrate procedures to assign
treatments using only partial network data, with the goal of either minimizing
estimator variance or optimally seeding. We derive single network asymptotic
results applicable to a variety of choices for an underlying graph model. We
validate our approach using simulated experiments on observed graphs with
applications to information diffusion in India and Malawi.
"
2406.14784,2024-06-24,Active Learning for Fair and Stable Online Allocations,"  We explore an active learning approach for dynamic fair resource allocation
problems. Unlike previous work that assumes full feedback from all agents on
their allocations, we consider feedback from a select subset of agents at each
epoch of the online resource allocation process. Despite this restriction, our
proposed algorithms provide regret bounds that are sub-linear in number of
time-periods for various measures that include fairness metrics commonly used
in resource allocation problems and stability considerations in matching
mechanisms. The key insight of our algorithms lies in adaptively identifying
the most informative feedback using dueling upper and lower confidence bounds.
With this strategy, we show that efficient decision-making does not require
extensive feedback and produces efficient outcomes for a variety of problem
classes.
"
2406.17567,2024-06-26,Transfer Learning for High Dimensional Robust Regression,"  Transfer learning has become an essential technique for utilizing information
from source datasets to improve the performance of the target task. However, in
the context of high-dimensional data, heterogeneity arises due to
heteroscedastic variance or inhomogeneous covariate effects. To solve this
problem, this paper proposes a robust transfer learning based on the Huber
regression, specifically designed for scenarios where the transferable source
data set is known. This method effectively mitigates the impact of data
heteroscedasticity, leading to improvements in estimation and prediction
accuracy. Moreover, when the transferable source data set is unknown, the paper
introduces an efficient detection algorithm to identify informative sources.
The effectiveness of the proposed method is proved through numerical simulation
and empirical analysis using superconductor data.
"
2406.19414,2024-07-01,"Stock Volume Forecasting with Advanced Information by Conditional
  Variational Auto-Encoder","  We demonstrate the use of Conditional Variational Encoder (CVAE) to improve
the forecasts of daily stock volume time series in both short and long term
forecasting tasks, with the use of advanced information of input variables such
as rebalancing dates. CVAE generates non-linear time series as out-of-sample
forecasts, which have better accuracy and closer fit of correlation to the
actual data, compared to traditional linear models. These generative forecasts
can also be used for scenario generation, which aids interpretation. We further
discuss correlations in non-stationary time series and other potential
extensions from the CVAE forecasts.
"
2406.19885,2024-10-10,"Fractal dimension, and the problems traps of its estimation","  This chapter deals with error and uncertainty in data. Treats their measuring
methods and meaning. It shows that uncertainty is a natural property of many
data sets. Uncertainty is fundamental for the survival os living species,
Uncertainty of the ""chaos"" type occurs in many systems, is fundamental to
understand these systems.
"
2407.00292,2024-07-02,Interpret the estimand framework from a causal inference perspective,"  The estimand framework proposed by ICH in 2017 has brought fundamental
changes in the pharmaceutical industry. It clearly describes how a treatment
effect in a clinical question should be precisely defined and estimated,
through attributes including treatments, endpoints and intercurrent events.
However, ideas around the estimand framework are commonly in text, and
different interpretations on this framework may exist. This article aims to
interpret the estimand framework through its underlying theories, the causal
inference framework based on potential outcomes. The statistical origin and
formula of an estimand is given through the causal inference framework, with
all attributes translated into statistical terms. How five strategies proposed
by ICH to analyze intercurrent events are incorporated in the statistical
formula of an estimand is described, and a new strategy to analyze intercurrent
events is also suggested. The roles of target populations and analysis sets in
the estimand framework are compared and discussed based on the statistical
formula of an estimand. This article recommends continuing study of causal
inference theories behind the estimand framework and improving the estimand
framework with greater methodological comprehensibility and availability.
"
2407.02401,2024-11-18,Novel Fuzzy Centrality Measures in Vague Social Networks,"  Social network analysis (SNA) helps us understand the relationships and
interactions between individuals, groups, organizations, or other social
entities. In the literature, ties are generally considered binary or weighted
based on their strength. Nonetheless, when the actors are individuals, these
relationships are often imprecise, and identifying them with simple scalars
leads to information loss. Indeed, social relationships are often vague in real
life, and although previous research has proposed the use of fuzzy networks,
these are typically characterized by crisp ties. The use of weighted links does
not align with the original philosophy of fuzzy logic, which instead aims to
preserve the vagueness inherent in human language and real life. For this
reason, this paper proposes a generalization of the so-called Fuzzy Social
Network Analysis (FSNA) to the context of imprecise relationships among actors.
Dealing with imprecise ties and introducing fuzziness in the definition of
relationships requires an extension of social network analysis, defining ties
as fuzzy numbers instead of crisp values and extending classical centrality
indices to fuzzy centrality indexes. The article presents the theory and
application of real data collected through a fascinating mouse-tracking
technique to study the fuzzy relationships in a collaboration network among the
members of a university department.
"
2407.05572,2025-01-23,"Reducing Total Trip Time and Vehicle Emission through Park-and-Ride --
  methods and case-study","  This study addresses important issues of traffic congestion and vehicle
emissions in urban areas by developing a comprehensive mathematical framework
to evaluate Park-and-Ride (PnR) systems. The proposed approach integrates
queueing theory and emissions modeling to simultaneously assess waiting times,
travel times, and vehicle emissions under various PnR usage scenarios. The
methodology employs a novel combination of Monte Carlo simulation and matrix
geometric analytic methods to analyze a queueing network representing PnR
facilities and road traffic. A case study of Tsukuba, Japan demonstrates the
model's applicability, revealing potential reductions in social costs related
to total trip time and emissions through optimized PnR policies. Specifically,
the study found that implementing optimal bus frequency and capacity policies
could reduce total social costs by up to 30\% compared to current conditions.
This research contributes to the literature by providing a unified framework
for evaluating PnR systems that considers both time and environmental costs,
offering valuable insights for urban planners and policymakers seeking to
improve transportation sustainability. The proposed model utilizes a single
server queue with a deterministic service time and multiple arrival streams to
represent traffic flow, incorporating both private cars and public buses.
Emissions are calculated using the Methodologies for Estimating Air Pollutant
Emissions from Transport (MEET) framework. The social cost of emissions and
total trip time (SCETT) is introduced as a comprehensive metric for evaluating
PnR system performance.
"
2407.06090,2024-07-09,"The Need for a Recurring Large-Scale Benchmarking Survey to Continually
  Evaluate Sampling Methods and Administration Modes: Lessons from the 2022
  Collaborative Midterm Survey","  As survey methods adapt to technological and societal changes, a growing body
of research seeks to understand the tradeoffs associated with various sampling
methods and administration modes. We show how the NSF-funded 2022 Collaborative
Midterm Survey (CMS) can be used as a dynamic and transparent framework for
evaluating which sampling approaches - or combination of approaches - are best
suited for various research goals. The CMS is ideally suited for this purpose
because it includes almost 20,000 respondents interviewed using two
administration modes (phone and online) and data drawn from random digit
dialing, random address-based sampling, a probability-based panel, two
nonprobability panels, and two nonprobability marketplaces. The analysis
considers three types of population benchmarks (election data, administrative
records, and large government surveys) and focuses on the national-level
estimates as well as oversamples in three states (California, Florida, and
Wisconsin). In addition to documenting how each of the survey strategies
performed, we develop a strategy to assess how different combinations of
approaches compare to different population benchmarks in order to guide
researchers combining sampling methods and sources. We conclude by providing
specific recommendations to public opinion and election survey researchers and
demonstrating how our approach could be applied to a large government survey
conducted at regular intervals to provide ongoing guidance to researchers,
government, businesses, and nonprofits regarding the most appropriate survey
sampling and administration methods.
"
2407.08835,2024-07-15,"More than Formulas -- Integrity, Communication, Computing and
  Reproducibility in Statistics Education","  This paper introduces a novel course design in the Master Program in
Biostatistics at the University of Zurich that integrates computing skills,
effective communication, reproducibility, and scientific integrity within one
course. Utilizing a flipped classroom model, the course aims to equip students
with the necessary competencies to handle real-world data analysis challenges
and effective statistical practice in general. The curriculum includes
practical tools such as version control with Git, dynamic reporting, unit
testing and containerization to foster reproducibility, and integrity in
statistical practice. Feedback gathered from both staff and students
post-implementation indicates that the course significantly enhances student
readiness for professional and academic environments, demonstrating the
effectiveness of this educational approach.
"
2407.09883,2024-07-16,"Toward a Complete Criterion for Value of Information in Insoluble
  Decision Problems","  In a decision problem, observations are said to be material if they must be
taken into account to perform optimally. Decision problems have an underlying
(graphical) causal structure, which may sometimes be used to evaluate certain
observations as immaterial. For soluble graphs - ones where important past
observations are remembered - there is a complete graphical criterion; one that
rules out materiality whenever this can be done on the basis of the graphical
structure alone. In this work, we analyse a proposed criterion for insoluble
graphs. In particular, we prove that some of the conditions used to prove
immateriality are necessary; when they are not satisfied, materiality is
possible. We discuss possible avenues and obstacles to proving necessity of the
remaining conditions.
"
2407.10577,2024-07-16,"Alternative proof for the bias of the hot hand statistic of streak
  length one","  For a sequence of $n$ random variables taking values $0$ or $1$, the hot hand
statistic of streak length $k$ counts what fraction of the streaks of length
$k$, that is, $k$ consecutive variables taking the value $1$, among the $n$
variables are followed by another $1$. Since this statistic does not use the
expected value of how many streaks of length $k$ are observed, but instead uses
the realization of the number of streaks present in the data, it may be a
biased estimator of the conditional probability of a fixed random variable
taking value $1$ if it is preceded by a streak of length $k$, as was first
studied and observed explicitly in [Miller and Sanjurjo, 2018]. In this short
note, we suggest an alternative proof for an explicit formula of the
expectation of the hot hand statistic for the case of streak length one. This
formula was obtained through a different argument in [Miller and Sanjurjo,
2018] and [Rinott and Bar-Hillel, 2015].
"
2407.11076,2024-08-07,A concise proof of Benford's law,"  This article presents a concise proof of the famous Benford's law when the
distribution has a Riemann integrable probability density function and provides
a criterion to judge whether a distribution obeys the law. The proof is
intuitive and elegant, accessible to anyone with basic knowledge of calculus,
revealing that the law originates from the basic property of the human number
system. The criterion can bring great convenience to the field of fraud
detection.
"
2407.11518,2024-07-17,Ensemble Transport Filter via Optimized Maximum Mean Discrepancy,"  In this paper, we present a new ensemble-based filter method by
reconstructing the analysis step of the particle filter through a transport
map, which directly transports prior particles to posterior particles. The
transport map is constructed through an optimization problem described by the
Maximum Mean Discrepancy loss function, which matches the expectation
information of the approximated posterior and reference posterior. The proposed
method inherits the accurate estimation of the posterior distribution from
particle filtering. To improve the robustness of Maximum Mean Discrepancy, a
variance penalty term is used to guide the optimization. It prioritizes
minimizing the discrepancy between the expectations of highly informative
statistics for the approximated and reference posteriors. The penalty term
significantly enhances the robustness of the proposed method and leads to a
better approximation of the posterior. A few numerical examples are presented
to illustrate the advantage of the proposed method over the ensemble Kalman
filter.
"
2407.11824,2025-03-19,The Future of Data Science Education,"  The definition of Data Science is a hotly debated topic. For many, the
definition is a simple shortcut to Artificial Intelligence or Machine Learning.
However, there is far more depth and nuance to the field of Data Science than a
simple shortcut can provide. The School of Data Science at the University of
Virginia has developed a novel model for the definition of Data Science. This
model is based on identifying a unified understanding of the data work done
across all areas of Data Science. It represents a generational leap forward in
how we understand and teach Data Science. In this paper we will present the
core features of the model and explain how it unifies various concepts going
far beyond the analytics component of AI. From this foundation we will present
our Undergraduate Major curriculum in Data Science and demonstrate how it
prepares students to be well-rounded Data Science team members and leaders. The
paper will conclude with an in-depth overview of the Foundations of Data
Science course designed to introduce students to the field while also
implementing proven STEM oriented pedagogical methods. These include, for
example, specifications grading, active learning lectures, guest lectures from
industry experts and weekly gamification labs.
"
2407.13495,2024-07-19,"Identifying Research Hotspots and Future Development Trends in Current
  Psychology: A Bibliometric Analysis of the Past Decade's Publications","  By conducting a bibliometric analysis on 4,869 publications in Current
Psychology from 2013 to 2022, this paper examined the annual publications and
annual citations, as well as the leading institutions, countries, and keywords.
CiteSpace, VOSviewer and SCImago Graphica were utilized for visualization
analysis. On one hand, this paper analyzed the academic influence of Current
Psychology over the past decade. On the other hand, it explored the research
hotspots and future development trends within the field of international
psychology. The results revealed that the three main research areas covered in
the publications of Current Psychology were: the psychological well-being of
young people, the negative emotions of adults, and self-awareness and
management. The latest research hotspots highlighted in the journal include
negative emotions, personality, and mental health. The three main development
trends of Current Psychology are: 1) exploring the personality psychology of
both adolescents and adults, 2) promoting the interdisciplinary research to
study social psychological issues through the use of diversified research
methods, and 3) emphasizing the emotional psychology of individuals and their
interaction with social reality, from a people-oriented perspective.
"
2407.14072,2024-08-06,FAVis: Visual Analytics of Factor Analysis for Psychological Research,"  Psychological research often involves understanding psychological constructs
through conducting factor analysis on data collected by a questionnaire, which
can comprise hundreds of questions. Without interactive systems for
interpreting factor models, researchers are frequently exposed to subjectivity,
potentially leading to misinterpretations or overlooked crucial information.
This paper introduces FAVis, a novel interactive visualization tool designed to
aid researchers in interpreting and evaluating factor analysis results. FAVis
enhances the understanding of relationships between variables and factors by
supporting multiple views for visualizing factor loadings and correlations,
allowing users to analyze information from various perspectives. The primary
feature of FAVis is to enable users to set optimal thresholds for factor
loadings to balance clarity and information retention. FAVis also allows users
to assign tags to variables, enhancing the understanding of factors by linking
them to their associated psychological constructs. Our user study demonstrates
the utility of FAVis in various tasks.
"
2407.17076,2025-04-16,The Analytic Stockwell Transform and its Zeros,"  A recent original line of research in time--frequency analysis has shifted
the interest in energy maxima toward zeros. Initially motivated by the
intriguing uniform spread of the zeros of the spectrogram of white noise, it
has led to fruitful theoretical developments combining probability theory,
complex analysis and signal processing. In this vein, the present work proposes
a characterization of the zeros of the Stockwell Transform of white noise,
which consists in an hybrid time--frequency multiresolution representation.
First of all, an analytic version of the Stockwell Transform is designed. Then,
analyticity is leveraged to establish a connection with the hyperbolic Gaussian
Analytic Function, whose zero set is invariant under the isometries of the
Poincar\'e disk. Finally, the theoretical spatial statistics of the zeros of
the hyperbolic Gaussian Analytic Function and the empirical statistics of the
zeros the Analytic Stockwell Transform of white noise are compared through
intensive Monte Carlo simulations, supporting the established connection. A
publicly available documented Python toolbox accompanies this work.
"
2407.18572,2024-07-29,Bernoulli amputation,"  An approach to amputation, the process of introducing missing values to a
complete dataset, is presented. It allows to construct missingness indicators
in a flexible and principled way via copulas and Bernoulli margins and to
incorporate dependence in missingness patterns. Besides more classical
missingness models such as missing completely at random, missing at random, and
missing not at random, the approach is able to model structured missingness
such as block missingness and, via mixtures, monotone missingness, which are
patterns of missing data frequently found in real-life datasets. Properties
such as joint missingness probabilities or missingness correlation are derived
mathematically. The approach is demonstrated with mathematical examples and
empirical illustrations in terms of a well-known dataset.
"
2407.18835,2024-11-20,Robust Estimation of Polychoric Correlation,"  Polychoric correlation is often an important building block in the analysis
of rating data, particularly for structural equation models. However, the
commonly employed maximum likelihood (ML) estimator is highly susceptible to
misspecification of the polychoric correlation model, for instance through
violations of latent normality assumptions. We propose a novel estimator that
is designed to be robust to partial misspecification of the polychoric model,
that is, the model is only misspecified for an unknown fraction of
observations, for instance (but not limited to) careless respondents. In
contrast to existing literature, our estimator makes no assumption on the type
or degree of model misspecification. It furthermore generalizes ML estimation,
is consistent as well as asymptotically normally distributed, and comes at no
additional computational cost. We demonstrate the robustness and practical
usefulness of our estimator in simulation studies and an empirical application
on a Big Five administration. In the latter, the polychoric correlation
estimates of our estimator and ML differ substantially, which, after further
inspection, is likely due to the presence of careless respondents that the
estimator helps identify.
"
2407.18909,2025-04-24,"Hybrid summary statistics: neural weak lensing inference beyond the
  power spectrum","  In inference problems, we often have domain knowledge which allows us to
define summary statistics that capture most of the information content in a
dataset. In this paper, we present a hybrid approach, where such physics-based
summaries are augmented by a set of compressed neural summary statistics that
are optimised to extract the extra information that is not captured by the
predefined summaries. The resulting statistics are very powerful inputs to
simulation-based or implicit inference of model parameters. We apply this
generalisation of Information Maximising Neural Networks (IMNNs) to parameter
constraints from tomographic weak gravitational lensing convergence maps to
find summary statistics that are explicitly optimised to complement angular
power spectrum estimates. We study several dark matter simulation resolutions
in low- and high-noise regimes. We show that i) the information-update
formalism extracts at least $3\times$ and up to $8\times$ as much information
as the angular power spectrum in all noise regimes, ii) the network summaries
are highly complementary to existing 2-point summaries, and iii) our formalism
allows for networks with smaller, physically-informed architectures to match
much larger regression networks with far fewer simulations needed to obtain
asymptotically optimal inference.
"
2407.19285,2024-08-12,"The Impact of Foreign Players in the English Premier League: A
  Mathematical Analys","  We undertake extensive analysis of English Premier League data over the
period 2009/10 to 2017/18 to identify and rank key factors affecting the
economic and footballing performances of the teams. Alternative end-of-season
league tables are generated by re-ranking the teams based on five different
descriptors - total expenditure, total funds spent on players, total funds
spent on foreign players, the ratio of foreign to British players and the
overall profit. The unequal distribution of resources and expenditure between
the clubs is analyzed through Lorenz curves. A comparative analysis of the
differences between the alternative tables and the conventional end-of-season
league table establishes the most likely factors to influence the performances
of the teams that we also rank using Principal Component Analysis. We find that
the top teams in the league are also those that tend to have the highest
expenditure overall, for all players, including foreign players; they also have
the highest ratios of foreign to British players. Our statistical and machine
learning study also indicates that successful performance on the field may not
guarantee healthy profits at the end of the season.
"
2407.19433,2024-12-04,"How Books Tell a History of Statistics in Portugal: Works of Foreigners,
  Estrangeirados, and Others","  Foreigners and ""estrangeirados"", an expression meaning ""people going to a
foreign country [""estrangeiro""] getting there further education"", had a leading
role in the development of Mathematical Statistics in Portugal. In what
concerns Statistics, ""estrangeirados"" in the nineteenth century were mainly
liberal intellectuals exiled for political reasons. From 1930 onwards, the
research funding authority sent university professors abroad, and hired foreign
researchers to stay in Portuguese institutions, and some of them were
instrumental in the importation of new concepts and methods of inferential
statistics. After 1970, there was a huge program of sending young researchers
abroad for doctoral studies. At the same time, many new universities and
polytechnic institutes have been created in Portugal. After that, aside from
foreigners who choose to have a research career in those institutions and the
""estrangeirados"" who had returned and created programs of doctoral studies,
others, who hadn't the opportunity of studying abroad, began to play a decisive
role in the development of Statistics in Portugal. The publication of handbooks
on Probability and Statistics, thesis and core papers in Portuguese scientific
journals, and also of works for the layman, reveals how Statistics progressed
from descriptive to a mathematical discipline used for inference in all fields
of knowledge, from natural sciences to methodology of scientific research.
"
2407.21190,2024-08-14,"Computational methods to simultaneously compare the predictive values of
  two diagnostic tests with missing data: EM-SEM algorithms and multiple
  imputation","  Predictive values are measures of the clinical accuracy of a binary
diagnostic test, and depend on the sensitivity and the specificity of the test
and on the disease prevalence among the population being studied. This article
studies hypothesis tests to simultaneously compare the predictive values of two
binary diagnostic tests in the presence of missing data. The hypothesis tests
were solved applying two computational methods: the EM and SEM algorithms and
multiple imputation. Simulation experiments were carried out to study the sizes
and the power of the hypothesis tests, giving some general rules of
application. Two R programmes were written to apply each method, and they are
available as supplementary material for the manuscript. The results were
applied to the diagnosis of Alzheimer's disease.
"
2407.21382,2024-09-02,"Comparison of the likelihood ratios of two diagnostic tests subject to a
  paired design: confidence intervals and sample size","  Positive and negative likelihood ratios are parameters which are used to
assess and compare the effectiveness of binary diagnostic tests. Both
parameters only depend on the sensitivity and specificity of the diagnostic
test and are equivalent to a relative risk. This article studies the comparison
of the likelihood ratios of two binary diagnostic tests subject to a paired
design through confidence intervals. Six approximate confidence intervals are
presented for the ratio of the likelihood ratios, and simulation experiments
are carried out to study the coverage probabilities and the average lengths of
the intervals considered, and some general rules of application are proposed. A
method is also proposed to determine the sample size necessary to estimate the
ratio between the likelihood ratios with a determined precision. The results
were applied to the diagnosis of coronary artery disease.
"
2407.21387,2024-09-02,"Asymptotic confidence intervals for the difference and the ratio of the
  weighted kappa coefficients of two diagnostic tests subject to a paired
  design","  The weighted kappa coefficient of a binary diagnostic test is a measure of
the beyond-chance agreement between the diagnostic test and the gold standard,
and depends on the sensitivity and specificity of the diagnostic test, on the
disease prevalence and on the relative importance between the false positives
and the false negatives. This article studies the comparison of the weighted
kappa coefficients of two binary diagnostic tests subject to a paired design
through confidence intervals. Three asymptotic confidence intervals are studied
for the difference between the parameters and five other intervals for the
ratio. Simulation experiments were carried out to study the coverage
probabilities and the average lengths of the intervals, giving some general
rules for application. A method is also proposed to calculate the sample size
necessary to compare the two weighted kappa coefficients through a confidence
interval. A program in R has been written to solve the problem studied and it
is available as supplementary material. The results were applied to a real
example of the diagnosis of malaria.
"
2408.04456,2024-08-09,"Modeling information spread across networks with communities using a
  multitype branching process framework","  The dynamics of information diffusion in complex networks is widely studied
in an attempt to understand how individuals communicate and how information
travels and reaches individuals through interactions. However, complex networks
often present community structure, and tools to analyse information diffusion
on networks with communities are needed. In this paper, we develop theoretical
tools using multi-type branching processes to model and analyse simple
contagion information spread on a broad class of networks with community
structure. We show how, by using limited information about the network -- the
degree distribution within and between communities -- we can calculate standard
statistical characteristics of the dynamics of information diffusion, such as
the extinction probability, hazard function, and cascade size distribution.
These properties can be estimated not only for the entire network but also for
each community separately. Furthermore, we estimate the probability of
information spreading from one community to another where it is not currently
spreading. We demonstrate the accuracy of our framework by applying it to two
specific examples: the Stochastic Block Model and a log-normal network with
community structure. We show how the initial seeding location affects the
observed cascade size distribution on a heavy-tailed network and that our
framework accurately captures this effect.
"
2408.14832,2024-12-05,"Cross-sectional personal network analysis of adult smoking in rural
  areas","  While research on adolescent smoking is extensive, little attention has been
given to smoking behaviors among rural middle-aged and older adults. This study
examines the role of personal networks and sociodemographic factors in
predicting smoking status in a rural Romanian community. Using a link-tracing
sampling method, we gathered data from 76 participants out of 83 in Leresti,
Arges County. Face-to-face interviews collected sociodemographic data and
network information, including smoking status and relational dynamics. We
applied multilevel logistic regression models to predict smoking behaviors
(current smokers, former smokers, and non-smokers) based on individual
characteristics and network influences. Results indicate that social networks
significantly influence smoking behaviors. For current smokers, having a
smoking family member greatly increased the odds of smoking (OR = 2.51, 95% CI:
1.62, 3.91, p < 0.001). Similarly, non-smoking family members increased the
likelihood of being a non-smoker (OR = 1.64, 95% CI: 1.04, 2.61, p < 0.05).
Women were less likely to smoke, highlighting sex differences in behavior.
These findings emphasize the critical role of social networks in shaping
smoking habits, advocating for targeted interventions in rural areas.
"
2409.01631,2025-01-30,"Doppler Power Spectrum in Channels with von Mises-Fisher Distribution of
  Scatterers","  This paper presents an analytical analysis of the Doppler spectrum in von
Mises-Fisher (vMF) scattering channels. A simple closed-form expression for the
Doppler spectrum is derived and used to investigate the impact of the vMF
scattering parameters, i.e., the mean direction and the degree of concentration
of scatterers. The spectrum is observed to exhibit exponential behavior for
mobile antenna motion parallel to the mean direction of scatterers, while
conforming to a Gaussian-like shape for the perpendicular motion. The validity
of the obtained results is verified by comparison against the results of Monte
Carlo simulations, where an exact match is observed.
"
2409.01647,2024-12-24,"Correlation Properties in Channels with von Mises-Fisher Distribution of
  Scatterers","  This letter presents simple analytical expressions for the spatial and
temporal correlation functions in channels with von Mises-Fisher (vMF)
scattering. In contrast to previous results, the expressions presented here are
exact and based only on elementary functions, clearly revealing the impact of
the underlying parameters. The derived results are validated by a comparison
against numerical integration result, where an exact match is observed. To
demonstrate their utility, the presented results are used to analyze spatial
correlation across different antenna array geometries and to investigate
temporal correlation of a fluctuating radar signal from a moving target.
"
2409.05263,2024-09-10,How to survive the Squid Games using probability theory,"  In this paper, we consider how probability theory can be used to determine
the survival strategy in two of the ``Squid Game"" and ``Squid Game: The
Challenge"" challenges: the Hopscotch and the Warships. We show how Hopscotch
can be easily tackled with the knowledge of the binomial distribution, taught
in introductory statistics courses, while Warships is a much more complex
problem, which can be tackled at different levels.
"
2409.05412,2024-09-10,"Censored Data Forecasting: Applying Tobit Exponential Smoothing with
  Time Aggregation","  This study introduces a novel approach to forecasting by Tobit Exponential
Smoothing with time aggregation constraints. This model, a particular case of
the Tobit Innovations State Space system, handles censored observed time series
effectively, such as sales data, with known and potentially variable censoring
levels over time. The paper provides a comprehensive analysis of the model
structure, including its representation in system equations and the optimal
recursive estimation of states. It also explores the benefits of time
aggregation in state space systems, particularly for inventory management and
demand forecasting. Through a series of case studies, the paper demonstrates
the effectiveness of the model across various scenarios, including hourly and
daily censoring levels. The results highlight the model's ability to produce
accurate forecasts and confidence bands comparable to those from uncensored
models, even under severe censoring conditions. The study further discusses the
implications for inventory policy, emphasizing the importance of avoiding
spiral-down effects in demand estimation. The paper concludes by showcasing the
superiority of the proposed model over standard methods, particularly in
reducing lost sales and excess stock, thereby optimizing inventory costs. This
research contributes to the field of forecasting by offering a robust model
that effectively addresses the challenges of censored data and time
aggregation.
"
2409.05764,2024-09-10,Jackknife Empirical Likelihood Ratio Test for Cauchy Distribution,"  Heavy-tailed distributions, such as the Cauchy distribution, are acknowledged
for providing more accurate models for financial returns, as the normal
distribution is deemed insufficient for capturing the significant fluctuations
observed in real-world assets. Data sets characterized by outlier sensitivity
are critically important in diverse areas, including finance, economics,
telecommunications, and signal processing. This article addresses a
goodness-of-fit test for the Cauchy distribution. The proposed test utilizes
empirical likelihood methods, including the jackknife empirical likelihood
(JEL) and adjusted jackknife empirical likelihood (AJEL). Extensive Monte Carlo
simulation studies are conducted to evaluate the finite sample performance of
the proposed test. The application of the proposed test is illustrated through
the analysing two real data sets.
"
2409.12102,2024-09-19,Cyclicity Analysis of the Ornstein-Uhlenbeck Process,"  In this thesis, we consider an $N$-dimensional Ornstein-Uhlenbeck (OU)
process satisfying the linear stochastic differential equation $d\mathbf x(t) =
- \mathbf B\mathbf x(t) dt + \boldsymbol \Sigma d \mathbf w(t).$ Here, $\mathbf
B$ is a fixed $N \times N$ circulant friction matrix whose eigenvalues have
positive real parts, $\boldsymbol \Sigma$ is a fixed $N \times M$ matrix. We
consider a signal propagation model governed by this OU process. In this model,
an underlying signal propagates throughout a network consisting of $N$ linked
sensors located in space. We interpret the $n$-th component of the OU process
as the measurement of the propagating effect made by the $n$-th sensor. The
matrix $\mathbf B$ represents the sensor network structure: if $\mathbf B$ has
first row $(b_1 \ , \ \dots \ , \ b_N),$ where $b_1>0$ and $b_2 \ , \ \dots \
,\ b_N \le 0,$ then the magnitude of $b_p$ quantifies how receptive the $n$-th
sensor is to activity within the $(n+p-1)$-th sensor. Finally, the $(m,n)$-th
entry of the matrix $\mathbf D = \frac{\boldsymbol \Sigma \boldsymbol
\Sigma^\text T}{2}$ is the covariance of the component noises injected into the
$m$-th and $n$-th sensors. For different choices of $\mathbf B$ and
$\boldsymbol \Sigma,$ we investigate whether Cyclicity Analysis enables us to
recover the structure of network. Roughly speaking, Cyclicity Analysis studies
the lead-lag dynamics pertaining to the components of a multivariate signal. We
specifically consider an $N \times N$ skew-symmetric matrix $\mathbf Q,$ known
as the lead matrix, in which the sign of its $(m,n)$-th entry captures the
lead-lag relationship between the $m$-th and $n$-th component OU processes. We
investigate whether the structure of the leading eigenvector of $\mathbf Q,$
the eigenvector corresponding to the largest eigenvalue of $\mathbf Q$ in
modulus, reflects the network structure induced by $\mathbf B.$
"
2409.14049,2024-10-10,"Adaptive radar detection of subspace-based distributed target in power
  heterogeneous clutter","  This paper investigates the problem of adaptive detection of distributed
targets in power heterogeneous clutter. In the considered scenario, all the
data share the identical structure of clutter covariance matrix, but with
varying and unknown power mismatches. To address this problem, we iteratively
estimate all the unknowns, including the coordinate matrix of the target, the
clutter covariance matrix, and the corresponding power mismatches, and propose
three detectors based on the generalized likelihood ratio test (GLRT), Rao and
the Wald tests. The results from simulated and real data both illustrate that
the detectors based on GLRT and Rao test have higher probabilities of detection
(PDs) than the existing competitors. Among them, the Rao test-based detector
exhibits the best overall detection performance. We also analyze the impact of
the target extended dimensions, the signal subspace dimensions, and the number
of training samples on the detection performance. Furthermore, simulation
experiments also demonstrate that the proposed detectors have a constant false
alarm rate (CFAR) property for the structure of clutter covariance matrix.
"
2409.14197,2024-09-24,"Advancing Employee Behavior Analysis through Synthetic Data: Leveraging
  ABMs, GANs, and Statistical Models for Enhanced Organizational Efficiency","  Success in todays data-driven corporate climate requires a deep understanding
of employee behavior. Companies aim to improve employee satisfaction, boost
output, and optimize workflow. This research study delves into creating
synthetic data, a powerful tool that allows us to comprehensively understand
employee performance, flexibility, cooperation, and team dynamics. Synthetic
data provides a detailed and accurate picture of employee activities while
protecting individual privacy thanks to cutting-edge methods like agent-based
models (ABMs), Generative Adversarial Networks (GANs), and statistical models.
Through the creation of multiple situations, this method offers insightful
viewpoints regarding increasing teamwork, improving adaptability, and
accelerating overall productivity. We examine how synthetic data has evolved
from a specialized field to an essential resource for researching employee
behavior and enhancing management efficiency. Keywords: Agent-Based Model,
Generative Adversarial Network, workflow optimization, organizational success
"
2409.14284,2024-09-24,Survey Data Integration for Distribution Function Estimation,"  Integration of probabilistic and non-probabilistic samples for the estimation
of finite population totals (or means) has recently received considerable
attention in the field of survey sampling; yet, to the best of our knowledge,
this framework has not been extended to cumulative distribution function (CDF)
estimation. To address this gap, we propose a novel CDF estimator that
integrates data from probability samples with data from (potentially large)
nonprobability samples. Assuming that a set of shared covariates are observed
in both samples, while the response variable is observed only in the latter,
the proposed estimator uses a survey-weighted empirical CDF of regression
residuals trained on the convenience sample to estimate the CDF of the response
variable. Under some regularity conditions, we show that our CDF estimator is
both design-consistent for the finite population CDF and asymptotically
normally distributed. Additionally, we define and study a quantile estimator
based on the proposed CDF estimator. Furthermore, we use both the bootstrap and
asymptotic formulae to estimate their respective sampling variances. Our
empirical results show that the proposed CDF estimator is robust to model
misspecification under ignorability, and robust to ignorability under model
misspecification. When both assumptions are violated, our residual-based CDF
estimator still outperforms its 'plug-in' mass imputation and naive siblings,
albeit with noted decreases in efficiency.
"
2409.14606,2024-11-25,"A Modified Satterthwaite (1941,1946) Effective Degrees of Freedom
  Approximation","  This study introduces a correction to the approximation of effective degrees
of freedom as proposed by Satterthwaite (1941, 1946), specifically addressing
scenarios where component degrees of freedom are small. The correction is
grounded in analytical results concerning the moments of standard normal random
variables. This modification is applicable to complex variance estimates that
involve both small and large degrees of freedom, offering an enhanced
approximation of the higher moments required by Satterthwaite's framework.
Additionally, this correction extends and partially validates the empirically
derived adjustment by Johnson & Rust (1992), as it is based on theoretical
foundations rather than simulations used to derive empirical transformation
constants.
"
2409.16527,2025-03-04,"Approximation of Smooth Numbers for Harmonic Samples A Stein method
  Approach","  We present a de Bruijn type approximation for quantifying the content of m
smooth numbers, derived from samples obtained through a probability measure
over the set of integers less than or equal to n, with point mass function at k
inversely proportional to k. Our analysis is based on a stochastic
representation of the measure of interest, utilizing weighted independent
geometric random variables. This representation is analyzed through the lens of
Stein method for the Dickman distribution. A pivotal element of our arguments
relies on precise estimations concerning the regularity properties of the
solution to the Dickman Stein equation for heaviside functions, recently
developed by Bhattacharjee and Schulte. Remarkably, our arguments remain mostly
in the realm of probability theory, with Mertens first and third theorems
standing as the only number theory estimations required.
"
2409.16613,2024-11-20,"Oral exams in introductory statistics class with non-native English
  speakers","  Oral exams are a powerful tool for educators to gauge student's learning.
This is particularly important in introductory statistics classes where many
students struggle to grasp a deep meaning of topics like $p$-values, confidence
intervals, hypothesis testing, and more. These challenges are only heightened
in a context where students are learning in a second language. In this paper, I
share my experience administering oral exams to an introductory statistics
class of non-native English speakers at a Japanese university. I explain the
context of the university and course that the exam was given in, before sharing
details about the two exams. Despite the challenges the students (and I myself)
faced, the exams seemed to truly test their statistical knowledge and not
merely their English proficiency, as I found little relationship between a
student's English ability and performance. I close with encouragements and
recommendations for practitioners hoping to implement these exams, all while
keeping an eye towards the unique difficulties faced by students not learning
in their mother tongue.
"
2409.16776,2024-09-26,Uncertainty Quantification for Agent Based Models: A Tutorial,"  We explore the application of uncertainty quantification methods to
agent-based models (ABMs) using a simple sheep and wolf predator-prey model.
This work serves as a tutorial on how techniques like emulation can be powerful
tools in this context. We also highlight the importance of advanced statistical
methods in effectively utilising computationally expensive ABMs. Specifically,
we implement stochastic Gaussian processes, Gaussian process classification,
sequential design, and history matching to address uncertainties in model input
parameters and outputs. Our results show that these methods significantly
enhance the robustness, accuracy, and predictive power of ABMs.
"
2409.17360,2024-09-27,Bi-Filtration and Stability of TDA Mapper for Point Cloud Data,"  Carlsson, Singh and Memoli's TDA mapper takes a point cloud dataset and
outputs a graph that depends on several parameter choices. Dey, Memoli, and
Wang developed Multiscale Mapper for abstract topological spaces so that
parameter choices can be analyzed via persistent homology. However, when
applied to actual data, one does not always obtain filtrations of mapper
graphs. DBSCAN, one of the most common clustering algorithms used in the TDA
mapper software, has two parameters, \textbf{$\epsilon$} and \textbf{MinPts}.
If \textbf{MinPts = 1} then DBSCAN is equivalent to single linkage clustering
with cutting height \textbf{$\epsilon$}. We show that if DBSCAN clustering is
used with \textbf{MinPts $>$ 2}, a filtration of mapper graphs may not exist
except in the absence of free-border points; but such filtrations exist if
DBSCAN clustering is used with \textbf{MinPts = 1} or \textbf{2} as the cover
size increases, \textbf{$\epsilon$} increases, and/or \textbf{MinPts}
decreases. However, the 1-dimensional filtration is unstable. If one adds noise
to a data set so that each data point has been perturbed by a distance at most
\textbf{$\delta$}, the persistent homology of the mapper graph of the perturbed
data set can be significantly different from that of the original data set. We
show that we can obtain stability by increasing both the cover size and
\textbf{$\epsilon$} at the same time. In particular, we show that the
bi-filtrations of the homology groups with respect to cover size and $\epsilon$
between these two datasets are \textbf{2$\delta$}-interleaved.
"
2409.18280,2024-09-30,"easylayout: an R package for interactive force-directed layouts within
  RStudio","  Motivation
  Network visualization is critical for effective communication in various
fields of knowledge. Currently, a gap separates network manipulation from
network visualization in programming environments. Users often export network
data to be laid out in external interactive software, like Cytoscape and Gephi.
We argue the current R package ecosystem lacks an interactive layout engine
well integrated with common data analysis workflows.
  Results
  We present easylayout, an R package that bridges network manipulation and
visualization by leveraging interactive force simulations within the IDE itself
(e.g., RStudio, VSCode). It is not yet another visualization library, but
instead aims to interconnect existing libraries and streamline their usage into
the R ecosystem. easylayout takes an igraph object and serializes it into a web
application integrated with the IDE's interface through a Shiny server. The app
lays out the network by simulating attraction and repulsion forces. Simulation
parameters can be adjusted in real-time. An editing mode allows moving and
rotating nodes. The implementation aims for performance, so that even lower-end
devices are able to work with relatively large networks. Once the user finishes
tweaking the layout, it is sent back to the R session to be plotted through
popular libraries like ggraph, igraph or even the base package itself. The
current implementation focuses on the R ecosystem, but using web technologies
makes it easily portable to similar environments, like Python/Jupyter
Notebooks. We expect this tool to reduce the time spent searching for suitable
network layouts, ultimately allowing researchers to generate more compelling
figures.
  Availability and implementation
  easylayout is freely available under an MIT license on GitHub
(https://github.com/dalmolingroup/easylayout). The package is implemented in
R/Shiny and JavaScript/Svelte.
"
2410.07569,2025-04-29,Language all the way down: Grammatical structures in mathematics,"  The ability to read, write, and speak mathematics is critical to students
becoming comfortable with statistical models and skills. Faster development of
those skills may act as encouragement to further engage with the discipline.
Vocabulary has been the focus of scholarship in existing literature on the
linguistics of mathematics and statistics but there are structures such as
grammar that go beyond the content of words and symbols. Here I introduce ideas
for grammar structures through a sequence of examples.
"
2410.08441,2024-10-14,"A scientific review on advances in statistical methods for crossover
  design","  A comprehensive review of the literature on crossover design is needed to
highlight its evolution, applications, and methodological advancements across
various fields. Given its widespread use in clinical trials and other research
domains, understanding this design's challenges, assumptions, and innovations
is essential for optimizing its implementation and ensuring accurate, unbiased
results. This article extensively reviews the history and statistical inference
methods for crossover designs. A primary focus is given to the AB-BA design as
it is the most widely used design in literature. Extension from two periods to
higher-order designs is discussed, and a general inference procedure for
continuous response is studied. Analysis of multivariate and categorical
responses is also reviewed in this context. A bunch of open problems in this
area are shortlisted.
"
2410.11399,2025-02-26,Convergence to the Truth,"  This article reviews and develops an epistemological tradition in the
philosophy of science, known as convergentism, which holds that inference
methods should be assessed based on their ability to converge to the truth
across a range of possible scenarios. Emphasis is placed on its historical
origins in the work of C. S. Peirce and its recent developments in formal
epistemology and data science (including statistics and machine learning).
Comparisons are made with three other traditions: (1) explanationism, which
holds that theory choice should be guided by a theory's overall balance of
explanatory virtues, such as simplicity and fit with data; (2) instrumentalism,
which maintains that scientific inference should be driven by the goal of
obtaining useful models rather than true theories; and (3) Bayesianism, which
shifts the focus from all-or-nothing beliefs to degrees of belief.
"
2410.12450,2024-10-17,A geometrical perspective on parametric psychometric models,"  Psychometrics and quantitative psychology rely strongly on statistical models
to measure psychological processes. As a branch of mathematics, geometry is
inherently connected to measurement and focuses on properties such as distance
and volume. However, despite the common root of measurement, geometry is
currently not used a lot in psychological measurement. In this paper, my aim is
to illustrate how ideas from non-Euclidean geometry may be relevant for
psychometrics.
"
2410.12736,2024-10-17,"A comparative study of self-starting CUSUM control charts for location
  shifts","  In recent years, self-starting methods have garnered increasing attention in
Statistical Process Control and Monitoring (SPC/M), as they offer real-time
disorder detection without the need for a calibration phase (Phase I). This
study focuses on evaluating parametric self-starting CUSUM-type control charts,
specifically the Bayesian Predictive Ratio CUSUM (PRC) developed by Bourazas et
al. (2023) and the frequentist alternative self-starting CUSUM proposed by
Hawkins and Olwell (1998). The performance of these methods is thoroughly
examined through an extensive simulation study under various scenarios
involving a change in the mean of Normal data. Additionally, a prior
sensitivity analysis for PRC is conducted. The work ands with concluding
remarks summarizing the findings.
"
2410.12930,2024-10-21,Probabilistic inference when the population space is open,"  In using observed data to make inferences about a population quantity, it is
commonly assumed that the sampling distribution from which the data were drawn
belongs to a given parametric family of distributions, or at least, a given
finite set of such families, i.e. the population space is assumed to be closed.
Here, we address the problem of how to determine an appropriate post-data
distribution for a given population quantity when such an assumption about the
underlying sampling distribution is not made, i.e. when the population space is
open. The strategy used to address this problem is based on the fact that even
though, due to an open population space being non-measurable, we are not able
to place a post-data distribution over all the sampling distributions contained
in such a population space, it is possible to partition this type of space into
a finite, countable or uncountable number of subsets such that a distribution
can be placed over a variable that simply indicates which of these subsets
contains the true sampling distribution. Moreover, it is argued that, by using
sampling distributions that belong to a number of parametric families, it is
possible to adequately and elegantly represent the sampling distributions that
belong to each of the subsets of such a partition. Since a statistical model is
conceived as being a model of a population space rather than a model of a
sampling distribution, it is also argued that neither the type of models that
are put forward nor the expression of pre-data knowledge via such models can be
directly brought into question by the data. Finally, the case is made that, as
well as not being required in the modelling process that is proposed, the
standard practice of using P values to measure the absolute compatibility of an
individual or family of sampling distributions with observed data is neither
meaningful nor useful.
"
2410.18062,2024-10-24,"Developing Consistency Among Undergraduate Graders Scoring Open-Ended
  Statistics Tasks","  Undergraduate graders are frequently important contributors to the teaching
team in post-secondary education settings. This study set out to investigate
agreement for a team of undergraduate graders as they acquired training and
experience for scoring responses to open-ended tasks. Results demonstrate
compelling evidence that undergraduate students can develop the ability to
establish and sustain substantial agreement with an instructor, especially when
equipped with proper training and a high-quality scoring rubric.
"
2410.18939,2024-10-25,Adaptive partition Factor Analysis,"  Factor Analysis has traditionally been utilized across diverse disciplines to
extrapolate latent traits that influence the behavior of multivariate observed
variables. Historically, the focus has been on analyzing data from a single
study, neglecting the potential study-specific variations present in data from
multiple studies. Multi-study factor analysis has emerged as a recent
methodological advancement that addresses this gap by distinguishing between
latent traits shared across studies and study-specific components arising from
artifactual or population-specific sources of variation. In this paper, we
extend the current methodologies by introducing novel shrinkage priors for the
latent factors, thereby accommodating a broader spectrum of scenarios -- from
the absence of study-specific latent factors to models in which factors pertain
only to small subgroups nested within or shared between the studies. For the
proposed construction we provide conditions for identifiability of factor
loadings and guidelines to perform straightforward posterior computation via
Gibbs sampling. Through comprehensive simulation studies, we demonstrate that
our proposed method exhibits competing performance across a variety of
scenarios compared to existing methods, yet providing richer insights. The
practical benefits of our approach are further illustrated through applications
to bird species co-occurrence data and ovarian cancer gene expression data.
"
2410.19555,2024-10-28,"Probability Proofs for Stirling (and More): the Ubiquitous Role of
  $\mathbf{\sqrt{2\pi}}$","  The Stirling approximation formula for $n!$ dates from 1730. Here we give new
and instructive proofs of this and related approximation formulae via tools of
probability and statistics. There are connections to the Central Limit Theorem
and also to approximations of marginal distributions in Bayesian setups.
Certain formulae emerge by working through particular instances, some
independently verifiable but others perhaps not. A particular case yielding new
formulae is that of summing independent uniforms, related to the Irwin--Hall
distribution. Yet further proofs of the Stirling flow from examining aspects of
limiting normality of the sample median of uniforms, and from these again we
find a proof for the Wallis product formula for $\pi$.
"
2410.22475,2024-10-31,Ethical Statistical Practice and Ethical AI,"  Artificial Intelligence (AI) is a field that utilizes computing and often,
data and statistics, intensively together to solve problems or make
predictions. AI has been evolving with literally unbelievable speed over the
past few years, and this has led to an increase in social, cultural,
industrial, scientific, and governmental concerns about the ethical development
and use of AI systems worldwide. The ASA has issued a statement on ethical
statistical practice and AI (ASA, 2024), which echoes similar statements from
other groups. Here we discuss the support for ethical statistical practice and
ethical AI that has been established in long-standing human rights law and
ethical practice standards for computing and statistics. There are multiple
sources of support for ethical statistical practice and ethical AI deriving
from these source documents, which are critical for strengthening the
operationalization of the ""Statement on Ethical AI for Statistics
Practitioners"". These resources are explicated for interested readers to
utilize to guide their development and use of AI in, and through, their
statistical practice.
"
2411.01394,2024-11-07,"Centrality in Collaboration: A Novel Algorithm for Social Partitioning
  Gradients in Community Detection for Multiple Oncology Clinical Trial
  Enrollments","  Patients at a comprehensive cancer center who do not achieve cure or
remission following standard treatments often become candidates for clinical
trials. Patients who participate in a clinical trial may be suitable for other
studies. A key factor influencing patient enrollment in subsequent clinical
trials is the structured collaboration between oncologists and most responsible
physicians. Possible identification of these collaboration networks can be
achieved through the analysis of patient movements between clinical trial
intervention types with social network analysis and community detection
algorithms. In the detection of oncologist working groups, the present study
evaluates three community detection algorithms: Girvan-Newman, Louvain and an
algorithm developed by the author. Girvan-Newman identifies each intervention
as their own community, while Louvain groups interventions in a manner that is
difficult to interpret. In contrast, the author's algorithm groups
interventions in a way that is both intuitive and informative, with a gradient
evident in social partitioning that is particularly useful for epidemiological
research. This lays the groundwork for future subgroup analysis of clustered
interventions.
"
2411.03955,2024-11-07,"Large Deviations Inequalities for Unequal Probability Sampling Without
  Replacement","  We provide bounds on the tail probabilities for simple procedures that
generate random samples _without replacement_, when the probabilities of being
selected need not be equal.
"
2411.05391,2025-02-26,"Impossibility results for equating the Youden Index with average scoring
  rules and Tjur $R^2$-like metrics","  We consider the Youden index fas well as measures evaluating predicted
probabilities for the maximum-likelihood estimate of a logistic regression
model with predictor the classifier. We give impossibility results showing that
the Youden index can not equal any average of a real scoring rule nor any
metric averaging over binary outcomes (0s and 1s) for any continuous
real-valued scoring rule. This shows the obstructions of such potential
equivalences and highlights the distinct roles these metrics play in diagnostic
assessment.
"
2411.08547,2025-02-26,Frequentist Statistics as Internalist Reliabilism,"  There has long been an impression that reliabilism implies externalism and
that frequentist statistics, due to its reliabilist nature, is inherently
externalist. I argue, however, that frequentist statistics can plausibly be
understood as a form of internalist reliabilism -- internalist in the
conventional sense, yet reliabilist in certain unconventional and intriguing
ways. Crucially, in developing the thesis that reliabilism does not imply
externalism, my aim is not to stretch the meaning of `reliabilism' merely to
sever the implication. Instead, it is to gain a deeper understanding of
frequentist statistics, which stands as one of the most sustained attempts by
scientists to develop an epistemology for their own use.
"
2411.16531,2025-03-14,"Good intentions, unintended consequences: exploring forecasting harms","  Organizations worldwide that rely on data-driven approaches regularly employ
forecasting methods to enhance their planning and decision-making processes.
While extensive research has examined the harms associated with traditional
machine learning applications, relatively little attention has been given to
the ethical implications of time series forecasting. However, forecasting
presents distinct ethical challenges due to its diverse organizational
applications, varied objectives, and unique data processing, model development,
and evaluation workflows. These distinctions complicate the direct application
of existing machine learning harm taxonomies to common forecasting scenarios.
To address this gap, we conduct multiple interviews with industry experts and
academic researchers, systematically identifying and analyzing underexplored
domains, use cases, and potential risks associated with forecasting. Our
objective is to develop a novel taxonomy of forecasting-specific harms. Drawing
inspiration from Microsoft Azure taxonomy for responsible innovation, we
integrate a human-led inductive coding approach with AI-driven analysis to
extract key categories of harm in forecasting. This taxonomy aims to support
researchers and practitioners by fostering ethical reflection on their
decision-making throughout the forecasting process. Additionally, we seek to
establish a research agenda focused on identifying measures to mitigate
potential harms in forecasting. By highlighting unique risks within
forecasting, our work contributes to the broader discourse on machine learning
ethics.
"
2411.18481,2025-05-06,"Bhirkuti's Test of Bias Acceptance (BTBA): Examining Its Performance in
  Psychometric Simulations","  We introduce Bhirkuti's Test of Bias Acceptance (BTBA), a standardized
framework for evaluating estimator bias in Monte Carlo simulation studies. BTBA
uses a simulation-specific standardized score (Z*) and a decision matrix to
assess bias acceptability based on the mean and variance of Z* distributions.
Under ideal conditions, Z* values should approximate a standard normal
distribution (Z-distribution) with a mean near zero and variance near one in
the context of simulation research. Systematic deviations from these patterns
such as shifted means or inflated variances indicate bias or estimator
instability in simulation-based research. BTBA visualizes these patterns using
ridgeline density plots, which reveal distributional features such as central
tendency, spread, skewness, and outliers. Demonstrated in a latent growth
modeling context, BTBA offers a reproducible and interpretable method for
diagnosing bias across varying simulation conditions. By addressing key
limitations of traditional relative bias (RB) metrics, BTBA provides a
theoretically grounded, distribution-aware, transparent, and replicable
alternative for evaluating estimator quality, particularly in psychometric
modeling, structural equation modeling, and missing data research. Through this
framework, we aim to enhance methodological decision-making by integrating
statistical reasoning with comprehensive visualization techniques.
"
2411.18838,2024-12-02,"Contrasting the optimal resource allocation to cybersecurity and cyber
  insurance using prospect theory versus expected utility theory","  Protecting against cyber-threats is vital for every organization and can be
done by investing in cybersecurity controls and purchasing cyber insurance.
However, these are interlinked since insurance premiums could be reduced by
investing more in cybersecurity controls. The expected utility theory and the
prospect theory are two alternative theories explaining decision-making under
risk and uncertainty, which can inform strategies for optimizing resource
allocation. While the former is considered a rational approach, research has
shown that most people make decisions consistent with the latter, including on
insurance uptakes. We compare and contrast these two approaches to provide
important insights into how the two approaches could lead to different optimal
allocations resulting in differing risk exposure as well as financial costs. We
introduce the concept of a risk curve and show that identifying the nature of
the risk curve is a key step in deriving the optimal resource allocation.
"
2411.19140,2024-12-02,Examining Multimodal Gender and Content Bias in ChatGPT-4o,"  This study investigates ChatGPT-4o's multimodal content generation,
highlighting significant disparities in its treatment of sexual content and
nudity versus violent and drug-related themes. Detailed analysis reveals that
ChatGPT-4o consistently censors sexual content and nudity, while showing
leniency towards violence and drug use. Moreover, a pronounced gender bias
emerges, with female-specific content facing stricter regulation compared to
male-specific content. This disparity likely stems from media scrutiny and
public backlash over past AI controversies, prompting tech companies to impose
stringent guidelines on sensitive issues to protect their reputations. Our
findings emphasize the urgent need for AI systems to uphold genuine ethical
standards and accountability, transcending mere political correctness. This
research contributes to the understanding of biases in AI-driven language and
multimodal models, calling for more balanced and ethical content moderation
practices.
"
2411.19902,2024-12-02,"Noncommutative Model Selection for Data Clustering and Dimension
  Reduction Using Relative von Neumann Entropy","  We propose a pair of completely data-driven algorithms for unsupervised
classification and dimension reduction, and we empirically study their
performance on a number of data sets, both simulated data in three-dimensions
and images from the COIL-20 data set. The algorithms take as input a set of
points sampled from a uniform distribution supported on a metric space, the
latter embedded in an ambient metric space, and they output a clustering or
reduction of dimension of the data. They work by constructing a natural family
of graphs from the data and selecting the graph which maximizes the relative
von Neumann entropy of certain normalized heat operators constructed from the
graphs. Once the appropriate graph is selected, the eigenvectors of the graph
Laplacian may be used to reduce the dimension of the data, and clusters in the
data may be identified with the kernel of the associated graph Laplacian.
Notably, these algorithms do not require information about the size of a
neighborhood or the desired number of clusters as input, in contrast to popular
algorithms such as $k$-means, and even more modern spectral methods such as
Laplacian eigenmaps, among others.
  In our computational experiments, our clustering algorithm outperforms
$k$-means clustering on data sets with non-trivial geometry and topology, in
particular data whose clusters are not concentrated around a specific point,
and our dimension reduction algorithm is shown to work well in several simple
examples.
"
2412.02367,2024-12-04,"Internalist Reliabilism in Statistics and Machine Learning: Thoughts on
  Jun Otsuka's Thinking about Statistics","  Otsuka (2023) argues for a correspondence between data science and
traditional epistemology: Bayesian statistics is internalist; classical
(frequentist) statistics is externalist, owing to its reliabilist nature; model
selection is pragmatist; and machine learning is a version of virtue
epistemology. Where he sees diversity, I see an opportunity for unity. In this
article, I argue that classical statistics, model selection, and machine
learning share a foundation that is reliabilist in an unconventional sense that
aligns with internalism. Hence a unification under internalist reliabilism.
"
2412.02969,2025-02-26,"Unified Inductive Logic: From Formal Learning to Statistical Inference
  to Supervised Learning","  While the traditional conception of inductive logic is Carnapian, I develop a
Peircean alternative and use it to unify formal learning theory, statistics,
and a significant part of machine learning: supervised learning. Some crucial
standards for evaluating non-deductive inferences have been assumed separately
in those areas, but can actually be justified by a unifying principle.
"
2412.04735,2024-12-09,A dynamical measure of algorithmically infused visibility,"  This work focuses on the nature of visibility in societies where the
behaviours of humans and algorithms influence each other - termed
algorithmically infused societies. We propose a quantitative measure of
visibility, with implications and applications to an array of disciplines
including communication studies, political science, marketing, technology
design, and social media analytics. The measure captures the basic
characteristics of the visibility of a given topic, in algorithm/AI-mediated
communication/social media settings. Topics, when trending, are ranked against
each other, and the proposed measure combines the following two attributes of a
topic: (i) the amount of time a topic spends at different ranks, and (ii) the
different ranks the topic attains. The proposed measure incorporates a tunable
parameter, termed the discrimination level, whose value determines the relative
weights of the two attributes that contribute to visibility. Analysis of a
large-scale, real-time dataset of trending topics, from one of the largest
social media platforms, demonstrates that the proposed measure can explain a
large share of the variability of the accumulated views of a topic.
"
2412.10296,2025-04-08,My Statistics is Better than Yours,"  Statistical schools-such as Bayesianism and Frequentism-are often presented
as competing frameworks, each claiming technical rigour and superiority.
Frequentism emphasizes objective inferences through repeated sampling, while
Bayesianism incorporates prior beliefs and updates them with new evidence.
Despite their strengths, neither school proves universally applicable, and the
pursuit of a single ""correct"" statistical framework is ultimately misguided.
Instead, this essay advocates for a context-dependent approach to statistical
norms, drawing on Douglas (2004)'s concept of ""operational objectivity"". The
idea is that by aligning the context of the research question with the value
judgments inherent to its field, a certain statistical paradigm is warranted.
This essay explores the decision-theoretic foundations of Bayesianism, examines
its descriptive limitations as highlighted by the Ellsberg paradox, and
addresses the challenges of comparing different normative systems.
"
2412.10643,2024-12-23,Scientific Realism vs. Anti-Realism: Toward a Common Ground,"  The debate between scientific realism and anti-realism remains at a
stalemate, making reconciliation seem hopeless. Yet, important work remains:
exploring a common ground, even if only to uncover deeper points of
disagreement and, ideally, to benefit both sides of the debate. I propose such
a common ground. Specifically, many anti-realists, such as instrumentalists,
have yet to seriously engage with Sober's call to justify their preferred
version of Ockham's razor through a positive account. Meanwhile, realists face
a similar challenge: providing a non-circular explanation of how their version
of Ockham's razor connects to truth. The common ground I propose addresses
these challenges for both sides; the key is to leverage the idea that everyone
values some truths and to draw on insights from scientific fields that study
scientific inference -- namely, statistics and machine learning. This common
ground also isolates a distinctively epistemic root of the irreconcilability in
the realism debate.
"
2412.11211,2024-12-17,"Deep Learning-based Approaches for State Space Models: A Selective
  Review","  State-space models (SSMs) offer a powerful framework for dynamical system
analysis, wherein the temporal dynamics of the system are assumed to be
captured through the evolution of the latent states, which govern the values of
the observations. This paper provides a selective review of recent advancements
in deep neural network-based approaches for SSMs, and presents a unified
perspective for discrete time deep state space models and continuous time ones
such as latent neural Ordinary Differential and Stochastic Differential
Equations. It starts with an overview of the classical maximum likelihood based
approach for learning SSMs, reviews variational autoencoder as a general
learning pipeline for neural network-based approaches in the presence of latent
variables, and discusses in detail representative deep learning models that
fall under the SSM framework. Very recent developments, where SSMs are used as
standalone architectural modules for improving efficiency in sequence modeling,
are also examined. Finally, examples involving mixed frequency and
irregularly-spaced time series data are presented to demonstrate the advantage
of SSMs in these settings.
"
2412.12233,2024-12-18,"Russian roulette: Why you can have a deterministic potential-outcome
  framework, or an asymmetric utility function, but not both","  It has been proposed in medical decision analysis to express the ``first do
no harm'' principle as an asymmetric utility function in which the loss from
killing a patient would count more than the gain from saving a life. Such a
utility depends on unrealized potential outcomes, and we show how this yields a
paradoxical decision recommendation in a simple hypothetical example involving
games of Russian roulette. The problem is resolved if we allow the potential
outcomes to be random variables. This leads us to conclude that, if you are
interested in this sort of asymmetric utility function, you need to move to the
stochastic potential outcome framework. We discuss the implications of the
choice of parameterization in this setting.
"
2412.13116,2024-12-18,"Equity in the Use of ChatGPT for the Classroom: A Comparison of the
  Accuracy and Precision of ChatGPT 3.5 vs. ChatGPT4 with Respect to Statistics
  and Data Science Exams","  A college education historically has been seen as method of moving upward
with regards to income brackets and social status. Indeed, many colleges
recognize this connection and seek to enroll talented low income students.
While these students might have their education, books, room, and board paid;
there are other items that they might be expected to use that are not part of
most college scholarship packages. One of those items that has recently
surfaced is access to generative AI platforms. The most popular of these
platforms is ChatGPT, and it has a paid version (ChatGPT4) and a free version
(ChatGPT3.5). We seek to explore differences in the free and paid versions in
the context of homework questions and data analyses as might be seen in a
typical introductory statistics course. We determine the extent to which
students who cannot afford newer and faster versions of generative AI programs
would be disadvantaged in terms of writing such projects and learning these
methods.
"
2412.14222,2025-01-14,"A Survey on Large Language Model-based Agents for Statistics and Data
  Science","  In recent years, data science agents powered by Large Language Models (LLMs),
known as ""data agents,"" have shown significant potential to transform the
traditional data analysis paradigm. This survey provides an overview of the
evolution, capabilities, and applications of LLM-based data agents,
highlighting their role in simplifying complex data tasks and lowering the
entry barrier for users without related expertise. We explore current trends in
the design of LLM-based frameworks, detailing essential features such as
planning, reasoning, reflection, multi-agent collaboration, user interface,
knowledge integration, and system design, which enable agents to address
data-centric problems with minimal human intervention. Furthermore, we analyze
several case studies to demonstrate the practical applications of various data
agents in real-world scenarios. Finally, we identify key challenges and propose
future research directions to advance the development of data agents into
intelligent statistical analysis software.
"
2412.16402,2024-12-24,"The Landscape of College-level Data Visualization Courses, and the
  Benefits of Incorporating Statistical Thinking","  Data visualization is a core part of statistical practice and is ubiquitous
in many fields. Although there are numerous books on data visualization,
instructors in statistics and data science may be unsure how to teach data
visualization, because it is such a broad discipline. To give guidance on
teaching data visualization from a statistical perspective, we make two
contributions. First, we conduct a survey of data visualization courses at top
colleges and universities in the United States, in order to understand the
landscape of data visualization courses. We find that most courses are not
taught by statistics and data science departments and do not focus on
statistical topics, especially those related to inference. Instead, most
courses focus on visual storytelling, aesthetic design, dashboard design, and
other topics specialized for other disciplines. Second, we outline three
teaching principles for incorporating statistical inference in data
visualization courses, and provide several examples that demonstrate how
instructors can follow these principles. The dataset from our survey allows
others to explore the diversity of data visualization courses, and our teaching
principles give guidance to instructors and departments who want to encourage
statistical thinking via data visualization. In this way, statistics-related
departments can provide a valuable perspective on data visualization that is
unique to current course offerings.
"
2412.16657,2024-12-25,"A Comprehensive Guide to Item Recovery Using the Multidimensional Graded
  Response Model in R","  The purpose of this study is to provide a step-by-step demonstration of item
recovery for the Multidimensional Graded Response Model (MGRM) in R. Within
this scope, a sample simulation design was constructed where the test lengths
were set to 20 and 40, the interdimensional correlations were varied as 0.3 and
0.7, and the sample size was fixed at 2000. Parameter estimates were derived
from the generated datasets for the 3-dimensional GRM, and bias and Root Mean
Square Error (RMSE) values were calculated and visualized. In line with the aim
of the study, R codes for all these steps were presented along with detailed
explanations, enabling researchers to replicate and adapt the procedures for
their own analyses. This study is expected to contribute to the literature by
serving as a practical guide for implementing item recovery in the MGRM. In
addition, the methods presented, including data generation, parameter
estimation, and result visualization, are anticipated to benefit researchers
even if they are not directly engaged in item recovery.
"
2412.19938,2024-12-31,Towards Strong AI: Transformational Beliefs and Scientific Creativity,"  Strong artificial intelligence (AI) is envisioned to possess general
cognitive abilities and scientific creativity comparable to human intelligence,
encompassing both knowledge acquisition and problem-solving. While remarkable
progress has been made in weak AI, the realization of strong AI remains a topic
of intense debate and critical examination. In this paper, we explore pivotal
innovations in the history of astronomy and physics, focusing on the discovery
of Neptune and the concept of scientific revolutions as perceived by
philosophers of science. Building on these insights, we introduce a simple
theoretical and statistical framework of weak beliefs, termed the
Transformational Belief (TB) framework, designed as a foundation for modeling
scientific creativity. Through selected illustrative examples in statistical
science, we demonstrate the TB framework's potential as a promising foundation
for understanding, analyzing, and even fostering creativity -- paving the way
toward the development of strong AI. We conclude with reflections on future
research directions and potential advancements.
"
2412.20175,2024-12-31,"An Undergraduate Course on the Statistical Principles of Research Study
  Design","  The undergraduate curriculum in statistics and data science is undergoing
changes to accommodate new methods, newly interested students, and the changing
role of statistics in society. Because of this, it is more important than ever
that students understand the role of study design and how to formulate
meaningful scientific and statistical research questions. While the traditional
Design of Experiments course is still extremely valuable for students heading
to industry and research careers, a broader study design course that
incorporates survey sampling, observational studies, and the basics of causal
inference with randomized experiment design is particularly useful for students
with a wide range of applied interests. Here, I describe such a course at a
small liberal arts college, along with ways to adapt it to meet different
student and instructor background and interests. The course serves as a
valuable bridge to advanced statistical coursework, meets key statistical
literacy and communication learning goals, and can be tailored to the desired
level of computational and mathematical fluency. Through reading, discussing,
and critiquing actual published research studies, students learn that
statistics is a living discipline with real consequences and become better
consumers and producers of scientific research and data-driven insights.
"
2501.00997,2025-01-03,Stochastic Simulation and Monte Carlo Method,"  These lecture notes are intended to cover some introductory topics in
stochastic simulation for scientific computing courses offered by the IT
department at Uppsala University, as taught by the author. Basic concepts in
probability theory are provided in the Appendix A, which you may review before
starting the upcoming sections or refer to as needed throughout the text.
"
2501.02126,2025-01-07,A Mathematical Lens for Teaching Data Science,"  Using the National Academies report, {\em Data Science for Undergraduates:
Opportunities and Options}, we connect data science curricula to the more
familiar pedagogy used by many mathematical scientists. We use their list of
``data acumen"" components to ground a discussion, which hopes to connect data
science curricula to the more familiar pedagogy used by many mathematical
scientists.
"
2501.03457,2025-01-08,A Bureaucratic Theory of Statistics,"  This commentary proposes a framework for understanding the role of statistics
in policy-making, regulation, and bureaucratic systems. I introduce the concept
of ""ex ante policy,"" describing statistical rules and procedures designed
before data collection to govern future actions. Through examining examples,
particularly clinical trials, I explore how ex ante policy serves as a calculus
of bureaucracy, providing numerical foundations for governance through clear,
transparent rules. The ex ante frame obviates heated debates about inferential
interpretations of probability and statistical tests, p-values, and rituals. I
conclude by calling for a deeper appreciation of statistics' bureaucratic
function and suggesting new directions for research in policy-oriented
statistical methodology.
"
2501.05584,2025-01-13,"The Impact of Question Framing on the Precision of Automatic Occupation
  Coding","  Occupational data play a vital role in research, official statistics, and
policymaking, yet their collection and accurate classification remain a
persistent challenge. This study investigates the effects of occupational
question wording on data variability and the performance of automatic coding
tools. Through a series of survey experiments conducted and replicated in
Germany, we tested two widely-used occupational question formats: one focusing
on ""job title"" (Berufsbezeichnung) and another on ""occupational tasks""
(berufliche T\""atigkeit). Our analysis reveals that automatic coding tools,
such as CASCOT and OccuCoDe, exhibit significant sensitivity to the form and
origin of the data. Specifically, these tools performed more efficiently when
coding responses to the job title question format compared to the occupational
task format. Additionally, we found that including examples of main tasks and
duties in the questions led respondents to provide more detailed but less
linguistically diverse responses. This reduced diversity may negatively affect
the precision of automatic coding. These findings highlight the importance of
tailoring automatic coding tools to the specific structure and origin of the
data they are applied to. We emphasize the need for further research to
optimize question design and coding tools for greater accuracy and
applicability in occupational data collection.
"
2501.08320,2025-01-15,"COMBO and COMMA: R packages for regression modeling and inference in the
  presence of misclassified binary mediator or outcome variables","  Misclassified binary outcome or mediator variables can cause unpredictable
bias in resulting parameter estimates. As more datasets that were not
originally collected for research purposes are being used for studies in the
social and health sciences, the need for methods that address data quality
concerns is growing. In this paper, we describe two R packages, COMBO and
COMMA, that implement bias-correction methods for misclassified binary outcome
and mediator variables, respectively. These likelihood-based approaches do not
require gold standard measures and allow for estimation of sensitivity and
specificity rates for the misclassified variable(s). In addition, these R
packages automatically apply crucial label switching corrections, allowing
researchers to circumvent the inherent permutation invariance of the
misclassification model likelihood. We demonstrate COMBO for single-outcome
cases using a study of bar exam passage. We develop and evaluate a risk
prediction model based on noisy indicators in a pretrial risk assessment study
to demonstrate COMBO for multi-outcome cases. In addition, we use COMMA to
evaluate the mediating effect of potentially misdiagnosed gestational
hypertension on the maternal ethnicity-birthweight relationship.
"
2501.08404,2025-01-16,Extremal events dictate population growth rate inference,"  Recent methods have been developed to map single-cell lineage statistics to
population growth. Because population growth selects for exponentially rare
phenotypes, these methods inherently depend on sampling large deviations from
finite data, which introduces systematic errors. A comprehensive understanding
of these errors in the context of finite data remains elusive. To address this
gap, we study the error in growth rate estimates across different models. We
show that under the usual bias-variance decomposition, the bias can be
decomposed into a finite-time bias and nonlinear averaging bias. We demonstrate
that finite-time bias, which dominates at short times, can be mitigated by
fitting its monotonic behavior. In contrast, at longer times, nonlinear
averaging bias becomes the predominant source of error, leading to a phase
transition. This transition can be understood through the Random Energy Model,
a mean-field model of disordered systems, where a few lineages dominate the
estimator. Applying these methods to experimental data demonstrates that
correcting for biases in lineage-based approaches yields consistent results for
the long-term growth rate across multiple methods and enables the
reverse-engineering of dynamic models. This new framework provides a
quantitative understanding of growth rate estimators, clarifies the conditions
under which they can be effectively applied to finite data, and introduces
model-free approaches for studying the connections between physiology and cell
growth.
"
2501.09171,2025-01-17,"Generative AI Takes a Statistics Exam: A Comparison of Performance
  between ChatGPT3.5, ChatGPT4, and ChatGPT4o-mini","  Many believe that use of generative AI as a private tutor has the potential
to shrink access and achievement gaps between students and schools with
abundant resources versus those with fewer resources. Shrinking the gap is
possible only if paid and free versions of the platforms perform with the same
accuracy. In this experiment, we investigate the performance of GPT versions
3.5, 4.0, and 4o-mini on the same 16-question statistics exam given to a class
of first-year graduate students. While we do not advocate using any generative
AI platform to complete an exam, the use of exam questions allows us to explore
aspects of ChatGPT's responses to typical questions that students might
encounter in a statistics course. Results on accuracy indicate that GPT 3.5
would fail the exam, GPT4 would perform well, and GPT4o-mini would perform
somewhere in between. While we acknowledge the existence of other Generative
AI/LLMs, our discussion concerns only ChatGPT because it is the most widely
used platform on college campuses at this time. We further investigate
differences among the AI platforms in the answers for each problem using
methods developed for text analytics, such as reading level evaluation and
topic modeling. Results indicate that GPT3.5 and 4o-mini have characteristics
that are more similar than either of them have with GPT4.
"
2501.10482,2025-01-22,Simulation of Random LR Fuzzy Intervals,"  Random fuzzy variables join the modeling of the impreciseness (due to their
``fuzzy part'') and randomness. Statistical samples of such objects are widely
used, and their direct, numerically effective generation is therefore
necessary. Usually, these samples consist of triangular or trapezoidal fuzzy
numbers. In this paper, we describe theoretical results and simulation
algorithms for another family of fuzzy numbers -- LR fuzzy numbers with
interval-valued cores. Starting from a simulation perspective on the piecewise
linear LR fuzzy numbers with the interval-valued cores, their limiting behavior
is then considered. This leads us to the numerically efficient algorithm for
simulating a sample consisting of such fuzzy values.
"
2501.10974,2025-02-10,"Sequential Change Detection for Learning in Piecewise Stationary Bandit
  Environments","  A finite-horizon variant of the quickest change detection problem is
investigated, which is motivated by a change detection problem that arises in
piecewise stationary bandits. The goal is to minimize the \emph{latency}, which
is smallest threshold such that the probability that the detection delay
exceeds the threshold is below a desired low level, while controlling the false
alarm probability to a desired low level. When the pre- and post-change
distributions are unknown, two tests are proposed as candidate solutions. These
tests are shown to attain order optimality in terms of the horizon.
Furthermore, the growth in their latencies with respect to the false alarm
probability and late detection probability satisfies a property that is
desirable in regret analysis for piecewise stationary bandits. Numerical
results are provided to validate the theoretical performance results.
"
2501.11813,2025-01-22,Utilising Deep Learning to Elicit Expert Uncertainty,"  Recent work [ 14 ] has introduced a method for prior elicitation that
utilizes records of expert decisions to infer a prior distribution. While this
method provides a promising approach to eliciting expert uncertainty, it has
only been demonstrated using tabular data, which may not entirely represent the
information used by experts to make decisions. In this paper, we demonstrate
how analysts can adopt a deep learning approach to utilize the method proposed
in [14 ] with the actual information experts use. We provide an overview of
deep learning models that can effectively model expert decision-making to
elicit distributions that capture expert uncertainty and present an example
examining the risk of colon cancer to show in detail how these models can be
used.
"
2501.12596,2025-01-23,"Adapting OpenAI's CLIP Model for Few-Shot Image Inspection in
  Manufacturing Quality Control: An Expository Case Study with Multiple
  Application Examples","  This expository paper introduces a simplified approach to image-based quality
inspection in manufacturing using OpenAI's CLIP (Contrastive Language-Image
Pretraining) model adapted for few-shot learning. While CLIP has demonstrated
impressive capabilities in general computer vision tasks, its direct
application to manufacturing inspection presents challenges due to the domain
gap between its training data and industrial applications. We evaluate CLIP's
effectiveness through five case studies: metallic pan surface inspection, 3D
printing extrusion profile analysis, stochastic textured surface evaluation,
automotive assembly inspection, and microstructure image classification. Our
results show that CLIP can achieve high classification accuracy with relatively
small learning sets (50-100 examples per class) for single-component and
texture-based applications. However, the performance degrades with complex
multi-component scenes. We provide a practical implementation framework that
enables quality engineers to quickly assess CLIP's suitability for their
specific applications before pursuing more complex solutions. This work
establishes CLIP-based few-shot learning as an effective baseline approach that
balances implementation simplicity with robust performance, demonstrated in
several manufacturing quality control applications.
"
2501.16008,2025-01-28,"Gaussian credible intervals in Bayesian nonparametric estimation of the
  unseen","  The unseen-species problem assumes $n\geq1$ samples from a population of
individuals belonging to different species, possibly infinite, and calls for
estimating the number $K_{n,m}$ of hitherto unseen species that would be
observed if $m\geq1$ new samples were collected from the same population. This
is a long-standing problem in statistics, which has gained renewed relevance in
biological and physical sciences, particularly in settings with large values of
$n$ and $m$. In this paper, we adopt a Bayesian nonparametric approach to the
unseen-species problem under the Pitman-Yor prior, and propose a novel
methodology to derive large $m$ asymptotic credible intervals for $K_{n,m}$,
for any $n\geq1$. By leveraging a Gaussian central limit theorem for the
posterior distribution of $K_{n,m}$, our method improves upon competitors in
two key aspects: firstly, it enables the full parameterization of the
Pitman-Yor prior, including the Dirichlet prior; secondly, it avoids the need
of Monte Carlo sampling, enhancing computational efficiency. We validate the
proposed method on synthetic and real data, demonstrating that it improves the
empirical performance of competitors by significantly narrowing the gap between
asymptotic and exact credible intervals for any $m\geq1$.
"
2501.17719,2025-01-30,"A Framework for Generating Realistic Synthetic Tabular Data in a
  Randomized Controlled Trial Setting","  Generation of realistic synthetic data has garnered considerable attention in
recent years, particularly in the health research domain due to its utility in,
for instance, sharing data while protecting patient privacy or determining
optimal clinical trial design. While much work has been concentrated on
synthetic image generation, generation of realistic and complex synthetic
tabular data of the type most commonly encountered in classic epidemiological
or clinical studies is still lacking, especially with regards to generating
data for randomized controlled trials (RTCs). There is no consensus regarding
the best way to generate synthetic tabular RCT data such that the underlying
multivariate data distribution is preserved. Motivated by an RCT in the
treatment of Human Immunodeficiency Virus, we empirically compared the ability
of several strategies and two generation techniques (one machine learning, the
other a more classical statistical method) to faithfully reproduce realistic
data. Our results suggest that using a sequential generation approach with a
R-vine copula model to generate baseline variables, followed by a simple random
treatment allocation to mimic the RCT setting, and subsequent regression models
for variables post-treatment allocation (such as the trial outcome) is the most
effective way to generate synthetic tabular RCT data that capture important and
realistic features of the real data.
"
2502.02927,2025-02-06,"Bayesian estimation of Unit-Weibull distribution based on dual
  generalized order statistics with application to the Cotton Production Data","  The Unit Weibull distribution with parameters $\alpha$ and $\beta$ is
considered to study in the context of dual generalized order statistics. For
the analysis purpose, Bayes estimators based on symmetric and asymmetric loss
functions are obtained. The methods which are utilized for Bayesian estimation
are approximation and simulation tools such as Lindley, Tierney-Kadane and
Markov chain Monte Carlo methods. The authors have considered squared error
loss function as symmetric and LINEX and general entropy loss function as
asymmetric loss functions. After presenting the mathematical results, a
simulation study is conducted to exhibit the performances of various derived
estimators. As this study is considered for the dual generalized order
statistics that is unification of models based distinct ordered random variable
such as order statistics, record values, etc. This provides flexibility in our
results and in continuation of this, the cotton production data of USA is
analyzed for both submodels of ordered random variables: order statistics and
record values.
"
2502.05336,2025-02-11,"Leveraging Order-Theoretic Tournament Graphs for Assessing Internal
  Consistency in Survey-Based Instruments Across Diverse Scenarios","  This paper introduces Monotone Delta, an order-theoretic measure designed to
enhance the reliability assessment of survey-based instruments in human-machine
interactions. Traditional reliability measures, such as Cronbach's Alpha and
McDonald's Omega, often yield misleading estimates due to their sensitivity to
redundancy, multidimensional constructs, and assumptions of normality and
uncorrelated errors. These limitations can compromise decision-making in
human-centric evaluations, where survey instruments inform adaptive interfaces,
cognitive workload assessments, and human-AI trust models. Monotone Delta
addresses these issues by quantifying internal consistency through the
minimization of ordinal contradictions and alignment with a unidimensional
latent order using weighted tournaments. Unlike traditional approaches, it
operates without parametric or model-based assumptions. We conducted
theoretical analyses and experimental evaluations on four challenging
scenarios: tau-equivalence, redundancy, multidimensionality, and non-normal
distributions, and proved that Monotone Delta provides more stable reliability
assessments compared to existing methods. The Monotone Delta is a valuable
alternative for evaluating questionnaire-based assessments in psychology, human
factors, healthcare, and interactive system design, enabling organizations to
optimize survey instruments, reduce costly redundancies, and enhance confidence
in human-system interactions.
"
2502.06628,2025-02-11,Random Variables aren't Random,"  This paper examines the foundational concept of random variables in
probability theory and statistical inference, demonstrating that their
mathematical definition requires no reference to randomization or hypothetical
repeated sampling. We show how measure-theoretic probability provides a
framework for modeling populations through distributions, leading to three key
contributions. First, we establish that random variables, properly understood
as measurable functions, can be fully characterized without appealing to
infinite hypothetical samples. Second, we demonstrate how this perspective
enables statistical inference through logical rather than probabilistic
reasoning, extending the reductio ad absurdum argument from deductive to
inductive inference. Third, we show how this framework naturally leads to
information-based assessment of statistical procedures, replacing traditional
inference metrics that emphasize bias and variance with information-based
approaches that better describe the families of distributions used in
parametric inference. This reformulation addresses long-standing debates in
statistical inference while providing a more coherent theoretical foundation.
Our approach offers an alternative to traditional frequentist inference that
maintains mathematical rigor while avoiding the philosophical complications
inherent in repeated sampling interpretations.
"
2502.07948,2025-02-13,The nature of mathematical models,"  Modeling has become a widespread, useful tool in mathematics applied to
diverse fields, from physics to economics to biomedicine. Practitioners of
modeling may use algebraic or differential equations, to the elements of which
they attribute an intuitive relationship with some relevant aspect of reality
they wish to represent. More sophisticated expressions may include
stochasticity, either as observation error or system noise. However, a clear,
unambiguous mathematical definition of what a model is and of what is the
relationship between the model and the real-life phenomena it purports to
represent has so far not been formulated. The present work aims to fill this
gap, motivating the definition of a mathematical model as an operator on a
Hilbert space of random variables, identifying the experimental realization as
the map between the theoretical space of model construction and the
computational space of statistical model identification, and tracing the
relationship of the geometry of the model manifold in the abstract setting with
the corresponding geometry of the prediction surfaces in statistical
estimation.
"
2502.11036,2025-02-19,A Survey: Potential Dimensionality Reduction Methods,"  Dimensionality reduction is a fundamental technique in machine learning and
data analysis, enabling efficient representation and visualization of
high-dimensional data. This paper explores five key methods: Principal
Component Analysis (PCA), Kernel PCA (KPCA), Sparse Kernel PCA, t-Distributed
Stochastic Neighbor Embedding (t-SNE), and Uniform Manifold Approximation and
Projection (UMAP). PCA provides a linear approach to capturing variance,
whereas KPCA and Sparse KPCA extend this concept to non-linear structures using
kernel functions. Meanwhile, t-SNE and UMAP focus on preserving local
relationships, making them effective for data visualization. Each method is
examined in terms of its mathematical formulation, computational complexity,
strengths, and limitations. The trade-offs between global structure
preservation, computational efficiency, and interpretability are discussed to
guide practitioners in selecting the appropriate technique based on their
application needs.
"
2502.11510,2025-04-28,"Here Be Dragons: Bimodal posteriors arise from numerical integration
  error in longitudinal models","  Longitudinal models with dynamics governed by differential equations may
require numerical integration alongside parameter estimation. We have
identified a situation where the numerical integration introduces error in such
a way that it becomes a novel source of non-uniqueness in estimation. We obtain
two very different sets of parameters, one of which is a good estimate of the
true values and the other a very poor one. The two estimates have forward
numerical projections statistically indistinguishable from each other because
of numerical error. In such cases, the posterior distribution for parameters is
bimodal, with a dominant mode closer to the true parameter value, and a second
cluster around the errant value. We demonstrate that multi-modality exists both
theoretically and empirically for an affine first order differential equation,
that a simulation workflow can test for evidence of the issue more generally,
and that Markov Chain Monte Carlo sampling with a suitable solution can avoid
bimodality. The issue of multi-modal posteriors arising from numerical error
has consequences for Bayesian inverse methods that rely on numerical
integration more broadly.
"
2502.11645,2025-02-18,"Deviation Ratings: A General, Clone-Invariant Rating Method","  Many real-world multi-agent or multi-task evaluation scenarios can be
naturally modelled as normal-form games due to inherent strategic (adversarial,
cooperative, and mixed motive) interactions. These strategic interactions may
be agentic (e.g. players trying to win), fundamental (e.g. cost vs quality), or
complementary (e.g. niche finding and specialization). In such a formulation,
it is the strategies (actions, policies, agents, models, tasks, prompts, etc.)
that are rated. However, the rating problem is complicated by redundancy and
complexity of N-player strategic interactions. Repeated or similar strategies
can distort ratings for those that counter or complement them. Previous work
proposed ``clone invariant'' ratings to handle such redundancies, but this was
limited to two-player zero-sum (i.e. strictly competitive) interactions. This
work introduces the first N-player general-sum clone invariant rating, called
deviation ratings, based on coarse correlated equilibria. The rating is
explored on several domains including LLMs evaluation.
"
2502.11820,2025-02-21,"A Diagnostic to Find and Help Combat Positivity Issues -- with a Focus
  on Continuous Treatments","  The positivity assumption is central in the identification of a causal
effect, yet is rarely discussed, especially in conjunction with continuous
treatments or Modified Treatment Policies. One common recommendation for
dealing with a violation is to change the estimand. However, an applied
researcher is faced with two problems: First, how can she tell whether there is
a positivity violation given her estimand of interest, preferably without
having to estimate a model first? Second, if she finds a problem with
positivity, how should she change her estimand in order to arrive at an
estimand which does not face the same issues? We suggest a novel diagnostic
which allows the researcher to answer both questions by providing insights into
how well an estimation for a certain estimand can be made for each observation
using the data at hand. We provide a simulation study on the general behaviour
of different MTPs at different levels of positivity violations and show how the
diagnostic helps understand where bias is to be expected. We illustrate the
application of our proposed diagnostic in a pharmacoepidemiological study based
on data from CHAPAS-3, a trial comparing different treatment regimens for
children living with HIV.
"
2502.12912,2025-02-19,"A Simplified and Numerically Stable Approach to the BG/NBD Churn
  Prediction model","  This study extends the BG/NBD churn probability model, addressing its
limitations in industries where customer behaviour is often influenced by
seasonal events and possibly high purchase counts. We propose a modified
definition of churn, considering a customer to have churned if they make no
purchases within M days. Our contribution is twofold: First, we simplify the
general equation for the specific case of zero purchases within M days. Second,
we derive an alternative expression using numerical techniques to mitigate
numerical overflow or underflow issues. This approach provides a more practical
and robust method for predicting customer churn in industries with irregular
purchase patterns.
"
2502.13106,2025-02-19,Score Matching Riemannian Diffusion Means,"  Estimating means on Riemannian manifolds is generally computationally
expensive because the Riemannian distance function is not known in closed-form
for most manifolds. To overcome this, we show that Riemannian diffusion means
can be efficiently estimated using score matching with the gradient of Brownian
motion transition densities using the same principle as in Riemannian diffusion
models. Empirically, we show that this is more efficient than Monte Carlo
simulation while retaining accuracy and is also applicable to learned
manifolds. Our method, furthermore, extends to computing the Fr\'echet mean and
the logarithmic map for general Riemannian manifolds. We illustrate the
applicability of the estimation of diffusion mean by efficiently extending
Euclidean algorithms to general Riemannian manifolds with a Riemannian
$k$-means algorithm and maximum likelihood Riemannian regression.
"
2502.13225,2025-02-20,"Entropy of spatial network with applications to non-extensive
  statistical mechanics","  A new method is proposed for analyzing complexity and studying the
information in random geometric networks using Tsallis entropy tool. Tsallis
entropy of the ensemble of random geometric networks is calculated based on the
components of the random connection model on the point process which is
obtained by connecting the points with a probability that depends on their
relative positions (10.1016/j.indag.2022.05.002, 2022). According to
information theory and conditional discussion, the bounds for Shannon and
Tsallis entropies of the ensemble of this random graph are presented. Using
this function and Lagrange's formula, the connection function that provides the
maximum Tsallis entropy based on general constraints is obtained. Then, a
simulation-based example is presented to clarify the application of the
proposed method in the study of ad hoc wireless networks. By observing the
obtained results, it can be stated that the wireless networks that adhere to
the model studied here are almost maximally complex. Also, Tsallis conditional
entropy maximizing function is compared with other connection functions using
numerical calculations and the optimal value for the maximization of
conditional entropies is obtained.
"
2502.14581,2025-05-13,A Statistical Case Against Empirical Human-AI Alignment,"  Empirical human-AI alignment aims to make AI systems act in line with
observed human behavior. While noble in its goals, we argue that empirical
alignment can inadvertently introduce statistical biases that warrant caution.
This position paper thus advocates against naive empirical alignment, offering
prescriptive alignment and a posteriori empirical alignment as alternatives. We
substantiate our principled argument by tangible examples like human-centric
decoding of language models.
"
2502.16988,2025-02-25,A tutorial on optimal dynamic treatment regimes,"  A dynamic treatment regime is a sequence of treatment decision rules tailored
to an individual's evolving status over time. In precision medicine, much focus
has been placed on finding an optimal dynamic treatment regime which, if
followed by everyone in the population, would yield the best outcome on
average; and extensive investigation has been conducted from both
methodological and applications standpoints. The aim of this tutorial is to
provide readers who are interested in optimal dynamic treatment regimes with a
systematic, detailed but accessible introduction, including the formal
definition and formulation of this topic within the framework of causal
inference, identification assumptions required to link the causal quantity of
interest to the observed data, existing statistical models and estimation
methods to learn the optimal regime from data, and application of these methods
to both simulated and real data.
"
2502.20281,2025-03-25,"Data Jamboree: A Party of Open-Source Software Solving Real-World Data
  Science Problems","  The evolving focus in statistics and data science education highlights the
growing importance of computing. This paper presents the Data Jamboree, a live
event that combines computational methods with traditional statistical
techniques to address real-world data science problems. Participants, ranging
from novices to experienced users, followed workshop leaders in using
open-source tools like Julia, Python, and R to perform tasks such as data
cleaning, manipulation, and predictive modeling. The Jamboree showcased the
educational benefits of working with open data, providing participants with
practical, hands-on experience. We compared the tools in terms of efficiency,
flexibility, and statistical power, with Julia excelling in performance, Python
in versatility, and R in statistical analysis and visualization. The paper
concludes with recommendations for designing similar events to encourage
collaborative learning and critical thinking in data science.
"
2503.02645,2025-03-05,A Generalized Theory of Mixup for Structure-Preserving Synthetic Data,"  Mixup is a widely adopted data augmentation technique known for enhancing the
generalization of machine learning models by interpolating between data points.
Despite its success and popularity, limited attention has been given to
understanding the statistical properties of the synthetic data it generates. In
this paper, we delve into the theoretical underpinnings of mixup, specifically
its effects on the statistical structure of synthesized data. We demonstrate
that while mixup improves model performance, it can distort key statistical
properties such as variance, potentially leading to unintended consequences in
data synthesis. To address this, we propose a novel mixup method that
incorporates a generalized and flexible weighting scheme, better preserving the
original data's structure. Through theoretical developments, we provide
conditions under which our proposed method maintains the (co)variance and
distributional properties of the original dataset. Numerical experiments
confirm that the new approach not only preserves the statistical
characteristics of the original data but also sustains model performance across
repeated synthesis, alleviating concerns of model collapse identified in
previous research.
"
2503.03484,2025-03-06,"The impact of the storytelling fallacy on real data examples in
  methodological research","  The term ""researcher degrees of freedom"" (RDF), which was introduced in
metascientific literature in the context of the replication crisis in science,
refers to the extent of flexibility a scientist has in making decisions related
to data analysis. These choices occur at all stages of the data analysis
process. In combination with selective reporting, RDF may lead to
over-optimistic statements and an increased rate of false positive findings.
Even though the concept has been mainly discussed in fields such as
epidemiology or psychology, similar problems affect methodological statistical
research. Researchers who develop and evaluate statistical methods are left
with a multitude of decisions when designing their comparison studies. This
leaves room for an over-optimistic representation of the performance of their
preferred method(s). The present paper defines and explores a particular RDF
that has not been previously identified and discussed. When interpreting the
results of real data examples that are most often part of methodological
evaluations, authors typically tell a domain-specific ""story"" that best
supports their argumentation in favor of their preferred method. However, there
are often plenty of other plausible stories that would support different
conclusions. We define the ""storytelling fallacy"" as the selective use of
anecdotal domain-specific knowledge to support the superiority of specific
methods in real data examples. While such examples fed by domain knowledge play
a vital role in methodological research, if deployed inappropriately they can
also harm the validity of conclusions on the investigated methods. The goal of
our work is to create awareness for this issue, fuel discussions on the role of
real data in generating evidence in methodological research and warn readers of
methodological literature against naive interpretations of real data examples.
"
2503.05963,2025-03-11,Bayesian Graph Traversal,"  This research considers Bayesian decision-analytic approaches toward the
traversal of an uncertain graph. Namely, a traveler progresses over a graph in
which rewards are gained upon a node's first visit and costs are incurred for
every edge traversal. The traveler knows the graph's adjacency matrix and his
starting position but does not know the rewards and costs. The traveler is a
Bayesian who encodes his beliefs about these values using a Gaussian process
prior and who seeks to maximize his expected utility over these beliefs.
Adopting a decision-analytic perspective, we develop sequential decision-making
solution strategies for this coupled information-collection and network-routing
problem. We show that the problem is NP-Hard and derive properties of the
optimal walk. These properties provide heuristics for the traveler's problem
that balance exploration and exploitation. We provide a practical case study
focused on the use of unmanned aerial systems for public safety and empirically
study policy performance in myriad Erdos-Renyi settings.
"
2503.08743,2025-03-13,Hard negative sampling in hyperedge prediction,"  Hypergraph, which allows each hyperedge to encompass an arbitrary number of
nodes, is a powerful tool for modeling multi-entity interactions. Hyperedge
prediction is a fundamental task that aims to predict future hyperedges or
identify existent but unobserved hyperedges based on those observed. In link
prediction for simple graphs, most observed links are treated as positive
samples, while all unobserved links are considered as negative samples.
However, this full-sampling strategy is impractical for hyperedge prediction,
due to the number of unobserved hyperedges in a hypergraph significantly
exceeds the number of observed ones. Therefore, one has to utilize some
negative sampling methods to generate negative samples, ensuring their quantity
is comparable to that of positive samples. In current hyperedge prediction,
randomly selecting negative samples is a routine practice. But through
experimental analysis, we discover a critical limitation of random selecting
that the generated negative samples are too easily distinguishable from
positive samples. This leads to premature convergence of the model and reduces
the accuracy of prediction. To overcome this issue, we propose a novel method
to generate negative samples, named as hard negative sampling (HNS). Unlike
traditional methods that construct negative hyperedges by selecting node sets
from the original hypergraph, HNS directly synthesizes negative samples in the
hyperedge embedding space, thereby generating more challenging and informative
negative samples. Our results demonstrate that HNS significantly enhances both
accuracy and robustness of the prediction. Moreover, as a plug-and-play
technique, HNS can be easily applied in the training of various hyperedge
prediction models based on representation learning.
"
2503.10710,2025-03-17,"How causal perspectives can inform problems in computational
  neuroscience","  Over the past two decades, considerable strides have been made in advancing
neuroscience techniques, yet the translation of these advancements into
clinically relevant insights for human mental health remains a challenge. This
review addresses a fundamental issue in neuroscience - attributing causality -
and advocates for the development of robust causal frameworks. We
systematically introduce the necessary definitions and concepts, emphasizing
the implicit role of causal frameworks in neuroscience investigations. We
illustrate how persistent challenges in neuroscience, such as batch effects and
selection biases, can be conceptualized and approached using causal frameworks.
Through theoretical development and real-world examples, we show how these
causal perspectives highlight numerous shortcomings of existing data collection
strategies and analytical approaches. We demonstrate how causal frameworks can
inform both experimental design and analysis, particularly for observational
studies where traditional randomization is infeasible. Using neuroimaging as a
detailed case study, we explore the advantages, shortcomings, and implications
for generalizability that these perspectives afford to existing and novel
research paradigms. Together, we believe that this perspective offers a
framework for conceptualizing, framing, and inspiring innovative approaches to
problems in neuroscience.
"
2503.10984,2025-05-15,"The Problem of the Priors, or Posteriors?","  The problem of the priors is well known: it concerns the challenge of
identifying norms that govern one's prior credences. I argue that a key to
addressing this problem lies in considering what I call the problem of the
posteriors -- the challenge of identifying norms that directly govern one's
posterior credences, which backward induce some norms on the priors via the
diachronic requirement of conditionalization. This forward-looking approach can
be summarized as: Think ahead, work backward. Although this idea can be traced
to Freedman (1963), Carnap (1963), and Shimony (1970), I believe that it has
not received enough attention. In this paper, I initiate a systematic defense
of forward-looking Bayesianism, addressing potential objections from more
traditional views (both subjectivist and objectivist). I also develop a
specific approach to forward-looking Bayesianism -- one that values the
convergence of posterior credences to the truth, and treats it as a fundamental
rather than derived norm. This approach, called {\em convergentist
Bayesianism}, is argued to be crucial for a Bayesian foundation of Ockham's
razor in statistics and machine learning.
"
2503.11289,2025-03-17,A quantile-based bivariate distribution,"  In this paper we present a flexible bivariate distribution specified by a
quantile function. The distribution contains as special cases new bivariate
exponential, Pareto I, Pareto II, beta, power, log logistic and uniform
distributions and also can approximate many other continuous models. Various
$L$-moment based properties of the distribution such as covariance, coskewness,
cokurtosis, $L$-correlation, etc are discussed. The distribution is used to
model two real data sets.
"
2503.13574,2025-03-19,The Hall of Fame Cut in Major League Baseball,"  I present a simple and transparent standard for career greatness in baseball:
any major league player with H > 2500 or HR > 350 or K > 2800 or W > 240 makes
my Hall of Fame Cut. Rate statistics are avoided due to small sample issues and
to ensure the standard is permanent once achieved. Hits and home runs were
chosen to represent the two extremes of batting styles. Strikeouts are chosen
as the most fundamental unit of pitching performance whereas wins are included
in deference to their historical importance as a benchmark. Most major league
batters and pitchers in the elected Hall of Fame also make my Hall of Fame Cut
but my quantitative standard shifts attention to several under-appreciated
players, such as Johnny Damon and Bartolo Colon, and allows us to celebrate
recent and active players without the waiting period (5 years post-retirement)
needed for Hall of Fame election. My Hall of Fame Cut is also agnostic to
performance enhancement or off-field issues and strongly favors longevity over
peak performance.
"
2503.14650,2025-03-20,Resolving Jeffreys-Lindley Paradox,"  Jeffreys-Lindley paradox is a case where frequentist and Bayesian hypothesis
testing methodologies contradict with each other. This has caused confusion
among data analysts for selecting a methodology for their statistical inference
tasks. Though the paradox goes back to mid 1930's so far there hasn't been a
satisfactory resolution given for it. In this paper we show that it arises
mainly due to the simple fact that, in the frequentist approach, the difference
between the hypothesized parameter value and the observed estimate of the
parameter is assessed in terms of the standard error of the estimate, no matter
what the actual numerical difference is and how small the standard error is,
whereas in the Bayesian methodology it has no effect due to the definition of
the Bayes factor in the context, even though such an assessment is present. In
fact, the paradox is an instance of conflict between statistical and practical
significance and a result of using a sharp null hypothesis to approximate an
acceptable small range of values for the parameter. Occurrence of type-I error
that is allowed in frequentist methodology plays important role in the paradox.
Therefore, the paradox is not a conflict between two inference methodologies
but an instance of not agreeing their conclusions
"
2503.14839,2025-03-20,"Bayesian hierarchical non-stationary hybrid modeling for threshold
  estimation in peak over threshold approach","  Extreme value theory (EVT) has been utilized to estimate crash risk from
traffic conflicts with the peak over threshold approach. However, it's
challenging to determine a suitable threshold to distinguish extreme conflicts
in an objective way. The subjective and arbitrary selection of the threshold in
the peak over threshold approach can result in biased estimation outcomes. This
study proposes a Bayesian hierarchical hybrid modeling (BHHM) framework for the
threshold estimation in the peak over threshold approach. Specifically, BHHM is
based on a piecewise function to model the general conflicts with specific
distribution while model the extreme conflicts with generalized Pareto
distribution (GPD). The Bayesian hierarchical structure is used to combine
traffic conflicts from different sites, incorporating covariates and
site-specific unobserved heterogeneity. Five non-stationary BHHM models,
including Normal-GPD, Cauchy-GPD, Logistic-GPD, Gamma-GPD, and Lognormal-GPD
models, were developed and compared. Traditional graphical diagnostic and
quantile regression approaches were also used for comparison. Traffic conflicts
collected from three signalized intersections in the city of Surrey, British
Columbia were used for the study. The results show that the proposed BHHM
approach could estimate the threshold parameter objectively. The Lognormal-GPD
model is superior to the other four BHHM models in terms of crash estimation
accuracy and model fit. The crash estimates using the threshold determined by
the BHHM outperform those estimated based on the graphical diagnostic and
quantile regression approaches, indicating the superiority of the proposed
threshold determination approach. The findings of this study contribute to
enhancing the existing EVT methods for providing a threshold determination
approach as well as producing reliable crash estimations.
"
2503.15382,2025-04-04,"The information mismatch, and how to fix it","  We live in unprecedented times in terms of our ability to use evidence to
inform medical care. For example, we can perform data-driven post-test
probability calculations. However, there is work to do. In current studies,
sensitivity and specificity, which play a key role in post-test probability
calculations, are defined as unadjusted for patient covariates. In light of
this, there have been multiple recommendations that sensitivity and specificity
be adjusted for covariates. However, there is less work on the downstream
clinical impact of unadjusted sensitivity and specificity. We discuss this
here. We argue that unadjusted sensitivity and specificity can lead to a
post-test probability that contains an ``information mismatch.'' We write the
equations behind such an information mismatch and discuss the steps that can be
taken to fix it.
"
2503.15821,2025-03-21,"Temporal Point Process Modeling of Aggressive Behavior Onset in
  Psychiatric Inpatient Youths with Autism","  Aggressive behavior, including aggression towards others and self-injury,
occurs in up to 80% of children and adolescents with autism, making it a
leading cause of behavioral health referrals and a major driver of healthcare
costs. Predicting when autistic youth will exhibit aggression is challenging
due to their communication difficulties. Many are minimally verbal or have poor
emotional insight. Recent advances in Machine Learning and wearable biosensing
enable short-term aggression predictions within a limited future window
(typically one to three minutes). However, existing models do not estimate
aggression probability within longer future windows nor the expected number of
aggression onsets over such a period. To address these limitations, we employ
Temporal Point Processes (TPPs) to model the generative process of aggressive
behavior onsets in inpatient youths with autism. We hypothesize that aggressive
behavior onsets follow a self-exciting process driven by short-term history,
making them well-suited for Hawkes Point Process modeling. We establish a
benchmark and demonstrate through Goodness-of-Fit statistics and predictive
metrics that TPPs perform well modeling aggressive behavior onsets in inpatient
youths with autism. Additionally, we gain insights into the onset generative
process, like the branching factor near criticality, and suggest TPPs may
enhance future clinical decision-making and preemptive interventions.
"
2503.17598,2025-04-04,Coarse-Grained Games: A Framework for Bounded Perception in Game Theory,"  In everyday life, we frequently make coarse-grained judgments. When we say
that Olivia and Noah excel in mathematics, we disregard the specific
differences in their mathematical abilities. Similarly, when we claim that a
particular automobile manufacturer produces high-quality cars, we overlook the
minor variations among individual vehicles. These coarse-grained assessments
are distinct from erroneous or deceptive judgments, such as those resulting
from student cheating or false advertising by corporations. Despite the
prevalence of such judgments, little attention has been given to their
underlying mathematical structure. In this paper, we introduce the concept of
coarse-graining into game theory, analyzing games where players may perceive
different payoffs as identical while preserving the underlying order structure.
We call it a Coarse-Grained Game (CGG). This framework allows us to examine the
rational inference processes that arise when players equate distinct
micro-level payoffs at a macro level, and to explore how Nash equilibria are
preserved or altered as a result. Our key findings suggest that CGGs possess
several desirable properties that make them suitable for modeling phenomena in
the social sciences. This paper demonstrates two such applications: first, in
cases of overly minor product updates, consumers may encounter an equilibrium
selection problem, resulting in market behavior that is not driven by objective
quality differences; second, the lemon market can be analyzed not only through
objective information asymmetry but also through asymmetries in perceptual
resolution or recognition ability.
"
2503.19068,2025-03-26,Minimum Volume Conformal Sets for Multivariate Regression,"  Conformal prediction provides a principled framework for constructing
predictive sets with finite-sample validity. While much of the focus has been
on univariate response variables, existing multivariate methods either impose
rigid geometric assumptions or rely on flexible but computationally expensive
approaches that do not explicitly optimize prediction set volume. We propose an
optimization-driven framework based on a novel loss function that directly
learns minimum-volume covering sets while ensuring valid coverage. This
formulation naturally induces a new nonconformity score for conformal
prediction, which adapts to the residual distribution and covariates. Our
approach optimizes over prediction sets defined by arbitrary norm balls,
including single and multi-norm formulations. Additionally, by jointly
optimizing both the predictive model and predictive uncertainty, we obtain
prediction sets that are tight, informative, and computationally efficient, as
demonstrated in our experiments on real-world datasets.
"
2503.20852,2025-03-28,"Teachable normal approximations to binomial and related probabilities or
  confidence bounds","  This document is an extended version of an abstract for a talk, with
approximately the same title, to be held at the 7th Joint Statistical Meeting
of the Deutsche Arbeitsgemeinschaft Statistik, from 24 to 28 March 2025 in
Berlin.
  Here ``teachable'' is meant to apply to people ranging from sufficiently
advanced high school pupils to university students in mathematics or
statistics: For understanding most of the proposed approximation results, it
should suffice to know binomial laws, their means and variances, and the
standard normal distribution function (but not necessarily the concept of a
corresponding normal random variable).
  Of the proposed approximations, some are well-known (at least to experts),
and some are based on teaching experience and research at Trier University.
"
2503.21719,2025-05-19,The Principle of Redundant Reflection,"  The fact that redundant information does not change a rational belief after
Bayesian updating implies uniqueness of Bayes rule. In fact, any updating rule
is uniquely specified by this principle. This is true for the classical
setting, as well as settings with improper or continuous priors. We prove this
result and illustrate it with two examples.
"
2503.22945,2025-04-01,Statistics at a Crossroads; Who is for the Challenge?,"  This project was sponsored by the National Science Foundation and organized
by a steering committee and a group of theme leaders. The six-member steering
committee, consisting of James Berger, Xuming He, David Madigan, Susan Murphy,
Bin Yu, and Jon Wellner, was responsible for the overall planning of the
project.
  This report is designed to be accessible to the wider audience of key
stakeholders in statistics and data science, including academic departments,
university administration, and funding agencies. After the role and the value
of Statistics and Data Science are discussed in Section 1, the report focuses
on the two goals related to emerging research and data-driven challenges in
applications. Section 2 identifies emerging research topics from the data
challenges arising from scientific and social applications, and Section 3
discusses a number of emerging areas in foundational research. How to engage
with those data-driven challenges and foster interdisciplinary collaborations
is also summarized in the Executive Summary. The third goal of creating a
vibrant research community and maintaining an appropriate balance is addressed
in Sections 4 (Professional Culture and Community Responsibilities) and 5
(Doctoral Education).
"
2504.01276,2025-04-03,"Online Fault Detection and Classification of Chemical Process Systems
  Leveraging Statistical Process Control and Riemannian Geometric Analysis","  In this work, we study an integrated fault detection and classification
framework called FARM for fast, accurate, and robust online chemical process
monitoring. The FARM framework integrates the latest advancements in
statistical process control (SPC) for monitoring nonparametric and
heterogeneous data streams with novel data analysis approaches based on
Riemannian geometry together in a hierarchical framework for online process
monitoring. We conduct a systematic evaluation of the FARM monitoring framework
using the Tennessee Eastman Process (TEP) dataset. Results show that FARM
performs competitively against state-of-the-art process monitoring algorithms
by achieving a good balance among fault detection rate (FDR), fault detection
speed (FDS), and false alarm rate (FAR). Specifically, FARM achieved an average
FDR of 96.97% while also outperforming benchmark methods in successfully
detecting hard-to-detect faults that are previously known, including Faults 3,
9 and 15, with FDRs being 97.08%, 96.30% and 95.99%, respectively. In terms of
FAR, our FARM framework allows practitioners to customize their choice of FAR,
thereby offering great flexibility. Moreover, we report a significant
improvement in average fault classification accuracy during online monitoring
from 61% to 82% when leveraging Riemannian geometric analysis, and further to
84.5% when incorporating additional features from SPC. This illustrates the
synergistic effect of integrating fault detection and classification in a
holistic, hierarchical monitoring framework.
"
2504.02960,2025-04-07,An Anytime Valid Test for Complete Spatial Randomness,"  A relevant question when analyzing spatial point patterns is that of spatial
randomness. More specifically, before any model can be fit to a point pattern a
first step is to test the data for departures from complete spatial randomness
(CSR). Traditional techniques employ distance or quadrat counts based methods
to test for CSR based on batched data. In this paper, we consider the practical
scenario of testing for CSR when the data are available sequentially (i.e.,
online). We present a sequential testing methodology called as {\em
PRe-process} that is based on e-values and is a fast, efficient and
nonparametric method. Simulation experiments with the truth departing from CSR
in two different scenarios show that the method is effective in capturing
inhomogeneity over time. Two real data illustrations considering lung cancer
cases in the Chorley-Ribble area, England from 1974 - 1983 and locations of
earthquakes in the state of Oklahoma, USA from 2000 - 2011 demonstrate the
utility of the PRe-process in sequential testing of CSR.
"
2504.05102,2025-04-08,Underreporting of Intimate Partner Violence in Brazil,"  According to WHO (2013), in general 30% of all women worldwide who have been
in a relationship have experienced physical and/or sexual violence by their
intimate partner. However, only a small percentage of intimate partner violence
(IPV) victims report it to the police. This phenomenon of under-reporting is
known as ``dark figure''. This paper aims to investigate the factors associated
with the reporting decision of IPV victims to the police in Brazil using the
third wave of the ``Pesquisa de Condi\c{c}\~{o}es Socioecon\^{o}micas e
Viol\^{e}ncia Dom\'{e}stica e Familiar contra a Mulher ($PCSVDF^{Mulher}$)''.
Using a bivariate probit regression model with sample selection, we found that
older white women, those who do not tolerate domestic violence, and women who
have experienced physical violence are more likely to report IPV to the police.
In contrast, married women, those with partners who abuse alcohol and those who
witnessed or knew that their mothers had experienced IPV, are less likely to
report it to law enforcement.
"
2504.06507,2025-04-10,"The Software Behind the Stats: A Student Exploration of Software Trends
  Across Disciplines","  This paper presents a student-led activity designed to explore the use of
statistical software in academic research across economics, political science,
and statistics. Students reviewed replication files from major journals and
repositories, gaining hands-on experience with reproducible workflows while
contributing to cross-disciplinary datasets. Web-scraped metadata and student
data collection, together covering more than 10,000 papers, reveal clear
disciplinary patterns: Stata remains dominant in economics, while R is
increasingly popular in political science and is the standard in statistics.
Within the social sciences, a growing number of articles also use multiple
software platforms within a single manuscript. Students reported increased
understanding of academic workflows and greater awareness of software diversity
in quantitative research. The activity is easy to adapt across course levels
and disciplines, and we offer suggestions for follow-up assignments that
reinforce key concepts in reproducibility and data fluency. The resulting
insights into current software practices are also valuable for instructors
seeking to align their teaching with evolving trends in research.
"
2504.06787,2025-04-10,"Communicating complex statistical models to a public health audience:
  translating science into action with the FARSI approach","  Background. Effectively communicating complex statistical model outputs is a
major challenge in public health. This study introduces the FARSI approach
(Fast, Accessible, Reliable, Secure, Informative) as a framework to enhance the
translation of intricate statistical findings into actionable insights for
policymakers and stakeholders. We apply this framework in a real-world case
study on chronic disease monitoring in Italy.
  Methods. The FARSI framework outlines key principles for developing
user-friendly tools that improve the translation of statistical results. We
applied these principles to create an open-access web application using R
Shiny, designed to communicate chronic disease prevalence estimates from a
Bayesian spatio-temporal logistic model. The case study highlights the
importance of an intuitive design for fast accessibility, validated data and
expert feedback for reliability, aggregated data for security, and insights
into prevalence population subgroups, which were previously unobservable, for
informativeness.
  Results. The web application enables stakeholders to explore disease
prevalence across populations and geographical area through dynamic
visualizations. It facilitates public health monitoring by, for instance,
identifying disparities at the local level and assessing risk factors such as
smoking. Its user-friendly interface enhances accessibility, making statistical
findings more actionable. Conclusions. The FARSI framework provides a
structured approach to improving the communication of complex research
findings. By making statistical models more accessible and interpretable, it
supports evidence-based decision-making in public health and increases the
societal impact of research.
"
2504.07704,2025-04-11,Measures of non-simplifyingness for conditional copulas and vines,"  In copula modeling, the simplifying assumption has recently been the object
of much interest. Although it is very useful to reduce the computational
burden, it remains far from obvious whether it is actually satisfied in
practice. We propose a theoretical framework which aims at giving a precise
meaning to the following question: how non-simplified or close to be simplified
is a given conditional copula? For this, we propose a theoretical framework
centered at the notion of measure of non-constantness. Then we discuss
generalizations of the simplifying assumption to the case where the conditional
marginal distributions may not be continuous, and corresponding measures of
non-simplifyingness in this case. The simplifying assumption is of particular
importance for vine copula models, and we therefore propose a notion of measure
of non-simplifyingness of a given copula for a particular vine structure, as
well as different scores measuring how non-simplified such a vine
decompositions would be for a general vine. Finally, we propose estimators for
these measures of non-simplifyingness given an observed dataset.
"
2504.08263,2025-04-14,"A roadmap for systematic identification and analysis of multiple biases
  in causal inference","  Observational studies examining causal effects rely on unverifiable causal
assumptions, the violation of which can induce multiple biases. Quantitative
bias analysis (QBA) methods examine the sensitivity of findings to such
violations, generally by producing bias-adjusted estimates under alternative
assumptions. Common strategies for QBA address either a single source of bias
or multiple sources one at a time, thus not informing the overall impact of the
potential biases. We propose a systematic approach (roadmap) for identifying
and analysing multiple biases together. Briefly, this consists of (i)
articulating the assumptions underlying the primary analysis through
specification and emulation of the ""ideal trial"" that defines the causal
estimand of interest and depicting these assumptions using casual diagrams;
(ii) depicting alternative assumptions under which biases arise using causal
diagrams; (iii) obtaining a single estimate simultaneously adjusted for all
biases under the alternative assumptions. We illustrate the roadmap in an
investigation of the effect of breastfeeding on risk of childhood asthma. We
further use simulations to evaluate a recent simultaneous adjustment approach
and illustrate the need for simultaneous rather than one-at-a-time adjustment
to examine the overall impact of biases. The proposed roadmap should facilitate
the conduct of high-quality multiple bias analyses.
"
2504.11035,2025-04-23,"A conceptual synthesis of causal assumptions for causal discovery and
  inference","  This work presents a conceptual synthesis of causal discovery and inference
frameworks, with a focus on how foundational assumptions -- causal sufficiency,
causal faithfulness, and the causal Markov condition -- are formalized and
operationalized across methodological traditions. Through structured tables and
comparative summaries, I map core assumptions, tasks, and analytical choices
from multiple causal frameworks, highlighting their connections and
differences. The synthesis provides practical guidance for researchers
designing causal studies, especially in settings where observational or
experimental constraints challenge standard approaches. This guide spans all
phases of causal analysis, including question formulation, formalization of
background knowledge, selection of appropriate frameworks, choice of study
design or algorithm, and interpretation. It is intended as a tool to support
rigorous causal reasoning across diverse empirical domains.
"
2504.12481,2025-04-18,"Understanding and Evaluating Engineering Creativity:Development and
  Validation of the Engineering Creativity Assessment Tool (ECAT)","  Creativity is essential in engineering education, enabling students to
develop innovative and practical solutions. However, assessing creativity
remains challenging due to a lack of reliable, domain-specific tools.
Traditional assessments like the Torrance Tests of Creative Thinking (TTCT) may
not fully capture the complexity of engineering creativity. This study
introduces and validates the Engineering Creativity Assessment Tool (ECAT),
designed specifically for engineering contexts. ECAT was tested with 199
undergraduate students who completed a hands-on design task. Five trained
raters evaluated the products using the ECAT rubric. Exploratory and
confirmatory factor analyses supported a four-factor structure: fluency,
originality, cognitive flexibility, and creative strengths. Reliability was
high, convergent and discriminant validity were examined using TTCT scores,
revealing moderate correlations that support ECATs domain specificity. ECAT
offers a reliable, valid framework for assessing creativity in engineering
education and provides actionable feedback to educators. Future work should
examine its broader applicability across disciplines and instructional
settings.
"
2504.15246,2025-04-22,"A Refreshment Stirred, Not Shaken (III): Can Swapping Be Differentially
  Private?","  The quest for a precise and contextually grounded answer to the question in
the present paper's title resulted in this stirred-not-shaken triptych, a
phrase that reflects our desire to deepen the theoretical basis, broaden the
practical applicability, and reduce the misperception of differential privacy
(DP)$\unicode{x2014}$all without shaking its core foundations. Indeed, given
the existence of more than 200 formulations of DP (and counting), before even
attempting to answer the titular question one must first precisely specify what
it actually means to be DP. Motivated by this observation, a theoretical
investigation into DP's fundamental essence resulted in Part I of this trio,
which introduces a five-building-block system explicating the who, where, what,
how and how much aspects of DP. Instantiating this system in the context of the
United States Decennial Census, Part II then demonstrates the broader
applicability and relevance of DP by comparing a swapping strategy like that
used in 2010 with the TopDown Algorithm$\unicode{x2014}$a DP method adopted in
the 2020 Census. This paper provides nontechnical summaries of the preceding
two parts as well as new discussion$\unicode{x2014}$for example, on how greater
awareness of the five building blocks can thwart privacy theatrics; how our
results bridging traditional SDC and DP allow a data custodian to reap the
benefits of both these fields; how invariants impact disclosure risk; and how
removing the implicit reliance on aleatoric uncertainty could lead to new
generalizations of DP.
"
2504.15290,2025-04-23,"Parental Imprints On Birth Weight: A Data-Driven Model For Neonatal
  Prediction In Low Resource Prenatal Care","  Accurate fetal birth weight prediction is a cornerstone of prenatal care, yet
traditional methods often rely on imaging technologies that remain inaccessible
in resource-limited settings. This study presents a novel machine
learning-based framework that circumvents these conventional dependencies,
using a diverse set of physiological, environmental, and parental factors to
refine birth weight estimation. A multi-stage feature selection pipeline
filters the dataset into an optimized subset, demonstrating previously
underexplored yet clinically relevant predictors of fetal growth. By
integrating advanced regression architectures and ensemble learning strategies,
the model captures non-linear relationships often overlooked by traditional
approaches, offering a predictive solution that is both interpretable and
scalable. Beyond predictive accuracy, this study addresses a question: whether
birth weight can be reliably estimated without conventional diagnostic tools.
The findings challenge entrenched methodologies by introducing an alternative
pathway that enhances accessibility without compromising clinical utility.
While limitations exist, the study lays the foundation for a new era in
prenatal analytics, one where data-driven inference competes with, and
potentially redefines, established medical assessments. By bridging
computational intelligence with obstetric science, this research establishes a
framework for equitable, technology-driven advancements in maternal-fetal
healthcare.
"
2504.15617,2025-04-23,"Spatiotemporal Assessment of Aircraft Noise Exposure Using Mobile
  Phone-Derived Population Estimates and High-Resolution Noise Measurements","  Aircraft noise exposure has traditionally been assessed using static
residential population data and long-term average noise metrics, often
overlooking the dynamic nature of human mobility and temporal variations in
operational conditions. This study proposes a data-driven framework that
integrates high-resolution noise measurements from airport monitoring terminals
with mobile phone-derived de facto population estimates to evaluate noise
exposure with fine spatio-temporal resolution. We develop hourly noise exposure
profiles and quantify the number of individuals affected across regions and
time windows, using both absolute counts and inequality metrics such as Gini
coefficients. This enables a nuanced examination of not only who is exposed,
but when and where the burden is concentrated. At our case study airport,
operational runway patterns resulted in recurring spatial shifts in noise
exposure. By incorporating de facto population data, we demonstrate that
identical noise operations can yield unequal impacts depending on the time and
location of population presence, highlighting the importance of accounting for
population dynamics in exposure assessment. Our approach offers a scalable
basis for designing population-sensitive noise abatement strategies,
contributing to more equitable and transparent aviation noise management.
"
2504.16186,2025-04-24,Analogy making as the basis of statistical inference,"  Standard statistical theory has arguably proved to be unsuitable as a basis
for constructing a satisfactory completely general framework for performing
statistical inference. For example, frequentist theory has never come close to
providing such a general inferential framework, which is not only attributable
to the question surrounding the soundness of this theory, but also to its focus
on attempting to address the problem of how to perform statistical inference
only in certain special cases. Also, theories of inference that are grounded in
the idea of deducing sample-based inferences about populations of interest from
a given set of universally acceptable axioms, e.g. many theories that aim to
justify Bayesian inference and theories of imprecise probability, suffer from
the difficulty of finding such axioms that are weak enough to be widely
acceptable, but strong enough to lead to methods of inference that can be
regarded as being efficient. These observations justify the need to look for an
alternative means by which statistical inference may be performed, and in
particular, to explore the one that is offered by analogy making. What is
presented here goes down this path. To be clear, this is done in a way that
does not simply endorse the common use of analogy making as a supplementary
means of understanding how statistical methods work, but formally develops
analogy making as the foundation of a general framework for performing
statistical inference. In the latter part of the paper, the use of this
framework is illustrated by applying some of the most important analogies
contained within it to a relatively simple but arguably still unresolved
problem of statistical inference, which naturally leads to an original way
being put forward of addressing issues that relate to Bartlett's and Lindley's
paradoxes.
"
2504.18695,2025-04-29,Local Polynomial Lp-norm Regression,"  The local least squares estimator for a regression curve cannot provide
optimal results when non-Gaussian noise is present. Both theoretical and
empirical evidence suggests that residuals often exhibit distributional
properties different from those of a normal distribution, making it worthwhile
to consider estimation based on other norms. It is suggested that $L_p$-norm
estimators be used to minimize the residuals when these exhibit non-normal
kurtosis. In this paper, we propose a local polynomial $L_p$-norm regression
that replaces weighted least squares estimation with weighted $L_p$-norm
estimation for fitting the polynomial locally. We also introduce a new method
for estimating the parameter $p$ from the residuals, enhancing the adaptability
of the approach. Through numerical and theoretical investigation, we
demonstrate our method's superiority over local least squares in
one-dimensional data and show promising outcomes for higher dimensions,
specifically in 2D.
"
2504.18982,2025-04-29,On Bitcoin Price Prediction,"  In recent years, cryptocurrencies have attracted growing attention from both
private investors and institutions. Among them, Bitcoin stands out for its
impressive volatility and widespread influence. This paper explores the
predictability of Bitcoin's price movements, drawing a parallel with
traditional financial markets. We examine whether the cryptocurrency market
operates under the efficient market hypothesis (EMH) or if inefficiencies still
allow opportunities for arbitrage. Our methodology combines theoretical
reviews, empirical analyses, machine learning approaches, and time series
modeling to assess the extent to which Bitcoin's price can be predicted. We
find that while, in general, the Bitcoin market tends toward efficiency,
specific conditions, including information asymmetries and behavioral
anomalies, occasionally create exploitable inefficiencies. However, these
opportunities remain difficult to systematically identify and leverage. Our
findings have implications for both investors and policymakers, particularly
regarding the regulation of cryptocurrency brokers and derivatives markets.
"
2504.20941,2025-04-30,"Conformal-DP: Differential Privacy on Riemannian Manifolds via Conformal
  Transformation","  Differential Privacy (DP) has been established as a safeguard for private
data sharing by adding perturbations to information release. Prior research on
DP has extended beyond data in the flat Euclidean space and addressed data on
curved manifolds, e.g., diffusion tensor MRI, social networks, or organ shape
analysis, by adding perturbations along geodesic distances. However, existing
manifold-aware DP methods rely on the assumption that samples are uniformly
distributed across the manifold. In reality, data densities vary, leading to a
biased noise imbalance across manifold regions, weakening the privacy-utility
trade-offs. To address this gap, we propose a novel mechanism: Conformal-DP,
utilizing conformal transformations on the Riemannian manifold to equalize
local sample density and to redefine geodesic distances accordingly while
preserving the intrinsic geometry of the manifold. Our theoretical analysis
yields two main results. First, we prove that the conformal factor computed
from local kernel-density estimates is explicitly data-density-aware; Second,
under the conformal metric, the mechanism satisfies $ \varepsilon
$-differential privacy on any complete Riemannian manifold and admits a
closed-form upper bound on the expected geodesic error that depends only on the
maximal density ratio, not on global curvatureof the manifold. Our experimental
results validate that the mechanism achieves high utility while providing the $
\varepsilon $-DP guarantee for both homogeneous and especially heterogeneous
manifold data.
"
2504.21566,2025-05-01,Rendering LaTeX in R,"  The xdvir package provides functions for rendering LaTeX fragments as labels,
annotations, and data symbols in R plots. There are convenient high-level
functions for rendering LaTeX fragments, including labels on ggplot2 plots,
plus lower-level functions for more fine control over the separate authoring,
typesetting, and rendering steps. There is support for making use of LaTeX
packages, including TikZ graphics. The rendered LaTeX output is fully
integrated with R graphics output in the sense that LaTeX output can be
positioned and sized relative to R graphics output and vice versa.
"
2505.00356,2025-05-02,Do global forecasting models require frequent retraining?,"  In an era of increasing computational capabilities and growing environmental
consciousness, organizations face a critical challenge in balancing the
accuracy of forecasting models with computational efficiency and
sustainability. Global forecasting models, lowering the computational time,
have gained significant attention over the years. However, the common practice
of retraining these models with new observations raises important questions
about the costs of forecasting. Using ten different machine learning and deep
learning models, we analyzed various retraining scenarios, ranging from
continuous updates to no retraining at all, across two large retail datasets.
We showed that less frequent retraining strategies maintain the forecast
accuracy while reducing the computational costs, providing a more sustainable
approach to large-scale forecasting. We also found that machine learning models
are a marginally better choice to reduce the costs of forecasting when coupled
with less frequent model retraining strategies as the frequency of the data
increases. Our findings challenge the conventional belief that frequent
retraining is essential for maintaining forecasting accuracy. Instead, periodic
retraining offers a good balance between predictive performance and efficiency,
both in the case of point and probabilistic forecasting. These insights provide
actionable guidelines for organizations seeking to optimize forecasting
pipelines while reducing costs and energy consumption.
"
2505.00854,2025-05-05,"Mapping the Intersection of Research and Policy in Centers for Medicare
  National Coverage Decision Memos","  Evidence is a crucial component of federal policy, but the interactions
between the various stakeholders involved in funding, producing, and using the
results of scientific research, an important class of evidence, for federal
policy are poorly understood. The national coverage determination process used
by the Centers for Medicare and Medicaid Services (CMS) to make significant
policies on healthcare coverage is an ideal candidate for studying the
interactions between stakeholders producing and utilizing scientific research
for policy. Memos produced during the national coverage determination process
contain information that identifies the organizations funding and producing
research articles cited by CMS policy staff. I use these data to map scientific
articles and their funding sources to discrete federal policies with
substantial economic and health impacts. My analysis highlights that
information derived from policy documents can facilitate transparency among the
stakeholders involved in funding, producing, and using evidence for federal
policy.
"
2505.02298,2025-05-06,"Statisticians Training STEM Educators in Statistics Methods and
  Pedagogy: A Case Study of Instructor Training in Bayesian Methods","  Educating the next generation of scientists in statistical methodology is an
important task. Educating their instructors in statistical content knowledge
and pedagogical knowledge is as important and provides an indirect impact of
students' learning. Statisticians are in a place to lead train-the-trainer
(TTT) programs in different methods. We present our instructor training program
in Bayesian methods as an effective case study of a TTT model. In addition to
describing the details of the structure of our training program, we share our
experience in designing and implementing our program including the challenges
we face, the opportunities created, and our recommendations for TTT programs
led by statisticians.
"
2505.04176,2025-05-08,Developing Assessment Methods for Evaluating Learning Experience,"  This research aims to investigate the gender-based learning experiences of
engineering students enrolled in the Probability and Statistics course,
focusing on the four different assessment methods employed namely direct
conceptual learning (DCL), symposium, applied deployment and collaborative
learning. The study encompasses 299 engineering students, comprising 90 females
and 209 males. Multivariate Analysis of Variance (MANOVA), is used to gain
deeper insights into the complex interplay between assessment methods and their
influence on student learning. The results of the statistical analysis reveal
that there are significant differences in the learning outcomes between female
and male engineering students in the assessment methods of direct conceptual
learning, symposium, and applied deployment. The findings suggest that there is
no significant difference in the learning outcomes between female and male
engineering students in the collaborative learning assessment method. The
graphical representation visually confirms the significant differences in
direct conceptual learning, symposium, and applied deployment, while
illustrating no significant difference in collaborative learning between female
and male engineering students.
"
2505.08395,2025-05-14,"Bayesian Estimation of Causal Effects Using Proxies of a Latent
  Interference Network","  Network interference occurs when treatments assigned to some units affect the
outcomes of others. Traditional approaches often assume that the observed
network correctly specifies the interference structure. However, in practice,
researchers frequently only have access to proxy measurements of the
interference network due to limitations in data collection or potential
mismatches between measured networks and actual interference pathways. In this
paper, we introduce a framework for estimating causal effects when only proxy
networks are available. Our approach leverages a structural causal model that
accommodates diverse proxy types, including noisy measurements, multiple data
sources, and multilayer networks, and defines causal effects as interventions
on population-level treatments. Since the true interference network is latent,
estimation poses significant challenges. To overcome them, we develop a
Bayesian inference framework. We propose a Block Gibbs sampler with Locally
Informed Proposals to update the latent network, thereby efficiently exploring
the high-dimensional posterior space composed of both discrete and continuous
parameters. We illustrate the performance of our method through numerical
experiments, demonstrating its accuracy in recovering causal effects even when
only proxies of the interference network are available.
"
2505.09619,2025-05-23,"Machine Learning Solutions Integrated in an IoT Healthcare Platform for
  Heart Failure Risk Stratification","  The management of chronic Heart Failure (HF) presents significant challenges
in modern healthcare, requiring continuous monitoring, early detection of
exacerbations, and personalized treatment strategies. In this paper, we present
a predictive model founded on Machine Learning (ML) techniques to identify
patients at HF risk. This model is an ensemble learning approach, a modified
stacking technique, that uses two specialized models leveraging clinical and
echocardiographic features and then a meta-model to combine the predictions of
these two models. We initially assess the model on a real dataset and the
obtained results suggest that it performs well in the stratification of
patients at HR risk. Specifically, we obtained high sensitivity (95\%),
ensuring that nearly all high-risk patients are identified. As for accuracy, we
obtained 84\%, which can be considered moderate in some ML contexts. However,
it is acceptable given our priority of identifying patients at risk of HF
because they will be asked to participate in the telemonitoring program of the
PrediHealth research project on which some of the authors of this paper are
working. The initial findings also suggest that ML-based risk stratification
models can serve as valuable decision-support tools not only in the PrediHealth
project but also for healthcare professionals, aiding in early intervention and
personalized patient management. To have a better understanding of the value
and of potentiality of our predictive model, we also contrasted its results
with those obtained by using three baseline models. The preliminary results
indicate that our predictive model outperforms these baselines that flatly
consider features, \ie not grouping them in clinical and echocardiographic
features.
"
2505.11944,2025-05-20,Basic model for ranking microfinance institutions,"  This paper discusses the challenges encountered in building a ranking model
for aggregator site products, using the example of ranking microfinance
institutions (MFIs) based on post-click conversion. We suggest which features
of MFIs should be considered, and using an algorithm based on Markov chains, we
demonstrate the ``usefulness'' of these features on real data. The ideas
developed in this work can be applied to aggregator websites in microinsurance,
especially when personal data is unavailable. Since we did not find similar
datasets in the public domain, we are publishing our dataset with a detailed
description of its attributes.
"
2505.14955,2025-05-22,"Bayesian Multivariate Approach to Subnational mortality graduation with
  Age-Varying Smoothness","  This work introduces a Bayesian smoothing approach for the joint graduation
of mortality rates across multiple populations. In particular, dynamical linear
models are used to induce smoothness across ages through structured dependence,
analogously to how temporal correlation is accommodated in state-space
time-indexed models. An essential issue in subnational mortality probabilistic
modelling is the lack or sparseness of information for some subpopulations. For
many countries, mortality data is severely limited, and approaches based on a
single population model can result in high uncertainty in the adjusted
mortality tables. Here, we recognize the interdependence within a group of
mortality data and pursue the pooling of information across several curves that
ideally share common characteristics, such as the influence of epidemics or
major economic shifts. Our proposal considers multivariate Bayesian dynamical
models with common parameters, allowing for borrowing of information across
mortality tables and enabling tests of convergence across populations. We also
employ discount factors, typical in DLMs, to regulate smoothness, with varying
discounting across ages, ensuring less smoothness at younger ages and greater
stability at adult ages. This setup implies a trade-off between stability and
adaptability. The discount parameter controls the responsiveness of the fit at
older ages to new data. The estimation is fully Bayesian, accommodating all
uncertainties in modelling and prediction. To illustrate the effectiveness of
our model, we analyse male and female mortality data from England and Wales
between 2010 and 2012, obtained from the Office for National Statistics. In
scenarios with simulated missing data, our approach showed strong performance
and flexibility in pooling information from related populations with more
complete data.
"
2505.16696,2025-05-23,"Sensitivity of ECG QRS Complexes to His-Purkinje Structure in
  Computational Heart Models","  Cardiac digital twins (CDT) are emerging as a potentially transformative tool
in cardiology. A critical yet understudied determinant of CDT accuracy is the
His-Purkinje system (HPS), which influences ventricular depolarization and
shapes the QRS complex of the electrocardiogram (ECG). Here, we quantify how
structural variations in the HPS alter QRS morphology and identify which
parameters drive this variability. We generated HPS structures using a
fractal-tree, rule-based algorithm, systematically varying nine model
parameters and assessing their effects on ten QRS-related metrics. We conducted
a Sobol sensitivity analysis to quantify direct and interaction-driven
contributions of each parameter to observed variability. Our results suggest
that most minor changes in HPS structure exert minimal influence on individual
QRS features; however, certain parameter combinations can produce abnormal QRS
morphologies. Wave durations and peak amplitudes of the QRS complex exhibit low
sensitivity to individual HPS parameter variations; however, we found that
specific parameter combinations can result in interactions that significantly
alter these aspects of QRS morphology. We found that certain HPS structures can
cause premature QRS formation, obscuring P-wave formation. QRS timing
variability was primarily driven by interactions among branch and fascicle
angles and branch repulsivity, though other parameters also showed notable
interaction effects. In addition to interactions, individual variations in the
number of branches in the HPS also affected QRS timing. While future models
should account for these potential sources of variability, this study indicates
that minor anatomical differences between a healthy patient's HPS and that of a
generic model are unlikely to significantly impact model fidelity or clinical
interpretation when both systems are physiologically normal.
"
